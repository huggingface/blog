---
title: 'Training and Fine-Tuning Sentence Transformers Models'
thumbnail: /blog/assets/87_training_st_models/thumbnail.png
---

<h1>
    Training and Fine-Tuning Sentence Transformers Models
</h1>

<div class="blog-metadata">
    <small>Published July 15, 2022.</small>
    <a target="_blank" class="btn no-underline text-sm mb-5 font-sans" href="https://github.com/huggingface/blog/blob/main/train-and-fine-tune-sentence-transformers-models.md">
        Update on GitHub
    </a>
</div>

<div class="author-card">
    <a href="/espejelomar"> 
        <img class="avatar avatar-user" src="https://bafybeidj6oxo7zm5pejnc2iezy24npw4qbt2jgpo4n6igt7oykc7rbvcxi.ipfs.dweb.link/omar_picture.png" title="Gravatar">
        <div class="bfc">
            <code>espejelomar</code>
            <span class="fullname">Omar Espejel</span>
        </div>
    </a>
</div>

Check out this tutorial with the Notebook Companion:
<a target="_blank" href="https://colab.research.google.com/github/huggingface/blog/blob/main/notebooks/80_getting_started_with_embeddings.ipynb">
    <img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/>
</a>

## How Sentence Transformers models work

In a Sentence Transformer model we are mapping a variable length text (or image pixels) to a fixed sized embedding representing the meaning of such input. For getting started on embeddings please refer to our [previous tutorial](https://huggingface.co/blog/getting-started-with-embeddings). In this post we will focus on text. 

This is how Sentence Transformers models work:

1. **Layer 1**: The input text passes trough a pre-trained Transformer model that can be obtained directly from the [Hugging Face Hub](https://huggingface.co/models?pipeline_tag=fill-mask&sort=downloads). This tutorial will use the "microsoft/deberta-v3-small" model. The outputs of the Transformer are contextualized word embeddings for all inputs tokens; imagine an embedding for each word of the text.
2. **Layer 2**: The word embeddings go trough a pooling process to obtain a single fix length embedding for the whole text. For example, mean pooling averages the word embeddings generated by the model.   

The following image summarizes the process:

![](assets/87_training_st_models/training_process.png)

Remember to install the Sentence Transormers library with `pip install -U sentence-transformers`. In code, this two steps process is simple: 

```py
from sentence_transformers import SentenceTransformer, models

## Step 1: use an existing language model
word_embedding_model = models.Transformer('bert-base-uncased', max_seq_length=256)

## Step 2: use a pool function over the token embeddings
pooling_model = models.Pooling(word_embedding_model.get_word_embedding_dimension())

## Join steps 1 and 2 using the modules argument
model = SentenceTransformer(modules=[word_embedding_model, pooling_model])
```

From the code above we can see that Sentence Transformers models are composed of modules, i.e. a list of layers which are executed consecutively. The input text enters the first layer and the final output comes from the last layer. The model above is the simplest Sentence Transformer possible, however, we can add additional Layers, for example, dense, bag of words, and convolutional.


## How to prepare your dataset for training a Sentence Transformers model

> "To represent our training data, we use the `InputExample` class to store training examples. As parameters, it accepts texts, which is a list of strings representing our pairs (or triplets). Further, we can also pass a label (either float or int)." - [Sentence Transformers Documentation](https://www.sbert.net/docs/training/overview.html#training-data).





## How to fine-tune a Sentence Transformer model

> "SentenceTransformers was designed in such way that fine-tuning your own sentence / text embeddings models is easy. It provides most of the building blocks that you can stick together to tune embeddings for your specific task." - [Sentence Transformers Documentation](https://www.sbert.net/docs/training/overview.html#training-overview).

training strategy to use greatly depends on your available data and on your target task.




Thanks for reading!

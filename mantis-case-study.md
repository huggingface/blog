---
title: "Why we’re switching to Hugging Face Inference Endpoints, and maybe you should too"
thumbnail: /blog/assets/78_ml_director_insights/mantis1.png
authors:
- user: mattupson
  guest: true
---

<h1>Why we’re switching to Hugging Face Inference Endpoints, and maybe you should too</h1>


<!-- {blog_metadata} -->
<!-- {authors} -->

Hugging Face recently launched Inference Endpoints; which as they put it: solves transformers in production. Inference Endpoints is a managed service that allows you to:

Deploy (almost) any model on Hugging Face Hub
To any cloud (AWS, and Azure, GCP on the way)
On a range of instance types (including GPU)
We’re switching some of our Machine Learning (ML) models that do inference on a CPU to this new service. This blog is about why, and why you might also want to consider it.

## What were we doing?


## What do we do now?


## What about Latency and Stability?


## What about the cost?


## Some notes and caveats:


## Other considerations


### Deployment Options


### Hosting multiple models on a single endpoint


## To conclude…


_This article was originally published on February 15, 2023 [in Medium](https://medium.com/mantisnlp/why-were-switching-to-hugging-face-inference-endpoints-and-maybe-you-should-too-829371dcd330)._

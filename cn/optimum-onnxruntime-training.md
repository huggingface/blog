---
title: "Optimum+ONNX Runtime - Easier, Faster training for your Hugging Face models"
thumbnail: /blog/assets/optimum_onnxruntime-training/thumbnail.png
authors:
- user: Jingya
- user: kshama-msft
  guest: true
- user: askhade
  guest: true
- user: weicwang
  guest: true
- user: zhijiang
  guest: true
---

# Optimum + ONNX Runtime: Easier, Faster training for your Hugging Face models

<!-- {blog_metadata} -->
<!-- {authors} -->

## Introduction

Transformer based models in language, vision and speech are getting larger to support complex multi-modal use cases for the end customer. Increasing model sizes directly impact the resources needed to train these models and scale them as the size increases. Hugging Face and Microsoft‚Äôs ONNX Runtime teams are working together to build advancements in finetuning large Language, Speech and Vision models. Hugging Face‚Äôs [Optimum library](https://huggingface.co/docs/optimum/index), through its integration with ONNX Runtime for training, provides an open solution to __improve training times by 35% or more__ for many popular Hugging Face models. We present details of both Hugging Face Optimum and the ONNX Runtime Training ecosystem, with performance numbers highlighting the benefits of using the Optimum library.

## Performance results

The chart below shows impressive acceleration __from 39% to 130%__ for Hugging Face models with Optimum when __using ONNX Runtime and DeepSpeed ZeRO Stage 1__ for training. The performance measurements were done on selected Hugging Face models with PyTorch as the baseline run, only ONNX Runtime for training as the second run, and ONNX Runtime + DeepSpeed ZeRO Stage 1 as the final run, showing maximum gains. The Optimizer used for the baseline PyTorch runs is the AdamW optimizer and the ORT Training runs use the Fused Adam Optimizer. The runs were performed on a single Nvidia A100 node with 8 GPUs.

<figure class="image table text-center m-0 w-full">
  <img src="assets/optimum_onnxruntime-training/onnxruntime-training-benchmark.png" alt="Optimum-onnxruntime Training Benchmark"/>
</figure>

Additional details on configuration settings to turn on Optimum for training acceleration can be found [here](https://huggingface.co/docs/optimum/onnxruntime/usage_guides/trainer). The version information used for these runs is as follows:
```
PyTorch: 1.14.0.dev20221103+cu116; ORT: 1.14.0.dev20221103001+cu116; DeepSpeed: 0.6.6; HuggingFace: 4.24.0.dev0; Optimum: 1.4.1.dev0; Cuda: 11.6.2
```

## Optimum Library

Hugging Face is a fast-growing open community and platform aiming to democratize good machine learning. We extended modalities from NLP to audio and vision, and now covers use cases across Machine Learning to meet our community's needs following the success of the [Transformers library](https://huggingface.co/docs/transformers/index). Now on [Hugging Face Hub](https://huggingface.co/models), there are more than 120K free and accessible model checkpoints for various machine learning tasks, 18K datasets, and 20K ML demo apps. However, scaling transformer models into production is still a challenge for the industry. Despite high accuracy, training and inference of transformer-based models can be time-consuming and expensive.

To target these needs, Hugging Face built two open-sourced libraries: __Accelerate__ and __Optimum__. While [ü§ó Accelerate](https://huggingface.co/docs/accelerate/index) focuses on out-of-the-box distributed training, [ü§ó Optimum](https://huggingface.co/docs/optimum/index), as an extension of transformers, accelerates model training and inference by leveraging the maximum efficiency of users‚Äô targeted hardware. Optimum integrated machine learning accelerators like ONNX Runtime and specialized hardware like [Intel's Habana Gaudi](https://huggingface.co/blog/habana-gaudi-2-benchmark), so users can benefit from considerable speedup in both training and inference. Besides, Optimum seamlessly integrates other Hugging Face‚Äôs tools while inheriting the same ease of use as Transformers. Developers can easily adapt their work to achieve lower latency with less computing power.

## ONNX Runtime Training

[ONNX Runtime](https://onnxruntime.ai/) accelerates [large model training](https://onnxruntime.ai/docs/get-started/training-pytorch.html) to speed up throughput by up to 40% standalone, and 130% when composed with [DeepSpeed](https://www.deepspeed.ai/tutorials/zero/) for popular HuggingFace transformer based models. ONNX Runtime is already integrated as part of Optimum and enables faster training through Hugging Face‚Äôs Optimum training framework.

ONNX Runtime Training achieves such throughput improvements via several memory and compute optimizations. The memory optimizations enable ONNX Runtime to maximize the batch size and utilize the available memory efficiently whereas the compute optimizations speed up the training time. These optimizations include, but are not limited to, efficient memory planning, kernel optimizations, multi tensor apply for Adam Optimizer (which batches the elementwise updates applied to all the model‚Äôs parameters into one or a few kernel launches), FP16 optimizer (which eliminates a lot of device to host memory copies), mixed precision training and graph optimizations like node fusions and node eliminations. ONNX Runtime Training supports both [NVIDIA](https://techcommunity.microsoft.com/t5/ai-machine-learning-blog/accelerate-pytorch-transformer-model-training-with-onnx-runtime/ba-p/2540471) and [AMD GPUs](https://cloudblogs.microsoft.com/opensource/2021/07/13/onnx-runtime-release-1-8-1-previews-support-for-accelerated-training-on-amd-gpus-with-the-amd-rocm-open-software-platform/), and offers extensibility with custom operators.

In short, it empowers AI developers to take full advantage of the ecosystem they are familiar with, like PyTorch and Hugging Face, and use acceleration from ONNX Runtime on the target device of their choice to save both time and resources.

## ONNX Runtime Training in Optimum

Optimum provides an `ORTTrainer` API that extends the `Trainer` in Transformers to use ONNX Runtime as the backend for acceleration. `ORTTrainer` is an easy-to-use API containing feature-complete training loop and evaluation loop. It supports features like hyperparameter search, mixed-precision training and distributed training with multiple GPUs. `ORTTrainer` enables AI developers to compose ONNX Runtime and other third-party acceleration techniques when training Transformers‚Äô models, which helps accelerate the training further and gets the best out of the hardware. For example, developers can combine ONNX Runtime Training with distributed data parallel and mixed-precision training integrated in Transformers‚Äô Trainer. Besides, `ORTTrainer` makes it easy to compose ONNX Runtime Training with DeepSpeed ZeRO-1, which saves memory by partitioning the optimizer states. After the pre-training or the fine-tuning is done, developers can either save the trained PyTorch model or convert it to the ONNX format with APIs that Optimum implemented for ONNX Runtime to ease the deployment for Inference. And just like `Trainer`, `ORTTrainer` has full integration with Hugging Face Hub: after the training, users can upload their model checkpoints to their Hugging Face Hub account.

So concretely, what should users do with Optimum to take advantage of the ONNX Runtime acceleration for training? If you are already using `Trainer`, you just need to adapt a few lines of code to benefit from all the improvements mentioned above. There are mainly two replacements that need to be applied. Firstly, replace `Trainer` with `ORTTrainer`, then replace `TrainingArguments` with `ORTTrainingArguments` which contains all the hyperparameters the trainer will use for training and evaluation. `ORTTrainingArguments` extends `TrainingArguments` to apply some extra arguments empowered by ONNX Runtime. For example, users can apply Fused Adam Optimizer for extra performance gain. Here is an example:

```diff
-from transformers import Trainer, TrainingArguments
+from optimum.onnxruntime import ORTTrainer, ORTTrainingArguments

# Step 1: Define training arguments
-training_args = TrainingArguments(
+training_args = ORTTrainingArguments(
    output_dir="path/to/save/folder/",
-   optim = "adamw_hf",
+   optim = "adamw_ort_fused",
    ...
)

# Step 2: Create your ONNX Runtime Trainer
-trainer = Trainer(
+trainer = ORTTrainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
+   feature="sequence-classification",
    ...
)

# Step 3: Use ONNX Runtime for training!ü§ó
trainer.train()
```

## Looking Forward

The Hugging Face team is working on open sourcing more large models and lowering the barrier for users to benefit from them with acceleration tools on both training and inference. We are collaborating with the ONNX Runtime training team to bring more training optimizations to newer and larger model architectures, including Whisper and Stable Diffusion. Microsoft has also packaged its state-of-the-art training acceleration technologies in the [Azure Container for PyTorch](https://techcommunity.microsoft.com/t5/ai-machine-learning-blog/enabling-deep-learning-with-azure-container-for-pytorch-in-azure/ba-p/3650489). This is a light-weight curated environment including DeepSpeed and ONNX Runtime to improve productivity for AI developers training with PyTorch. In addition to large model training, the ONNX Runtime training team is also building new solutions for learning on the edge ‚Äì training on devices that are constrained on memory and power.

## Getting Started

We invite you to check out the links below to learn more about, and get started with, Optimum ONNX Runtime Training for your Hugging Face models.

* [Optimum ONNX Runtime Training Documentation](https://huggingface.co/docs/optimum/onnxruntime/usage_guides/trainer)
* [Optimum ONNX Runtime Training Examples](https://github.com/huggingface/optimum/tree/main/examples/onnxruntime/training)
* [Optimum Github repo](https://github.com/huggingface/optimum/tree/main)
* [ONNX Runtime Training Examples](https://github.com/microsoft/onnxruntime-training-examples/)
* [ONNX Runtime Training Github repo](https://github.com/microsoft/onnxruntime/tree/main/orttraining)
* [ONNX Runtime](https://onnxruntime.ai/)
* [DeepSpeed](https://www.deepspeed.ai/) and [ZeRO](https://www.deepspeed.ai/tutorials/zero/) Tutorial
* [Azure Container for PyTorch](https://techcommunity.microsoft.com/t5/ai-machine-learning-blog/enabling-deep-learning-with-azure-container-for-pytorch-in-azure/ba-p/3650489)

---

# Optimum + ONNX Runtime: Êõ¥ÂÆπÊòì„ÄÅÊõ¥Âø´Âú∞ËÆ≠ÁªÉ‰Ω†ÁöÑHugging FaceÊ®°Âûã

## ‰ªãÁªç

Âü∫‰∫éËØ≠Ë®Ä„ÄÅËßÜËßâÂíåËØ≠Èü≥ÁöÑ Transformer Ê®°ÂûãË∂äÊù•Ë∂äÂ§ßÔºå‰ª•ÊîØÊåÅÁªàÁ´ØÁî®Êà∑Â§çÊùÇÁöÑÂ§öÊ®°ÊÄÅÁî®‰æã„ÄÇÂ¢ûÂä†Ê®°ÂûãÂ§ßÂ∞èÁõ¥Êé•ÂΩ±ÂìçËÆ≠ÁªÉËøô‰∫õÊ®°ÂûãÊâÄÈúÄÁöÑËµÑÊ∫êÔºåÂπ∂ÈöèÁùÄÊ®°ÂûãÂ§ßÂ∞èÁöÑÂ¢ûÂä†ËÄåÊâ©Â±ïÂÆÉ‰ª¨„ÄÇHugging Face ÂíåÂæÆËΩØÁöÑ ONNX Runtime Âõ¢ÈòüÊ≠£Âú®‰∏ÄËµ∑Âä™ÂäõÔºåÂú®ÂæÆË∞ÉÂ§ßÂûãËØ≠Ë®Ä„ÄÅËØ≠Èü≥ÂíåËßÜËßâÊ®°ÂûãÊñπÈù¢ÂèñÂæóËøõÊ≠•„ÄÇHugging Face ÁöÑ [Optimum Â∫ì](https://huggingface.co/docs/optimum/index)ÔºåÈÄöËøáÂíå ONNX Runtime ÁöÑÈõÜÊàêËøõË°åËÆ≠ÁªÉÔºå‰∏∫ËÆ∏Â§öÊµÅË°åÁöÑ Hugging Face Ê®°ÂûãÊèê‰æõ‰∫Ü‰∏Ä‰∏™ÂºÄÊîæÁöÑËß£ÂÜ≥ÊñπÊ°àÔºå**ÂèØ‰ª•Â∞ÜËÆ≠ÁªÉÊó∂Èó¥Áº©Áü≠35%ÊàñÊõ¥Â§ö**„ÄÇÊàë‰ª¨Â±ïÁé∞‰∫Ü Hugging Face Optimum Âíå ONNX Runtime Training ÁîüÊÄÅÁ≥ªÁªüÁöÑÁªÜËäÇÔºåÊÄßËÉΩÊï∞ÊçÆÁ™ÅÂá∫‰∫Ü‰ΩøÁî® Optimum Â∫ìÁöÑÂ•ΩÂ§Ñ„ÄÇ

## ÊÄßËÉΩÊµãËØïÁªìÊûú

‰∏ãÈù¢ÁöÑÂõæË°®Ë°®ÊòéÔºåÂΩì**‰ΩøÁî® ONNX Runtime Âíå DeepSpeed ZeRO Stage 1 **ËøõË°åËÆ≠ÁªÉÊó∂ÔºåÁî® Optimum ÁöÑ Hugging Face Ê®°ÂûãÁöÑÂä†ÈÄü**‰ªé39%ÊèêÈ´òÂà∞130%**„ÄÇÊÄßËÉΩÊµãËØïÁöÑÂü∫ÂáÜËøêË°åÊòØÂú®ÈÄâÂÆöÁöÑ Hugging Face PyTorchÊ®°Âûã‰∏äËøõË°åÁöÑÔºåÁ¨¨‰∫åÊ¨°ËøêË°åÊòØÂè™Áî® ONNX Runtime ËÆ≠ÁªÉÔºåÊúÄÂêé‰∏ÄÊ¨°ËøêË°åÊòØ ONNX Runtime + DeepSpeed ZeRO Stage 1ÔºåÂõæ‰∏≠ÊòæÁ§∫‰∫ÜÊúÄÂ§ßÁöÑÊî∂Áõä„ÄÇÂü∫Á∫ø PyTorch ËøêË°åÊâÄÁî®ÁöÑ‰ºòÂåñÂô®ÊòØAdamW OptimizerÔºåORT ËÆ≠ÁªÉÁî®ÁöÑ‰ºòÂåñÂô®ÊòØ Fused Adam Optimizer„ÄÇËøô‰∫õËøêË°åÊòØÂú®Â∏¶Êúâ8‰∏™ GPU ÁöÑÂçï‰∏™ Nvidia A100 ËäÇÁÇπ‰∏äÊâßË°åÁöÑ„ÄÇ

![](https://huggingface.co/blog/assets/optimum_onnxruntime-training/onnxruntime-training-benchmark.png)

Êõ¥Â§öÂÖ≥‰∫éÂºÄÂêØ Optimum ËøõË°åËÆ≠ÁªÉÂä†ÈÄüÁöÑÈÖçÁΩÆÁªÜËäÇÂèØ‰ª•Âú®[ËøôÈáå](https://huggingface.co/docs/optimum/onnxruntime/usage_guides/trainer)ÊâæÂà∞„ÄÇÁî®‰∫éËøô‰∫õËøêË°åÁöÑÁâàÊú¨‰ø°ÊÅØÂ¶Ç‰∏ãÔºö

```
PyTorch: 1.14.0.dev20221103+cu116; ORT: 1.14.0.dev20221103001+cu116; DeepSpeed: 0.6.6; HuggingFace: 4.24.0.dev0; Optimum: 1.4.1.dev0; Cuda: 11.6.2
```

## Optimum Â∫ì

Hugging Face ÊòØ‰∏Ä‰∏™Âø´ÈÄüÂèëÂ±ïÁöÑÂºÄÊîæÁ§æÂå∫ÂíåÂπ≥Âè∞ÔºåÊó®Âú®Â∞Ü‰ºòÁßÄÁöÑÊú∫Âô®Â≠¶‰π†Â§ß‰ºóÂåñ„ÄÇÈöèÁùÄ [Transformer Â∫ì](https://huggingface.co/docs/transformers/index)ÁöÑÊàêÂäüÔºåÊàë‰ª¨Â∞ÜÊ®°ÊÄÅ‰ªé NLP Êâ©Â±ïÂà∞Èü≥È¢ëÂíåËßÜËßâÔºåÁé∞Âú®Ê∂µÁõñ‰∫ÜË∑®Êú∫Âô®Â≠¶‰π†ÁöÑÁî®‰æãÔºå‰ª•Êª°Ë∂≥Êàë‰ª¨Á§æÂå∫ÁöÑÈúÄÊ±Ç„ÄÇÁé∞Âú®Âú® [Hugging Face Hub](https://huggingface.co/models) ‰∏äÔºåÊúâË∂ÖËøá12‰∏á‰∏™ÂÖçË¥πÂíåÂèØËÆøÈóÆÁöÑÊ®°Âûã checkpoints Áî®‰∫éÂêÑÁßçÊú∫Âô®Â≠¶‰π†‰ªªÂä°Ôºå1.8‰∏á‰∏™Êï∞ÊçÆÈõÜÂíå 2‰∏á‰∏™Êú∫Âô®Â≠¶‰π†ÊºîÁ§∫Â∫îÁî®„ÄÇÁÑ∂ËÄåÔºåÂ∞Ü Transformer Ê®°ÂûãÊâ©Â±ïÂà∞Áîü‰∫ß‰∏≠‰ªçÁÑ∂ÊòØÂ∑•‰∏öÁïåÁöÑ‰∏Ä‰∏™ÊåëÊàò„ÄÇÂ∞ΩÁÆ°ÂáÜÁ°ÆÊÄßÂæàÈ´òÔºå‰ΩÜÂü∫‰∫é Transformer ÁöÑÊ®°ÂûãÁöÑËÆ≠ÁªÉÂíåÊé®ÁêÜÂèØËÉΩËÄóÊó∂‰∏îÊòÇË¥µ„ÄÇ

‰∏∫‰∫ÜÊª°Ë∂≥Ëøô‰∫õÈúÄÊ±ÇÔºåHugging Face ÊûÑÂª∫‰∫Ü‰∏§‰∏™ÂºÄÊ∫êÂ∫ìÔºö**Accelerate** Âíå **Optimum**„ÄÇ[ü§óAccelerate](https://huggingface.co/docs/accelerate/index) ‰∏ìÊ≥®‰∫éÂºÄÁÆ±Âç≥Áî®ÁöÑÂàÜÂ∏ÉÂºèËÆ≠ÁªÉÔºåËÄå [ü§óOptimum](https://huggingface.co/docs/optimum/index) ‰Ωú‰∏∫ Transformer ÁöÑÊâ©Â±ïÔºåÈÄöËøáÂà©Áî®Áî®Êà∑ÁõÆÊ†áÁ°¨‰ª∂ÁöÑÊúÄÂ§ßÊïàÁéáÊù•Âä†ÈÄüÊ®°ÂûãËÆ≠ÁªÉÂíåÊé®ÁêÜ„ÄÇOptimum ÈõÜÊàê‰∫ÜÊú∫Âô®Â≠¶‰π†Âä†ÈÄüÂô®Â¶Ç ONNX RuntimeÔºåÂíå‰∏ì‰∏öÁöÑÁ°¨‰ª∂Â¶Ç[Ëã±ÁâπÂ∞îÁöÑ Habana Gaudi](https://huggingface.co/blog/habana-gaudi-2-benchmark)ÔºåÂõ†Ê≠§Áî®Êà∑ÂèØ‰ª•‰ªéËÆ≠ÁªÉÂíåÊé®ÁêÜÁöÑÊòæËëóÂä†ÈÄü‰∏≠ÂèóÁõä„ÄÇÊ≠§Â§ñÔºåOptimum Êó†ÁºùÈõÜÊàê‰∫ÜÂÖ∂‰ªñ Hugging Face ÁöÑÂ∑•ÂÖ∑ÔºåÂêåÊó∂ÁªßÊâø‰∫Ü Transformer ÁöÑÊòìÁî®ÊÄß„ÄÇÂºÄÂèë‰∫∫ÂëòÂèØ‰ª•ËΩªÊùæÂú∞Ë∞ÉÊï¥‰ªñ‰ª¨ÁöÑÂ∑•‰ΩúÔºå‰ª•Êõ¥Â∞ëÁöÑËÆ°ÁÆóËÉΩÂäõÂÆûÁé∞Êõ¥‰ΩéÁöÑÂª∂Ëøü„ÄÇ

## ONNX Runtime ËÆ≠ÁªÉ

[ONNX Runtime](https://onnxruntime.ai/) Âä†ÈÄü[Â§ßÂûãÊ®°ÂûãËÆ≠ÁªÉ](https://onnxruntime.ai/docs/get-started/training-pytorch.html)ÔºåÂçïÁã¨‰ΩøÁî®Êó∂Â∞ÜÂêûÂêêÈáèÊèêÈ´ò40%Ôºå‰∏é [DeepSpeed](https://www.deepspeed.ai/tutorials/zero/) ÁªÑÂêàÂêéÂ∞ÜÂêûÂêêÈáèÊèêÈ´ò130%ÔºåÁî®‰∫éÊµÅË°åÁöÑÂü∫‰∫éHugging Face Transformer ÁöÑÊ®°Âûã„ÄÇONNX Runtime Â∑≤ÁªèÈõÜÊàê‰∏∫ Optimum ÁöÑ‰∏ÄÈÉ®ÂàÜÔºåÂπ∂ÈÄöËøá Hugging Face ÁöÑ Optimum ËÆ≠ÁªÉÊ°ÜÊû∂ÂÆûÁé∞Êõ¥Âø´ÁöÑËÆ≠ÁªÉ„ÄÇ

ONNX Runtime Training ÈÄöËøá‰∏Ä‰∫õÂÜÖÂ≠òÂíåËÆ°ÁÆó‰ºòÂåñÂÆûÁé∞‰∫ÜËøôÊ†∑ÁöÑÂêûÂêêÈáèÊîπËøõ„ÄÇÂÜÖÂ≠ò‰ºòÂåñ‰Ωø ONNX Runtime ËÉΩÂ§üÊúÄÂ§ßÂåñÊâπÂ§ßÂ∞èÂπ∂ÊúâÊïàÂà©Áî®ÂèØÁî®ÁöÑÂÜÖÂ≠òÔºåËÄåËÆ°ÁÆó‰ºòÂåñÂàôÂä†Âø´‰∫ÜËÆ≠ÁªÉÊó∂Èó¥„ÄÇËøô‰∫õ‰ºòÂåñÂåÖÊã¨‰ΩÜ‰∏çÈôê‰∫éÔºåÈ´òÊïàÁöÑÂÜÖÂ≠òËßÑÂàíÔºåÂÜÖÊ†∏‰ºòÂåñÔºåÈÄÇÁî®‰∫é Adam ‰ºòÂåñÂô®ÁöÑÂ§öÂº†ÈáèÂ∫îÁî®ÔºàÂ∞ÜÂ∫îÁî®‰∫éÊâÄÊúâÊ®°ÂûãÂèÇÊï∞ÁöÑÊåâÂÖÉÁ¥†Êõ¥Êñ∞ÂàÜÊâπÂà∞‰∏Ä‰∏™ÊàñÂá†‰∏™ÂÜÖÊ†∏ÂêØÂä®‰∏≠ÔºâÔºåFP16 ‰ºòÂåñÂô®ÔºàÊ∂àÈô§‰∫ÜÂ§ßÈáèÁî®‰∫é‰∏ªÊú∫ÂÜÖÂ≠òÊã∑Ë¥ùÁöÑËÆæÂ§áÔºâÔºåÊ∑∑ÂêàÁ≤æÂ∫¶ËÆ≠ÁªÉÂíåÂõæ‰ºòÂåñÔºåÂ¶ÇËäÇÁÇπËûçÂêàÂíåËäÇÁÇπÊ∂àÈô§„ÄÇONNX Runtime Training ÊîØÊåÅ [NVIDIA](https://techcommunity.microsoft.com/t5/ai-machine-learning-blog/accelerate-pytorch-transformer-model-training-with-onnx-runtime/ba-p/2540471) Âíå [AMD GPU](https://cloudblogs.microsoft.com/opensource/2021/07/13/onnx-runtime-release-1-8-1-previews-support-for-accelerated-training-on-amd-gpus-with-the-amd-rocm-open-software-platform/)ÔºåÂπ∂Êèê‰æõËá™ÂÆö‰πâÊìç‰ΩúÁöÑÂèØÊâ©Â±ïÊÄß„ÄÇ

ÁÆÄËÄåË®Ä‰πãÔºåÂÆÉ‰Ωø AI ÂºÄÂèë‰∫∫ÂëòËÉΩÂ§üÂÖÖÂàÜÂà©Áî®‰ªñ‰ª¨ÁÜüÊÇâÁöÑÁîüÊÄÅÁ≥ªÁªüÔºåÂ¶Ç PyTorch Âíå Hugging FaceÔºåÂπ∂Âú®‰ªñ‰ª¨ÈÄâÊã©ÁöÑÁõÆÊ†áËÆæÂ§á‰∏ä‰ΩøÁî® ONNX Runtime ËøõË°åÂä†ÈÄüÔºå‰ª•ËäÇÁúÅÊó∂Èó¥ÂíåËµÑÊ∫ê„ÄÇ

## Optimum ‰∏≠ÁöÑ ONNX Runtime Training

Optimum Êèê‰æõ‰∫Ü‰∏Ä‰∏™ `ORTTrainer` APIÔºåÂÆÉÊâ©Â±ï‰∫Ü Transformer ‰∏≠ÁöÑ `Trainer`Ôºå‰ª•‰ΩøÁî® ONNX Runtime ‰Ωú‰∏∫ÂêéÁ´ØËøõË°åÂä†ÈÄü„ÄÇ`ORTTrainer` ÊòØ‰∏Ä‰∏™Êòì‰∫é‰ΩøÁî®ÁöÑ APIÔºåÂåÖÂê´ÂÆåÊï¥ÁöÑËÆ≠ÁªÉÂæ™ÁéØÂíåËØÑ‰º∞Âæ™ÁéØ„ÄÇÂÆÉÊîØÊåÅÂÉèË∂ÖÂèÇÊï∞ÊêúÁ¥¢„ÄÅÊ∑∑ÂêàÁ≤æÂ∫¶ËÆ≠ÁªÉÂíåÂ§ö GPU ÂàÜÂ∏ÉÂºèËÆ≠ÁªÉÁ≠âÂäüËÉΩ„ÄÇ`ORTTrainer` ‰Ωø AI ÂºÄÂèë‰∫∫ÂëòÂú®ËÆ≠ÁªÉ Transformer Ê®°ÂûãÊó∂ËÉΩÂ§üÁªÑÂêà ONNX Runtime ÂíåÂÖ∂‰ªñÁ¨¨‰∏âÊñπÂä†ÈÄüÊäÄÊúØÔºåËøôÊúâÂä©‰∫éËøõ‰∏ÄÊ≠•Âä†ÈÄüËÆ≠ÁªÉÔºåÂπ∂ÂÖÖÂàÜÂèëÊå•Á°¨‰ª∂ÁöÑ‰ΩúÁî®„ÄÇ‰æãÂ¶ÇÔºåÂºÄÂèë‰∫∫ÂëòÂèØ‰ª•Â∞Ü ONNX Runtime Training ‰∏é Transformer ËÆ≠ÁªÉÂô®‰∏≠ÈõÜÊàêÁöÑÂàÜÂ∏ÉÂºèÊï∞ÊçÆÂπ∂Ë°åÂíåÊ∑∑ÂêàÁ≤æÂ∫¶ËÆ≠ÁªÉÁõ∏ÁªìÂêà„ÄÇÊ≠§Â§ñÔºå`ORTTrainer` ‰Ωø‰Ω†ÂèØ‰ª•ËΩªÊùæÂú∞Â∞Ü DeepSpeed ZeRO-1 Âíå ONNX Runtime Training ÁªÑÂêàÔºåÈÄöËøáÂØπ‰ºòÂåñÂô®Áä∂ÊÄÅËøõË°åÂàÜÂå∫Êù•ËäÇÁúÅÂÜÖÂ≠ò„ÄÇÂú®ÂÆåÊàêÈ¢ÑËÆ≠ÁªÉÊàñÂæÆË∞ÉÂêéÔºåÂºÄÂèë‰∫∫ÂëòÂèØ‰ª•‰øùÂ≠òÂ∑≤ËÆ≠ÁªÉÁöÑ PyTorch Ê®°ÂûãÔºåÊàñ‰ΩøÁî® OptimumÂÆûÁé∞ÁöÑ API Â∞ÜÂÖ∂ËΩ¨‰∏∫ ONNX Ê†ºÂºèÔºå‰ª•ÁÆÄÂåñÊé®ÁêÜÁöÑÈÉ®ÁΩ≤„ÄÇÂíå `Trainer` ‰∏ÄÊ†∑Ôºå`ORTTrainer` ‰∏é Hugging Face HubÂÆåÂÖ®ÈõÜÊàêÔºöËÆ≠ÁªÉÁªìÊùüÂêéÔºåÁî®Êà∑ÂèØ‰ª•Â∞Ü‰ªñ‰ª¨ÁöÑÊ®°Âûã checkpoints ‰∏ä‰º†Âà∞ Hugging Face Hub Ë¥¶Êà∑„ÄÇ

Âõ†Ê≠§ÂÖ∑‰ΩìÊù•ËØ¥ÔºåÁî®Êà∑Â∫îËØ•Â¶Ç‰ΩïÂà©Áî® ONNX Runtime Âä†ÈÄüËøõË°åËÆ≠ÁªÉÔºüÂ¶ÇÊûú‰Ω†Â∑≤ÁªèÂú®‰ΩøÁî® `Trainer`Ôºå‰Ω†Âè™ÈúÄË¶Å‰øÆÊîπÂá†Ë°å‰ª£Á†ÅÂ∞±ÂèØ‰ª•‰ªé‰∏äÈù¢ÊèêÂà∞ÁöÑÊâÄÊúâÊîπËøõ‰∏≠ÂèóÁõä„ÄÇ‰∏ªË¶ÅÊúâ‰∏§‰∏™ÊõøÊç¢ÈúÄË¶ÅÂ∫îÁî®„ÄÇÈ¶ñÂÖàÔºåÂ∞Ü `Trainer` ÊõøÊç¢‰∏∫ `ORTTrainer`ÔºåÁÑ∂ÂêéÂ∞Ü `TrainingArguments` ÊõøÊç¢‰∏∫`ORTTrainingArguments`ÔºåÂÖ∂‰∏≠ÂåÖÂê´ËÆ≠ÁªÉÂô®Â∞ÜÁî®‰∫éËÆ≠ÁªÉÂíåËØÑ‰º∞ÁöÑÊâÄÊúâË∂ÖÂèÇÊï∞„ÄÇ`ORTTrainingArguments` Êâ©Â±ï‰∫Ü `TrainingArguments`Ôºå‰ª•Â∫îÁî® ONNX Runtime ÊéàÊùÉÁöÑ‰∏Ä‰∫õÈ¢ùÂ§ñÂèÇÊï∞„ÄÇ‰æãÂ¶ÇÔºåÁî®Êà∑ÂèØ‰ª•‰ΩøÁî® Fused Adam ‰ºòÂåñÂô®Êù•Ëé∑ÂæóÈ¢ùÂ§ñÁöÑÊÄßËÉΩÊî∂Áõä„ÄÇ‰∏ãÈù¢ÊòØ‰∏Ä‰∏™‰æãÂ≠êÔºö

```python
-from transformers import Trainer, TrainingArguments
+from optimum.onnxruntime import ORTTrainer, ORTTrainingArguments

# Step 1: Define training arguments
-training_args = TrainingArguments(
+training_args = ORTTrainingArguments(
    output_dir="path/to/save/folder/",
-   optim = "adamw_hf",
+   optim = "adamw_ort_fused",
    ...
)

# Step 2: Create your ONNX Runtime Trainer
-trainer = Trainer(
+trainer = ORTTrainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
+   feature="sequence-classification",
    ...
)

# Step 3: Use ONNX Runtime for training!ü§ó
trainer.train()
```

## Â±ïÊúõÊú™Êù•

Hugging Face Âõ¢ÈòüÊ≠£Âú®ÂºÄÊ∫êÊõ¥Â§öÁöÑÂ§ßÂûãÊ®°ÂûãÔºåÂπ∂ÈÄöËøáËÆ≠ÁªÉÂíåÊé®ÁêÜÁöÑÂä†ÈÄüÂ∑•ÂÖ∑‰ª•Èôç‰ΩéÁî®Êà∑‰ªéÊ®°Âûã‰∏≠Ëé∑ÁõäÁöÑÈó®Êßõ„ÄÇÊàë‰ª¨Ê≠£Âú®‰∏é ONNX Runtime Training Âõ¢ÈòüÂêà‰ΩúÔºå‰∏∫Êõ¥Êñ∞ÂíåÊõ¥Â§ßÁöÑÊ®°ÂûãÊû∂ÊûÑÂ∏¶Êù•Êõ¥Â§öÁöÑËÆ≠ÁªÉ‰ºòÂåñÔºåÂåÖÊã¨ Whisper Âíå Stable Diffusion„ÄÇÂæÆËΩØËøòÂ∞ÜÂÖ∂ÊúÄÂÖàËøõÁöÑËÆ≠ÁªÉÂä†ÈÄüÊäÄÊúØÊâìÂåÖÂú® [PyTorch ÁöÑ Azure ÂÆπÂô®](https://techcommunity.microsoft.com/t5/ai-machine-learning-blog/enabling-deep-learning-with-azure-container-for-pytorch-in-azure/ba-p/3650489)‰∏≠„ÄÇËøôÊòØ‰∏Ä‰∏™ËΩªÈáèÁ∫ßÁöÑÁ≤æÂøÉËê•ÈÄ†ÁöÑÁéØÂ¢ÉÔºåÂåÖÊã¨ DeepSpeed Âíå ONNX RuntimeÔºå‰ª•ÊèêÈ´ò AI ÂºÄÂèëËÄÖ‰ΩøÁî® PyTorch ËÆ≠ÁªÉÁöÑÁîü‰∫ßÂäõ„ÄÇÈô§‰∫ÜÂ§ßÂûãÊ®°ÂûãËÆ≠ÁªÉÂ§ñÔºåONNX Runtime Training Âõ¢ÈòüËøòÂú®‰∏∫ËæπÁºòÂ≠¶‰π†ÊûÑÂª∫Êñ∞ÁöÑËß£ÂÜ≥ÊñπÊ°à‚Äî‚ÄîÂú®ÂÜÖÂ≠òÂíåÁîµÊ∫êÂèóÈôêÁöÑËÆæÂ§á‰∏äËøõË°åËÆ≠ÁªÉ„ÄÇ

## ÂáÜÂ§áÂºÄÂßã

Êàë‰ª¨ÈÇÄËØ∑‰Ω†Êü•Áúã‰∏ãÈù¢ÁöÑÈìæÊé•Ôºå‰ª•‰∫ÜËß£Êõ¥Â§öÂÖ≥‰∫é Hugging Face Ê®°ÂûãÁöÑ Optimum ONNX Runtime TrainingÔºåÂπ∂ÂºÄÂßã‰ΩøÁî®„ÄÇ

- [Optimum ONNX Runtime Training ÊñáÊ°£](https://huggingface.co/docs/optimum/onnxruntime/usage_guides/trainer)
- [Optimum ONNX Runtime Training Á§∫‰æã](https://github.com/huggingface/optimum/tree/main/examples/onnxruntime/training)
- [Optimum Github ‰ªìÂ∫ì](https://github.com/huggingface/optimum/tree/main)
- [ONNX Runtime Training Á§∫‰æã](https://github.com/microsoft/onnxruntime-training-examples/)
- [ONNX Runtime Training Github ‰ªìÂ∫ì](https://github.com/microsoft/onnxruntime/tree/main/orttraining)
- [ONNX Runtime](https://onnxruntime.ai/)
- [DeepSpeed](https://www.deepspeed.ai/) Âíå [ZeRO](https://www.deepspeed.ai/tutorials/zero/) ÊïôÁ®ã
- [PyTorch ÁöÑAzure ÂÆπÂô®](https://techcommunity.microsoft.com/t5/ai-machine-learning-blog/enabling-deep-learning-with-azure-container-for-pytorch-in-azure/ba-p/3650489)

ÊÑüË∞¢ÈòÖËØªÔºÅÂ¶ÇÊûú‰Ω†Êúâ‰ªª‰ΩïÈóÆÈ¢òÔºåËØ∑ÈÄöËøá [Github](https://github.com/huggingface/optimum/issues) Êàñ[ËÆ∫Âùõ](https://discuss.huggingface.co/c/optimum/)ÈöèÊó∂ËÅîÁ≥ªÊàë‰ª¨„ÄÇ‰Ω†‰πüÂèØ‰ª•Âú®[Twitter](https://twitter.com/Jhuaplin) Êàñ [LinkedIn](https://www.linkedin.com/in/jingya-huang-96158b15b/) ‰∏äËÅîÁ≥ªÊàë„ÄÇ



> ÂéüÊñáÔºö[Optimum+ONNX Runtime - Easier, Faster training for your Hugging Face models](https://huggingface.co/blog/optimum-onnxruntime-training)
>
> ËØëËÄÖÔºöAIboy1993ÔºàÊùéÊó≠‰∏úÔºâ
# Running SD On Toasters

## Open-sourcing Knowledge Distillation code and weights of SD-Small and SD-Tiny

![image]("assets/distill_sd/Picture1.png")

Isn‚Äôt it the ideal utopian (or dystopian, for some) world we are zooming into? LLMs, generative models, and AGI so ubiquitous and pervasive that they can run even on a mundane home appliance like a toaster? The past few months have seen an explosion of larger and larger language models such as GPT-4, Falcon 40B, LLaMa-2 70B etc. The world is heading towards large all-knowing generalized models, increasing the complexity and compute capacities required to run such models. On the imaging side too, Stable diffusion has progressively moved towards larger and larger image generation models such as SD2.1 and SDXL, to deliver highly versatile, state-of-the-art image generations with just a single model, similar to what MidJourney has been doing for the past couple of years.

We, at Segmind, have been working on how to make the models faster and cheaper. Last year, we have open-sourced our accelerated SD-WebUI library called [voltaML](https://github.com/VoltaML/voltaML-fast-stable-diffusion), which is a AITemplate/TensorRT based inference acceleration library that has delivered between 4-6X increase in the inference speed. To continue towards the goal of making generative models faster, smaller and cheaper, we are open-sourcing the weights and training code of our compressed SD models; SD-Small and SD-Tiny. The pretrained checkpoints are available on [Huggingface ü§ó](https://huggingface.co/segmind)

## Knowledge Distillation:

![image]("assets/distill_sd/Picture2.png")

Our new compressed models have been trained on Knowledge-Distillation (KD) techniques and the work has been largely based on this paper. The authors describe a Block-removal Knowledge-Distillation method where some of the UNet layers are removed and the student model weights are trained. Using the KD methods described in the paper, we were able to train two compressed models using the Huggingface ü§ó [Diffusers üß®](https://github.com/huggingface/diffusers) library ; Small and Tiny, that have 35% and 55% fewer parameters respectively than the base model while achieving comparable image fidelity as the base model. We have open sourced our distillation code in this [repo](https://github.com/segmind/distill-sd) and pretrained checkpoints on [Huggingface ü§ó](https://huggingface.co/segmind)

Knowledge-Distillation training a neural network is similar to a teacher guiding a student step-by-step. A large teacher model is pre-trained on a large amount of data and then a smaller model is trained on a smaller dataset, to imitate the outputs of the larger model along with classical training on the dataset.

In this particular type of knowledge distillation, the student model is trained to do the normal diffusion task of recovering an image from pure noise, but at the same time, the model is made to match the output of the larger teacher model. The matching of outputs happens at every block of the U-nets, hence the model quality is mostly preserved. So, using the previous analogy, we can say that during this kind of distillation, the student will not only try to learn from the Questions and Answers but also from the Teacher‚Äôs answers, as well as the step by step method of getting to the answer. We have 3 components in the loss function to achieve this, firstly the traditional loss between latents of the target image and latents of the generated image. Secondly, the loss between latents of the image generated by the teacher and latents of image generated by the student. And lastly, and the most important component, is the feature level loss, which is the loss between the outputs of each of the blocks of the teacher and the student.

Combining all of this makes up the Knowledge-Distillation training. Below is an architecture of the Block Removed UNet used in the KD as described in the paper.

![image]("assets/distill_sd/Picture3.png")

Image taken from the [paper](https://arxiv.org/pdf/2305.15798.pdf)  ‚ÄúOn Architectural Compression of Text-to-Image Diffusion Models‚Äù by Shinkook. et. al


We have taken Realistic-Vision 4.0 as our base teacher model and have trained on the LAION Art Aesthetic dataset with image scores above 7.5, because of their high quality image descriptions. Unlike the paper, we have chosen to train the two models on 1M images for 100K steps for the Small and 125K steps for the Tiny mode respectively. The code for the distillation training can be found [here](https://github.com/segmind/distill-sd).

## Speed in terms of inference latency:

We have observed that distilled models are up to 100% faster than the original base models.

![image]("assets/distill_sd/Picture4.png")

## Fine-tuning SD-tiny model on portrait dataset

We have fine-tuned our sd-tiny model on portrait images generated with the Realistic Vision v4.0 model. Below are the fine tuning parameters used.

- Steps: 131000
- Learning rate: 1e-4
- Batch size: 32
- Gradient accumulation steps: 4
- Image resolution: 768
- Dataset size - 7k images
- Mixed-precision: fp16

We were able to produce image quality close to the images produced by the original model, with almost 40% fewer parameters and the sample results below speak for themselves:

![image]("assets/distill_sd/Picture5.png")

The code for fine-tuning the base models can be found [here](https://github.com/segmind/distill-sd/blob/master/checkpoint_training.py)

## LoRA Training

One of the advantages of LoRA training on a distilled model is faster training. Below are some of the images of the first LoRA we trained on the distilled model on some abstract concepts. The code for the LoRA training can be found [here](https://github.com/segmind/distill-sd/blob/master/lora_training.py).

![image]("assets/distill_sd/Picture6.png")

## Open-source collaboration

We invite the open-source community to help us improve and achieve wider adoption of these distilled SD models. Users can join our [Discord](https://discord.gg/s6E6eHJk) server, where we will be announcing the latest updates to these models, releasing more checkpoints and some exciting new LoRAs. And if you like our work, please give us a star on our [Github](https://github.com/segmind/distill-sd).
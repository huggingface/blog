---
title: "ğŸ¤— PEFTï¼šåœ¨ä½èµ„æºç¡¬ä»¶ä¸Šå¯¹åäº¿è§„æ¨¡æ¨¡å‹è¿›è¡Œå‚æ•°é«˜æ•ˆå¾®è°ƒ"
thumbnail: /blog/assets/130_peft/thumbnail.png
authors:
- user: smangrul
- user: sayakpaul
---

## ğŸ¤— PEFTï¼šåœ¨ä½èµ„æºç¡¬ä»¶ä¸Šå¯¹åäº¿è§„æ¨¡æ¨¡å‹è¿›è¡Œå‚æ•°é«˜æ•ˆå¾®è°ƒ


## åŠ¨æœº

åŸºäº Transformers æ¶æ„çš„å¤§å‹è¯­è¨€æ¨¡å‹ (LLM)ï¼Œå¦‚ GPTã€T5 å’Œ BERTï¼Œå·²ç»åœ¨å„ç§è‡ªç„¶è¯­è¨€å¤„ç† (NLP) ä»»åŠ¡ä¸­å–å¾—äº†æœ€å…ˆè¿›çš„ç»“æœã€‚æ­¤å¤–ï¼Œè¿˜å¼€å§‹æ¶‰è¶³å…¶ä»–é¢†åŸŸï¼Œä¾‹å¦‚è®¡ç®—æœºè§†è§‰ (CV) (VITã€Stable Diffusionã€LayoutLM) å’ŒéŸ³é¢‘ (Whisperã€XLS-R)ã€‚ä¼ ç»Ÿçš„èŒƒå¼æ˜¯å¯¹é€šç”¨ç½‘ç»œè§„æ¨¡æ•°æ®è¿›è¡Œå¤§è§„æ¨¡é¢„è®­ç»ƒï¼Œç„¶åå¯¹ä¸‹æ¸¸ä»»åŠ¡è¿›è¡Œå¾®è°ƒã€‚ä¸ä½¿ç”¨å¼€ç®±å³ç”¨çš„é¢„è®­ç»ƒ LLM (ä¾‹å¦‚ï¼Œé›¶æ ·æœ¬æ¨ç†) ç›¸æ¯”ï¼Œåœ¨ä¸‹æ¸¸æ•°æ®é›†ä¸Šå¾®è°ƒè¿™äº›é¢„è®­ç»ƒ LLM ä¼šå¸¦æ¥å·¨å¤§çš„æ€§èƒ½æå‡ã€‚

ç„¶è€Œï¼Œéšç€æ¨¡å‹å˜å¾—è¶Šæ¥è¶Šå¤§ï¼Œåœ¨æ¶ˆè´¹çº§ç¡¬ä»¶ä¸Šå¯¹æ¨¡å‹è¿›è¡Œå…¨éƒ¨å‚æ•°çš„å¾®è°ƒå˜å¾—ä¸å¯è¡Œã€‚æ­¤å¤–ï¼Œä¸ºæ¯ä¸ªä¸‹æ¸¸ä»»åŠ¡ç‹¬ç«‹å­˜å‚¨å’Œéƒ¨ç½²å¾®è°ƒæ¨¡å‹å˜å¾—éå¸¸æ˜‚è´µï¼Œå› ä¸ºå¾®è°ƒæ¨¡å‹ä¸åŸå§‹é¢„è®­ç»ƒæ¨¡å‹çš„å¤§å°ç›¸åŒã€‚å‚æ•°é«˜æ•ˆå¾®è°ƒ(PEFT) æ–¹æ³•æ—¨åœ¨è§£å†³è¿™ä¸¤ä¸ªé—®é¢˜ï¼

PEFT æ–¹æ³•ä»…å¾®è°ƒå°‘é‡ (é¢å¤–) æ¨¡å‹å‚æ•°ï¼ŒåŒæ—¶å†»ç»“é¢„è®­ç»ƒ LLM çš„å¤§éƒ¨åˆ†å‚æ•°ï¼Œä»è€Œå¤§å¤§é™ä½äº†è®¡ç®—å’Œå­˜å‚¨æˆæœ¬ã€‚è¿™ä¹Ÿå…‹æœäº†[ç¾éš¾æ€§é—å¿˜](https://arxiv.org/abs/1312.6211)çš„é—®é¢˜ï¼Œè¿™æ˜¯åœ¨ LLM çš„å…¨å‚æ•°å¾®è°ƒæœŸé—´è§‚å¯Ÿåˆ°çš„ä¸€ç§ç°è±¡ã€‚PEFT æ–¹æ³•ä¹Ÿæ˜¾ç¤ºå‡ºåœ¨ä½æ•°æ®çŠ¶æ€ä¸‹æ¯”å¾®è°ƒæ›´å¥½ï¼Œå¯ä»¥æ›´å¥½åœ°æ³›åŒ–åˆ°åŸŸå¤–åœºæ™¯ã€‚å®ƒå¯ä»¥åº”ç”¨äºå„ç§æ¨¡æ€ï¼Œä¾‹å¦‚[å›¾åƒåˆ†ç±»](https://github.com/huggingface/peft/tree/main/examples/image_classification)ä»¥åŠ [Stable diffusion dreambooth](https://github.com/huggingface/peft/tree/main/examples/lora_dreambooth)ã€‚

PEFT æ–¹æ³•è¿˜æœ‰åŠ©äºæé«˜è½»ä¾¿æ€§ï¼Œå…¶ä¸­ç”¨æˆ·å¯ä»¥ä½¿ç”¨ PEFT æ–¹æ³•è°ƒæ•´æ¨¡å‹ï¼Œä»¥è·å¾—ä¸å®Œå…¨å¾®è°ƒçš„å¤§å‹æ£€æŸ¥ç‚¹ç›¸æ¯”ï¼Œå¤§å°ä»…å‡  MB çš„å¾®å°æ£€æŸ¥ç‚¹ã€‚ä¾‹å¦‚ï¼Œ `bigscience/mt0-xxl` å ç”¨ 40GB çš„å­˜å‚¨ç©ºé—´ï¼Œå…¨å‚æ•°å¾®è°ƒå°†å¯¼è‡´æ¯ä¸ªä¸‹æ¸¸æ•°æ®é›†æœ‰å¯¹åº” 40GB æ£€æŸ¥ç‚¹ã€‚è€Œä½¿ç”¨ PEFT æ–¹æ³•ï¼Œæ¯ä¸ªä¸‹æ¸¸æ•°æ®é›†åªå ç”¨å‡  MB çš„å­˜å‚¨ç©ºé—´ï¼ŒåŒæ—¶å®ç°ä¸å…¨å‚æ•°å¾®è°ƒç›¸å½“çš„æ€§èƒ½ã€‚æ¥è‡ª PEFT æ–¹æ³•çš„å°‘é‡è®­ç»ƒæƒé‡è¢«æ·»åŠ åˆ°é¢„è®­ç»ƒ LLM é¡¶å±‚ã€‚å› æ­¤ï¼ŒåŒä¸€ä¸ª LLM å¯ä»¥é€šè¿‡æ·»åŠ å°çš„æƒé‡æ¥ç”¨äºå¤šä¸ªä»»åŠ¡ï¼Œè€Œæ— éœ€æ›¿æ¢æ•´ä¸ªæ¨¡å‹ã€‚

**ç®€è€Œè¨€ä¹‹ï¼ŒPEFT æ–¹æ³•ä½¿æ‚¨èƒ½å¤Ÿè·å¾—ä¸å…¨å‚æ•°å¾®è°ƒç›¸å½“çš„æ€§èƒ½ï¼ŒåŒæ—¶åªæœ‰å°‘é‡å¯è®­ç»ƒå‚æ•°ã€‚**

ä»Šå¤©ï¼Œæˆ‘ä»¬å¾ˆé«˜å…´åœ°ä»‹ç» [ğŸ¤— PEFT](https://github.com/huggingface/peft) åº“ã€‚å®ƒæä¾›äº†æœ€æ–°çš„å‚æ•°é«˜æ•ˆå¾®è°ƒæŠ€æœ¯ï¼Œä¸ ğŸ¤— Transformers å’Œ ğŸ¤— Accelerate æ— ç¼é›†æˆã€‚è¿™ä½¿å¾—èƒ½å¤Ÿä½¿ç”¨æ¥è‡ª Transformers çš„æœ€æµè¡Œå’Œé«˜æ€§èƒ½çš„æ¨¡å‹ï¼Œä»¥åŠ Accelerate çš„ç®€å•æ€§å’Œå¯æ‰©å±•æ€§ã€‚ä»¥ä¸‹æ˜¯ç›®å‰æ”¯æŒçš„ PEFT æ–¹æ³•ï¼Œå³å°†æ¨å‡ºæ›´å¤š:

1. LoRA: [LORA: LOW-RANK ADAPTATION OF LARGE LANGUAGE MODELS](https://arxiv.org/pdf/2106.09685.pdf)
2. Prefix Tuning: [P-Tuning v2: Prompt Tuning Can Be Comparable to Fine-tuning Universally Across Scales and Tasks](https://arxiv.org/pdf/2110.07602.pdf)
3. Prompt Tuning: [The Power of Scale for Parameter-Efficient Prompt Tuning](https://arxiv.org/pdf/2104.08691.pdf) 
4. P-Tuning: [GPT Understands, Too](https://arxiv.org/pdf/2103.10385.pdf) 

## ç”¨ä¾‹

æˆ‘ä»¬åœ¨ GitHub PEFT åº“ä¸­æ¢ç´¢äº†è®¸å¤šæœ‰è¶£çš„[ç”¨ä¾‹](https://github.com/huggingface/peft#use-cases)ã€‚ä»¥ä¸‹ç½—åˆ—çš„æ˜¯å…¶ä¸­æœ€æœ‰è¶£çš„:

1. ä½¿ç”¨ ğŸ¤— PEFT LoRA åœ¨å…·æœ‰ 11GB RAM çš„æ¶ˆè´¹çº§ç¡¬ä»¶ä¸Šè°ƒæ•´ `bigscience/T0_3B` æ¨¡å‹ (30 äº¿ä¸ªå‚æ•°)ï¼Œä¾‹å¦‚ Nvidia GeForce RTX 2080 Tiã€Nvidia GeForce RTX 3080 ç­‰ï¼Œå¹¶ä¸”ä½¿ç”¨ ğŸ¤— Accelerate çš„ DeepSpeed é›†æˆ: [peft_lora_seq2seq_accelerate_ds_zero3_offload.py](https://github.com/huggingface/peft/blob/main/examples/conditional_generation/peft_lora_seq2seq_accelerate_ds_zero3_offload.py)ã€‚è¿™æ„å‘³ç€æ‚¨å¯ä»¥åœ¨ Google Colab ä¸­è°ƒæ•´å¦‚æ­¤å¤§çš„ LLMã€‚

2. é€šè¿‡ä½¿ç”¨ ğŸ¤— PEFT LoRA å’Œ [bitsandbytes](https://github.com/TimDettmers/bitsandbytes) åœ¨ Google Colab ä¸­å¯ç”¨ OPT-6.7b æ¨¡å‹ (67 äº¿ä¸ªå‚æ•°) çš„ INT8 è°ƒæ•´ï¼Œå°†å‰é¢çš„ç¤ºä¾‹æå‡ä¸€ä¸ªæ¡£æ¬¡: [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1jCkpikz0J2o20FBQmYmAGdiKmJGOMo-o?usp=sharing)ã€‚

3. åœ¨å…·æœ‰ 11GB RAM çš„æ¶ˆè´¹çº§ç¡¬ä»¶ä¸Šä½¿ç”¨ ğŸ¤— PEFT è¿›è¡Œç¨³å®šçš„ Diffusion Dreambooth è®­ç»ƒï¼Œä¾‹å¦‚ Nvidia GeForce RTX 2080 Tiã€Nvidia GeForce RTX 3080 ç­‰ã€‚è¯•ç”¨ Space æ¼”ç¤ºï¼Œå®ƒåº”è¯¥å¯ä»¥åœ¨ T4 å®ä¾‹ (16GB GPU) ä¸Šæ— ç¼è¿è¡Œ: [smangrul/peft-lora-sd-dreambooth](https://huggingface.co/spaces/smangrul/peft-lora-sd-dreambooth)ã€‚

<p align="center">
    <img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/peft/peft_lora_dreambooth_gradio_space.png" alt="peft lora dreambooth gradio space"><br>
    <em>PEFT LoRA Dreambooth Gradio Space</em>
</p>

## ä½¿ç”¨ ğŸ¤— PEFT è®­ç»ƒæ‚¨çš„æ¨¡å‹

è®©æˆ‘ä»¬è€ƒè™‘ä½¿ç”¨ LoRA å¾®è°ƒ [`bigscience/mt0-large`](https://huggingface.co/bigscience/mt0-large) çš„æƒ…å†µã€‚

1. å¼•è¿›å¿…è¦çš„åº“

```diff
  from transformers import AutoModelForSeq2SeqLM
+ from peft import get_peft_model, LoraConfig, TaskType
  model_name_or_path = "bigscience/mt0-large"
  tokenizer_name_or_path = "bigscience/mt0-large"
```

2. åˆ›å»º PEFT æ–¹æ³•å¯¹åº”çš„é…ç½®
```py
peft_config = LoraConfig(
    task_type=TaskType.SEQ_2_SEQ_LM, inference_mode=False, r=8, lora_alpha=32, lora_dropout=0.1
)
```

3. é€šè¿‡è°ƒç”¨ `get_peft_model` åŒ…è£…åŸºç¡€ ğŸ¤— Transformer æ¨¡å‹
```diff
  model = AutoModelForSeq2SeqLM.from_pretrained(model_name_or_path)
+ model = get_peft_model(model, peft_config)
+ model.print_trainable_parameters()
# output: trainable params: 2359296 || all params: 1231940608 || trainable%: 0.19151053100118282
```

å°±æ˜¯è¿™æ ·ï¼è®­ç»ƒå¾ªç¯çš„å…¶ä½™éƒ¨åˆ†ä¿æŒä¸å˜ã€‚æœ‰å…³ç«¯åˆ°ç«¯ç¤ºä¾‹ï¼Œè¯·å‚é˜…ç¤ºä¾‹ [peft_lora_seq2seq.ipynb](https://github.com/huggingface/peft/blob/main/examples/conditional_generation/peft_lora_seq2seq.ipynb)ã€‚

4. å½“æ‚¨å‡†å¤‡å¥½ä¿å­˜æ¨¡å‹ä»¥ä¾›æ¨ç†æ—¶ï¼Œåªéœ€æ‰§è¡Œä»¥ä¸‹æ“ä½œã€‚
```py
model.save_pretrained("output_dir") 
# model.push_to_hub("my_awesome_peft_model") also works
```

è¿™åªä¼šä¿å­˜ç»è¿‡è®­ç»ƒçš„å¢é‡ PEFT æƒé‡ã€‚ä¾‹å¦‚ï¼Œæ‚¨å¯ä»¥åœ¨æ­¤å¤„çš„ `twitter_complaints` raft æ•°æ®é›†ä¸Šæ‰¾åˆ°ä½¿ç”¨ LoRA è°ƒæ•´çš„ `bigscience/T0_3B`: [smangrul/twitter_complaints_bigscience_T0_3B_LORA_SEQ_2_SEQ_LM](https://huggingface.co/smangrul/twitter_complaints_bigscience_T0_3B_LORA_SEQ_2_SEQ_LM)ã€‚è¯·æ³¨æ„ï¼Œå®ƒåªåŒ…å« 2 ä¸ªæ–‡ä»¶: adapter_config.json å’Œ adapter_model.binï¼Œåè€…åªæœ‰ 19MBã€‚

5. è¦åŠ è½½å®ƒè¿›è¡Œæ¨ç†ï¼Œè¯·éµå¾ªä»¥ä¸‹ä»£ç ç‰‡æ®µ:
```diff
  from transformers import AutoModelForSeq2SeqLM
+ from peft import PeftModel, PeftConfig

  peft_model_id = "smangrul/twitter_complaints_bigscience_T0_3B_LORA_SEQ_2_SEQ_LM"
  config = PeftConfig.from_pretrained(peft_model_id)
  model = AutoModelForSeq2SeqLM.from_pretrained(config.base_model_name_or_path)
+ model = PeftModel.from_pretrained(model, peft_model_id)
  tokenizer = AutoTokenizer.from_pretrained(config.base_model_name_or_path)

  model = model.to(device)
  model.eval()
  inputs = tokenizer("Tweet text : @HondaCustSvc Your customer service has been horrible during the recall process. I will never purchase a Honda again. Label :", return_tensors="pt")

  with torch.no_grad():
      outputs = model.generate(input_ids=inputs["input_ids"].to("cuda"), max_new_tokens=10)
      print(tokenizer.batch_decode(outputs.detach().cpu().numpy(), skip_special_tokens=True)[0])
# 'complaint'
```

## ä¸‹ä¸€æ­¥

æˆ‘ä»¬å‘å¸ƒäº† PEFT æ–¹æ³•ï¼Œä½œä¸ºåœ¨ä¸‹æ¸¸ä»»åŠ¡å’ŒåŸŸä¸Šè°ƒæ•´å¤§å‹ LLM çš„æœ‰æ•ˆæ–¹å¼ï¼ŒèŠ‚çœäº†å¤§é‡è®¡ç®—å’Œå­˜å‚¨ï¼ŒåŒæ—¶å®ç°ä¸å…¨å‚æ•°å¾®è°ƒç›¸å½“çš„æ€§èƒ½ã€‚åœ¨æ¥ä¸‹æ¥çš„å‡ ä¸ªæœˆä¸­ï¼Œæˆ‘ä»¬å°†æ¢ç´¢æ›´å¤š PEFT æ–¹æ³•ï¼Œä¾‹å¦‚ (IA)3 å’Œç“¶é¢ˆé€‚é…å™¨ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å°†å…³æ³¨æ–°çš„ç”¨ä¾‹ï¼Œä¾‹å¦‚ Google Colab ä¸­[`whisper-large`](https://huggingface.co/openai/whisper-large) æ¨¡å‹çš„ INT8 è®­ç»ƒä»¥åŠä½¿ç”¨ PEFT æ–¹æ³•è°ƒæ•´ RLHF ç»„ä»¶ (ä¾‹å¦‚ç­–ç•¥å’Œæ’åºå™¨)ã€‚

ä¸æ­¤åŒæ—¶ï¼Œæˆ‘ä»¬å¾ˆé«˜å…´çœ‹åˆ°è¡Œä¸šä»ä¸šè€…å¦‚ä½•å°† PEFT åº”ç”¨äºä»–ä»¬çš„ç”¨ä¾‹ - å¦‚æœæ‚¨æœ‰ä»»ä½•é—®é¢˜æˆ–åé¦ˆï¼Œè¯·åœ¨æˆ‘ä»¬çš„ [GitHub ä»“åº“](https://github.com/huggingface/peft) ä¸Šæå‡ºé—®é¢˜ ğŸ¤—ã€‚

ç¥ä½ æœ‰ä¸€è¶Ÿå¿«ä¹çš„å‚æ•°é«˜æ•ˆå¾®è°ƒä¹‹æ—…ï¼

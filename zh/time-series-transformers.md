---
title: "ä½¿ç”¨ ğŸ¤— Transformers è¿›è¡Œæ¦‚ç‡æ—¶é—´åºåˆ—é¢„æµ‹"
thumbnail: /blog/assets/118_time-series-transformers/thumbnail.png
authors:
- user: nielsr
- user: kashif
translators:
- user: zhongdongy
---

# ä½¿ç”¨ ğŸ¤— Transformers è¿›è¡Œæ¦‚ç‡æ—¶é—´åºåˆ—é¢„æµ‹


<script async defer src="https://unpkg.com/medium-zoom-element@0/dist/medium-zoom-element.min.js"></script>

<a target="_blank" href="https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/time-series-transformers.ipynb">
    <img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/>
</a>

## ä»‹ç»

æ—¶é—´åºåˆ—é¢„æµ‹æ˜¯ä¸€ä¸ªé‡è¦çš„ç§‘å­¦å’Œå•†ä¸šé—®é¢˜ï¼Œå› æ­¤æœ€è¿‘é€šè¿‡ä½¿ç”¨ [åŸºäºæ·±åº¦å­¦ä¹ ](https://dl.acm.org/doi/abs/10.1145/3533382) è€Œä¸æ˜¯ [ç»å…¸æ–¹æ³•](https://otexts.com/fpp3/) çš„æ¨¡å‹ä¹Ÿæ¶Œç°å‡ºè¯¸å¤šåˆ›æ–°ã€‚ARIMA ç­‰ç»å…¸æ–¹æ³•ä¸æ–°é¢–çš„æ·±åº¦å­¦ä¹ æ–¹æ³•ä¹‹é—´çš„ä¸€ä¸ªé‡è¦åŒºåˆ«å¦‚ä¸‹ã€‚

##  æ¦‚ç‡é¢„æµ‹

é€šå¸¸ï¼Œç»å…¸æ–¹æ³•é’ˆå¯¹æ•°æ®é›†ä¸­çš„æ¯ä¸ªæ—¶é—´åºåˆ—å•ç‹¬æ‹Ÿåˆã€‚è¿™äº›é€šå¸¸è¢«ç§°ä¸ºâ€œå•ä¸€â€æˆ–â€œå±€éƒ¨â€æ–¹æ³•ã€‚ç„¶è€Œï¼Œå½“å¤„ç†æŸäº›åº”ç”¨ç¨‹åºçš„å¤§é‡æ—¶é—´åºåˆ—æ—¶ï¼Œåœ¨æ‰€æœ‰å¯ç”¨æ—¶é—´åºåˆ—ä¸Šè®­ç»ƒä¸€ä¸ªâ€œå…¨å±€â€æ¨¡å‹æ˜¯æœ‰ç›Šçš„ï¼Œè¿™ä½¿æ¨¡å‹èƒ½å¤Ÿä»è®¸å¤šä¸åŒçš„æ¥æºå­¦ä¹ æ½œåœ¨çš„è¡¨ç¤ºã€‚

ä¸€äº›ç»å…¸æ–¹æ³•æ˜¯ç‚¹å€¼çš„ (point-valued)(æ„æ€æ˜¯æ¯ä¸ªæ—¶é—´æ­¥åªè¾“å‡ºä¸€ä¸ªå€¼)ï¼Œå¹¶ä¸”é€šè¿‡æœ€å°åŒ–å…³äºåŸºæœ¬äº‹å®æ•°æ®çš„ L2 æˆ– L1 ç±»å‹çš„æŸå¤±æ¥è®­ç»ƒæ¨¡å‹ã€‚ç„¶è€Œï¼Œç”±äºé¢„æµ‹ç»å¸¸ç”¨äºå®é™…å†³ç­–æµç¨‹ä¸­ï¼Œç”šè‡³åœ¨å¾ªç¯ä¸­æœ‰äººçš„å¹²é¢„ï¼Œè®©æ¨¡å‹åŒæ—¶ä¹Ÿæä¾›é¢„æµ‹çš„ä¸ç¡®å®šæ€§æ›´åŠ æœ‰ç›Šã€‚è¿™ä¹Ÿç§°ä¸ºâ€œæ¦‚ç‡é¢„æµ‹â€ï¼Œè€Œä¸æ˜¯â€œç‚¹é¢„æµ‹â€ã€‚è¿™éœ€è¦å¯¹å¯ä»¥é‡‡æ ·çš„æ¦‚ç‡åˆ†å¸ƒè¿›è¡Œå»ºæ¨¡ã€‚

æ‰€ä»¥ç®€è€Œè¨€ä¹‹ï¼Œæˆ‘ä»¬å¸Œæœ›è®­ç»ƒ **å…¨å±€æ¦‚ç‡æ¨¡å‹**ï¼Œè€Œä¸æ˜¯è®­ç»ƒå±€éƒ¨ç‚¹é¢„æµ‹æ¨¡å‹ã€‚æ·±åº¦å­¦ä¹ éå¸¸é€‚åˆè¿™ä¸€ç‚¹ï¼Œå› ä¸ºç¥ç»ç½‘ç»œå¯ä»¥ä»å‡ ä¸ªç›¸å…³çš„æ—¶é—´åºåˆ—ä¸­å­¦ä¹ è¡¨ç¤ºï¼Œå¹¶å¯¹æ•°æ®çš„ä¸ç¡®å®šæ€§è¿›è¡Œå»ºæ¨¡ã€‚

åœ¨æ¦‚ç‡è®¾å®šä¸­å­¦ä¹ æŸäº›é€‰å®šå‚æ•°åˆ†å¸ƒçš„æœªæ¥å‚æ•°å¾ˆå¸¸è§ï¼Œä¾‹å¦‚é«˜æ–¯åˆ†å¸ƒ (Gaussian) æˆ– Student-Tï¼Œæˆ–è€…å­¦ä¹ æ¡ä»¶åˆ†ä½æ•°å‡½æ•° (conditional quantile function)ï¼Œæˆ–ä½¿ç”¨é€‚åº”æ—¶é—´åºåˆ—è®¾ç½®çš„å…±å‹é¢„æµ‹ (Conformal Prediction) æ¡†æ¶ã€‚æ–¹æ³•çš„é€‰æ‹©ä¸ä¼šå½±å“åˆ°å»ºæ¨¡ï¼Œå› æ­¤é€šå¸¸å¯ä»¥å°†å…¶è§†ä¸ºå¦ä¸€ä¸ªè¶…å‚æ•°ã€‚é€šè¿‡é‡‡ç”¨ç»éªŒå‡å€¼æˆ–ä¸­å€¼ï¼Œäººä»¬æ€»æ˜¯å¯ä»¥å°†æ¦‚ç‡æ¨¡å‹è½¬å˜ä¸ºç‚¹é¢„æµ‹æ¨¡å‹ã€‚

## æ—¶é—´åºåˆ— Transformer

æ­£å¦‚äººä»¬æ‰€æƒ³è±¡çš„é‚£æ ·ï¼Œåœ¨å¯¹æœ¬æ¥å°±è¿ç»­çš„æ—¶é—´åºåˆ—æ•°æ®å»ºæ¨¡æ–¹é¢ï¼Œç ”ç©¶äººå‘˜æå‡ºäº†ä½¿ç”¨å¾ªç¯ç¥ç»ç½‘ç»œ (RNN) (å¦‚ LSTM æˆ– GRU) æˆ–å·ç§¯ç½‘ç»œ (CNN) çš„æ¨¡å‹ï¼Œæˆ–åˆ©ç”¨æœ€è¿‘å…´èµ·çš„åŸºäº Transformer çš„è®­ç»ƒæ–¹æ³•ï¼Œéƒ½å¾ˆè‡ªç„¶åœ°é€‚åˆæ—¶é—´åºåˆ—é¢„æµ‹åœºæ™¯ã€‚

åœ¨è¿™ç¯‡åšæ–‡ä¸­ï¼Œæˆ‘ä»¬å°†åˆ©ç”¨ä¼ ç»Ÿ vanilla Transformer (å‚è€ƒ [(Vaswani et al., 2017)](https://arxiv.org/abs/1706.03762)) è¿›è¡Œå•å˜é‡æ¦‚ç‡é¢„æµ‹ (**univariate** probabilistic forecasting) ä»»åŠ¡ (å³é¢„æµ‹æ¯ä¸ªæ—¶é—´åºåˆ—çš„ä¸€ç»´åˆ†å¸ƒ)ã€‚ç”±äº Encoder-Decoder Transformer å¾ˆå¥½åœ°å°è£…äº†å‡ ä¸ªå½’çº³åå·®ï¼Œæ‰€ä»¥å®ƒæˆä¸ºäº†æˆ‘ä»¬é¢„æµ‹çš„è‡ªç„¶é€‰æ‹©ã€‚

é¦–å…ˆï¼Œä½¿ç”¨ Encoder-Decoder æ¶æ„åœ¨æ¨ç†æ—¶å¾ˆæœ‰å¸®åŠ©ã€‚é€šå¸¸å¯¹äºä¸€äº›è®°å½•çš„æ•°æ®ï¼Œæˆ‘ä»¬å¸Œæœ›æå‰é¢„çŸ¥æœªæ¥çš„ä¸€äº›é¢„æµ‹æ­¥éª¤ã€‚å¯ä»¥è®¤ä¸ºè¿™ä¸ªè¿‡ç¨‹ç±»ä¼¼äºæ–‡æœ¬ç”Ÿæˆä»»åŠ¡ï¼Œå³ç»™å®šä¸Šä¸‹æ–‡ï¼Œé‡‡æ ·ä¸‹ä¸€ä¸ªè¯å…ƒ (token) å¹¶å°†å…¶ä¼ å›è§£ç å™¨ (ä¹Ÿç§°ä¸ºâ€œè‡ªå›å½’ç”Ÿæˆâ€) ã€‚ç±»ä¼¼åœ°ï¼Œæˆ‘ä»¬ä¹Ÿå¯ä»¥åœ¨ç»™å®šæŸç§åˆ†å¸ƒç±»å‹çš„æƒ…å†µä¸‹ï¼Œä»ä¸­æŠ½æ ·ä»¥æä¾›é¢„æµ‹ï¼Œç›´åˆ°æˆ‘ä»¬æœŸæœ›çš„é¢„æµ‹èŒƒå›´ã€‚è¿™è¢«ç§°ä¸ºè´ªå©ªé‡‡æ · (Greedy Sampling)/æœç´¢ï¼Œ[æ­¤å¤„](https://huggingface.co/blog/zh/how-to-generate) æœ‰ä¸€ç¯‡å…³äº NLP åœºæ™¯é¢„æµ‹çš„ç²¾å½©åšæ–‡ã€‚

å…¶æ¬¡ï¼ŒTransformer å¸®åŠ©æˆ‘ä»¬è®­ç»ƒå¯èƒ½åŒ…å«æˆåƒä¸Šä¸‡ä¸ªæ—¶é—´ç‚¹çš„æ—¶é—´åºåˆ—æ•°æ®ã€‚ç”±äºæ³¨æ„åŠ›æœºåˆ¶çš„æ—¶é—´å’Œå†…å­˜é™åˆ¶ï¼Œä¸€æ¬¡æ€§å°† *æ‰€æœ‰* æ—¶é—´åºåˆ—çš„å®Œæ•´å†å²è¾“å…¥æ¨¡å‹æˆ–è®¸ä¸å¤ªå¯è¡Œã€‚å› æ­¤ï¼Œåœ¨ä¸ºéšæœºæ¢¯åº¦ä¸‹é™ (SGD) æ„å»ºæ‰¹æ¬¡æ—¶ï¼Œå¯ä»¥è€ƒè™‘é€‚å½“çš„ä¸Šä¸‹æ–‡çª—å£å¤§å°ï¼Œå¹¶ä»è®­ç»ƒæ•°æ®ä¸­å¯¹è¯¥çª—å£å’Œåç»­é¢„æµ‹é•¿åº¦å¤§å°çš„çª—å£è¿›è¡Œé‡‡æ ·ã€‚å¯ä»¥å°†è°ƒæ•´è¿‡å¤§å°çš„ä¸Šä¸‹æ–‡çª—å£ä¼ é€’ç»™ç¼–ç å™¨ã€é¢„æµ‹çª—å£ä¼ é€’ç»™ *ausal-masked* è§£ç å™¨ã€‚è¿™æ ·ä¸€æ¥ï¼Œè§£ç å™¨åœ¨å­¦ä¹ ä¸‹ä¸€ä¸ªå€¼æ—¶åªèƒ½æŸ¥çœ‹ä¹‹å‰çš„æ—¶é—´æ­¥ã€‚è¿™ç›¸å½“äºäººä»¬è®­ç»ƒç”¨äºæœºå™¨ç¿»è¯‘çš„ vanilla Transformer çš„è¿‡ç¨‹ï¼Œç§°ä¸ºâ€œæ•™å¸ˆå¼ºåˆ¶ (Teacher Forcing)â€ã€‚

Transformers ç›¸å¯¹äºå…¶ä»–æ¶æ„çš„å¦ä¸€ä¸ªå¥½å¤„æ˜¯ï¼Œæˆ‘ä»¬å¯ä»¥å°†ç¼ºå¤±å€¼ (è¿™åœ¨æ—¶é—´åºåˆ—åœºæ™¯ä¸­å¾ˆå¸¸è§) ä½œä¸ºç¼–ç å™¨æˆ–è§£ç å™¨çš„é¢å¤–æ©è”½å€¼ (mask)ï¼Œå¹¶ä¸”ä»ç„¶å¯ä»¥åœ¨ä¸è¯‰è¯¸äºå¡«å……æˆ–æ’è¡¥çš„æƒ…å†µä¸‹è¿›è¡Œè®­ç»ƒã€‚è¿™ç›¸å½“äº Transformers åº“ä¸­ BERT å’Œ GPT-2 ç­‰æ¨¡å‹çš„ `attention_mask`ï¼Œåœ¨æ³¨æ„åŠ›çŸ©é˜µ (attention matrix) çš„è®¡ç®—ä¸­ä¸åŒ…æ‹¬å¡«å……è¯å…ƒã€‚

ç”±äºä¼ ç»Ÿ vanilla Transformer çš„å¹³æ–¹è¿ç®—å’Œå†…å­˜è¦æ±‚ï¼ŒTransformer æ¶æ„çš„ä¸€ä¸ªç¼ºç‚¹æ˜¯ä¸Šä¸‹æ–‡å’Œé¢„æµ‹çª—å£çš„å¤§å°å—åˆ°é™åˆ¶ã€‚å…³äºè¿™ä¸€ç‚¹ï¼Œå¯ä»¥å‚é˜… [Tay et al., 2020](https://arxiv.org/abs/2009.06732)ã€‚æ­¤å¤–ï¼Œç”±äº Transformer æ˜¯ä¸€ç§å¼ºå¤§çš„æ¶æ„ï¼Œä¸ [å…¶ä»–æ–¹æ³•](https://openreview.net/pdf?id=D7YBmfX_VQy) ç›¸æ¯”ï¼Œå®ƒå¯èƒ½ä¼šè¿‡æ‹Ÿåˆæˆ–æ›´å®¹æ˜“å­¦ä¹ è™šå‡ç›¸å…³æ€§ã€‚

ğŸ¤— Transformers åº“å¸¦æœ‰ä¸€ä¸ªæ™®é€šçš„æ¦‚ç‡æ—¶é—´åºåˆ— Transformer æ¨¡å‹ï¼Œç®€ç§°ä¸º [Time Series Transformer](https://huggingface.co/docs/transformers/model_doc/time_series_transformer)ã€‚åœ¨è¿™ç¯‡æ–‡ç« åé¢çš„å†…å®¹ä¸­ï¼Œæˆ‘ä»¬å°†å±•ç¤ºå¦‚ä½•åœ¨è‡ªå®šä¹‰æ•°æ®é›†ä¸Šè®­ç»ƒæ­¤ç±»æ¨¡å‹ã€‚


## è®¾ç½®ç¯å¢ƒ

é¦–å…ˆï¼Œè®©æˆ‘ä»¬å®‰è£…å¿…è¦çš„åº“: ğŸ¤— Transformersã€ğŸ¤— Datasetsã€ğŸ¤— Evaluateã€ğŸ¤— Accelerate å’Œ [GluonTS](https://github.com/awslabs/gluonts)ã€‚

æ­£å¦‚æˆ‘ä»¬å°†å±•ç¤ºçš„é‚£æ ·ï¼ŒGluonTS å°†ç”¨äºè½¬æ¢æ•°æ®ä»¥åˆ›å»ºç‰¹å¾ä»¥åŠåˆ›å»ºé€‚å½“çš„è®­ç»ƒã€éªŒè¯å’Œæµ‹è¯•æ‰¹æ¬¡ã€‚


```python
!pip install -q transformers

!pip install -q datasets

!pip install -q evaluate

!pip install -q accelerate

!pip install -q gluonts ujson
```

## åŠ è½½æ•°æ®é›†

åœ¨è¿™ç¯‡åšæ–‡ä¸­ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨ [Hugging Face Hub](https://huggingface.co/datasets/monash_tsf) ä¸Šæä¾›çš„ `tourism_monthly` æ•°æ®é›†ã€‚è¯¥æ•°æ®é›†åŒ…å«æ¾³å¤§åˆ©äºš 366 ä¸ªåœ°åŒºçš„æ¯æœˆæ—…æ¸¸æµé‡ã€‚

æ­¤æ•°æ®é›†æ˜¯ [Monash Time Series Forecasting](https://forecastingdata.org/) å­˜å‚¨åº“çš„ä¸€éƒ¨åˆ†ï¼Œè¯¥å­˜å‚¨åº“æ”¶çº³äº†æ˜¯æ¥è‡ªå¤šä¸ªé¢†åŸŸçš„æ—¶é—´åºåˆ—æ•°æ®é›†ã€‚å®ƒå¯ä»¥çœ‹ä½œæ˜¯æ—¶é—´åºåˆ—é¢„æµ‹çš„ GLUE åŸºå‡†ã€‚


```python
from datasets import load_dataset

dataset = load_dataset("monash_tsf", "tourism_monthly")
```

å¯ä»¥çœ‹å‡ºï¼Œæ•°æ®é›†åŒ…å« 3 ä¸ªç‰‡æ®µ: è®­ç»ƒã€éªŒè¯å’Œæµ‹è¯•ã€‚

```python
dataset

>>> DatasetDict({
        train: Dataset({
            features: ['start', 'target', 'feat_static_cat', 'feat_dynamic_real', 'item_id'],
            num_rows: 366
        })
        test: Dataset({
            features: ['start', 'target', 'feat_static_cat', 'feat_dynamic_real', 'item_id'],
            num_rows: 366
        })
        validation: Dataset({
            features: ['start', 'target', 'feat_static_cat', 'feat_dynamic_real', 'item_id'],
            num_rows: 366
        })
    })
```

æ¯ä¸ªç¤ºä¾‹éƒ½åŒ…å«ä¸€äº›é”®ï¼Œå…¶ä¸­ `start` å’Œ `target` æ˜¯æœ€é‡è¦çš„é”®ã€‚è®©æˆ‘ä»¬çœ‹ä¸€ä¸‹æ•°æ®é›†ä¸­çš„ç¬¬ä¸€ä¸ªæ—¶é—´åºåˆ—:

```python
train_example = dataset['train'][0]
train_example.keys()

>>> dict_keys(['start', 'target', 'feat_static_cat', 'feat_dynamic_real', 'item_id'])
```

`start` ä»…æŒ‡ç¤ºæ—¶é—´åºåˆ—çš„å¼€å§‹ (ç±»å‹ä¸º `datetime`) ï¼Œè€Œ `target` åŒ…å«æ—¶é—´åºåˆ—çš„å®é™…å€¼ã€‚

`start` å°†æœ‰åŠ©äºå°†æ—¶é—´ç›¸å…³çš„ç‰¹å¾æ·»åŠ åˆ°æ—¶é—´åºåˆ—å€¼ä¸­ï¼Œä½œä¸ºæ¨¡å‹çš„é¢å¤–è¾“å…¥ (ä¾‹å¦‚â€œä¸€å¹´ä¸­çš„æœˆä»½â€) ã€‚å› ä¸ºæˆ‘ä»¬å·²ç»çŸ¥é“æ•°æ®çš„é¢‘ç‡æ˜¯ `æ¯æœˆ`ï¼Œæ‰€ä»¥ä¹Ÿèƒ½æ¨ç®—ç¬¬äºŒä¸ªå€¼çš„æ—¶é—´æˆ³ä¸º `1979-02-01`ï¼Œç­‰ç­‰ã€‚


```python
print(train_example['start'])
print(train_example['target'])

>>> 1979-01-01 00:00:00
    [1149.8699951171875, 1053.8001708984375, ..., 5772.876953125]
```

éªŒè¯é›†åŒ…å«ä¸è®­ç»ƒé›†ç›¸åŒçš„æ•°æ®ï¼Œåªæ˜¯æ•°æ®æ—¶é—´èŒƒå›´å»¶é•¿äº† `prediction_length` é‚£ä¹ˆå¤šã€‚è¿™ä½¿æˆ‘ä»¬èƒ½å¤Ÿæ ¹æ®çœŸå®æƒ…å†µéªŒè¯æ¨¡å‹çš„é¢„æµ‹ã€‚

ä¸éªŒè¯é›†ç›¸æ¯”ï¼Œæµ‹è¯•é›†è¿˜æ˜¯æ¯”éªŒè¯é›†å¤šåŒ…å« `prediction_length` æ—¶é—´çš„æ•°æ® (æˆ–è€…ä½¿ç”¨æ¯”è®­ç»ƒé›†å¤šå‡ºæ•°ä¸ª `prediction_length` æ—¶é•¿æ•°æ®çš„æµ‹è¯•é›†ï¼Œå®ç°åœ¨å¤šé‡æ»šåŠ¨çª—å£ä¸Šçš„æµ‹è¯•ä»»åŠ¡)ã€‚

```python
validation_example = dataset['validation'][0]
validation_example.keys()

>>> dict_keys(['start', 'target', 'feat_static_cat', 'feat_dynamic_real', 'item_id'])
```

éªŒè¯çš„åˆå§‹å€¼ä¸ç›¸åº”çš„è®­ç»ƒç¤ºä¾‹å®Œå…¨ç›¸åŒ:

```python
print(validation_example['start'])
print(validation_example['target'])

>>> 1979-01-01 00:00:00
    [1149.8699951171875, 1053.8001708984375, ..., 5985.830078125]
```

ä½†æ˜¯ï¼Œä¸è®­ç»ƒç¤ºä¾‹ç›¸æ¯”ï¼Œæ­¤ç¤ºä¾‹å…·æœ‰ `prediction_length=24` ä¸ªé¢å¤–çš„æ•°æ®ã€‚è®©æˆ‘ä»¬éªŒè¯ä¸€ä¸‹ã€‚

```python
freq = "1M"
prediction_length = 24

assert len(train_example["target"]) + prediction_length == len(
    validation_example["target"]
)
```

è®©æˆ‘ä»¬å¯è§†åŒ–ä¸€ä¸‹:


```python
import matplotlib.pyplot as plt

figure, axes = plt.subplots()
axes.plot(train_example["target"], color="blue")
axes.plot(validation_example["target"], color="red", alpha=0.5)

plt.show()
```
    
![png](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/time-series-transformers/output_21_0.png)
    
ä¸‹é¢æ‹†åˆ†æ•°æ®:

```python
train_dataset = dataset["train"]
test_dataset = dataset["test"]
```

## å°† `start` æ›´æ–°ä¸º `pd.Period`

æˆ‘ä»¬è¦åšçš„ç¬¬ä¸€ä»¶äº‹æ˜¯æ ¹æ®æ•°æ®çš„ `freq` å€¼å°†æ¯ä¸ªæ—¶é—´åºåˆ—çš„ `start` ç‰¹å¾è½¬æ¢ä¸º pandas çš„ `Period` ç´¢å¼•:

```python
from functools import lru_cache

import pandas as pd
import numpy as np

@lru_cache(10_000)
def convert_to_pandas_period(date, freq):
    return pd.Period(date, freq)

def transform_start_field(batch, freq):
    batch["start"] = [convert_to_pandas_period(date, freq) for date in batch["start"]]
    return batch
```

è¿™é‡Œæˆ‘ä»¬ä½¿ç”¨ `datasets` çš„ [`set_transform`](https://huggingface.co/docs/datasets/v2.7.0/en/package_reference/main_classes#datasets.Dataset.set_transform) æ¥å®ç°:

```python
from functools import partial

train_dataset.set_transform(partial(transform_start_field, freq=freq))
test_dataset.set_transform(partial(transform_start_field, freq=freq))
```

## å®šä¹‰æ¨¡å‹

æ¥ä¸‹æ¥ï¼Œè®©æˆ‘ä»¬å®ä¾‹åŒ–ä¸€ä¸ªæ¨¡å‹ã€‚è¯¥æ¨¡å‹å°†ä»å¤´å¼€å§‹è®­ç»ƒï¼Œå› æ­¤æˆ‘ä»¬ä¸ä½¿ç”¨ `from_pretrained` æ–¹æ³•ï¼Œè€Œæ˜¯ä» [`config`](https://huggingface.co/docs/transformers/model_doc/time_series_transformer#transformers.TimeSeriesTransformerConfig) ä¸­éšæœºåˆå§‹åŒ–æ¨¡å‹ã€‚

æˆ‘ä»¬ä¸ºæ¨¡å‹æŒ‡å®šäº†å‡ ä¸ªé™„åŠ å‚æ•°:
- `prediction_length` (åœ¨æˆ‘ä»¬çš„ä¾‹å­ä¸­æ˜¯ `24` ä¸ªæœˆ) : è¿™æ˜¯ Transformer çš„è§£ç å™¨å°†å­¦ä¹ é¢„æµ‹çš„èŒƒå›´;
- `context_length`: å¦‚æœæœªæŒ‡å®š `context_length`ï¼Œæ¨¡å‹ä¼šå°† `context_length` (ç¼–ç å™¨çš„è¾“å…¥) è®¾ç½®ä¸ºç­‰äº `prediction_length`;
- ç»™å®šé¢‘ç‡çš„ `lags`(æ»å): è¿™å°†å†³å®šæ¨¡å‹â€œå›å¤´çœ‹â€çš„ç¨‹åº¦ï¼Œä¹Ÿä¼šä½œä¸ºé™„åŠ ç‰¹å¾ã€‚ä¾‹å¦‚å¯¹äº `Daily` é¢‘ç‡ï¼Œæˆ‘ä»¬å¯èƒ½ä¼šè€ƒè™‘å›é¡¾ `[1, 2, 7, 30, ...]`ï¼Œä¹Ÿå°±æ˜¯å›é¡¾ 1ã€2â€¦â€¦å¤©çš„æ•°æ®ï¼Œè€Œå¯¹äº Minuteæ•°æ®ï¼Œæˆ‘ä»¬å¯èƒ½ä¼šè€ƒè™‘ `[1, 30, 60, 60*24, ...]` ç­‰;
- æ—¶é—´ç‰¹å¾çš„æ•°é‡: åœ¨æˆ‘ä»¬çš„ä¾‹å­ä¸­è®¾ç½®ä¸º `2`ï¼Œå› ä¸ºæˆ‘ä»¬å°†æ·»åŠ  `MonthOfYear` å’Œ `Age` ç‰¹å¾;
- é™æ€ç±»åˆ«å‹ç‰¹å¾çš„æ•°é‡: åœ¨æˆ‘ä»¬çš„ä¾‹å­ä¸­ï¼Œè¿™å°†åªæ˜¯ `1`ï¼Œå› ä¸ºæˆ‘ä»¬å°†æ·»åŠ ä¸€ä¸ªâ€œæ—¶é—´åºåˆ— IDâ€ç‰¹å¾;
- åŸºæ•°: å°†æ¯ä¸ªé™æ€ç±»åˆ«å‹ç‰¹å¾çš„å€¼çš„æ•°é‡æ„æˆä¸€ä¸ªåˆ—è¡¨ï¼Œå¯¹äºæœ¬ä¾‹æ¥è¯´å°†æ˜¯ `[366]`ï¼Œå› ä¸ºæˆ‘ä»¬æœ‰ 366 ä¸ªä¸åŒçš„æ—¶é—´åºåˆ—;
- åµŒå…¥ç»´åº¦: æ¯ä¸ªé™æ€ç±»åˆ«å‹ç‰¹å¾çš„åµŒå…¥ç»´åº¦ï¼Œä¹Ÿæ˜¯æ„æˆåˆ—è¡¨ã€‚ä¾‹å¦‚ `[3]` æ„å‘³ç€æ¨¡å‹å°†ä¸ºæ¯ä¸ª `366` æ—¶é—´åºåˆ— (åŒºåŸŸ) å­¦ä¹ å¤§å°ä¸º `3` çš„åµŒå…¥å‘é‡ã€‚

è®©æˆ‘ä»¬ä½¿ç”¨ GluonTS ä¸ºç»™å®šé¢‘ç‡ (â€œæ¯æœˆâ€) æä¾›çš„é»˜è®¤æ»åå€¼:

```python
from gluonts.time_feature import get_lags_for_frequency

lags_sequence = get_lags_for_frequency(freq)
print(lags_sequence)

>>> [1, 2, 3, 4, 5, 6, 7, 11, 12, 13, 23, 24, 25, 35, 36, 37]
```

è¿™æ„å‘³ç€æˆ‘ä»¬æ¯ä¸ªæ—¶é—´æ­¥å°†å›é¡¾é•¿è¾¾ 37 ä¸ªæœˆçš„æ•°æ®ï¼Œä½œä¸ºé™„åŠ ç‰¹å¾ã€‚

æˆ‘ä»¬è¿˜æ£€æŸ¥ GluonTS ä¸ºæˆ‘ä»¬æä¾›çš„é»˜è®¤æ—¶é—´ç‰¹å¾:


```python
from gluonts.time_feature import time_features_from_frequency_str

time_features = time_features_from_frequency_str(freq)
print(time_features)

>>> [<function month_of_year at 0x7fa496d0ca70>]
```

åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œåªæœ‰ä¸€ä¸ªç‰¹å¾ï¼Œå³â€œä¸€å¹´ä¸­çš„æœˆä»½â€ã€‚è¿™æ„å‘³ç€å¯¹äºæ¯ä¸ªæ—¶é—´æ­¥é•¿ï¼Œæˆ‘ä»¬å°†æ·»åŠ æœˆä»½ä½œä¸ºæ ‡é‡å€¼ (ä¾‹å¦‚ï¼Œå¦‚æœæ—¶é—´æˆ³ä¸º "january"ï¼Œåˆ™ä¸º `1`ï¼›å¦‚æœæ—¶é—´æˆ³ä¸º "february"ï¼Œåˆ™ä¸º `2`ï¼Œç­‰ç­‰) ã€‚

æˆ‘ä»¬ç°åœ¨å‡†å¤‡å¥½å®šä¹‰æ¨¡å‹éœ€è¦çš„æ‰€æœ‰å†…å®¹äº†:


```python
from transformers import TimeSeriesTransformerConfig, TimeSeriesTransformerForPrediction

config = TimeSeriesTransformerConfig(
    prediction_length=prediction_length,
    # context length:
    context_length=prediction_length * 2,
    # lags coming from helper given the freq:
    lags_sequence=lags_sequence,
    # we'll add 2 time features ("month of year" and "age", see further):
    num_time_features=len(time_features) + 1,
    # we have a single static categorical feature, namely time series ID:
    num_static_categorical_features=1,
    # it has 366 possible values:
    cardinality=[len(train_dataset)],
    # the model will learn an embedding of size 2 for each of the 366 possible values:
    embedding_dimension=[2],
    
    # transformer params:
    encoder_layers=4,
    decoder_layers=4,
    d_model=32,
)

model = TimeSeriesTransformerForPrediction(config)
```

è¯·æ³¨æ„ï¼Œä¸ ğŸ¤— Transformers åº“ä¸­çš„å…¶ä»–æ¨¡å‹ç±»ä¼¼ï¼Œ[`TimeSeriesTransformerModel`](https://huggingface.co/docs/transformers/model_doc/time_series_transformer#transformers.TimeSeriesTransformerModel) å¯¹åº”äºæ²¡æœ‰ä»»ä½•é¡¶éƒ¨å‰ç½®å¤´çš„ç¼–ç å™¨-è§£ç å™¨ Transformerï¼Œè€Œ [`TimeSeriesTransformerForPrediction`](https://huggingface.co/docs/transformers/model_doc/time_series_transformer#transformers.TimeSeriesTransformerForPrediction) å¯¹åº”äºé¡¶éƒ¨æœ‰ä¸€ä¸ªåˆ†å¸ƒå‰ç½®å¤´ (**distribution head**) çš„ `TimeSeriesTransformerForPrediction`ã€‚é»˜è®¤æƒ…å†µä¸‹ï¼Œè¯¥æ¨¡å‹ä½¿ç”¨ Student-t åˆ†å¸ƒ (ä¹Ÿå¯ä»¥è‡ªè¡Œé…ç½®):

```python
model.config.distribution_output

>>> student_t
```

è¿™æ˜¯å…·ä½“å®ç°å±‚é¢ä¸ç”¨äº NLP çš„ Transformers çš„ä¸€ä¸ªé‡è¦åŒºåˆ«ï¼Œå…¶ä¸­å¤´éƒ¨é€šå¸¸ç”±ä¸€ä¸ªå›ºå®šçš„åˆ†ç±»åˆ†å¸ƒç»„æˆï¼Œå®ç°ä¸º `nn.Linear` å±‚ã€‚

## å®šä¹‰è½¬æ¢

æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å®šä¹‰æ•°æ®çš„è½¬æ¢ï¼Œå°¤å…¶æ˜¯éœ€è¦åŸºäºæ ·æœ¬æ•°æ®é›†æˆ–é€šç”¨æ•°æ®é›†æ¥åˆ›å»ºå…¶ä¸­çš„æ—¶é—´ç‰¹å¾ã€‚

åŒæ ·ï¼Œæˆ‘ä»¬ç”¨åˆ°äº† GluonTS åº“ã€‚è¿™é‡Œå®šä¹‰äº†ä¸€ä¸ª `Chain` (æœ‰ç‚¹ç±»ä¼¼äºå›¾åƒè®­ç»ƒçš„ `torchvision.transforms.Compose`) ã€‚å®ƒå…è®¸æˆ‘ä»¬å°†å¤šä¸ªè½¬æ¢ç»„åˆåˆ°ä¸€ä¸ªæµæ°´çº¿ä¸­ã€‚

```python
from gluonts.time_feature import (
    time_features_from_frequency_str,
    TimeFeature,
    get_lags_for_frequency,
)
from gluonts.dataset.field_names import FieldName
from gluonts.transform import (
    AddAgeFeature,
    AddObservedValuesIndicator,
    AddTimeFeatures,
    AsNumpyArray,
    Chain,
    ExpectedNumInstanceSampler,
    InstanceSplitter,
    RemoveFields,
    SelectFields,
    SetField,
    TestSplitSampler,
    Transformation,
    ValidationSplitSampler,
    VstackFeatures,
    RenameFields,
)
```

ä¸‹é¢çš„è½¬æ¢ä»£ç å¸¦æœ‰æ³¨é‡Šä¾›å¤§å®¶æŸ¥çœ‹å…·ä½“çš„æ“ä½œæ­¥éª¤ã€‚ä»å…¨å±€æ¥è¯´ï¼Œæˆ‘ä»¬å°†è¿­ä»£æ•°æ®é›†çš„å„ä¸ªæ—¶é—´åºåˆ—å¹¶æ·»åŠ ã€åˆ é™¤æŸäº›å­—æ®µæˆ–ç‰¹å¾:

```python
from transformers import PretrainedConfig

def create_transformation(freq: str, config: PretrainedConfig) -> Transformation:
    remove_field_names = []
    if config.num_static_real_features == 0:
        remove_field_names.append(FieldName.FEAT_STATIC_REAL)
    if config.num_dynamic_real_features == 0:
        remove_field_names.append(FieldName.FEAT_DYNAMIC_REAL)
    if config.num_static_categorical_features == 0:
        remove_field_names.append(FieldName.FEAT_STATIC_CAT)

    # a bit like torchvision.transforms.Compose
    return Chain(
        # step 1: remove static/dynamic fields if not specified
        [RemoveFields(field_names=remove_field_names)]
        # step 2: convert the data to NumPy (potentially not needed)
        + (
            [
                AsNumpyArray(
                    field=FieldName.FEAT_STATIC_CAT,
                    expected_ndim=1,
                    dtype=int,
                )
            ]
            if config.num_static_categorical_features > 0
            else []
        )
        + (
            [
                AsNumpyArray(
                    field=FieldName.FEAT_STATIC_REAL,
                    expected_ndim=1,
                )
            ]
            if config.num_static_real_features > 0
            else []
        )
        + [
            AsNumpyArray(
                field=FieldName.TARGET,
                # we expect an extra dim for the multivariate case:
                expected_ndim=1 if config.input_size == 1 else 2,
            ),
            # step 3: handle the NaN's by filling in the target with zero
            # and return the mask (which is in the observed values)
            # true for observed values, false for nan's
            # the decoder uses this mask (no loss is incurred for unobserved values)
            # see loss_weights inside the xxxForPrediction model
            AddObservedValuesIndicator(
                target_field=FieldName.TARGET,
                output_field=FieldName.OBSERVED_VALUES,
            ),
            # step 4: add temporal features based on freq of the dataset
            # month of year in the case when freq="M"
            # these serve as positional encodings
            AddTimeFeatures(
                start_field=FieldName.START,
                target_field=FieldName.TARGET,
                output_field=FieldName.FEAT_TIME,
                time_features=time_features_from_frequency_str(freq),
                pred_length=config.prediction_length,
            ),
            # step 5: add another temporal feature (just a single number)
            # tells the model where in its life the value of the time series is,
            # sort of a running counter
            AddAgeFeature(
                target_field=FieldName.TARGET,
                output_field=FieldName.FEAT_AGE,
                pred_length=config.prediction_length,
                log_scale=True,
            ),
            # step 6: vertically stack all the temporal features into the key FEAT_TIME
            VstackFeatures(
                output_field=FieldName.FEAT_TIME,
                input_fields=[FieldName.FEAT_TIME, FieldName.FEAT_AGE]
                + (
                    [FieldName.FEAT_DYNAMIC_REAL]
                    if config.num_dynamic_real_features > 0
                    else []
                ),
            ),
            # step 7: rename to match HuggingFace names
            RenameFields(
                mapping={
                    FieldName.FEAT_STATIC_CAT: "static_categorical_features",
                    FieldName.FEAT_STATIC_REAL: "static_real_features",
                    FieldName.FEAT_TIME: "time_features",
                    FieldName.TARGET: "values",
                    FieldName.OBSERVED_VALUES: "observed_mask",
                }
            ),
        ]
    )
```

## å®šä¹‰ `InstanceSplitter`

å¯¹äºè®­ç»ƒã€éªŒè¯ã€æµ‹è¯•æ­¥éª¤ï¼Œæ¥ä¸‹æ¥æˆ‘ä»¬åˆ›å»ºä¸€ä¸ª `InstanceSplitter`ï¼Œç”¨äºä»æ•°æ®é›†ä¸­å¯¹çª—å£è¿›è¡Œé‡‡æ · (å› ä¸ºç”±äºæ—¶é—´å’Œå†…å­˜é™åˆ¶ï¼Œæˆ‘ä»¬æ— æ³•å°†æ•´ä¸ªå†å²å€¼ä¼ é€’ç»™ Transformer)ã€‚

å®ä¾‹æ‹†åˆ†å™¨ä»æ•°æ®ä¸­éšæœºé‡‡æ ·å¤§å°ä¸º `context_length` å’Œåç»­å¤§å°ä¸º `prediction_length` çš„çª—å£ï¼Œå¹¶å°† `past_` æˆ– `future_` é”®é™„åŠ åˆ°å„ä¸ªçª—å£çš„ä»»ä½•ä¸´æ—¶é”®ã€‚è¿™ç¡®ä¿äº† `values` è¢«æ‹†åˆ†ä¸º `past_values` å’Œåç»­çš„ `future_values` é”®ï¼Œå®ƒä»¬å°†åˆ†åˆ«ç”¨ä½œç¼–ç å™¨å’Œè§£ç å™¨çš„è¾“å…¥ã€‚åŒæ ·æˆ‘ä»¬è¿˜éœ€è¦ä¿®æ”¹ `time_series_fields` å‚æ•°ä¸­çš„æ‰€æœ‰é”®:

```python
from gluonts.transform.sampler import InstanceSampler
from typing import Optional

def create_instance_splitter(
    config: PretrainedConfig,
    mode: str,
    train_sampler: Optional[InstanceSampler] = None,
    validation_sampler: Optional[InstanceSampler] = None,
) -> Transformation:
    assert mode in ["train", "validation", "test"]

    instance_sampler = {
        "train": train_sampler
        or ExpectedNumInstanceSampler(
            num_instances=1.0, min_future=config.prediction_length
        ),
        "validation": validation_sampler
        or ValidationSplitSampler(min_future=config.prediction_length),
        "test": TestSplitSampler(),
    }[mode]

    return InstanceSplitter(
        target_field="values",
        is_pad_field=FieldName.IS_PAD,
        start_field=FieldName.START,
        forecast_start_field=FieldName.FORECAST_START,
        instance_sampler=instance_sampler,
        past_length=config.context_length + max(config.lags_sequence),
        future_length=config.prediction_length,
        time_series_fields=["time_features", "observed_mask"],
    )
```

## åˆ›å»º PyTorch æ•°æ®åŠ è½½å™¨

æœ‰äº†æ•°æ®ï¼Œä¸‹ä¸€æ­¥éœ€è¦åˆ›å»º PyTorch DataLoadersã€‚å®ƒå…è®¸æˆ‘ä»¬æ‰¹é‡å¤„ç†æˆå¯¹çš„ (è¾“å…¥, è¾“å‡º) æ•°æ®ï¼Œå³ (`past_values`, `future_values`)ã€‚


```python
from typing import Iterable

import torch
from gluonts.itertools import Cached, Cyclic
from gluonts.dataset.loader import as_stacked_batches


def create_train_dataloader(
    config: PretrainedConfig,
    freq,
    data,
    batch_size: int,
    num_batches_per_epoch: int,
    shuffle_buffer_length: Optional[int] = None,
    cache_data: bool = True,
    **kwargs,
) -> Iterable:
    PREDICTION_INPUT_NAMES = [
        "past_time_features",
        "past_values",
        "past_observed_mask",
        "future_time_features",
    ]
    if config.num_static_categorical_features > 0:
        PREDICTION_INPUT_NAMES.append("static_categorical_features")

    if config.num_static_real_features > 0:
        PREDICTION_INPUT_NAMES.append("static_real_features")

    TRAINING_INPUT_NAMES = PREDICTION_INPUT_NAMES + [
        "future_values",
        "future_observed_mask",
    ]

    transformation = create_transformation(freq, config)
    transformed_data = transformation.apply(data, is_train=True)
    if cache_data:
        transformed_data = Cached(transformed_data)

    # we initialize a Training instance
    instance_splitter = create_instance_splitter(config, "train")

    # the instance splitter will sample a window of
    # context length + lags + prediction length (from the 366 possible transformed time series)
    # randomly from within the target time series and return an iterator.
    stream = Cyclic(transformed_data).stream()
    training_instances = instance_splitter.apply(
        stream, is_train=True
    )
    
    return as_stacked_batches(
        training_instances,
        batch_size=batch_size,
        shuffle_buffer_length=shuffle_buffer_length,
        field_names=TRAINING_INPUT_NAMES,
        output_type=torch.tensor,
        num_batches_per_epoch=num_batches_per_epoch,
    )
```


```python
def create_test_dataloader(
    config: PretrainedConfig,
    freq,
    data,
    batch_size: int,
    **kwargs,
):
    PREDICTION_INPUT_NAMES = [
        "past_time_features",
        "past_values",
        "past_observed_mask",
        "future_time_features",
    ]
    if config.num_static_categorical_features > 0:
        PREDICTION_INPUT_NAMES.append("static_categorical_features")

    if config.num_static_real_features > 0:
        PREDICTION_INPUT_NAMES.append("static_real_features")

    transformation = create_transformation(freq, config)
    transformed_data = transformation.apply(data, is_train=False)

    # we create a Test Instance splitter which will sample the very last
    # context window seen during training only for the encoder.
    instance_sampler = create_instance_splitter(config, "test")

    # we apply the transformations in test mode
    testing_instances = instance_sampler.apply(transformed_data, is_train=False)
    
    return as_stacked_batches(
        testing_instances,
        batch_size=batch_size,
        output_type=torch.tensor,
        field_names=PREDICTION_INPUT_NAMES,
    )
```


```python
train_dataloader = create_train_dataloader(
    config=config,
    freq=freq,
    data=train_dataset,
    batch_size=256,
    num_batches_per_epoch=100,
)

test_dataloader = create_test_dataloader(
    config=config,
    freq=freq,
    data=test_dataset,
    batch_size=64,
)
```

è®©æˆ‘ä»¬æ£€æŸ¥ç¬¬ä¸€æ‰¹:


```python
batch = next(iter(train_dataloader))
for k, v in batch.items():
    print(k, v.shape, v.type())

>>> past_time_features torch.Size([256, 85, 2]) torch.FloatTensor
    past_values torch.Size([256, 85]) torch.FloatTensor
    past_observed_mask torch.Size([256, 85]) torch.FloatTensor
    future_time_features torch.Size([256, 24, 2]) torch.FloatTensor
    static_categorical_features torch.Size([256, 1]) torch.LongTensor
    future_values torch.Size([256, 24]) torch.FloatTensor
    future_observed_mask torch.Size([256, 24]) torch.FloatTensor
```

å¯ä»¥çœ‹å‡ºï¼Œæˆ‘ä»¬æ²¡æœ‰å°† `input_ids` å’Œ `attention_mask` æä¾›ç»™ç¼–ç å™¨ (è®­ç»ƒ NLP æ¨¡å‹æ—¶ä¹Ÿæ˜¯è¿™ç§æƒ…å†µ)ï¼Œè€Œæ˜¯æä¾› `past_values`ï¼Œä»¥åŠ `past_observed_mask`ã€`past_time_features`ã€`static_categorical_features` å’Œ `static_real_features` å‡ é¡¹æ•°æ®ã€‚

è§£ç å™¨çš„è¾“å…¥åŒ…æ‹¬ `future_values`ã€`future_observed_mask` å’Œ `future_time_features`ã€‚`future_values` å¯ä»¥çœ‹ä½œç­‰åŒäº NLP è®­ç»ƒä¸­çš„ `decoder_input_ids`ã€‚

æˆ‘ä»¬å¯ä»¥å‚è€ƒ [Time Series Transformer æ–‡æ¡£](https://huggingface.co/docs/transformers/model_doc/time_series_transformer#transformers.TimeSeriesTransformerForPrediction.forward.past_values) ä»¥è·å¾—å¯¹å®ƒä»¬ä¸­æ¯ä¸€ä¸ªçš„è¯¦ç»†è§£é‡Šã€‚

## å‰å‘ä¼ æ’­

è®©æˆ‘ä»¬å¯¹åˆšåˆšåˆ›å»ºçš„æ‰¹æ¬¡æ‰§è¡Œä¸€æ¬¡å‰å‘ä¼ æ’­:

```python
# perform forward pass
outputs = model(
    past_values=batch["past_values"],
    past_time_features=batch["past_time_features"],
    past_observed_mask=batch["past_observed_mask"],
    static_categorical_features=batch["static_categorical_features"]
    if config.num_static_categorical_features > 0
    else None,
    static_real_features=batch["static_real_features"]
    if config.num_static_real_features > 0
    else None,
    future_values=batch["future_values"],
    future_time_features=batch["future_time_features"],
    future_observed_mask=batch["future_observed_mask"],
    output_hidden_states=True,
)
```

```python
print("Loss:", outputs.loss.item())

>>> Loss: 9.069628715515137
```
ç›®å‰ï¼Œè¯¥æ¨¡å‹è¿”å›äº†æŸå¤±å€¼ã€‚è¿™æ˜¯ç”±äºè§£ç å™¨ä¼šè‡ªåŠ¨å°† `future_values` å‘å³ç§»åŠ¨ä¸€ä¸ªä½ç½®ä»¥è·å¾—æ ‡ç­¾ã€‚è¿™å…è®¸è®¡ç®—é¢„æµ‹ç»“æœå’Œæ ‡ç­¾å€¼ä¹‹é—´çš„è¯¯å·®ã€‚

å¦è¯·æ³¨æ„ï¼Œè§£ç å™¨ä½¿ç”¨ Causal Mask æ¥é¿å…é¢„æµ‹æœªæ¥ï¼Œå› ä¸ºå®ƒéœ€è¦é¢„æµ‹çš„å€¼åœ¨ `future_values` å¼ é‡ä¸­ã€‚

## è®­ç»ƒæ¨¡å‹

æ˜¯æ—¶å€™è®­ç»ƒæ¨¡å‹äº†ï¼æˆ‘ä»¬å°†ä½¿ç”¨æ ‡å‡†çš„ PyTorch è®­ç»ƒå¾ªç¯ã€‚

è¿™é‡Œæˆ‘ä»¬ç”¨åˆ°äº† ğŸ¤— [Accelerate](https://huggingface.co/docs/accelerate/index) åº“ï¼Œå®ƒä¼šè‡ªåŠ¨å°†æ¨¡å‹ã€ä¼˜åŒ–å™¨å’Œæ•°æ®åŠ è½½å™¨æ”¾ç½®åœ¨é€‚å½“çš„ `device` ä¸Šã€‚


```python
from accelerate import Accelerator
from torch.optim import AdamW

accelerator = Accelerator()
device = accelerator.device

model.to(device)
optimizer = AdamW(model.parameters(), lr=6e-4, betas=(0.9, 0.95), weight_decay=1e-1)

model, optimizer, train_dataloader = accelerator.prepare(
    model,
    optimizer,
    train_dataloader,
)

model.train()
for epoch in range(40):
    for idx, batch in enumerate(train_dataloader):
        optimizer.zero_grad()
        outputs = model(
            static_categorical_features=batch["static_categorical_features"].to(device)
            if config.num_static_categorical_features > 0
            else None,
            static_real_features=batch["static_real_features"].to(device)
            if config.num_static_real_features > 0
            else None,
            past_time_features=batch["past_time_features"].to(device),
            past_values=batch["past_values"].to(device),
            future_time_features=batch["future_time_features"].to(device),
            future_values=batch["future_values"].to(device),
            past_observed_mask=batch["past_observed_mask"].to(device),
            future_observed_mask=batch["future_observed_mask"].to(device),
        )
        loss = outputs.loss

        # Backpropagation
        accelerator.backward(loss)
        optimizer.step()

        if idx % 100 == 0:
            print(loss.item())
```


## æ¨ç†

åœ¨æ¨ç†æ—¶ï¼Œå»ºè®®ä½¿ç”¨ `generate()` æ–¹æ³•è¿›è¡Œè‡ªå›å½’ç”Ÿæˆï¼Œç±»ä¼¼äº NLP æ¨¡å‹ã€‚

é¢„æµ‹çš„è¿‡ç¨‹ä¼šä»æµ‹è¯•å®ä¾‹é‡‡æ ·å™¨ä¸­è·å¾—æ•°æ®ã€‚é‡‡æ ·å™¨ä¼šå°†æ•°æ®é›†çš„æ¯ä¸ªæ—¶é—´åºåˆ—çš„æœ€å `context_length` é‚£ä¹ˆé•¿æ—¶é—´çš„æ•°æ®é‡‡æ ·å‡ºæ¥ï¼Œç„¶åè¾“å…¥æ¨¡å‹ã€‚è¯·æ³¨æ„ï¼Œè¿™é‡Œéœ€è¦æŠŠæå‰å·²çŸ¥çš„ `future_time_features` ä¼ é€’ç»™è§£ç å™¨ã€‚

è¯¥æ¨¡å‹å°†ä»é¢„æµ‹åˆ†å¸ƒä¸­è‡ªå›å½’é‡‡æ ·ä¸€å®šæ•°é‡çš„å€¼ï¼Œå¹¶å°†å®ƒä»¬ä¼ å›è§£ç å™¨æœ€ç»ˆå¾—åˆ°é¢„æµ‹è¾“å‡º:

```python
model.eval()

forecasts = []

for batch in test_dataloader:
    outputs = model.generate(
        static_categorical_features=batch["static_categorical_features"].to(device)
        if config.num_static_categorical_features > 0
        else None,
        static_real_features=batch["static_real_features"].to(device)
        if config.num_static_real_features > 0
        else None,
        past_time_features=batch["past_time_features"].to(device),
        past_values=batch["past_values"].to(device),
        future_time_features=batch["future_time_features"].to(device),
        past_observed_mask=batch["past_observed_mask"].to(device),
    )
    forecasts.append(outputs.sequences.cpu().numpy())
```

è¯¥æ¨¡å‹è¾“å‡ºä¸€ä¸ªè¡¨ç¤ºç»“æ„çš„å¼ é‡ (`batch_size`, `number of samples`, `prediction length`)ã€‚

ä¸‹é¢çš„è¾“å‡ºè¯´æ˜: å¯¹äºå¤§å°ä¸º `64` çš„æ‰¹æ¬¡ä¸­çš„æ¯ä¸ªç¤ºä¾‹ï¼Œæˆ‘ä»¬å°†è·å¾—æ¥ä¸‹æ¥ `24` ä¸ªæœˆå†…çš„ `100` ä¸ªå¯èƒ½çš„å€¼:


```python
forecasts[0].shape

>>> (64, 100, 24)
```

æˆ‘ä»¬å°†å‚ç›´å †å å®ƒä»¬ï¼Œä»¥è·å¾—æµ‹è¯•æ•°æ®é›†ä¸­æ‰€æœ‰æ—¶é—´åºåˆ—çš„é¢„æµ‹:

```python
forecasts = np.vstack(forecasts)
print(forecasts.shape)

>>> (366, 100, 24)
```

æˆ‘ä»¬å¯ä»¥æ ¹æ®æµ‹è¯•é›†ä¸­å­˜åœ¨çš„æ ·æœ¬å€¼ï¼Œæ ¹æ®çœŸå®æƒ…å†µè¯„ä¼°ç”Ÿæˆçš„é¢„æµ‹ã€‚è¿™é‡Œæˆ‘ä»¬ä½¿ç”¨æ•°æ®é›†ä¸­çš„æ¯ä¸ªæ—¶é—´åºåˆ—çš„ [MASE](https://huggingface.co/spaces/evaluate-metric/mase) å’Œ [sMAPE](https://huggingface.co/spaces/evaluate-metric/smape) æŒ‡æ ‡ (metrics) æ¥è¯„ä¼°:

```python
from evaluate import load
from gluonts.time_feature import get_seasonality

mase_metric = load("evaluate-metric/mase")
smape_metric = load("evaluate-metric/smape")

forecast_median = np.median(forecasts, 1)

mase_metrics = []
smape_metrics = []
for item_id, ts in enumerate(test_dataset):
    training_data = ts["target"][:-prediction_length]
    ground_truth = ts["target"][-prediction_length:]
    mase = mase_metric.compute(
        predictions=forecast_median[item_id], 
        references=np.array(ground_truth), 
        training=np.array(training_data), 
        periodicity=get_seasonality(freq))
    mase_metrics.append(mase["mase"])
    
    smape = smape_metric.compute(
        predictions=forecast_median[item_id], 
        references=np.array(ground_truth), 
    )
    smape_metrics.append(smape["smape"])
```


```python
print(f"MASE: {np.mean(mase_metrics)}")

>>> MASE: 1.2564196892177717

print(f"sMAPE: {np.mean(smape_metrics)}")

>>> sMAPE: 0.1609541520852549
```

æˆ‘ä»¬è¿˜å¯ä»¥å•ç‹¬ç»˜åˆ¶æ•°æ®é›†ä¸­æ¯ä¸ªæ—¶é—´åºåˆ—çš„ç»“æœæŒ‡æ ‡ï¼Œå¹¶è§‚å¯Ÿåˆ°å…¶ä¸­å°‘æ•°æ—¶é—´åºåˆ—å¯¹æœ€ç»ˆæµ‹è¯•æŒ‡æ ‡çš„å½±å“å¾ˆå¤§:

```python
plt.scatter(mase_metrics, smape_metrics, alpha=0.3)
plt.xlabel("MASE")
plt.ylabel("sMAPE")
plt.show()
```

![png](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/time-series-transformers/output_scatter.png)

ä¸ºäº†æ ¹æ®åŸºæœ¬äº‹å®æµ‹è¯•æ•°æ®ç»˜åˆ¶ä»»ä½•æ—¶é—´åºåˆ—çš„é¢„æµ‹ï¼Œæˆ‘ä»¬å®šä¹‰äº†ä»¥ä¸‹è¾…åŠ©ç»˜å›¾å‡½æ•°:

```python
import matplotlib.dates as mdates

def plot(ts_index):
    fig, ax = plt.subplots()

    index = pd.period_range(
        start=test_dataset[ts_index][FieldName.START],
        periods=len(test_dataset[ts_index][FieldName.TARGET]),
        freq=freq,
    ).to_timestamp()

    # Major ticks every half year, minor ticks every month,
    ax.xaxis.set_major_locator(mdates.MonthLocator(bymonth=(1, 7)))
    ax.xaxis.set_minor_locator(mdates.MonthLocator())

    ax.plot(
        index[-2*prediction_length:], 
        test_dataset[ts_index]["target"][-2*prediction_length:],
        label="actual",
    )

    plt.plot(
        index[-prediction_length:], 
        np.median(forecasts[ts_index], axis=0),
        label="median",
    )
    
    plt.fill_between(
        index[-prediction_length:],
        forecasts[ts_index].mean(0) - forecasts[ts_index].std(axis=0), 
        forecasts[ts_index].mean(0) + forecasts[ts_index].std(axis=0), 
        alpha=0.3, 
        interpolate=True,
        label="+/- 1-std",
    )
    plt.legend()
    plt.show()
```

ä¾‹å¦‚:

```python
plot(334)
```

![png](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/time-series-transformers/output_65_1.png)
    
æˆ‘ä»¬å¦‚ä½•ä¸å…¶ä»–æ¨¡å‹è¿›è¡Œæ¯”è¾ƒï¼Ÿ[Monash Time Series Repository](https://forecastingdata.org/#results) æœ‰ä¸€ä¸ªæµ‹è¯•é›† MASE æŒ‡æ ‡çš„æ¯”è¾ƒè¡¨ã€‚æˆ‘ä»¬å¯ä»¥å°†è‡ªå·±çš„ç»“æœæ·»åŠ åˆ°å…¶ä¸­ä½œæ¯”è¾ƒ:

|Dataset | 	SES| 	Theta | 	TBATS| 	ETS	| (DHR-)ARIMA| 	PR|	CatBoost |	FFNN	| DeepAR | 	N-BEATS | 	WaveNet| 	**Transformer** (Our) |
|:------------------:|:-----------------:|:--:|:--:|:--:|:--:|:--:|:--:|:---:|:---:|:--:|:--:|:--:|
|Tourism Monthly | 	3.306 |	1.649 |	1.751 |	1.526|	1.589|	1.678	|1.699|	1.582	| 1.409	| 1.574|	1.482	|  **1.256**|

è¯·æ³¨æ„ï¼Œæˆ‘ä»¬çš„æ¨¡å‹å‡»è´¥äº†æ‰€æœ‰å·²çŸ¥çš„å…¶ä»–æ¨¡å‹ (å¦è¯·å‚è§ç›¸åº” [è®ºæ–‡](https://openreview.net/pdf?id=wEc1mgAjU-) ä¸­çš„è¡¨ 2) ï¼Œå¹¶ä¸”æˆ‘ä»¬æ²¡æœ‰åšä»»ä½•è¶…å‚æ•°ä¼˜åŒ–ã€‚æˆ‘ä»¬ä»…ä»…èŠ±äº† 40 ä¸ªå®Œæ•´è®­ç»ƒè°ƒå‚å‘¨æœŸæ¥è®­ç»ƒ Transformerã€‚

å½“ç„¶ï¼Œæˆ‘ä»¬åº”è¯¥è°¦è™šã€‚ä»å†å²å‘å±•çš„è§’åº¦æ¥çœ‹ï¼Œç°åœ¨è®¤ä¸ºç¥ç»ç½‘ç»œè§£å†³æ—¶é—´åºåˆ—é¢„æµ‹é—®é¢˜æ˜¯æ­£é€”ï¼Œå°±å¥½æ¯”å½“å¹´çš„è®ºæ–‡å¾—å‡ºäº† â€œ[ä½ éœ€è¦çš„å°±æ˜¯ XGBoost](https://www.sciencedirect.com/science/article/pii/S0169207021001679)â€ çš„ç»“è®ºã€‚æˆ‘ä»¬åªæ˜¯å¾ˆå¥½å¥‡ï¼Œæƒ³çœ‹çœ‹ç¥ç»ç½‘ç»œèƒ½å¸¦æˆ‘ä»¬èµ°å¤šè¿œï¼Œä»¥åŠ Transformer æ˜¯å¦ä¼šåœ¨è¿™ä¸ªé¢†åŸŸå‘æŒ¥ä½œç”¨ã€‚è¿™ä¸ªç‰¹å®šçš„æ•°æ®é›†ä¼¼ä¹è¡¨æ˜å®ƒç»å¯¹å€¼å¾—æ¢ç´¢ã€‚

## ä¸‹ä¸€æ­¥

æˆ‘ä»¬é¼“åŠ±è¯»è€…å°è¯•æˆ‘ä»¬çš„ [Jupyter Notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/time-series-transformers.ipynb) å’Œæ¥è‡ª Hugging Face [Hub](https://huggingface.co/datasets/monash_tsf) çš„å…¶ä»–æ—¶é—´åºåˆ—æ•°æ®é›†ï¼Œå¹¶æ›¿æ¢é€‚å½“çš„é¢‘ç‡å’Œé¢„æµ‹é•¿åº¦å‚æ•°ã€‚å¯¹äºæ‚¨çš„æ•°æ®é›†ï¼Œéœ€è¦å°†å®ƒä»¬è½¬æ¢ä¸º GluonTS çš„æƒ¯ç”¨æ ¼å¼ï¼Œåœ¨ä»–ä»¬çš„ [æ–‡æ¡£](https://ts.gluon.ai/stable/tutorials/forecasting/extended_tutorial.html#What-is-in-a-dataset?) é‡Œæœ‰éå¸¸æ¸…æ™°çš„è¯´æ˜ã€‚æˆ‘ä»¬è¿˜å‡†å¤‡äº†ä¸€ä¸ªç¤ºä¾‹ Notebookï¼Œå‘æ‚¨å±•ç¤ºå¦‚ä½•å°†æ•°æ®é›†è½¬æ¢ä¸º [ğŸ¤— Hugging Face æ•°æ®é›†æ ¼å¼](https://github.com/huggingface/notebooks/blob/main/examples/time_series_datasets.ipynb)ã€‚

æ­£å¦‚æ—¶é—´åºåˆ—ç ”ç©¶äººå‘˜æ‰€çŸ¥ï¼Œäººä»¬å¯¹â€œå°†åŸºäº Transformer çš„æ¨¡å‹åº”ç”¨äºæ—¶é—´åºåˆ—â€é—®é¢˜å¾ˆæ„Ÿå…´è¶£ã€‚ä¼ ç»Ÿ vanilla Transformer åªæ˜¯ä¼—å¤šåŸºäºæ³¨æ„åŠ› (Attention) çš„æ¨¡å‹ä¹‹ä¸€ï¼Œå› æ­¤éœ€è¦å‘åº“ä¸­è¡¥å……æ›´å¤šæ¨¡å‹ã€‚

ç›®å‰æ²¡æœ‰ä»€ä¹ˆèƒ½å¦¨ç¢æˆ‘ä»¬ç»§ç»­æ¢ç´¢å¯¹å¤šå˜é‡æ—¶é—´åºåˆ— (multivariate time series) è¿›è¡Œå»ºæ¨¡ï¼Œä½†æ˜¯ä¸ºæ­¤éœ€è¦ä½¿ç”¨å¤šå˜é‡åˆ†å¸ƒå¤´ (multivariate distribution head) æ¥å®ä¾‹åŒ–æ¨¡å‹ã€‚ç›®å‰å·²ç»æ”¯æŒäº†å¯¹è§’ç‹¬ç«‹åˆ†å¸ƒ (diagonal independent distributions)ï¼Œåç»­ä¼šå¢åŠ å…¶ä»–å¤šå…ƒåˆ†å¸ƒæ”¯æŒã€‚è¯·ç»§ç»­å…³æ³¨æœªæ¥çš„åšå®¢æ–‡ç« ä»¥åŠå…¶ä¸­çš„æ•™ç¨‹ã€‚

è·¯çº¿å›¾ä¸Šçš„å¦ä¸€ä»¶äº‹æ˜¯æ—¶é—´åºåˆ—åˆ†ç±»ã€‚è¿™éœ€è¦å°†å¸¦æœ‰åˆ†ç±»å¤´çš„æ—¶é—´åºåˆ—æ¨¡å‹æ·»åŠ åˆ°åº“ä¸­ï¼Œä¾‹å¦‚ç”¨äºå¼‚å¸¸æ£€æµ‹è¿™ç±»ä»»åŠ¡ã€‚ 

å½“å‰çš„æ¨¡å‹ä¼šå‡è®¾æ—¥æœŸæ—¶é—´å’Œæ—¶é—´åºåˆ—å€¼éƒ½å­˜åœ¨ï¼Œä½†åœ¨ç°å®ä¸­è¿™å¯èƒ½ä¸èƒ½å®Œå…¨æ»¡è¶³ã€‚ä¾‹å¦‚ [WOODS](https://woods-benchmarks.github.io/) ç»™å‡ºçš„ç¥ç»ç§‘å­¦æ•°æ®é›†ã€‚å› æ­¤ï¼Œæˆ‘ä»¬è¿˜éœ€è¦å¯¹å½“å‰æ¨¡å‹è¿›è¡Œæ³›åŒ–ï¼Œä½¿æŸäº›è¾“å…¥åœ¨æ•´ä¸ªæµæ°´çº¿ä¸­å¯é€‰ã€‚

æœ€åï¼ŒNLP/CV é¢†åŸŸä» [å¤§å‹é¢„è®­ç»ƒæ¨¡å‹](https://arxiv.org/abs/1810.04805) ä¸­è·ç›ŠåŒªæµ…ï¼Œä½†æ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œæ—¶é—´åºåˆ—é¢†åŸŸå¹¶éå¦‚æ­¤ã€‚åŸºäº Transformer çš„æ¨¡å‹ä¼¼ä¹æ˜¯è¿™ä¸€ç ”ç©¶æ–¹å‘çš„å¿…ç„¶ä¹‹é€‰ï¼Œæˆ‘ä»¬è¿«ä¸åŠå¾…åœ°æƒ³çœ‹çœ‹ç ”ç©¶äººå‘˜å’Œä»ä¸šè€…ä¼šå‘ç°å“ªäº›çªç ´ï¼

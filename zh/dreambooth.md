---
title: ä½¿ç”¨ Diffusers é€šè¿‡ Dreambooth æŠ€æœ¯æ¥è®­ç»ƒ Stable Diffusion
thumbnail: /blog/assets/sd_dreambooth_training/thumbnail.jpg
authors:
- user: valhalla
- user: pcuenq
- user: 9of9
- user: youyuanrsq
  guest: true
translators:
- user: innovation64
- user: inferjay
  proofreader: true
---

# ä½¿ç”¨ Diffusers é€šè¿‡ Dreambooth æŠ€æœ¯æ¥è®­ç»ƒ Stable Diffusion


[Dreambooth](https://dreambooth.github.io/) æ˜¯ä¸€ç§ä½¿ç”¨ç‰¹æ®Šçš„å¾®è°ƒæ–¹å¼æ¥æ•™ä¼š [Stable Diffusion](https://huggingface.co/blog/stable_diffusion) æ–°æ¦‚å¿µçš„æŠ€æœ¯ã€‚åˆ©ç”¨è¿™ä¸ªæŠ€æœ¯ï¼Œæœ‰çš„äººä»…ä»…ç”¨ä»–ä»¬è‡ªå·±å¾ˆå°‘çš„ç…§ç‰‡å°±å°†è‡ªå·±ç½®èº«äºŽå¥‡å¦™çš„å¢ƒç•Œä¹‹ä¸­ï¼Œè€Œæœ‰äº›äººåˆ™ç»“åˆå®ƒç”Ÿæˆæ–°çš„é£Žæ ¼ã€‚ðŸ§¨Diffusersæä¾›ä¸€ä¸ª [DreamBooth è®­ç»ƒè„šæœ¬](https://github.com/huggingface/diffusers/tree/main/examples/DreamBooth)ã€‚ä½¿ç”¨è¿™ä¸ªè„šæœ¬è®­ç»ƒä¸ä¼šèŠ±è´¹å¾ˆé•¿çš„æ—¶é—´ï¼Œä½†æ˜¯æ¯”è¾ƒéš¾ç­›é€‰æ­£ç¡®çš„è¶…å‚æ•°ï¼Œå¹¶ä¸”å®¹æ˜“è¿‡æ‹Ÿåˆã€‚

> [Dreambooth](https://dreambooth.github.io/) is a technique to teach new concepts to [Stable Diffusion](https://huggingface.co/blog/stable_diffusion) using a specialized form of fine-tuning. Some people have been using it with a few of their photos to place themselves in fantastic situations, while others are using it to incorporate new styles. [ðŸ§¨ Diffusers](https://github.com/huggingface/diffusers) provides a Dreambooth [training script](https://github.com/huggingface/diffusers/tree/main/examples/dreambooth). It doesn't take long to train, but it's hard to select the right set of hyperparameters and it's easy to overfit.

æˆ‘ä»¬åšäº†è®¸å¤šå®žéªŒæ¥åˆ†æžä¸åŒå‚æ•°é…ç½®ä¸‹`DreamBooth`çš„æ•ˆæžœã€‚æœ¬æ–‡å±•ç¤ºäº†æˆ‘ä»¬çš„å‘çŽ°å’Œä¸€äº›å°æŠ€å·§æ¥å¸®åŠ©ä½ åœ¨ç”¨ `DreamBooth`å¾®è°ƒ`Stable Diffusion`çš„æ—¶å€™æå‡ï¼ˆç”Ÿæˆå›¾ç‰‡çš„ï¼‰æ•ˆæžœã€‚

> We conducted a lot of experiments to analyze the effect of different settings in Dreambooth. This post presents our findings and some tips to improve your results when fine-tuning Stable Diffusion with Dreambooth.

åœ¨å¼€å§‹ä¹‹å‰ï¼Œè¯·æ³¨æ„è¯¥æ–¹æ³•ç¦æ­¢åº”ç”¨åœ¨æ¶æ„è¡Œä¸ºä¸Šï¼Œæ¥ç”Ÿæˆä¸€äº›æœ‰å®³çš„ä¸œè¥¿ï¼Œæˆ–è€…åœ¨æ²¡æœ‰ç›¸å…³èƒŒæ™¯ä¸‹å†’å……æŸäººã€‚è¯¥æ¨¡åž‹çš„è®­ç»ƒå‚ç…§ [CreativeML Open RAIL-M è®¸å¯](https://huggingface.co/spaces/CompVis/stable-diffusion-license)ã€‚

> Before we start, please be aware that this method should never be used for malicious purposes, to generate harm in any way, or to impersonate people without their knowledge. Models trained with it are still bound by the [CreativeML Open RAIL-M license](https://huggingface.co/spaces/CompVis/stable-diffusion-license) that governs distribution of Stable Diffusion models.

æ³¨æ„ï¼šè¯¥å¸–å­çš„å…ˆå‰ç‰ˆæœ¬å·²å‡ºç‰ˆä¸º [Wï¼†B æŠ¥å‘Š](https://wandb.ai/psuraj/dreambooth/reports/Dreambooth-Training-Analysis--VmlldzoyNzk0NDc3)

> _Note: a previous version of this post was published [as a W&B report](https://wandb.ai/psuraj/dreambooth/reports/Dreambooth-Training-Analysis--VmlldzoyNzk0NDc3)_.

## TL;DR: æŽ¨èçš„è®¾ç½®ï¼ˆTL;DR: Recommended Settingsï¼‰

* `DreamBooth`å¾ˆå®¹æ˜“å¿«é€Ÿè¿‡æ‹Ÿåˆï¼Œä¸ºäº†èŽ·å–é«˜è´¨é‡å›¾ç‰‡ï¼Œæˆ‘ä»¬å¿…é¡»åœ¨è®­ç»ƒæ­¥éª¤ï¼ˆstepsï¼‰å’Œå­¦ä¹ çŽ‡ä¹‹é—´æ‰¾åˆ°ä¸€ä¸ª "sweet spot"ã€‚æˆ‘ä»¬æŽ¨èä½¿ç”¨è¾ƒå°çš„å­¦ä¹ çŽ‡ä»¥åŠé€æ­¥å¢žåŠ æ­¥æ•°ç›´åˆ°å¾—åˆ°æ¯”è¾ƒæ»¡æ„çš„ç»“æžœçš„ç­–ç•¥ï¼›
* > Dreambooth tends to overfit quickly. To get good-quality images, we must find a 'sweet spot' between the number of training steps and the learning rate. We recommend using a low learning rate and progressively increasing the number of steps until the results are satisfactory.
* å¯¹äºŽè„¸éƒ¨å›¾åƒè€Œè¨€`DreamBooth`éœ€è¦æ›´å¤šçš„è®­ç»ƒæ­¥æ•°ï¼ˆstepsï¼‰ã€‚åœ¨æˆ‘ä»¬çš„å®žéªŒä¸­ï¼Œå½“batch sizeè®¾ç½®ä¸º2ï¼Œå­¦ä¹ çŽ‡è®¾ç½®ä¸º`1e-6`æ—¶ï¼Œå°†stepsè®¾ç½®ä¸º800-1200æ­¥æ•ˆæžœä¸é”™ï¼›
* > Dreambooth needs more training steps for faces. In our experiments, 800-1200 steps worked well when using a batch size of 2 and LR of 1e-6.
* å½“é’ˆå¯¹è„¸éƒ¨å›¾åƒï¼ˆç”Ÿæˆï¼‰è¿›è¡Œè®­ç»ƒæ—¶ï¼Œäº‹å…ˆä¿å­˜ï¼ˆprior perservationï¼‰å¯¹äºŽé¿å…è¿‡æ‹Ÿåˆéžå¸¸é‡è¦ï¼Œä½†å¯¹äºŽå…¶ä»–ä¸»é¢˜å¯èƒ½å½±å“å°±æ²¡é‚£ä¹ˆå¤§äº†ï¼›
* > Prior preservation is important to avoid overfitting when training on faces. For other subjects, it doesn't seem to make a huge difference.
* å¦‚æžœä½ çœ‹åˆ°ç”Ÿæˆçš„å›¾ç‰‡å™ªå£°å¾ˆå¤§ä¸”è´¨é‡å¾ˆä½Žã€‚è¿™é€šå¸¸æ„å‘³ç€è¿‡æ‹Ÿåˆäº†ã€‚é¦–å…ˆï¼Œå…ˆå°è¯•ä¸Šè¿°æ­¥éª¤åŽ»é¿å…ä»–ï¼Œå¦‚æžœç”Ÿæˆçš„å›¾ç‰‡ä¾æ—§å……æ»¡å™ªå£°ã€‚ä½¿ç”¨`DDIM`è°ƒåº¦å™¨ï¼ˆschedulerï¼‰æˆ–è€…è¿­ä»£æ›´å¤šæŽ¨ç†æ­¥éª¤ï¼ˆstepsï¼‰ï¼ˆå¯¹äºŽæˆ‘ä»¬çš„å®žéªŒå¤§æ¦‚ 100 å·¦å³å°±å¾ˆå¥½äº†ï¼‰ï¼›
* > If you see that the generated images are noisy or the quality is degraded, it likely means overfitting. First, try the steps above to avoid it. If the generated images are still noisy, use the DDIM scheduler or run more inference steps (~100 worked well in our experiments).
* è®­ç»ƒæ–‡æœ¬ç¼–ç å™¨è€Œä¸æ˜¯ï¼ˆè®­ç»ƒï¼‰`Unet`å¯¹ï¼ˆå›¾åƒçš„ï¼‰è´¨é‡æœ‰å¾ˆå¤§å½±å“ã€‚æˆ‘ä»¬æœ€å¥½çš„ç»“æžœæ˜¯é€šè¿‡ä½¿ç”¨æ–‡æœ¬ç¼–ç å™¨å¾®è°ƒã€è¾ƒä½Žçš„å­¦ä¹ çŽ‡å’Œé€‚å½“çš„æ­¥æ•°çš„ç»„åˆæ¥èŽ·å¾—çš„ã€‚ä½†æ˜¯ï¼Œå¾®è°ƒæ–‡æœ¬ç¼–ç å™¨éœ€è¦æ›´å¤šçš„å†…å­˜ï¼Œå› æ­¤è‡³å°‘å…·æœ‰24GBå†…å­˜çš„GPUã€‚ä½¿ç”¨8ä½`Adam`ã€fp16è®­ç»ƒæˆ–æ¢¯åº¦ç´¯ç§¯ç­‰æŠ€æœ¯ï¼Œå¯ä»¥åœ¨åƒGoogle Colabæˆ–Kaggleæä¾›çš„16GB GPUä¸Šè¿›è¡Œè®­ç»ƒã€‚
* > Training the text encoder in addition to the UNet has a big impact on quality. Our best results were obtained using a combination of text encoder fine-tuning, low LR, and a suitable number of steps. However, fine-tuning the text encoder requires more memory, so a GPU with at least 24 GB of RAM is ideal. Using techniques like 8-bit Adam, `fp16` training or gradient accumulation, it is possible to train on 16 GB GPUs like the ones provided by Google Colab or Kaggle.
* `EMA`å¯¹äºŽå¾®è°ƒä¸é‡è¦ï¼›
* > Fine-tuning with or without EMA produced similar results.
* æ²¡æœ‰å¿…è¦ç”¨`sks`è¯æ±‡è®­ç»ƒ`DreamBooth`ã€‚æœ€æ—©çš„å®žçŽ°ä½¿ç”¨å®ƒæ˜¯å› ä¸ºè¿™ä¸ªtokenåœ¨è¯æ±‡ä¸­å¾ˆç½•è§ï¼Œä½†å®žé™…ä¸Šæ˜¯ä¸€ç§ rifleã€‚æˆ‘ä»¬çš„å®žéªŒæˆ–å…¶ä»–åƒ [@nitrosocke](https://huggingface.co/nitrosocke) çš„ä¾‹å­éƒ½è¡¨æ˜Žä½¿ç”¨è‡ªç„¶è¯­è¨€æè¿°ä½ çš„ç›®æ ‡å°±è¶³å¤Ÿäº†ã€‚
* > There's no need to use the `sks` word to train Dreambooth. One of the first implementations used it because it was a rare token in the vocabulary, but it's actually a kind of rifle. Our experiments, and those by for example [@nitrosocke](https://huggingface.co/nitrosocke) show that it's ok to select terms that you'd naturally use to describe your target.

## å­¦ä¹ çŽ‡çš„å½±å“ï¼ˆLearning Rate Impactï¼‰

`Dreambooth`å¾ˆå®¹æ˜“è¿‡æ‹Ÿåˆã€‚ä¸ºäº†èŽ·å¾—è‰¯å¥½çš„ç»“æžœï¼Œè¯·è°ƒæ•´å­¦ä¹ çŽ‡å’Œè®­ç»ƒæ­¥æ•°æ¥é€‚é…ä½ çš„æ•°æ®é›†ã€‚åœ¨æˆ‘ä»¬çš„å®žéªŒä¸­ï¼ˆè¯¦è§ä¸‹æ–‡ï¼‰ï¼Œæˆ‘ä»¬ä½¿ç”¨è¾ƒé«˜å’Œè¾ƒä½Žçš„å­¦ä¹ çŽ‡å¯¹å››ä¸ªä¸åŒçš„æ•°æ®é›†è¿›è¡Œäº†å¾®è°ƒã€‚åœ¨æ‰€æœ‰æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬éƒ½ä½¿ç”¨è¾ƒä½Žçš„å­¦ä¹ çŽ‡èŽ·å¾—äº†æ›´å¥½çš„ç»“æžœã€‚

> Dreambooth overfits very quickly. To get good results, tune the learning rate and the number of training steps in a way that makes sense for your dataset. In our experiments (detailed below), we fine-tuned on four different datasets with high and low learning rates. In all cases, we got better results with a low learning rate.

## å®žéªŒè®¾ç½®ï¼ˆExperiments Settingsï¼‰

æˆ‘ä»¬æ‰€æœ‰çš„å®žéªŒéƒ½æ˜¯åœ¨ä½¿ç”¨`AdamW` ä¼˜åŒ–å™¨çš„ [`train_deambooth.py` è„šæœ¬](https://github.com/huggingface/diffusers/tree/main/examples/DreamBooth)ä¸Šï¼Œä½¿ç”¨2ä¸ª40GBçš„A100ä¸Šè¿›è¡Œçš„ã€‚æˆ‘ä»¬åœ¨æ‰€æœ‰è¿è¡Œä¸­éƒ½ä½¿ç”¨ç›¸åŒçš„ç§å­å’Œç›¸åŒçš„è¶…å‚æ•°ï¼Œé™¤äº†å­¦ä¹ çŽ‡ã€è®­ç»ƒæ­¥æ•°å’Œæ˜¯å¦ä½¿ç”¨äº‹å…ˆä¿å­˜ï¼ˆprior preservationï¼‰ã€‚

> All our experiments were conducted using the [`train_dreambooth.py`](https://github.com/huggingface/diffusers/tree/main/examples/dreambooth) script with the `AdamW` optimizer on 2x 40GB A100s. We used the same seed and kept all hyperparameters equal across runs, except LR, number of training steps and the use of prior preservation.

å¯¹äºŽå‰ä¸‰ä¸ªç¤ºä¾‹ï¼ˆå„ç§ç‰©ä½“ï¼‰ï¼Œæˆ‘ä»¬è®¾ç½®batch sizeå¤§å°ä¸º4ï¼ˆæ¯ä¸ªGPUä¸º2ï¼‰å¹¶è¿›è¡Œäº†400æ­¥çš„å¾®è°ƒã€‚æˆ‘ä»¬ä½¿ç”¨äº†é«˜å­¦ä¹ çŽ‡`5e-6`å’Œä½Žå­¦ä¹ çŽ‡`2e-6`ã€‚æ²¡æœ‰ä½¿ç”¨äº‹å…ˆä¿å­˜ï¼ˆprior preservationï¼‰ã€‚

> For the first 3 examples (various objects), we fine-tuned the model with a batch size of 4 (2 per GPU) for 400 steps. We used a high learning rate of `5e-6` and a low learning rate of `2e-6`. No prior preservation was used.

æœ€åŽä¸€ä¸ªå®žéªŒå°è¯•æŠŠäººåŠ å…¥æ¨¡åž‹ï¼Œæˆ‘ä»¬ä½¿ç”¨äº‹å…ˆä¿å­˜ï¼ˆprior preservationï¼‰å¹¶å°†batch sizeè®¾ç½®ä¸º2 (æ¯ä¸ªGPUåˆ†1ä¸ª)ï¼Œè®­ç»ƒ800-1200æ­¥ã€‚æˆ‘ä»¬ä½¿ç”¨çš„é«˜å­¦ä¹ çŽ‡ä¸º`5e-6`ï¼Œä½Žå­¦ä¹ çŽ‡ä¸º`2e-6`ã€‚

> The last experiment attempts to add a human subject to the model. We used prior preservation with a batch size of 2 (1 per GPU), 800 and 1200 steps in this case. We used a high learning rate of `5e-6` and a low learning rate of `2e-6`.

ä½ å¯ä»¥ä½¿ç”¨8bit `Adam`ï¼Œ`fp16` ç²¾åº¦è®­ç»ƒæˆ–è€…æ¢¯åº¦ç´¯è®¡åŽ»å‡å°‘å†…å­˜çš„éœ€è¦ï¼Œå¹¶åœ¨ä¸€ä¸ª16Gæ˜¾å­˜çš„æœºå™¨ä¸Šè¿è¡Œç›¸åŒçš„å®žéªŒã€‚

> Note that you can use 8-bit Adam, `fp16` training or gradient accumulation to reduce memory requirements and run similar experiments on GPUs with 16 GB of memory.

### Toy çŒ«ï¼ˆCat Toyï¼‰

é«˜å­¦ä¹ çŽ‡ High Learning Rate(`5e-6`)

![Cat Toy, High Learning Rate](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/dreambooth-assets/1_cattoy_hlr.jpg)

ä½Žå­¦ä¹ çŽ‡ Low Learning Rate (`2e-6`)

![Cat Toy, Low Learning Rate](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/dreambooth-assets/2_cattoy_llr.jpg)

### çŒªå¤´ï¼ˆPigheadï¼‰

é«˜å­¦ä¹ çŽ‡ (`5e-6`) è¯·æ³¨æ„ï¼Œé¢œè‰²ä¼ªå½±æ˜¯å™ªå£°æ®‹ç•™ç‰©-è¿è¡Œæ›´å¤šçš„æŽ¨ç†æ­¥éª¤å¯ä»¥å¸®åŠ©è§£å†³å…¶ä¸­ä¸€äº›ç»†èŠ‚é—®é¢˜ã€‚

> High Learning Rate (`5e-6`). Note that the color artifacts are noise remnants â€“ running more inference steps could help resolve some of those details.

![Pighead, High Learning Rate](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/dreambooth-assets/3_pighead_hlr.jpg)

ä½Žå­¦ä¹ çŽ‡ Low Learning Rate(`2e-6`)

![Pighead, Low Learning Rate](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/dreambooth-assets/4_pighead_llr.jpg)

### åœŸè±†å…ˆç”Ÿçš„å¤´ï¼ˆMr. Potato Headï¼‰

é«˜å­¦ä¹ çŽ‡ (`5e-6`) è¯·æ³¨æ„ï¼Œé¢œè‰²ä¼ªåƒæ˜¯å™ªå£°æ®‹ä½™ç‰©-è¿è¡Œæ›´å¤šçš„æŽ¨ç†æ­¥éª¤å¯ä»¥å¸®åŠ©è§£å†³å…¶ä¸­ä¸€äº›ç»†èŠ‚é—®é¢˜ã€‚

> High Learning Rate (`5e-6`). Note that the color artifacts are noise remnants â€“ running more inference steps could help resolve some of those details.

![Potato Head, High Learning Rate](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/dreambooth-assets/5_potato_hlr.jpg)

ä½Žå­¦ä¹ çŽ‡ Low Learning Rate(`2e-6`)

![Potato Head, Low Learning Rate](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/dreambooth-assets/6_potato_llr.jpg)

### äººè„¸ï¼ˆHuman Faceï¼‰

æˆ‘ä»¬å°è¯•å°†Seinfeldä¸­çš„Kramerè§’è‰²èžå…¥åˆ°`Stable Diffusion`ä¸­ã€‚æ­£å¦‚ä¹‹å‰æåˆ°çš„ï¼Œæˆ‘ä»¬ä½¿ç”¨æ›´å°çš„batch sizeè®­ç»ƒäº†æ›´å¤šçš„æ­¥éª¤ã€‚å³ä¾¿å¦‚æ­¤ï¼Œç»“æžœä¹Ÿä¸æ˜¯ç‰¹åˆ«å‡ºè‰²ã€‚ä¸ºäº†ç®€æ´èµ·è§ï¼Œæˆ‘ä»¬çœç•¥äº†è¿™äº›ç¤ºä¾‹å›¾åƒï¼Œå¹¶å°†è¯»è€…æŽ¨èåˆ°æŽ¥ä¸‹æ¥çš„éƒ¨åˆ†ï¼Œå…¶ä¸­äººè„¸è®­ç»ƒå°†æˆä¸ºæˆ‘ä»¬åŠªåŠ›çš„é‡ç‚¹ã€‚

> We tried to incorporate the Kramer character from Seinfeld into Stable Diffusion. As previously mentioned, we trained for more steps with a smaller batch size. Even so, the results were not stellar. For the sake of brevity, we have omitted these sample images and defer the reader to the next sections, where face training became the focus of our efforts.

### åˆæ­¥ç»“æžœæ€»ç»“ï¼ˆSummary of Initial Resultsï¼‰

ä¸ºäº†ç”¨`DreamBooth`èŽ·å–æ›´å¥½çš„`Stable Diffusion`ç»“æžœï¼Œé’ˆå¯¹ä½ çš„æ•°æ®é›†è°ƒæ•´ä½ çš„å­¦ä¹ çŽ‡å’Œè®­ç»ƒæ­¥æ•°éžå¸¸é‡è¦ã€‚

> To get good results training Stable Diffusion with Dreambooth, it's important to tune the learning rate and training steps for your dataset.

* å­¦ä¹ çŽ‡è¿‡é«˜å’Œè¿‡å¤šçš„è®­ç»ƒæ­¥éª¤ä¼šå¯¼è‡´è¿‡æ‹Ÿåˆã€‚æ— è®ºä½¿ç”¨ä»€ä¹ˆæç¤ºï¼Œæ¨¡åž‹å¤§å¤šä¼šç”Ÿæˆä¸Žè®­ç»ƒæ•°æ®ç›¸ä¼¼çš„å›¾åƒã€‚
* > High learning rates and too many training steps will lead to overfitting. The model will mostly generate images from your training data, no matter what prompt is used.
* å­¦ä¹ çŽ‡è¿‡ä½Žå’Œæ­¥éª¤è¿‡å°‘ä¼šå¯¼è‡´æ¬ æ‹Ÿåˆï¼šæ¨¡åž‹æ— æ³•ç”Ÿæˆæˆ‘ä»¬å°è¯•èžåˆçš„æ¦‚å¿µã€‚
* > Low learning rates and too few steps will lead to underfitting: the model will not be able to generate the concept we were trying to incorporate.

é¢éƒ¨æ›´éš¾è®­ç»ƒã€‚åœ¨æˆ‘ä»¬çš„å®žéªŒä¸­ï¼Œå­¦ä¹ çŽ‡ä¸º`2e-6`ï¼Œ400ä¸ªè®­ç»ƒæ­¥éª¤å¯¹äºŽç‰©ä½“è¡¨çŽ°è‰¯å¥½ï¼Œä½†å¯¹äºŽé¢éƒ¨éœ€è¦`1e-6`ï¼ˆæˆ–`2e-6`ï¼‰å’Œçº¦1200æ­¥æ‰è¡Œã€‚

> Faces are harder to train. In our experiments, a learning rate of `2e-6` with `400` training steps works well for objects but faces required `1e-6` (or `2e-6`) with ~1200 steps.

å¦‚æžœæ¨¡åž‹è¿‡åº¦æ‹Ÿåˆï¼Œå›¾åƒè´¨é‡ä¼šä¸¥é‡é™ä½Žï¼Œè¿™ä¼šå‘ç”Ÿåœ¨ä»¥ä¸‹æƒ…å†µä¸‹:

* å­¦ä¹ çŽ‡è¿‡é«˜
* è®­ç»ƒæ­¥æ•°è¿‡å¤š
* å¯¹äºŽé¢éƒ¨è€Œè¨€ï¼Œå¦‚æžœä¸ä½¿ç”¨äº‹å…ˆä¿å­˜ï¼ˆprior preservationï¼‰ï¼Œå¦‚ä¸‹ä¸€èŠ‚æ‰€ç¤ºï¼Œä¹Ÿä¼šå‘ç”Ÿè¿‡æ‹Ÿåˆ

> Image quality degrades a lot if the model overfits, and this happens if:
> * The learning rate is too high.
> * We run too many training steps.
> * In the case of faces, when no prior preservation is used, as shown in the next section.

## åœ¨è®­ç»ƒäººè„¸æ—¶ä½¿ç”¨äº‹å…ˆä¿å­˜ï¼ˆUsing Prior Preservation when training Facesï¼‰

äº‹å…ˆä¿å­˜ï¼ˆprior preservationï¼‰æ˜¯ä¸€ç§æŠ€æœ¯ï¼Œå®ƒä½¿ç”¨åŒä¸€ç±»åˆ«çš„é¢å¤–å›¾åƒä½œä¸ºå¾®è°ƒè¿‡ç¨‹çš„ä¸€éƒ¨åˆ†ã€‚ä¾‹å¦‚ï¼Œå¦‚æžœæˆ‘ä»¬å°è¯•å°†ä¸€ä¸ªæ–°çš„äººç‰©èžå…¥åˆ°æ¨¡åž‹ä¸­ï¼Œæˆ‘ä»¬æƒ³è¦ä¿å­˜çš„*ç±»åˆ«*å¯èƒ½æ˜¯*äºº*ã€‚äº‹å…ˆä¿å­˜ï¼ˆprior preservationï¼‰é€šè¿‡ä½¿ç”¨æ–°äººç‰©çš„ç…§ç‰‡ä¸Žå…¶ä»–äººçš„ç…§ç‰‡ç›¸ç»“åˆæ¥å‡å°‘è¿‡æ‹Ÿåˆã€‚å¥½å¤„åœ¨äºŽï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨`Stable Diffusion`æ¨¡åž‹ç”Ÿæˆè¿™äº›é¢å¤–çš„ç±»åˆ«å›¾åƒï¼å¦‚æžœä½ æ„¿æ„ï¼Œè®­ç»ƒè„šæœ¬ä¼šè‡ªåŠ¨å¤„ç†è¿™äº›é¢å¤–çš„ç±»åˆ«å›¾åƒï¼Œä½†ä½ ä¹Ÿå¯ä»¥æä¾›ä¸€ä¸ªåŒ…å«è‡ªå·±å…ˆå‰ä¿å­˜å›¾åƒçš„æ–‡ä»¶å¤¹ã€‚

> Prior preservation is a technique that uses additional images of the same class we are trying to train as part of the fine-tuning process. For example, if we try to incorporate a new person into the model, the _class_ we'd want to preserve could be _person_. Prior preservation tries to reduce overfitting by using photos of the new person combined with photos of other people. The nice thing is that we can generate those additional class images using the Stable Diffusion model itself! The training script takes care of that automatically if you want, but you can also provide a folder with your own prior preservation images.

äº‹å…ˆä¿å­˜ï¼ˆprior preservationï¼‰ï¼Œ1200 æ­¥æ•°ï¼Œå­¦ä¹ çŽ‡ = `2e-6` ï¼ˆPrior preservation, 1200 steps, lr=`2e-6`ï¼‰

![Faces, prior preservation](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/dreambooth-assets/7_faces_with_prior.jpg)

æ— äº‹å…ˆä¿å­˜ï¼Œ1200 æ­¥æ•°ï¼Œå­¦ä¹ çŽ‡ = `2e-6`ï¼ˆNo prior preservation, 1200 steps, lr=`2e-6`ï¼‰

![Faces, prior preservation](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/dreambooth-assets/8_faces_no_prior.jpg)

å¦‚ä½ æ‰€è§ï¼Œå½“ä½¿ç”¨äº‹å…ˆä¿å­˜ï¼ˆprior preservationï¼‰æ—¶ï¼Œç»“æžœä¼šæ›´å¥½ï¼Œä½†æ˜¯ä»ç„¶æœ‰å˜ˆæ‚çš„æ–‘ç‚¹ã€‚æ˜¯æ—¶å€™ä½¿ç”¨ä¸€äº›å…¶ä»–æŠ€å·§äº†ã€‚

> As you can see, results are better when prior preservation is used, but there are still noisy blotches. It's time for some additional tricks!

## è°ƒåº¦å™¨çš„å½±å“ï¼ˆEffect of Schedulersï¼‰

åœ¨ä¹‹å‰çš„ç¤ºä¾‹ä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨`PNDM`è°ƒåº¦å™¨åœ¨æŽ¨ç†è¿‡ç¨‹ä¸­å¯¹å›¾åƒè¿›è¡Œé‡‡æ ·ã€‚æˆ‘ä»¬è§‚å¯Ÿåˆ°ï¼Œå½“æ¨¡åž‹å‡ºçŽ°è¿‡æ‹Ÿåˆæ—¶ï¼Œ`DDIM`é€šå¸¸æ¯”`PNDM`å’Œ`LMSDiscrete`è¡¨çŽ°æ›´å¥½ã€‚æ­¤å¤–ï¼Œå¯ä»¥é€šè¿‡è¿è¡Œæ›´å¤šæ­¥éª¤æ¥æé«˜è´¨é‡ï¼š100æ­¥ä¼¼ä¹Žæ˜¯ä¸€ä¸ªä¸é”™çš„é€‰æ‹©ã€‚é¢å¤–çš„æ­¥éª¤æœ‰åŠ©äºŽå°†ä¸€äº›å™ªå£°è¡¥ä¸è½¬åŒ–ä¸ºå›¾åƒç»†èŠ‚ã€‚

> In the previous examples, we used the `PNDM` scheduler to sample images during the inference process. We observed that when the model overfits, `DDIM` usually works much better than `PNDM` and `LMSDiscrete`. In addition, quality can be improved by running inference for more steps: 100 seems to be a good choice. The additional steps help resolve some of the noise patches into image details.

`PNDM`, Kramerè„¸éƒ¨ ï¼ˆ`PNDM`, Kramer faceï¼‰

![PNDM Cosmo](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/dreambooth-assets/9_cosmo_pndm.jpg)

`LMSDiscrete`, Kramerè„¸éƒ¨ã€‚ç»“æžœå¾ˆç³Ÿç³• ï¼ˆ`LMSDiscrete`, Kramer face. Results are terrible!ï¼‰

![LMSDiscrete Cosmo](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/dreambooth-assets/a_cosmo_lmsd.jpg)

`DDIM`, Kramerè„¸éƒ¨ã€‚æ•ˆæžœå¥½å¤šäº†ï¼ˆ`DDIM`, Kramer face. Much betterï¼‰

![DDIM Cosmo](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/dreambooth-assets/b_cosmo_ddim.jpg)

å¯¹äºŽå…¶ä»–ç‰©ä½“ï¼Œå¯ä»¥è§‚å¯Ÿåˆ°ç±»ä¼¼çš„è¡Œä¸ºï¼Œå°½ç®¡ç¨‹åº¦è¾ƒå°ã€‚

> A similar behaviour can be observed for other subjects, although to a lesser extent.

`PNDM`, åœŸè±†å¤´ï¼ˆ`PNDM`, Potato Headï¼‰

![PNDM åœŸè±†å¤´](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/dreambooth-assets/c_potato_pndm.jpg)

`LMSDiscrete`, åœŸè±†å¤´ï¼ˆ`LMSDiscrete`, Potato Headï¼‰

![LMSDiscrite Potato](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/dreambooth-assets/d_potato_lmsd.jpg)

`DDIM`, åœŸè±†å¤´ï¼ˆ`DDIM`, Potato Headï¼‰

![DDIM Potato](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/dreambooth-assets/e_potato_ddim.jpg)

## å¾®è°ƒæ–‡æœ¬ç¼–ç å™¨ï¼ˆFine-tuning the Text Encoderï¼‰

åŽŸå§‹çš„`Dreambooth`è®ºæ–‡æè¿°äº†ä¸€ç§å¾®è°ƒæ¨¡åž‹ä¸­`UNet`ç»„ä»¶çš„æ–¹æ³•ï¼Œä½†æ˜¯ä¿æŒæ–‡æœ¬ç¼–ç å™¨ä¸å˜ã€‚ç„¶è€Œï¼Œæˆ‘ä»¬è§‚å¯Ÿåˆ°å¾®è°ƒç¼–ç å™¨å¯ä»¥äº§ç”Ÿæ›´å¥½çš„ç»“æžœã€‚åœ¨çœ‹åˆ°å…¶ä»–`Dreambooth`çš„å®žçŽ°ä¸­ä½¿ç”¨è¯¥æ–¹æ³•åŽï¼Œæˆ‘ä»¬è¿›è¡Œäº†å®žéªŒï¼Œç»“æžœéžå¸¸æ˜¾è‘—ï¼

> The original Dreambooth paper describes a method to fine-tune the UNet component of the model but keeps the text encoder frozen. However, we observed that fine-tuning the encoder produces better results. We experimented with this approach after seeing it used in other Dreambooth implementations, and the results are striking!

å†»ç»“æ–‡æœ¬ç¼–ç å™¨ï¼ˆFrozen text encoderï¼‰

![Frozen text encoder](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/dreambooth-assets/f_froxen_encoder.jpg)

å¾®è°ƒæ–‡æœ¬ç¼–ç å™¨ï¼ˆFine-tuned text encoderï¼‰

![Fine-tuned text encoder](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/dreambooth-assets/g_unfrozen_encoder.jpg)

å¾®è°ƒæ–‡æœ¬ç¼–ç å™¨å¯ä»¥äº§ç”Ÿæœ€ä½³ç»“æžœï¼Œç‰¹åˆ«æ˜¯å¯¹äºŽé¢éƒ¨ã€‚å®ƒç”Ÿæˆæ›´åŠ é€¼çœŸçš„å›¾åƒï¼Œæ›´ä¸å®¹æ˜“è¿‡æ‹Ÿåˆï¼ŒåŒæ—¶è¿˜å¯ä»¥å®žçŽ°æ›´å¥½çš„æç¤ºå¯è§£é‡Šæ€§ï¼Œèƒ½å¤Ÿå¤„ç†æ›´å¤æ‚çš„æç¤ºã€‚

> Fine-tuning the text encoder produces the best results, especially with faces. It generates more realistic images, it's less prone to overfitting and it also achieves better prompt interpretability, being able to handle more complex prompts.

## åŽè®°ï¼šTextual Inversion + DreamBoothï¼ˆEpilogue: Textual Inversion + Dreamboothï¼‰

æˆ‘ä»¬è¿˜è¿›è¡Œäº†æœ€åŽä¸€ä¸ªå®žéªŒï¼Œå°†[Textual Inversion](https://textual-inversion.github.io/)ä¸Ž`Dreambooth`ç›¸ç»“åˆã€‚è¿™ä¸¤ç§æŠ€æœ¯å…·æœ‰ç±»ä¼¼çš„ç›®æ ‡ï¼Œä½†å®ƒä»¬çš„æ–¹æ³•ä¸åŒã€‚

> We also ran a final experiment where we combined [Textual Inversion](https://textual-inversion.github.io) with Dreambooth. Both techniques have a similar goal, but their approaches are different.

åœ¨è¿™ä¸ªå®žéªŒä¸­ï¼Œæˆ‘ä»¬é¦–å…ˆè¿è¡Œäº†2000æ­¥çš„Textual Inversionã€‚ç„¶åŽä»Žè¯¥æ¨¡åž‹ä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨å­¦ä¹ çŽ‡ä¸º`1e-6`çš„`Dreambooth`è¿›è¡Œäº†é¢å¤–çš„500æ­¥è®­ç»ƒã€‚ä»¥ä¸‹æ˜¯ç»“æžœï¼š

> In this experiment we first ran textual inversion for 2000 steps. From that model, we then ran Dreambooth for an additional 500 steps using a learning rate of `1e-6`. These are the results:

![Textual Inversion + Dreambooth](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/dreambooth-assets/h_textual_inversion_dreambooth.jpg)

æˆ‘ä»¬è®¤ä¸ºè¿™äº›ç»“æžœæ¯”ä»…ä½¿ç”¨`Dreambooth`è¦å¥½å¾—å¤šï¼Œä½†ä¸å¦‚å¾®è°ƒæ•´ä¸ªæ–‡æœ¬ç¼–ç å™¨æ—¶é‚£ä¹ˆå¥½ã€‚å®ƒä¼¼ä¹Žæ›´å¤šåœ°å¤åˆ¶äº†è®­ç»ƒå›¾åƒçš„é£Žæ ¼ï¼Œæ‰€ä»¥å¯èƒ½å‡ºçŽ°äº†è¿‡æ‹ŸåˆçŽ°è±¡ã€‚æˆ‘ä»¬æ²¡æœ‰è¿›ä¸€æ­¥æŽ¢ç´¢è¿™ç§ç»„åˆï¼Œä½†å®ƒå¯èƒ½æ˜¯ä¸€ä¸ªæœ‰è¶£çš„æ›¿ä»£æ–¹æ¡ˆï¼Œå¯ä»¥æ”¹è¿›`Dreambooth`å¹¶è¿™ä¸ªæ–¹æ¡ˆä»å¯ä»¥åœ¨16GBçš„GPUä¸Šè¿è¡Œã€‚è¯·éšæ„æŽ¢ç´¢å¹¶å‘Šè¯‰æˆ‘ä»¬æ‚¨çš„ç»“æžœï¼

> We think the results are much better than doing plain Dreambooth but not as good as when we fine-tune the whole text encoder. It seems to copy the style of the training images a bit more, so it could be overfitting to them. We didn't explore this combination further, but it could be an interesting alternative to improve Dreambooth and still fit the process in a 16GB GPU. Feel free to explore and tell us about your results!
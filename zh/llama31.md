---
title: "Llama 3.1ï¼š405B/70B/8B æ¨¡å‹çš„å¤šè¯­è¨€ä¸é•¿ä¸Šä¸‹æ–‡èƒ½åŠ›è§£æ" 
thumbnail: /blog/assets/llama31/thumbnail.jpg
authors:
- user: philschmid
- user: osanseviero
- user: alvarobartt
- user: lvwerra
- user: dvilasuero
- user: reach-vb
- user: marcsun13
- user: pcuenq
translators:
- user: AdinaY
---

# Llama 3.1 - 405Bã€70B å’Œ 8B çš„å¤šè¯­è¨€ä¸é•¿ä¸Šä¸‹æ–‡èƒ½åŠ›è§£æ

Llama 3.1 å‘å¸ƒäº†ï¼ä»Šå¤©æˆ‘ä»¬è¿æ¥äº† Llama å®¶æ—çš„æ–°æˆå‘˜ Llama 3.1 è¿›å…¥ Hugging Face å¹³å°ã€‚æˆ‘ä»¬å¾ˆé«˜å…´ä¸ Meta åˆä½œï¼Œç¡®ä¿åœ¨ Hugging Face ç”Ÿæ€ç³»ç»Ÿä¸­å®ç°æœ€ä½³é›†æˆã€‚Hub ä¸Šç°æœ‰å…«ä¸ªå¼€æºæƒé‡æ¨¡å‹ (3 ä¸ªåŸºç¡€æ¨¡å‹å’Œ 5 ä¸ªå¾®è°ƒæ¨¡å‹)ã€‚

Llama 3.1 æœ‰ä¸‰ç§è§„æ ¼: 8B é€‚åˆåœ¨æ¶ˆè´¹è€…çº§ GPU ä¸Šè¿›è¡Œé«˜æ•ˆéƒ¨ç½²å’Œå¼€å‘ï¼Œ70B é€‚åˆå¤§è§„æ¨¡ AI åŸç”Ÿåº”ç”¨ï¼Œè€Œ 405B åˆ™é€‚ç”¨äºåˆæˆæ•°æ®ã€å¤§è¯­è¨€æ¨¡å‹ (LLM) ä½œä¸ºè¯„åˆ¤è€…æˆ–è’¸é¦ã€‚è¿™ä¸‰ä¸ªè§„æ ¼éƒ½æä¾›åŸºç¡€ç‰ˆå’ŒæŒ‡ä»¤è°ƒä¼˜ç‰ˆã€‚

é™¤äº†å…­ä¸ªç”Ÿæˆæ¨¡å‹ï¼ŒMeta è¿˜å‘å¸ƒäº†ä¸¤ä¸ªæ–°æ¨¡å‹: Llama Guard 3 å’Œ Prompt Guardã€‚Prompt Guard æ˜¯ä¸€ä¸ªå°å‹åˆ†ç±»å™¨ï¼Œå¯ä»¥æ£€æµ‹æç¤ºæ³¨å…¥å’Œè¶Šç‹±ã€‚Llama Guard 3 æ˜¯ä¸€ä¸ªä¿æŠ¤æ¨¡å‹ï¼Œèƒ½å¤Ÿåˆ†ç±» LLM è¾“å…¥å’Œç”Ÿæˆçš„å†…å®¹ã€‚

æ­¤æ¬¡å‘å¸ƒçš„ä¸€äº›åŠŸèƒ½å’Œé›†æˆåŒ…æ‹¬:

- [Hub ä¸Šçš„æ¨¡å‹](https://huggingface.co/collections/meta-llama/llama-31-669fc079a0c406a149a5738f)
- Hugging Face Transformers å’Œ TGI é›†æˆ
- [Meta Llama 3.1 405B Instruct çš„ Hugging Chat é›†æˆ](https://huggingface.co/chat/models/meta-llama/Meta-Llama-3.1-405b-instruct/)
- ä½¿ç”¨æ¨ç†ç«¯ç‚¹ã€Google Cloudã€Amazon SageMaker å’Œ DELL Enterprise Hub è¿›è¡Œæ¨ç†å’Œéƒ¨ç½²é›†æˆ
- FP8ã€AWQ å’Œ GPTQ çš„é‡åŒ–ï¼Œä¾¿äºæ¨ç†
- ä½¿ç”¨ ğŸ¤— TRL åœ¨å•ä¸ª GPU ä¸Šå¾®è°ƒ Llama 3.1 8B
- ä½¿ç”¨ Distilabel ç”Ÿæˆ Llama 3.1 70B å’Œ 405B çš„åˆæˆæ•°æ®

## ç›®å½•

  - [Llama 3.1 çš„æ–°åŠŸèƒ½](#whats-new-with-llama-31)
  - [Llama 3.1 éœ€è¦å¤šå°‘å†…å­˜ï¼Ÿ](#how-much-memory-does-llama-31-need)
    - [æ¨ç†å†…å­˜éœ€æ±‚](#inference-memory-requirements)
    - [è®­ç»ƒå†…å­˜éœ€æ±‚](#training-memory-requirements)
  - [Llama 3.1 è¯„ä¼°](#llama-31-evaluation)
  - [ä½¿ç”¨ Hugging Face Transformers](#using-hugging-face-transformers)
  - [å¦‚ä½•ä½¿ç”¨ Llama 3.1](#how-to-prompt-llama-31)
    - [å†…ç½®å·¥å…·è°ƒç”¨](#built-in-tool-calling)
  - [è‡ªå®šä¹‰å·¥å…·è°ƒç”¨](#custom-tool-calling)
  - [æ¼”ç¤º](#demo)
  - [Llama 3.1 405B çš„ FP8ã€AWQ å’Œ GPTQ é‡åŒ–](#llama-31-405b-quantization-with-fp8-awq-and-gptq)
  - [æ¨ç†é›†æˆ](#inference-integrations)
    - [Hugging Face æ¨ç† API](#hugging-face-inference-api)
    - [Hugging Face æ¨ç†ç«¯ç‚¹](#hugging-face-inference-endpoints)
  - [Hugging Face åˆä½œä¼™ä¼´é›†æˆ](#hugging-face-partner-integrations)
  - [ä½¿ç”¨ Hugging Face TRL è¿›è¡Œå¾®è°ƒ](#fine-tuning-with-hugging-face-trl)
  - [ä½¿ç”¨ distilabel ç”Ÿæˆåˆæˆæ•°æ®](#synthetic-data-generation-with-distilabel)
  - [é™„åŠ èµ„æº](#additional-resources)
  - [è‡´è°¢](#acknowledgments)

## Llama 3.1 çš„æ–°åŠŸèƒ½

Llama 3.1 ä¸ºä»€ä¹ˆä»¤äººå…´å¥‹ï¼Ÿåœ¨å‰ä»£äº§å“çš„åŸºç¡€ä¸Šï¼ŒLlama 3.1 å¢åŠ äº†ä¸€äº›å…³é”®æ–°åŠŸèƒ½:

- 128K token çš„é•¿ä¸Šä¸‹æ–‡èƒ½åŠ› (ç›¸è¾ƒäºåŸæ¥çš„ 8K)
- å¤šè¯­è¨€æ”¯æŒ
- å·¥å…·ä½¿ç”¨åŠŸèƒ½
- æ‹¥æœ‰ 4050 äº¿å‚æ•°çš„è¶…å¤§ç¨ å¯†æ¨¡å‹
- æ›´å®½æ¾çš„è®¸å¯è¯

è®©æˆ‘ä»¬æ·±å…¥äº†è§£è¿™äº›æ–°åŠŸèƒ½ï¼

Llama 3.1 ç‰ˆæœ¬å¼•å…¥äº†åŸºäº Llama 3 æ¶æ„çš„å…­ä¸ªæ–°å¼€æº LLM æ¨¡å‹ã€‚å®ƒä»¬æœ‰ä¸‰ç§è§„æ ¼: 8Bã€70B å’Œ 405B å‚æ•°ï¼Œæ¯ç§éƒ½æœ‰åŸºç¡€ç‰ˆ (é¢„è®­ç»ƒ) å’ŒæŒ‡ä»¤è°ƒä¼˜ç‰ˆã€‚æ‰€æœ‰ç‰ˆæœ¬éƒ½æ”¯æŒ 128K token çš„ä¸Šä¸‹æ–‡é•¿åº¦å’Œ 8 ç§è¯­è¨€ï¼ŒåŒ…æ‹¬è‹±è¯­ã€å¾·è¯­ã€æ³•è¯­ã€æ„å¤§åˆ©è¯­ã€è‘¡è„ç‰™è¯­ã€å°åœ°è¯­ã€è¥¿ç­ç‰™è¯­å’Œæ³°è¯­ã€‚Llama 3.1 ç»§ç»­ä½¿ç”¨åˆ†ç»„æŸ¥è¯¢æ³¨æ„åŠ› (GQA)ï¼Œè¿™æ˜¯ä¸€ç§é«˜æ•ˆçš„è¡¨ç¤ºæ–¹å¼ï¼Œæœ‰åŠ©äºå¤„ç†æ›´é•¿çš„ä¸Šä¸‹æ–‡ã€‚

- [Meta-Llama-3.1-8B](https://huggingface.co/meta-llama/Meta-Llama-3.1-8B): åŸºç¡€ 8B æ¨¡å‹
- [Meta-Llama-3.1-8B-Instruct](https://huggingface.co/meta-llama/Meta-Llama-3.1-8B-Instruct): åŸºç¡€ 8B æ¨¡å‹çš„æŒ‡ä»¤è°ƒä¼˜ç‰ˆ
- [Meta-Llama-3.1-70B](https://huggingface.co/meta-llama/Meta-Llama-3.1-70B): åŸºç¡€ 70B æ¨¡å‹
- [Meta-Llama-3.1-70B-Instruct](https://huggingface.co/meta-llama/Meta-Llama-3.1-70B-Instruct): åŸºç¡€ 70B æ¨¡å‹çš„æŒ‡ä»¤è°ƒä¼˜ç‰ˆ
- [Meta-Llama-3.1-405B](https://huggingface.co/meta-llama/Meta-Llama-3.1-405B): åŸºç¡€ 405B æ¨¡å‹
- [Meta-Llama-3.1-405B-Instruct](https://huggingface.co/meta-llama/Meta-Llama-3.1-405B-Instruct): åŸºç¡€ 405B æ¨¡å‹çš„æŒ‡ä»¤è°ƒä¼˜ç‰ˆ

é™¤äº†è¿™å…­ä¸ªè¯­è¨€æ¨¡å‹ï¼Œè¿˜å‘å¸ƒäº† Llama Guard 3 å’Œ Prompt Guardã€‚

- [Llama Guard 3](https://huggingface.co/meta-llama/Llama-Guard-3-8B) æ˜¯ Llama Guard å®¶æ—çš„æœ€æ–°ç‰ˆæœ¬ï¼ŒåŸºäº Llama 3.1 8B è¿›è¡Œå¾®è°ƒã€‚å®ƒä¸ºç”Ÿäº§ç”¨ä¾‹è€Œè®¾è®¡ï¼Œå…·æœ‰ 128k çš„ä¸Šä¸‹æ–‡é•¿åº¦å’Œå¤šè¯­è¨€èƒ½åŠ›ã€‚Llama Guard 3 å¯ä»¥åˆ†ç±» LLM çš„è¾“å…¥ (æç¤º) å’Œè¾“å‡ºï¼Œä»¥æ£€æµ‹åœ¨é£é™©åˆ†ç±»ä¸­è¢«è®¤ä¸ºä¸å®‰å…¨çš„å†…å®¹ã€‚
- [Prompt Guard](https://huggingface.co/meta-llama/Prompt-Guard-86M)ï¼Œå¦ä¸€æ–¹é¢ï¼Œæ˜¯ä¸€ä¸ªå°å‹ 279M å‚æ•°çš„åŸºäº BERT çš„åˆ†ç±»å™¨ï¼Œå¯ä»¥æ£€æµ‹æç¤ºæ³¨å…¥å’Œè¶Šç‹±ã€‚å®ƒåœ¨å¤§è§„æ¨¡æ”»å‡»è¯­æ–™åº“ä¸Šè®­ç»ƒï¼Œå¹¶å»ºè®®ä½¿ç”¨ç‰¹å®šåº”ç”¨çš„æ•°æ®è¿›è¡Œè¿›ä¸€æ­¥å¾®è°ƒã€‚

ä¸ Llama 3 ç›¸æ¯”ï¼ŒLlama 3.1 çš„æ–°ç‰¹ç‚¹æ˜¯æŒ‡ä»¤æ¨¡å‹åœ¨å·¥å…·è°ƒç”¨æ–¹é¢è¿›è¡Œäº†å¾®è°ƒï¼Œé€‚ç”¨äºæ™ºèƒ½ä½“ç”¨ä¾‹ã€‚å†…ç½®äº†ä¸¤ä¸ªå·¥å…· (æœç´¢ï¼Œä½¿ç”¨ Wolfram Alpha è¿›è¡Œæ•°å­¦æ¨ç†)ï¼Œå¯ä»¥æ‰©å±•ä¸ºè‡ªå®šä¹‰ JSON åŠŸèƒ½ã€‚

Llama 3.1 æ¨¡å‹åœ¨å®šåˆ¶ GPU é›†ç¾¤ä¸Šè®­ç»ƒäº†è¶…è¿‡ 15 ä¸‡äº¿ tokenï¼Œæ€»è®¡ 39.3M GPU å°æ—¶ (8B 1.46Mï¼Œ70B 7.0Mï¼Œ405B 30.84M)ã€‚æˆ‘ä»¬ä¸çŸ¥é“è®­ç»ƒæ•°æ®é›†æ··åˆçš„å…·ä½“ç»†èŠ‚ï¼Œä½†æˆ‘ä»¬çŒœæµ‹å®ƒåœ¨å¤šè¯­è¨€æ–¹é¢æœ‰æ›´å¹¿æ³›çš„ç­–åˆ’ã€‚Llama 3.1 Instruct å·²ä¼˜åŒ–ç”¨äºæŒ‡ä»¤è·Ÿéšï¼Œå¹¶åœ¨å…¬å¼€å¯ç”¨çš„æŒ‡ä»¤æ•°æ®é›†ä»¥åŠè¶…è¿‡ 2500 ä¸‡åˆæˆç”Ÿæˆçš„ç¤ºä¾‹ä¸Šè¿›è¡Œç›‘ç£å¾®è°ƒ (SFT) å’Œäººç±»åé¦ˆçš„å¼ºåŒ–å­¦ä¹  (RLHF)ã€‚Meta å¼€å‘äº†åŸºäº LLM çš„åˆ†ç±»å™¨ï¼Œä»¥åœ¨æ•°æ®æ··åˆåˆ›å»ºè¿‡ç¨‹ä¸­è¿‡æ»¤å’Œç­–åˆ’é«˜è´¨é‡çš„æç¤ºå’Œå“åº”ã€‚

å…³äºè®¸å¯æ¡æ¬¾ï¼ŒLlama 3.1 å…·æœ‰éå¸¸ç›¸ä¼¼çš„è®¸å¯è¯ï¼Œä½†æœ‰ä¸€ä¸ªå…³é”®åŒºåˆ«: **å®ƒå…è®¸ä½¿ç”¨æ¨¡å‹è¾“å‡ºæ¥æ”¹è¿›å…¶ä»– LLM**ã€‚è¿™æ„å‘³ç€åˆæˆæ•°æ®ç”Ÿæˆå’Œè’¸é¦æ˜¯å…è®¸çš„ï¼Œå³ä½¿æ˜¯ä¸åŒçš„æ¨¡å‹ï¼è¿™å¯¹ 405B æ¨¡å‹å°¤å…¶é‡è¦ï¼Œå¦‚åé¢æ‰€è®¨è®ºçš„ã€‚è®¸å¯è¯å…è®¸å†åˆ†å‘ã€å¾®è°ƒå’Œåˆ›å»ºè¡ç”Ÿä½œå“ï¼Œä»ç„¶è¦æ±‚æ´¾ç”Ÿæ¨¡å‹åœ¨å…¶åç§°çš„å¼€å¤´åŒ…æ‹¬ â€œLlamaâ€ï¼Œå¹¶ä¸”ä»»ä½•è¡ç”Ÿä½œå“æˆ–æœåŠ¡å¿…é¡»æåŠ â€œBuilt with Llamaâ€ã€‚æœ‰å…³å®Œæ•´è¯¦æƒ…ï¼Œè¯·ç¡®ä¿é˜…è¯» [å®˜æ–¹è®¸å¯è¯](https://huggingface.co/meta-llama/Meta-Llama-3.1-8B-Instruct/blob/main/LICENSE)ã€‚

## Llama 3.1 éœ€è¦å¤šå°‘å†…å­˜ï¼Ÿ

Llama 3.1 å¸¦æ¥äº†ä»¤äººå…´å¥‹çš„è¿›æ­¥ã€‚ç„¶è€Œï¼Œè¿è¡Œå®ƒéœ€è¦ä»”ç»†è€ƒè™‘ç¡¬ä»¶èµ„æºã€‚æˆ‘ä»¬åˆ†è§£äº†ä¸‰ç§æ¨¡å‹è§„æ ¼åœ¨è®­ç»ƒå’Œæ¨ç†ä¸­çš„å†…å­˜éœ€æ±‚ã€‚

### æ¨ç†å†…å­˜éœ€æ±‚

å¯¹äºæ¨ç†ï¼Œå†…å­˜éœ€æ±‚å–å†³äºæ¨¡å‹è§„æ ¼å’Œæƒé‡çš„ç²¾åº¦ã€‚ä»¥ä¸‹æ˜¯ä¸åŒé…ç½®æ‰€éœ€çš„è¿‘ä¼¼å†…å­˜:

<table>
  <tr>
   <td><strong> æ¨¡å‹è§„æ ¼ </strong>
   </td>
   <td><strong>FP16</strong>
   </td>
   <td><strong>FP8</strong>
   </td>
   <td><strong>INT4</strong>
   </td>
  </tr>
  <tr>
   <td>8B
   </td>
   <td>16 GB
   </td>
   <td>8 GB
   </td>
   <td>4 GB
   </td>
  </tr>
  <tr>
   <td>70B
   </td>
   <td>140 GB
   </td>
   <td>70 GB
   </td>
   <td>35 GB
   </td>
  </tr>
  <tr>
   <td>405B
   </td>
   <td>810 GB
   </td>
   <td>405 GB
   </td>
   <td>203 GB
   </td>
  </tr>
</table>

_æ³¨æ„: ä¸Šé¢å¼•ç”¨çš„æ•°å­—è¡¨ç¤ºä»…åŠ è½½æ¨¡å‹æ£€æŸ¥ç‚¹æ‰€éœ€çš„ GPU VRAMã€‚å®ƒä»¬ä¸åŒ…æ‹¬å†…æ ¸æˆ– CUDA å›¾å½¢çš„ torch ä¿ç•™ç©ºé—´ã€‚_

ä¾‹å¦‚ï¼Œä¸€ä¸ª H100 èŠ‚ç‚¹ (8x H100) æœ‰çº¦ 640GB çš„ VRAMï¼Œå› æ­¤ 405B æ¨¡å‹éœ€è¦åœ¨å¤šèŠ‚ç‚¹è®¾ç½®ä¸­è¿è¡Œæˆ–ä»¥è¾ƒä½ç²¾åº¦ (ä¾‹å¦‚ FP8) è¿è¡Œï¼Œè¿™æ˜¯æ¨èçš„æ–¹æ³•ã€‚

è¯·è®°ä½ï¼Œè¾ƒä½ç²¾åº¦ (ä¾‹å¦‚ INT4) å¯èƒ½ä¼šå¯¼è‡´ä¸€äº›ç²¾åº¦æŸå¤±ï¼Œä½†å¯ä»¥æ˜¾è‘—å‡å°‘å†…å­˜éœ€æ±‚å¹¶æé«˜æ¨ç†é€Ÿåº¦ã€‚é™¤äº†æ¨¡å‹æƒé‡å¤–ï¼Œæ‚¨è¿˜éœ€è¦å°† KV ç¼“å­˜ä¿æŒåœ¨å†…å­˜ä¸­ã€‚å®ƒåŒ…å«æ¨¡å‹ä¸Šä¸‹æ–‡ä¸­æ‰€æœ‰ token çš„é”®å’Œå€¼ï¼Œä»¥ä¾¿åœ¨ç”Ÿæˆæ–° token æ—¶ä¸éœ€è¦é‡æ–°è®¡ç®—ã€‚ç‰¹åˆ«æ˜¯å½“åˆ©ç”¨å¯ç”¨çš„é•¿ä¸Šä¸‹æ–‡é•¿åº¦æ—¶ï¼Œå®ƒå˜å¾—è‡³å…³é‡è¦ã€‚åœ¨ FP16 ä¸­ï¼ŒKV ç¼“å­˜å†…å­˜éœ€æ±‚å¦‚ä¸‹:

<table>
  <tr>
   <td><strong> æ¨¡å‹è§„æ ¼ </strong>
   </td>
   <td><strong>1k token</strong>
   </td>
   <td><strong>16k token</strong>
   </td>
   <td><strong>128k token</strong>
   </td>
  </tr>
  <tr>
   <td>8B
   </td>
   <td>0.125 GB
   </td>
   <td>1.95 GB
   </td>
   <td>15.62 GB
   </td>
</tr>
  <tr>
   <td>70B
   </td>
   <td>0.313 GB
   </td>
   <td>4.88 GB
   </td>
   <td>39.06 GB
   </td>
  </tr>
  <tr>
   <td>405B
   </td>
   <td>0.984 GB
   </td>
   <td>15.38
   </td>
   <td>123.05 GB
   </td>
  </tr>
</table>

ç‰¹åˆ«æ˜¯å¯¹äºå°è§„æ ¼æ¨¡å‹ï¼Œå½“æ¥è¿‘ä¸Šä¸‹æ–‡é•¿åº¦ä¸Šé™æ—¶ï¼Œç¼“å­˜ä½¿ç”¨çš„å†…å­˜ä¸æƒé‡ä¸€æ ·å¤šã€‚

### è®­ç»ƒå†…å­˜éœ€æ±‚

ä»¥ä¸‹è¡¨æ ¼æ¦‚è¿°äº†ä½¿ç”¨ä¸åŒæŠ€æœ¯è®­ç»ƒ Llama 3.1 æ¨¡å‹çš„å¤§è‡´å†…å­˜éœ€æ±‚:

<table>
  <tr>
   <td><strong>æ¨¡å‹è§„æ ¼</strong>
   </td>
   <td><strong>1k token</strong>
   </td>
   <td><strong>16k token</strong>
   </td>
   <td><strong>128k token</strong>
   </td>
  </tr>
  <tr>
   <td>8B
   </td>
   <td>0.125 GB
   </td>
   <td>1.95 GB
   </td>
   <td>15.62 GB
   </td>


  </tr>
  <tr>
   <td>70B
   </td>
   <td>0.313 GB
   </td>
   <td>4.88 GB
   </td>
   <td>39.06 GB
   </td>
  </tr>
  <tr>
   <td>405B
   </td>
   <td>0.984 GB
   </td>
   <td>15.38
   </td>
   <td>123.05 GB
   </td>
  </tr>
</table>

_æ³¨æ„: è¿™äº›æ˜¯ä¼°è®¡å€¼ï¼Œå¯èƒ½ä¼šæ ¹æ®å…·ä½“å®ç°ç»†èŠ‚å’Œä¼˜åŒ–æƒ…å†µæœ‰æ‰€ä¸åŒã€‚_

## Llama 3.1 è¯„ä¼°

_æ³¨æ„: æˆ‘ä»¬ç›®å‰æ­£åœ¨æ–°çš„ [Open LLM Leaderboard 2](https://huggingface.co/spaces/open-llm-leaderboard/open_llm_leaderboard) ä¸Šå•ç‹¬è¯„ä¼° Llama 3.1ï¼Œå¹¶å°†åœ¨ä»Šå¤©æ™šäº›æ—¶å€™æ›´æ–°æ­¤éƒ¨åˆ†ã€‚ä»¥ä¸‹æ˜¯ Meta å®˜æ–¹è¯„ä¼°çš„æ‘˜å½•ã€‚_


<table>
  <tr>
   <td><strong><em>ç±»åˆ«</em></strong>
   </td>
   <td><strong><em>åŸºå‡†</em></strong>
   </td>
   <td><strong><em>æ ·æœ¬æ•°</em></strong>
   </td>
   <td><strong><em>æŒ‡æ ‡</em></strong>
   </td>
   <td><strong><em>Llama 3 8B</em></strong>
   </td>
   <td><strong><em>Llama 3.1 8B</em></strong>
   </td>
   <td><strong><em>Llama 3 70B</em></strong>
   </td>
   <td><strong><em>Llama 3.1 70B</em></strong>
   </td>
   <td><strong><em>Llama 3.1 405B</em></strong>
   </td>
  </tr>
  <tr>
   <td><em>ç»¼åˆ</em>
   </td>
   <td><em>MMLU</em>
   </td>
   <td><em>5</em>
   </td>
   <td><em>å®è§‚å¹³å‡/å­—ç¬¦å‡†ç¡®ç‡</em></td>
   <td><em>66.7</em>
   </td>
   <td><em>66.7</em>
   </td>
   <td><em>79.5</em>
   </td>
   <td><em>79.3</em>
   </td>
   <td><em>85.2</em></td>
  </tr>
  <tr>
   <td></td>
   <td><em>MMLU PROï¼ˆCoTï¼‰</em></td>
   <td><em>5</em></td>
   <td><em>å®è§‚å¹³å‡/å­—ç¬¦å‡†ç¡®ç‡</em></td>
   <td><em>36.2</em></td>
   <td><em>37.1</em></td>
   <td><em>55.0</em></td>
   <td><em>53.8</em></td>
   <td><em>61.6</em></td>
  </tr>
  <tr>
   <td></td>
   <td><em>AGIEval è‹±è¯­</em></td>
   <td><em>3-5</em></td>
   <td><em>å¹³å‡/å­—ç¬¦å‡†ç¡®ç‡</em></td>
   <td><em>47.1</em></td>
   <td><em>47.8</em></td>
   <td><em>63.0</em></td>
   <td><em>64.6</em></td>
   <td><em>71.6</em></td>
  </tr>
  <tr>
   <td></td>
   <td><em>CommonSenseQA</em></td>
   <td><em>7</em></td>
   <td><em>å­—ç¬¦å‡†ç¡®ç‡</em></td>
   <td><em>72.6</em></td>
   <td><em>75.0</em></td>
   <td><em>83.8</em></td>
   <td><em>84.1</em></td>
   <td><em>85.8</em></td>
  </tr>
  <tr>
   <td></td>
   <td><em>Winogrande</em></td>
   <td><em>5</em></td>
   <td><em>å­—ç¬¦å‡†ç¡®ç‡</em></td>
   <td><em>-</em></td>
   <td><em>60.5</em></td>
   <td><em>-</em></td>
   <td><em>83.3</em></td>
   <td><em>86.7</em></td>
  </tr>
  <tr>
   <td></td>
   <td><em>BIG-Bench Hardï¼ˆCoTï¼‰</em></td>
   <td><em>3</em></td>
   <td><em>å¹³å‡/å®Œå…¨åŒ¹é…</em></td>
   <td><em>61.1</em></td>
   <td><em>64.2</em></td>
   <td><em>81.3</em></td>
   <td><em>81.6</em></td>
   <td><em>85.9</em></td>
  </tr>
  <tr>
   <td></td>
   <td><em>ARC-Challenge</em></td>
   <td><em>25</em></td>
   <td><em>å­—ç¬¦å‡†ç¡®ç‡</em></td>
   <td><em>79.4</em></td>
   <td><em>79.7</em></td>
   <td><em>93.1</em></td>
   <td><em>92.9</em></td>
   <td><em>96.1</em></td>
  </tr>
  <tr>
   <td><em>çŸ¥è¯†æ¨ç†</em></td>
   <td><em>TriviaQA-Wiki</em></td>
   <td><em>5</em></td>
   <td><em>å®Œå…¨åŒ¹é…</em></td>
   <td><em>78.5</em></td>
   <td><em>77.6</em></td>
   <td><em>89.7</em></td>
   <td><em>89.8</em></td>
   <td><em>91.8</em></td>
  </tr>
  <tr>
   <td></td>
   <td><em>SQuAD</em></td>
   <td><em>1</em></td>
   <td><em>å®Œå…¨åŒ¹é…</em></td>
   <td><em>76.4</em></td>
   <td><em>77.0</em></td>
   <td><em>85.6</em></td>
   <td><em>81.8</em></td>
   <td><em>89.3</em></td>
  </tr>
  <tr>
   <td><em>é˜…è¯»ç†è§£</em></td>
   <td><em>QuACï¼ˆF1ï¼‰</em></td>
   <td><em>1</em></td>
   <td><em>F1</em></td>
   <td><em>44.4</em></td>
   <td><em>44.9</em></td>
   <td><em>51.1</em></td>
   <td><em>51.1</em></td>
   <td><em>53.6</em></td>
  </tr>
  <tr>
   <td></td>
   <td><em>BoolQ</em></td>
   <td><em>0

</em></td>
   <td><em>å­—ç¬¦å‡†ç¡®ç‡</em></td>
   <td><em>75.7</em></td>
   <td><em>75.0</em></td>
   <td><em>79.0</em></td>
   <td><em>79.4</em></td>
   <td><em>80.0</em></td>
  </tr>
  <tr>
   <td></td>
   <td><em>DROPï¼ˆF1ï¼‰</em></td>
   <td><em>3</em></td>
   <td><em>F1</em></td>
   <td><em>58.4</em></td>
   <td><em>59.5</em></td>
   <td><em>79.7</em></td>
   <td><em>79.6</em></td>
   <td><em>84.8</em></td>
  </tr>
</table>

## ä½¿ç”¨ Hugging Face Transformers

Llama 3.1 éœ€è¦è¿›è¡Œå°‘é‡å»ºæ¨¡æ›´æ–°ï¼Œä»¥æœ‰æ•ˆå¤„ç† RoPE ç¼©æ”¾ã€‚ä½¿ç”¨ Transformers [4.43 ç‰ˆ](https://github.com/huggingface/transformers/tags)ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨æ–°çš„ Llama 3.1 æ¨¡å‹ï¼Œå¹¶åˆ©ç”¨ Hugging Face ç”Ÿæ€ç³»ç»Ÿä¸­çš„æ‰€æœ‰å·¥å…·ã€‚ç¡®ä¿ä½¿ç”¨æœ€æ–°çš„ `transformers` ç‰ˆæœ¬:

```bash
pip install "transformers>=4.43" --upgrade
```

å‡ ä¸ªç»†èŠ‚:

- Transformers é»˜è®¤ä»¥ bfloat16 åŠ è½½æ¨¡å‹ã€‚è¿™æ˜¯ Meta å‘å¸ƒçš„åŸå§‹æ£€æŸ¥ç‚¹ä½¿ç”¨çš„ç±»å‹ï¼Œå› æ­¤è¿™æ˜¯ç¡®ä¿æœ€ä½³ç²¾åº¦æˆ–è¿›è¡Œè¯„ä¼°çš„æ¨èæ–¹æ³•ã€‚
- åŠ©æ‰‹å“åº”å¯èƒ½ä»¥ç‰¹æ®Š token `<|eot_id|>` ç»“å°¾ï¼Œä½†æˆ‘ä»¬è¿˜å¿…é¡»åœ¨æ‰¾åˆ°å¸¸è§„ EOS token æ—¶åœæ­¢ç”Ÿæˆã€‚æˆ‘ä»¬å¯ä»¥é€šè¿‡åœ¨ `eos_token_id` å‚æ•°ä¸­æä¾›ç»ˆæ­¢ç¬¦åˆ—è¡¨æ¥æå‰åœæ­¢ç”Ÿæˆã€‚
- æˆ‘ä»¬ä½¿ç”¨äº† Meta ä»£ç åº“ä¸­çš„é»˜è®¤é‡‡æ ·å‚æ•° (`temperature` å’Œ `top_p` )ã€‚æˆ‘ä»¬è¿˜æ²¡æœ‰æ—¶é—´è¿›è¡Œå¹¿æ³›æµ‹è¯•ï¼Œè¯·éšæ„æ¢ç´¢ï¼

ä»¥ä¸‹ä»£ç æ®µæ˜¾ç¤ºäº†å¦‚ä½•ä½¿ç”¨ `meta-llama/Meta-Llama-3.1-8B-Instruct` ã€‚å®ƒå¤§çº¦éœ€è¦ 16 GB çš„ VRAMï¼Œé€‚åˆè®¸å¤šæ¶ˆè´¹è€…çº§ GPUã€‚ç›¸åŒçš„ä»£ç æ®µé€‚ç”¨äº `meta-llama/Meta-Llama-3.1-70B-Instruct` ï¼Œåœ¨ 140GB VRAM å’Œ `meta-llama/Meta-Llama-3.1-405B-Instruct` (éœ€è¦ 810GB VRAM)ï¼Œä½¿å…¶æˆä¸ºç”Ÿäº§ç”¨ä¾‹çš„éå¸¸æœ‰è¶£çš„æ¨¡å‹ã€‚å¯ä»¥é€šè¿‡ä»¥ 8 ä½æˆ– 4 ä½æ¨¡å¼åŠ è½½è¿›ä¸€æ­¥å‡å°‘å†…å­˜æ¶ˆè€—ã€‚

```python
from transformers import pipeline
import torch

model_id = "meta-llama/Meta-Llama-3.1-8B-Instruct"
pipe = pipeline(
    "text-generation",
    model=model_id,
    model_kwargs={"torch_dtype": torch.bfloat16},
    device="cuda",
)

messages = [
    {"role": "user", "content": "Who are you? Please, answer in pirate-speak."},
]
outputs = pipe(
    messages,
    max_new_tokens=256,
    do_sample=False,
)
assistant_response = outputs[0]["generated_text"][-1]["content"]
print(assistant_response)
# Arrrr, me hearty! Yer lookin' fer a bit o' information about meself, eh? Alright then, matey! I be a language-generatin' swashbuckler, a digital buccaneer with a penchant fer spinnin' words into gold doubloons o' knowledge! Me name be... (dramatic pause)...Assistant! Aye, that be me name, and I be here to help ye navigate the seven seas o' questions and find the hidden treasure o' answers! So hoist the sails and set course fer adventure, me hearty! What be yer first question?
```

æ‚¨è¿˜å¯ä»¥è‡ªåŠ¨é‡åŒ–æ¨¡å‹ï¼Œä»¥ 8 ä½ç”šè‡³ 4 ä½æ¨¡å¼åŠ è½½ï¼Œä½¿ç”¨ bitsandbytesã€‚4 ä½åŠ è½½å¤§ 70B ç‰ˆæœ¬å¤§çº¦éœ€è¦ 34 GB çš„å†…å­˜è¿è¡Œã€‚è¿™æ˜¯å¦‚ä½•ä»¥ 4 ä½æ¨¡å¼åŠ è½½ç”Ÿæˆç®¡é“:

```python
pipeline = pipeline(
    "text-generation",
    model=model_id,
    model_kwargs={
        "torch_dtype": torch.bfloat16,
        "quantization_config": {"load_in_4bit": True}
    },
)
```

æœ‰å…³ä½¿ç”¨ `transformers` æ¨¡å‹çš„æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·æŸ¥çœ‹ [æ¨¡å‹å¡ç‰‡](https://huggingface.co/meta-llama/Meta-Llama-3.1-8B-Instruct)ã€‚

_æ³¨æ„: Transformers å¤„ç†æ‰€æœ‰æ£˜æ‰‹çš„æç¤ºæ¨¡æ¿é—®é¢˜ï¼Œå¦‚æœæ‚¨æƒ³äº†è§£æ›´å¤šå…³äºæç¤ºçš„ä¿¡æ¯ï¼Œè¯·æŸ¥çœ‹ä¸‹ä¸€éƒ¨åˆ†ã€‚_

## å¦‚ä½•ä½¿ç”¨ Llama 3.1

åŸºç¡€æ¨¡å‹æ²¡æœ‰æç¤ºæ ¼å¼ã€‚åƒå…¶ä»–åŸºç¡€æ¨¡å‹ä¸€æ ·ï¼Œå®ƒä»¬å¯ä»¥ç”¨äºç»§ç»­è¾“å…¥åºåˆ—å¹¶è¿›è¡Œåˆç†çš„å»¶ç»­æˆ–é›¶æ ·æœ¬/å°‘æ ·æœ¬æ¨ç†ã€‚å®ƒä»¬ä¹Ÿæ˜¯å¾®è°ƒæ‚¨è‡ªå·±ç”¨ä¾‹çš„ç»ä½³åŸºç¡€ã€‚

æŒ‡ä»¤ç‰ˆæœ¬æ”¯æŒå…·æœ‰ 4 ä¸ªè§’è‰²çš„å¯¹è¯æ ¼å¼:

1. **system:** è®¾ç½®å¯¹è¯çš„ä¸Šä¸‹æ–‡ã€‚å®ƒå…è®¸åŒ…æ‹¬è§„åˆ™ã€æŒ‡å—æˆ–å¿…è¦çš„ä¿¡æ¯ï¼Œå¸®åŠ©æœ‰æ•ˆå“åº”ã€‚å®ƒä¹Ÿç”¨äºåœ¨é€‚å½“æƒ…å†µä¸‹å¯ç”¨å·¥å…·ä½¿ç”¨ã€‚
2. **user:** ç”¨æˆ·è¾“å…¥ã€å‘½ä»¤å’Œå¯¹æ¨¡å‹çš„é—®é¢˜ã€‚
3. **assistant:** åŠ©æ‰‹çš„å“åº”ï¼ŒåŸºäº `system` å’Œ `user` æç¤ºä¸­æä¾›çš„ä¸Šä¸‹æ–‡ã€‚
4. **ipython:** Llama 3.1 ä¸­å¼•å…¥çš„æ–°è§’è‰²ã€‚å½“å·¥å…·è°ƒç”¨è¿”å›ç»™ LLM æ—¶ä½œä¸ºè¾“å‡ºä½¿ç”¨ã€‚

æŒ‡ä»¤ç‰ˆæœ¬ä½¿ç”¨ä»¥ä¸‹å¯¹è¯ç»“æ„è¿›è¡Œç®€å•å¯¹è¯:

```
<|begin_of_text|><|start_header_id|>system<|end_header_id|>

{{ system_prompt }}<|eot_id|><|start_header_id|>user<|end_header_id|>

{{ user_msg_1 }}<|eot_id|><|start_header_id|>assistant<|end_header_id|>

{{ model_answer_1 }}<|eot_id|>
```

Llama 3.1 æŒ‡ä»¤æ¨¡å‹ç°åœ¨æ”¯æŒå·¥å…·è°ƒç”¨ï¼ŒåŒ…æ‹¬ä¸‰ä¸ªå†…ç½®å·¥å…· (brave_searchã€wolfram_alpha å’Œ code_interpreter) å’Œé€šè¿‡ JSON å‡½æ•°è°ƒç”¨çš„è‡ªå®šä¹‰å·¥å…·è°ƒç”¨ã€‚å†…ç½®å·¥å…·ä½¿ç”¨ Python è¯­æ³•ã€‚ç”Ÿæˆ Python ä»£ç ä»¥è¿›è¡Œå‡½æ•°è°ƒç”¨æ˜¯ä»£ç è§£é‡Šå™¨å·¥å…·çš„ä¸€éƒ¨åˆ†ï¼Œå¿…é¡»åœ¨ç³»ç»Ÿæç¤ºä¸­ä½¿ç”¨ `Environment` å…³é”®å­—å¯ç”¨ï¼Œå¦‚ä¸‹æ‰€ç¤ºã€‚

### å†…ç½®å·¥å…·è°ƒç”¨

åŒ…æ‹¬ "Environment: ipython" ä¼šæ‰“å¼€ä»£ç è§£é‡Šå™¨æ¨¡å¼ï¼Œæ¨¡å‹å¯ä»¥ç”Ÿæˆå®ƒæœŸæœ›è¢«æ‰§è¡Œçš„ Python ä»£ç ã€‚åŠ©æ‰‹å“åº”çš„æ¶ˆæ¯ä½“ä»¥ç‰¹æ®Šæ ‡è®° `<|python_tag|>` å¼€å¤´ï¼Œä»¥ `<|eom_id|>` ç»“å°¾ï¼Œè€Œä¸æ˜¯æ ‡å‡† `<|eot_id|>`ã€‚åè€…è¡¨ç¤ºå›åˆç»“æŸï¼Œè€Œå‰è€…è¡¨ç¤ºç»§ç»­å¤šæ­¥æ¨ç†ã€‚

<details close>
<summary>å†…ç½®å·¥å…·è°ƒç”¨ç¤ºä¾‹</summary>

```
<|begin_of_text|><|start_header_id|>system<|end_header_id|>


Environment: ipython
Tools: brave_search, wolfram_alpha

Cutting Knowledge Date: 01 March 2023
Today's Date: 13 July 2024


You are a helpful Assistant.<|eot_id|><|start_header_id|>user<|end_header_id|>

Weather in Menlo Park, California<|eot_id|><|start_header_id|>assistant<|end_header_id|>
```

æ­¤æ—¶æ¨¡å‹çš„å“åº”å°†åŒ…æ‹¬è°ƒç”¨å…¶ä¸­ä¸€ä¸ªæ”¯æŒçš„å·¥å…· (åœ¨æœ¬ä¾‹ä¸­ä¸º `brave_search` ) çš„ Python ä»£ç :

```
<|python_tag|>brave_search.call(query="current weather in Menlo Park, California")<|eom_id|>
```

æ‰§è¡Œè°ƒç”¨çš„å“åº”ç„¶åå‘é€å›æ¨¡å‹ä»¥æ£€ç´¢æœ€ç»ˆå“åº”ã€‚ä¸ºäº†ç®€æ´ï¼Œä»¥ä¸‹å†…å®¹å°†é™„åŠ åˆ°å‰é¢ä»£ç æ®µä¸­æ˜¾ç¤ºçš„æ¶ˆæ¯:

```
<|python_tag|>brave_search.call(query="Menlo Park California weather")<|eom_id|><|start_header_id|>ipython<|end_header_id|>

{"query": "Menlo Park California weather", "top_k": [{"title": "10-Day Weather Forecast for West Menlo Park, CA - The Weather Channel | weather.com", "url": "https://weather.com/weather/tenday/l/West+Menlo+Park+CA?canonicalCityId=b2375713aa1943aad7d1a13a85e1c0adad13c1b10563b2bbaad70734dc61cf11", "description": "Be prepared with the most accurate 10-day forecast for West <strong>Menlo</strong> <strong>Park</strong>, CA with highs, lows, chance of precipitation from The <strong>Weather</strong> Channel and <strong>Weather</strong>.com", "type": "search_result"},....}<|eot_id|><|start_header_id|>assistant<|end_header_id|>
```

LLM çš„æœ€ç»ˆå“åº”å°†æ˜¯:

```
The current weather in Menlo Park, California is mostly sunny with a high of 77Â°F and a low of 56Â°F.<|eot_id|>
```

</details>

## è‡ªå®šä¹‰å·¥å…·è°ƒç”¨

Llama 3.1 æŒ‡ä»¤æ”¯æŒä»å•ä¸ªç”¨æˆ·æ¶ˆæ¯ä¸­è°ƒç”¨è‡ªå®šä¹‰å‡½æ•°ã€‚ä»¥ä¸‹æç¤ºæä¾›äº†å¦‚ä½•ä»æ¨¡å‹è¾“å‡ºè°ƒç”¨è‡ªå®šä¹‰å‡½æ•°çš„ç¤ºä¾‹ã€‚åœ¨è‡ªå®šä¹‰å‡½æ•°è°ƒç”¨ä¸­ï¼Œæ¨¡å‹è¾“å‡º `<|eot_id|>` è€Œä¸æ˜¯ `<|eom_id|>` ã€‚éœ€è¦è°ƒæ•´ç³»ç»Ÿæç¤ºä»¥å‘ŠçŸ¥æ¨¡å‹å¦‚ä½•å¤„ç†å‡½æ•°è°ƒç”¨è¾“å‡ºã€‚

<details close>
<summary>è‡ªå®šä¹‰å·¥å…·è°ƒç”¨ JSON å‡½æ•°</summary>

```
<|begin_of_text|><|start_header_id|>system<|end_header_id|>

You are a helpful assistant with tool calling capabilities. When you receive a tool call response, use the output to format an answer to the orginal user question.<|eot_id|><|start_header_id|>user<|end_header_id|>

Given the following functions, please respond with a JSON for a function call with its proper arguments that best answers the given prompt.

Respond in the format {"name": function name, "parameters": dictionary of argument name and its value}. Do not use variables.

{
    "type": "function",
    "function": {
    "name": "get_current_conditions",
    "description": "Get the current weather conditions for a specific location",
    "parameters": {
        "type": "object",
        "properties": {
        "location": {
            "type": "string",
            "description": "The city and state, e.g., San Francisco, CA"
        },
        "unit": {
            "type": "string",
            "enum": ["Celsius", "Fahrenheit"],
            "description": "The temperature unit to use. Infer this from the user's location."
        }
        },
        "required": ["location", "unit"]
    }
    }
}

Question: what is the weather like in Menlo Park?<|eot_id|><|start_header_id|>assitant<|end_header_id|>

{"name": "get_current_conditions", "parameters": {"location": "Menlo Park, CA", "unit": "Fahrenheit"}}<|eot_id|><|start_header_id|>ipython<|end_header_id|>
```

å½“æˆ‘ä»¬ä»é€‰å®šçš„å·¥å…·æ£€ç´¢è¾“å‡ºæ—¶ï¼Œæˆ‘ä»¬å°†å…¶ä¼ å›æ¨¡å‹ï¼Œä½¿ç”¨ç›¸åŒçš„ `<|python_tag|>` åˆ†éš”ç¬¦ã€‚`<|python_tag|>` ä¸æ„å‘³ç€ä½¿ç”¨ Pythonã€‚å®ƒä»…ç”¨äºè¡¨ç¤ºä»»ä½•å·¥å…·çš„è¾“å‡ºå¼€å§‹ã€‚

```
<|python_tag|>{
    "tool_call_id": "get_current_conditions"
    "output": "Clouds giving way to sun Hi: 76Â° Tonight: Mainly clear early, then areas of low clouds forming Lo: 56Â°"
}<|eot_id|><|start_header_id|>assistant<|end_header_id|>

The weather in Menlo Park is currently cloudy with a high of 76Â° and a low of 56Â°, with clear skies expected tonight.<|eot_id|>
```

è¿™ç§æ ¼å¼å¿…é¡»ç²¾ç¡®å¤åˆ¶æ‰èƒ½æœ‰æ•ˆä½¿ç”¨ã€‚transformers ä¸­å¯ç”¨çš„èŠå¤©æ¨¡æ¿ä½¿å…¶æ˜“äºæ­£ç¡®æ ¼å¼åŒ–æç¤ºã€‚

</details>

## æ¼”ç¤º

æ‚¨å¯ä»¥åœ¨ä»¥ä¸‹æ¼”ç¤ºä¸­è¯•éªŒä¸‰ç§æŒ‡ä»¤æ¨¡å‹:

- Llama 3.1 405B çš„ Hugging Chat [https://huggingface.co/chat/models/meta-llama/Meta-Llama-3.1-405b-instruct/](https://huggingface.co/chat/models/meta-llama/Meta-Llama-3.1-405b-instruct/)
- Llama 3.1 70B çš„ Hugging Chat [https://huggingface.co/chat/models/meta-llama/Meta-Llama-3.1-70b-instruct/](https://huggingface.co/chat/models/meta-llama/Meta-Llama-3.1-70b-instruct/)
- Llama 3.1 8B æ¼”ç¤ºçš„ Gradio é©±åŠ¨çš„ Space [https://huggingface.co/spaces/ysharma/Chat_with_Meta_llama3_1_8b](https://huggingface.co/spaces/ysharma/Chat_with_Meta_llama3_1_8b)

æ•´ä¸ªå †æ ˆéƒ½æ˜¯å¼€æºçš„ã€‚Hugging Chat ç”± [chat-ui](https://github.com/huggingface/chat-ui) å’Œ [text-generation-inference](https://github.com/huggingface/text-generation-inference) æä¾›æ”¯æŒã€‚

## Llama 3.1 405B çš„ FP8ã€AWQ å’Œ GPTQ é‡åŒ–

Meta åˆ›å»ºäº† [Llama 3.1 405B çš„å®˜æ–¹ FP8 é‡åŒ–ç‰ˆæœ¬](https://huggingface.co/meta-llama/Meta-Llama-3.1-405B-Instruct-FP8)ï¼Œç²¾åº¦æŸå¤±æœ€å°ã€‚ä¸ºå®ç°è¿™ä¸€ç›®æ ‡ï¼ŒFP8 é‡åŒ–ä»…åº”ç”¨äºæ¨¡å‹çš„ä¸»è¦çº¿æ€§è¿ç®—ç¬¦ï¼Œä¾‹å¦‚ FFNs çš„é—¨å’Œä¸Šå‡åŠä¸‹é™æŠ•å½± (æ¶µç›– 75% çš„æ¨ç† FLOPs)ã€‚æˆ‘ä»¬å…±åŒåŠªåŠ›ï¼Œç¡®ä¿æ­¤ FP8 é‡åŒ–æ£€æŸ¥ç‚¹åœ¨ç¤¾åŒºä¸­å…¼å®¹ (transformers, TGI, VLLM)ã€‚

æ­¤å¤–ï¼Œæˆ‘ä»¬ä½¿ç”¨ AutoAWQ å’Œ AutoGPTQ åˆ›å»ºäº† INT4 çš„ AWQ å’Œ GPTQ é‡åŒ–å˜ä½“ã€‚å¯¹äº AWQï¼Œæ‰€æœ‰çº¿æ€§å±‚éƒ½ä½¿ç”¨ GEMM å†…æ ¸è¿›è¡Œé‡åŒ–ï¼Œå°†é›¶ç‚¹é‡åŒ–åˆ° 4 ä½ï¼Œç»„å¤§å°ä¸º 128; å¯¹äº GPTQï¼Œç›¸åŒçš„è®¾ç½®ä»…ä½¿ç”¨ GPTQ å†…æ ¸ã€‚æˆ‘ä»¬ç¡®ä¿ INT4 æ£€æŸ¥ç‚¹ä¸ transformers å’Œ TGI å…¼å®¹ï¼ŒåŒ…æ‹¬ Marlin å†…æ ¸æ”¯æŒï¼Œä»¥åŠ å¿« TGI ä¸­ GPTQ é‡åŒ–çš„æ¨ç†é€Ÿåº¦ã€‚

å¯ç”¨çš„ Llama 3.1 405B çš„é‡åŒ–æƒé‡:

- [meta-llama/Meta-Llama-3.1-405B-Base-FP8](https://huggingface.co/meta-llama/Meta-Llama-3.1-405B-FP8): å®˜æ–¹ FP8 é‡åŒ–æƒé‡ï¼Œå¯åœ¨ 8xH100 ä¸Šè¿è¡Œ
- [meta-llama/Meta-Llama-3.1-405B-Instruct-FP8](https://huggingface.co/sllhf/Meta-Llama-3.1-405B-Instruct-FP8): å®˜æ–¹ FP8 é‡åŒ–æƒé‡ï¼Œå¯åœ¨ 8xH100 ä¸Šè¿è¡Œ
- [hugging-quants/Meta-Llama-3.1-405B-Instruct-AWQ-INT4](https://huggingface.co/hugging-quants/Meta-Llama-3.1-405B-Instruct-AWQ-INT4): Hugging Face é‡åŒ–æƒé‡ï¼Œå¯åœ¨ 8xA100 80GB, 8xH100 80GB å’Œ 8xA100 40GB (å‡å°‘ KV ç¼“å­˜ä¸”æ—  CUDA å›¾å½¢) ä¸Šè¿è¡Œ
- [hugging-quants/Meta-Llama-3.1-405B-Instruct-GPTQ-INT4:](https://huggingface.co/hugging-quants/Meta-Llama-3.1-405B-Instruct-GPTQ-INT4): Hugging Face é‡åŒ–æƒé‡ï¼Œå¯åœ¨ 8xA100 80GB, 8xH100 80GB å’Œ 8xA100 40GB (å‡å°‘ KV ç¼“å­˜ä¸”æ—  CUDA å›¾å½¢) ä¸Šè¿è¡Œ
- [hugging-quants/Meta-Llama-3.1-405B-BNB-NF4](https://huggingface.co/hugging-quants/Meta-Llama-3.1-405B-BNB-NF4): Hugging Face é‡åŒ–æƒé‡ï¼Œé€‚ç”¨äº QLoRA å¾®è°ƒ
- [hugging-quants/Meta-Llama-3.1-405B-Instruct-BNB-NF4](https://huggingface.co/hugging-quants/Meta-Llama-3.1-405B-Instruct-BNB-NF4): Hugging Face é‡åŒ–æƒé‡ï¼Œé€‚ç”¨äºåœ¨ 8xA100 å’Œ 4xH100 ä¸Šæ¨ç†

[Hugging Quants ç»„ç»‡](https://huggingface.co/hugging-quants) è¿˜åŒ…å« 70B å’Œ 8B ç‰ˆæœ¬çš„é‡åŒ–æ£€æŸ¥ç‚¹ã€‚

## æ¨ç†é›†æˆ

### Hugging Face æ¨ç† API

[Hugging Face PRO ç”¨æˆ·ç°åœ¨å¯ä»¥è®¿é—®ç‹¬å®¶ API ç«¯ç‚¹](https://huggingface.co/blog/inference-pro)ï¼Œæ‰˜ç®¡ Llama 3.1 8B Instructã€Llama 3.1 70B Instruct å’Œ Llama 3.1 405B Instruct AWQï¼Œç”± [text-generation-inference](https://github.com/huggingface/text-generation-inference) æä¾›æ”¯æŒã€‚æ‰€æœ‰ç‰ˆæœ¬éƒ½æ”¯æŒ Messages APIï¼Œå› æ­¤ä¸ OpenAI å®¢æˆ·ç«¯åº“å…¼å®¹ï¼ŒåŒ…æ‹¬ LangChain å’Œ LlamaIndexã€‚

_æ³¨æ„: ä½¿ç”¨ `pip install "huggingface_hub>=0.24.1"` æ›´æ–°åˆ°æœ€æ–°çš„ `huggingface_hub` ç‰ˆæœ¬ã€‚_

```python
from huggingface_hub import InferenceClient

# åˆå§‹åŒ–å®¢æˆ·ç«¯ï¼ŒæŒ‡å‘ä¸€ä¸ªå¯ç”¨çš„æ¨¡å‹
client = InferenceClient()

chat_completion = client.chat.completions.create(
    model="meta-llama/Meta-Llama-3.1-405B-Instruct-FP8",
    messages=[
        {"role": "system", "content": "You are a helpful and honest programming assistant."},
        {"role": "user", "content": "Is Rust better than Python?"},
    ],
    stream=True,
    max_tokens=500
)

# è¿­ä»£å¹¶æ‰“å°æµ
for message in chat_completion:
    print(message.choices[0].delta.content, end="")
```

æœ‰å…³ä½¿ç”¨ Messages API çš„æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·æŸ¥çœ‹ [æ­¤å¸–å­](https://huggingface.co/blog/tgi-messages-api)ã€‚

### Hugging Face æ¨ç†ç«¯ç‚¹

æ‚¨å¯ä»¥åœ¨ Hugging Face çš„ [æ¨ç†ç«¯ç‚¹](https://ui.endpoints.huggingface.co/) ä¸Šéƒ¨ç½² Llama 3.1ï¼Œå®ƒä½¿ç”¨ Text Generation Inference ä½œä¸ºåç«¯ã€‚Text Generation Inference æ˜¯ Hugging Face å¼€å‘çš„ç”Ÿäº§å°±ç»ªæ¨ç†å®¹å™¨ï¼Œæ”¯æŒ FP8ã€è¿ç»­æ‰¹å¤„ç†ã€token æµã€å¼ é‡å¹¶è¡Œï¼Œä»¥ä¾¿åœ¨å¤šä¸ª GPU ä¸Šå¿«é€Ÿæ¨ç†ã€‚è¦éƒ¨ç½² Llama 3.1ï¼Œè¯·è½¬åˆ° [æ¨¡å‹é¡µé¢](https://huggingface.co/meta-llama/Meta-Llama-3-70B-instruct) å¹¶ç‚¹å‡»éƒ¨ç½² -> æ¨ç†ç«¯ç‚¹å°éƒ¨ä»¶:

- [Meta-Llama-3.1-8B-Instruct](https://huggingface.co/meta-llama/Meta-Llama-3.1-8B-Instruct) æ¨èåœ¨ 1x NVIDIA A10G æˆ– L4 GPU ä¸Šè¿è¡Œ
- [Meta-Llama-3.1-70B-Instruct](https://huggingface.co/meta-llama/Meta-Llama-3.1-70B-Instruct) æ¨èåœ¨ 4x NVIDIA A100 æˆ–é‡åŒ–ä¸º AWQ/GPTQ åœ¨ 2x A100 ä¸Šè¿è¡Œ
- [Meta-Llama-3.1-405B-Instruct-FP8](https://huggingface.co/sllhf/Meta-Llama-3.1-405B-Instruct-FP8) æ¨èåœ¨ 8x NVIDIA H100 ä¸Šä»¥ FP è¿è¡Œæˆ–é‡åŒ–ä¸º [AWQ](https://huggingface.co/hugging-quants/Meta-Llama-3.1-405B-Instruct-AWQ-INT4)/[GPTQ](https://huggingface.co/hugging-quants/Meta-Llama-3.1-405B-Instruct-GPTQ-INT4) åœ¨ 8x A100 ä¸Šè¿è¡Œ

```python
from huggingface_hub import InferenceClient

# åˆå§‹åŒ–å®¢æˆ·ç«¯ï¼ŒæŒ‡å‘ä¸€ä¸ªå¯ç”¨çš„æ¨¡å‹
client = InferenceClient(
    base_url="<ENDPOINT_URL>",
)

# åˆ›å»ºä¸€ä¸ªèŠå¤©å®Œæˆ
chat_completion = client.chat.completions.create(
    model="ENDPOINT",
    messages=[
        {"role": "system", "content": "You are a helpful and honest programming assistant."},
        {"role": "user", "content": "Is Rust better than Python?"},
    ],
    stream=True,
    max_tokens=500
)

# è¿­ä»£å¹¶æ‰“å°æµ
for message in chat_completion:
    print(message.choices[0].delta.content, end="")
```

## Hugging Face åˆä½œä¼™ä¼´é›†æˆ

_æ³¨æ„: æˆ‘ä»¬ç›®å‰æ­£åœ¨ä¸æˆ‘ä»¬çš„åˆä½œä¼™ä¼´ AWSã€Google Cloudã€Microsoft Azure å’Œ DELL åˆä½œï¼Œå°† Llama 3.1 8Bã€70B å’Œ 405B æ·»åŠ åˆ° Amazon SageMakerã€Google Kubernetes Engineã€Vertex AI Model Catalogã€Azure AI Studioã€DELL Enterprise Hubã€‚æˆ‘ä»¬å°†åœ¨å®¹å™¨å¯ç”¨æ—¶æ›´æ–°æ­¤éƒ¨åˆ† - æ‚¨å¯ä»¥ [è®¢é˜… Hugging Squad ä»¥è·å–ç”µå­é‚®ä»¶æ›´æ–°](https://mailchi.mp/huggingface/squad)ã€‚_

## ä½¿ç”¨ Hugging Face TRL è¿›è¡Œå¾®è°ƒ

åœ¨æœ¬èŠ‚ä¸­ï¼Œæˆ‘ä»¬å°†æŸ¥çœ‹ Hugging Face ç”Ÿæ€ç³»ç»Ÿä¸­å¯ç”¨çš„å·¥å…·ï¼Œä»¥ä¾¿åœ¨æ¶ˆè´¹è€…çº§ GPU ä¸Šé«˜æ•ˆè®­ç»ƒ Llama 3.1ã€‚ä¸‹é¢æ˜¯ä¸€ä¸ªç¤ºä¾‹å‘½ä»¤ï¼Œç”¨äºåœ¨ OpenAssistant çš„ [chat æ•°æ®é›†](https://huggingface.co/datasets/OpenAssistant/oasst_top1_2023-08-25) ä¸Šå¾®è°ƒ Llama 3.1 8Bã€‚æˆ‘ä»¬ä½¿ç”¨ 4 ä½é‡åŒ–å’Œ [QLoRA](https://arxiv.org/abs/2305.14314) æ¥èŠ‚çœå†…å­˜ï¼Œä»¥é’ˆå¯¹æ‰€æœ‰æ³¨æ„åŠ›å—çš„çº¿æ€§å±‚ã€‚

<details close>
<summary>ä½¿ç”¨ Hugging Face TRL çš„å¾®è°ƒç¤ºä¾‹</summary>

é¦–å…ˆï¼Œå®‰è£…æœ€æ–°ç‰ˆæœ¬çš„ ğŸ¤— TRL å¹¶å…‹éš† repo ä»¥è®¿é—® [è®­ç»ƒè„šæœ¬](https://github.com/huggingface/trl/blob/main/examples/scripts/sft.py):

```
pip install "transformers>=4.43" --upgrade
pip install --upgrade bitsandbytes
pip install --ugprade peft
pip install git+https://github.com/huggingface/trl
git clone https://github.com/huggingface/trl
cd trl
```

ç„¶åä½ å¯ä»¥è¿è¡Œè„šæœ¬:

```
python \
    examples/scripts/sft.py \
    --model_name meta-llama/Meta-Llama-3.1-8B \
    --dataset_name OpenAssistant/oasst_top1_2023-08-25 \
    --dataset_text_field="text" \
    --per_device_train_batch_size 1 \
    --per_device_eval_batch_size 1 \
    --gradient_accumulation_steps 4 \
    --learning_rate 2e-4 \
    --report_to "none" \
    --bf16 \
    --max_seq_length 1024 \
    --lora_r 16 --lora_alpha 32 \
    --lora_target_modules q_proj k_proj v_proj o_proj \
    --load_in_4bit \
    --use_peft \
    --attn_implementation "flash_attention_2" \
    --logging_steps=10 \
    --gradient_checkpointing \
    --output_dir llama31
```

å¦‚æœæ‚¨æœ‰æ›´å¤šçš„ GPUï¼Œå¯ä»¥ä½¿ç”¨ DeepSpeed å’Œ ZeRO Stage 3 è¿è¡Œè®­ç»ƒ:

```
accelerate launch --config_file=examples/accelerate_configs/deepspeed_zero3.yaml \
    examples/scripts/sft.py \
    --model_name meta-llama/Meta-Llama-3.1-8B \
    --dataset_name OpenAssistant/oasst_top1_2023-08-25 \
    --dataset_text_field="text" \
    --per_device train batch_size 1 \
    --per_device_eval_batch_size 1 \
    --gradient_accumulation_steps 4 \
    --learning_rate 2e-5 \
    --report_to wandb \
    --bf16 \
    --max_seq_length 1024 \
    --attn_implementation eager \
    --logging_steps=10 \
    --gradient_checkpointing \
    --output_dir models/llama
```

</details>

## ä½¿ç”¨ distilabel ç”Ÿæˆåˆæˆæ•°æ®

Llama 3.1 è®¸å¯è¯çš„ä¸€ä¸ªé‡å¤§å˜åŒ–æ˜¯ï¼Œå®ƒå…è®¸ä½¿ç”¨æ¨¡å‹è¾“å‡ºæ¥æ”¹è¿›å…¶ä»– LLMï¼Œè¿™æ„å‘³ç€æ‚¨å¯ä»¥ä½¿ç”¨ Llama 3.1 æ¨¡å‹ç”Ÿæˆåˆæˆæ•°æ®é›†ï¼Œå¹¶ä½¿ç”¨å®ƒä»¬æ¥å¾®è°ƒæ›´å°ã€æ›´ä¸“ä¸šçš„æ¨¡å‹ã€‚

è®©æˆ‘ä»¬çœ‹ä¸€ä¸ªç¤ºä¾‹ï¼Œå¦‚ä½•ä½¿ç”¨ [distilabel](https://github.com/argilla-io/distilabel)ï¼Œä¸€ä¸ªç”¨äºç”Ÿæˆåˆæˆæ•°æ®çš„å¼€æºæ¡†æ¶ï¼Œç”Ÿæˆä¸€ä¸ªåå¥½æ•°æ®é›†ã€‚è¯¥æ•°æ®é›†å¯ç”¨äºä½¿ç”¨ TRL æä¾›çš„åå¥½ä¼˜åŒ–æ–¹æ³• (å¦‚ DPO æˆ– KTO) å¾®è°ƒæ¨¡å‹ã€‚

é¦–å…ˆå®‰è£…æœ€æ–°çš„ `distilabel` ç‰ˆæœ¬ï¼ŒåŒ…æ‹¬ `hf-inference-endpoints` é¢å¤–ç»„ä»¶ï¼Œä½¿ç”¨ `pip` å¦‚ä¸‹:

```bash
pip install â€œdistilabel[hf-inference-endpoints]â€ --upgrade
```

ç„¶åå®šä¹‰ä¸€ä¸ªç®¡é“:

- ä» Hugging Face Hub åŠ è½½å¸¦æœ‰æŒ‡ä»¤çš„æ•°æ®é›†ã€‚
- ä½¿ç”¨ Hugging Face æ¨ç†ç«¯ç‚¹ï¼Œé€šè¿‡ Llama 3.1 70B Instruct å’Œ Llama 3.1 405B Instruct ç”Ÿæˆå“åº”ã€‚
- æœ€åï¼Œä½¿ç”¨ Llama 3.1 405B Instruct ä½œä¸ºè£åˆ¤ï¼Œä½¿ç”¨ UltraFeedback æç¤ºå¯¹å“åº”è¿›è¡Œè¯„åˆ†ã€‚ä»è¿™äº›è¯„åˆ†ä¸­ï¼Œå¯ä»¥é€‰æ‹©å’Œæ‹’ç»å“åº”ï¼Œå¹¶ä½¿ç”¨åå¥½ä¼˜åŒ–æ–¹æ³•å¾®è°ƒæ¨¡å‹ã€‚

è¯·å‚é˜…ä¸‹é¢çš„ä»£ç ä»¥å®šä¹‰ç®¡é“ï¼Œæˆ–ä½¿ç”¨æ­¤ [Colab ç¬”è®°æœ¬](https://colab.research.google.com/drive/1o0ALge7DHBmcKgdyrk59yOL70tcGS3v4?usp=sharing) è‡ªè¡Œè¿è¡Œå¹¶æ¢ç´¢ç”Ÿæˆçš„æ•°æ®é›†ã€‚

```python
from distilabel.llms import InferenceEndpointsLLM
from distilabel.pipeline import Pipeline
from distilabel.steps import LoadDataFromHub, CombineColumns
from distilabel.steps.tasks import TextGeneration, UltraFeedback

llama70B = InferenceEndpointsLLM(
    model_id="meta-llama/Meta-Llama-3.1-70B-Instruct"
)
llama405B = InferenceEndpointsLLM(
    model_id="meta-llama/Meta-Llama-3.1-405B-Instruct-FP8"
)

with Pipeline(name="synthetic-data-with-llama3") as pipeline:
    # åŠ è½½å¸¦æœ‰æç¤ºçš„æ•°æ®é›†
    load_dataset = LoadDataFromHub(
        repo_id="argilla/10Kprompts-mini"
    )
    # ä¸ºæ¯ä¸ªæç¤ºç”Ÿæˆä¸¤ä¸ªå“åº”
    generate = [
        TextGeneration(llm=llama70B),
        TextGeneration(llm=llama405B)
    ]
    # å°†å“åº”ç»„åˆåˆ°ä¸€ä¸ªåˆ—ä¸­
    combine = CombineColumns(
        columns=["generation", "model_name"],
        output_columns=["generations", "model_names"]
    )
    # ä½¿ç”¨ 405B LLM-as-a-judge å¯¹å“åº”è¿›è¡Œè¯„åˆ†
    rate = UltraFeedback(aspect="overall-rating", llm=llama405B)
    # å®šä¹‰ç®¡é“
    load_dataset >> generate >> combine >> rate

if __name__ == "__main__":
    distiset = pipeline.run()
```

æ¥ä¸‹æ¥æ˜¯ä»€ä¹ˆï¼Ÿé™¤äº†ä¸Šè¿°ç¤ºä¾‹ï¼Œ `distilabel` è¿˜æä¾›äº†ä½¿ç”¨ LLM åœ¨å¹¿æ³›çš„åœºæ™¯å’Œä¸»é¢˜ä¸­ç”Ÿæˆåˆæˆæ•°æ®çš„ä»¤äººå…´å¥‹çš„æ–¹æ³•ã€‚å®ƒåŒ…æ‹¬å½“å‰ SOTA æ–‡çŒ®ä¸­çš„å®ç°ï¼Œç”¨äºä»»åŠ¡å¦‚ä½¿ç”¨ LLM-as-a-judge æ–¹æ³•è¯„ä¼°è¾“å‡ºã€è¿›åŒ–æŒ‡ä»¤ã€æ•°æ®è¿‡æ»¤ä»¥åŠå®šä¹‰è‡ªå®šä¹‰ç»„ä»¶ã€‚

## é™„åŠ èµ„æº

- [Hub ä¸Šçš„æ¨¡å‹](https://huggingface.co/collections/meta-llama/llama-31-669fc079a0c406a149a5738f)
- [Hugging Face Llama Recipes](https://github.com/huggingface/huggingface-llama-recipes)
- [Open LLM Leaderboard](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard)
- [Llama 3.1 405B Instruct çš„ Hugging Chat æ¼”ç¤º](https://huggingface.co/chat/models/meta-llama/Meta-Llama-3.1-405b-instruct/)
- [Meta åšå®¢](https://ai.meta.com/blog/meta-llama-3-1/)

## è‡´è°¢

æ²¡æœ‰æˆåƒä¸Šä¸‡ç¤¾åŒºæˆå‘˜å¯¹ transformersã€tgiã€vllmã€pytorchã€LM Eval Harness å’Œè®¸å¤šå…¶ä»–é¡¹ç›®çš„è´¡çŒ®ï¼Œè¿™äº›æ¨¡å‹çš„å‘å¸ƒå’Œç”Ÿæ€ç³»ç»Ÿä¸­çš„æ”¯æŒä¸è¯„ä¼°æ˜¯ä¸å¯èƒ½å®ç°çš„ã€‚è¿™æ¬¡å‘å¸ƒç¦»ä¸å¼€ [ClÃ©mentine](https://huggingface.co/clefourrier) å’Œ [Nathan](https://huggingface.co/SaylorTwift) å¯¹ LLM è¯„ä¼°çš„æ”¯æŒ; [Nicolas](https://huggingface.co/Narsil)ã€[Olivier Dehaene](https://huggingface.co/olivierdehaene) å’Œ [DaniÃ«l de Kok](https://huggingface.co/danieldk) å¯¹ Text Generation Inference æ”¯æŒçš„è´¡çŒ®; [Arthur](https://huggingface.co/ArthurZ)ã€[Matthew Carrigan](https://huggingface.co/Rocketknight1)ã€[Zachary Mueller](https://huggingface.co/muellerzr)ã€[Joao](https://huggingface.co/joaogante)ã€[Joshua Lochner](https://huggingface.co/Xenova) å’Œ [Lysandre](https://huggingface.co/lysandre) å¯¹ Llama 3.1 é›†æˆåˆ° `transformers` çš„è´¡çŒ®; [Matthew Douglas](https://huggingface.co/mdouglas) å¯¹é‡åŒ–æ”¯æŒçš„è´¡çŒ®; [Gabriel MartÃ­n BlÃ¡zquez](https://huggingface.co/gabrielmbmb) å¯¹ `distilabel` æ”¯æŒçš„è´¡çŒ®; [Merve Noyan](https://huggingface.co/merve) å’Œ [Aymeric Roucher](https://huggingface.co/m-ric) å¯¹å®¡æ ¸çš„è´¡çŒ®; [hysts](huggingface.co/hysts) å’Œ [Yuvi](huggingface.co/ysharma) å¯¹æ¼”ç¤ºçš„è´¡çŒ®; [Ellie](https://huggingface.co/eliebak) å¯¹å¾®è°ƒæµ‹è¯•çš„è´¡çŒ®; [Brigitte Tousignant](https://huggingface.co/BrigitteTousi) å’Œ [Florent Daudens](https://huggingface.co/fdaudens) å¯¹æ²Ÿé€šçš„è´¡çŒ®; [Nathan](https://huggingface.co/nsarrazin) å’Œ [Victor](https://huggingface.co/victor) å¯¹ Hugging Chat ä¸­ Llama 3.1 çš„å¯ç”¨æ€§çš„è´¡çŒ®ã€‚

æ„Ÿè°¢ Meta å›¢é˜Ÿå‘å¸ƒ Llama 3.1 å¹¶ä½¿å…¶åœ¨å¼€æº AI ç¤¾åŒºä¸­å¯ç”¨ï¼
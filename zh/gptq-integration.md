---
title: "ä½¿ç”¨ AutoGPTQ å’Œ transformers è®©å¤§è¯­è¨€æ¨¡å‹æ›´è½»é‡åŒ–" 
thumbnail: /blog/assets/159_autogptq_transformers/thumbnail.jpg
authors:
- user: marcsun13
- user: fxmarty
- user: PanEa
  guest: true
- user: qwopqwop
  guest: true
- user: ybelkada
- user: TheBloke
  guest: true
translators:
- user: PanEa
  guest: true
---

# ä½¿ç”¨ AutoGPTQ å’Œ transformers è®©å¤§è¯­è¨€æ¨¡å‹æ›´è½»é‡åŒ–

å¤§è¯­è¨€æ¨¡å‹åœ¨ç†è§£å’Œç”Ÿæˆäººç±»æ°´å¹³çš„æ–‡å­—æ–¹é¢æ‰€å±•ç°å‡ºçš„éå‡¡èƒ½åŠ›ï¼Œæ­£åœ¨è®¸å¤šé¢†åŸŸå¸¦æ¥åº”ç”¨ä¸Šçš„é©æ–°ã€‚ç„¶è€Œï¼Œåœ¨æ¶ˆè´¹çº§ç¡¬ä»¶ä¸Šè®­ç»ƒå’Œéƒ¨ç½²å¤§è¯­è¨€æ¨¡å‹çš„éœ€æ±‚ä¹Ÿå˜å¾—è¶Šæ¥è¶Šéš¾ä»¥æ»¡è¶³ã€‚

ğŸ¤— Hugging Face çš„æ ¸å¿ƒä½¿å‘½æ˜¯ _è®©ä¼˜ç§€çš„æœºå™¨å­¦ä¹ å¤§ä¼—åŒ–_ (TODO: æ˜¯å¦æœ‰å®˜æ–¹æ­£å¼çš„ä¸­è¯‘ç‰ˆæœ¬ï¼Ÿ)ï¼Œè€Œè¿™æ­£åŒ…æ‹¬äº†å°½å¯èƒ½åœ°è®©æ‰€æœ‰äººéƒ½èƒ½å¤Ÿä½¿ç”¨ä¸Šå¤§æ¨¡å‹ã€‚æœ¬ç€[ä¸ bitsandbytes åˆä½œ](https://huggingface.co/blog/4bit-transformers-bitsandbytes) ä¸€æ ·çš„ç²¾ç¥ï¼Œæˆ‘ä»¬å°† [AutoGPTQ](https://github.com/PanQiWei/AutoGPTQ) ä»£ç åº“é›†æˆåˆ°äº† Transformers ä¸­ï¼Œè®©ç”¨æˆ·ä½¿ç”¨ GPTQ ç®—æ³• ([Frantar et al. 2023](https://arxiv.org/pdf/2210.17323.pdf)) åœ¨8ä½ã€4ä½ã€3ä½ï¼Œç”šè‡³æ˜¯2ä½ç²¾åº¦ä¸‹é‡åŒ–å’Œè¿è¡Œæ¨¡å‹æˆä¸ºå¯èƒ½ã€‚å½“ä½¿ç”¨4ä½é‡åŒ–æ—¶ï¼Œç²¾åº¦çš„ä¸‹é™å¯ä»¥å¿½ç•¥ä¸è®¡ï¼ŒåŒæ—¶åœ¨å°æ‰¹é‡æ¨ç†ä¸Šä¿æŒç€ä¸ `fp16` åŸºçº¿ç›¸å½“çš„é€Ÿåº¦ã€‚ éœ€è¦æ³¨æ„çš„æ˜¯ï¼ŒGPTQ æ–¹æ³•ä¸ bitsandbytes æå‡ºçš„è®­ç»ƒåé‡åŒ–æ–¹æ³•æœ‰æ‰€ä¸åŒï¼šå®ƒéœ€è¦åœ¨é‡åŒ–é˜¶æ®µæä¾›ä¸€ä¸ªæ ¡å‡†æ•°æ®é›†ã€‚

è¿™ä¸€é›†æˆæ”¯æŒè‹±ä¼Ÿè¾¾ GPU å’ŒåŸºäº RoCm çš„ AMD GPUã€‚

## ç›®å½•

- [ç›¸å…³èµ„æº](#ç›¸å…³èµ„æº)
- [**GPTQ è®ºæ–‡æ€»ç»“**](#--gptq-è®ºæ–‡æ€»ç»“--)
- [AutoGPTQ ä»£ç åº“â€”â€”ä¸€ç«™å¼åœ°å°† GPTQ æ–¹æ³•åº”ç”¨äºå¤§è¯­è¨€æ¨¡å‹](#autogptq-ä»£ç åº“â€”â€”ä¸€ç«™å¼åœ°å°†-gptq-æ–¹æ³•åº”ç”¨äºå¤§è¯­è¨€æ¨¡å‹)
- [ğŸ¤— Transformers å¯¹ GPTQ æ¨¡å‹çš„æœ¬åœ°åŒ–æ”¯æŒ](#---transformers-å¯¹-gptq-æ¨¡å‹çš„æœ¬åœ°åŒ–æ”¯æŒ)
- [ä½¿ç”¨ **Optimum ä»£ç åº“** é‡åŒ–æ¨¡å‹](#ä½¿ç”¨---optimum-ä»£ç åº“---é‡åŒ–æ¨¡å‹)
- [é€šè¿‡ ***Text-Generation-Inference*** ä½¿ç”¨ GPTQ æ¨¡å‹](#é€šè¿‡----text-generation-inference----ä½¿ç”¨-gptq-æ¨¡å‹)
- [**ä½¿ç”¨ PEFT å¾®è°ƒé‡åŒ–åçš„æ¨¡å‹**](#--ä½¿ç”¨-peft-å¾®è°ƒé‡åŒ–åçš„æ¨¡å‹--)
- [æ”¹è¿›ç©ºé—´](#æ”¹è¿›ç©ºé—´)
  * [å·²æ”¯æŒçš„æ¨¡å‹](#å·²æ”¯æŒçš„æ¨¡å‹)
- [ç»“è®ºå’Œç»“è¯­](#ç»“è®ºå’Œç»“è¯­)
- [è‡´è°¢](#è‡´è°¢)


## ç›¸å…³èµ„æº

æœ¬æ–‡åŠæœ‰å…³ç‰ˆæœ¬å‘å¸ƒæä¾›äº†ä¸€äº›èµ„æºæ¥å¸®åŠ©ç”¨æˆ·å¼€å¯ GPTQ é‡åŒ–çš„æ—…ç¨‹ï¼š

- [åŸå§‹è®ºæ–‡](https://arxiv.org/pdf/2210.17323.pdf)
- [è¿è¡Œäº Google Colab ç¬”è®°æœ¬ä¸Šçš„åŸºç¡€ç”¨ä¾‹](https://colab.research.google.com/drive/1_TIrmuKOFhuRRiTWN94iLKUFu6ZX4ceb?usp=sharing) â€”â€” è¯¥ç¬”è®°æœ¬ä¸Šçš„ç”¨ä¾‹å±•ç¤ºäº†å¦‚ä½•ä½¿ç”¨ GPTQ æ–¹æ³•é‡åŒ–ä½ çš„ transformers æ¨¡å‹ã€å¦‚ä½•è¿›è¡Œé‡åŒ–æ¨¡å‹çš„æ¨ç†ï¼Œä»¥åŠå¦‚ä½•ä½¿ç”¨é‡åŒ–åçš„æ¨¡å‹è¿›è¡Œå¾®è°ƒã€‚
- Transformers ä¸­é›†æˆ GPTQ çš„[è¯´æ˜æ–‡æ¡£](https://huggingface.co/docs/transformers/main/en/main_classes/quantization)
- Optimum ä¸­é›†æˆ GPTQ çš„[è¯´æ˜æ–‡æ¡£](https://huggingface.co/docs/optimum/llm_quantization/usage_guides/quantization)
- TheBloke [æ¨¡å‹ä»“åº“](https://huggingface.co/TheBloke?sort_models=likes#models) ä¸­çš„ GPTQ æ¨¡å‹ã€‚


## **GPTQ è®ºæ–‡æ€»ç»“**

é€šå¸¸ï¼Œé‡åŒ–æ–¹æ³•å¯ä»¥åˆ†ä¸ºä»¥ä¸‹ä¸¤ç±»ï¼š

1. è®­ç»ƒåé‡åŒ– (Post Training Quantization, PTQ)ï¼šé€‚åº¦åœ°ä½¿ç”¨ä¸€äº›èµ„æºæ¥é‡åŒ–é¢„è®­ç»ƒå¥½çš„æ¨¡å‹ï¼Œå¦‚ä¸€ä¸ªæ ¡å‡†æ•°æ®é›†å’Œå‡ å°æ—¶çš„ç®—åŠ›ã€‚
2. é‡åŒ–æ„ŸçŸ¥è®­ç»ƒ (Quantization Aware Training, QAT)ï¼šåœ¨è®­ç»ƒæˆ–è¿›ä¸€æ­¥å¾®è°ƒä¹‹å‰æ‰§è¡Œé‡åŒ–ã€‚

GPTQ å±äºè®­ç»ƒåé‡åŒ–ï¼Œè¿™å¯¹äºå¤§æ¨¡å‹è€Œè¨€æ ¼å¤–æœ‰æ„ä¹‰(TODO: è¿˜æ˜¯éµå¾ªåŸæ–‡çš„ interesting ç¿»è¯‘æˆæœ‰è¶£ï¼Ÿ)ï¼Œå› ä¸ºå¯¹å…¶è¿›è¡Œå…¨å‚æ•°çš„è®­ç»ƒç”šè‡³ä»…ä»…æ˜¯å¾®è°ƒéƒ½ååˆ†æ˜‚è´µã€‚

å…·ä½“è€Œè¨€ï¼ŒGPTQ é‡‡ç”¨ int4/fp16 (W4A16) çš„æ··åˆé‡åŒ–æ–¹æ¡ˆï¼Œå…¶ä¸­æ¨¡å‹æƒé‡è¢«é‡åŒ–ä¸º int4 æ•°å€¼ç±»å‹ï¼Œè€Œæ¿€æ´»å€¼åˆ™ä¿ç•™åœ¨ float16ã€‚åœ¨æ¨ç†é˜¶æ®µï¼Œæ¨¡å‹æƒé‡è¢«åŠ¨æ€åœ°åé‡åŒ–å› float16 å¹¶åœ¨è¯¥æ•°å€¼ç±»å‹ä¸‹è¿›è¡Œå®é™…çš„è¿ç®—ã€‚

è¯¥æ–¹æ¡ˆæœ‰ä»¥ä¸‹ä¸¤æ–¹é¢çš„ä¼˜ç‚¹ï¼š

- int4 é‡åŒ–èƒ½å¤ŸèŠ‚çœæ¥è¿‘4å€çš„å†…å­˜ï¼Œè¿™æ˜¯å› ä¸ºåé‡åŒ–æ“ä½œå‘ç”Ÿåœ¨ç®—å­çš„è®¡ç®—å•å…ƒé™„è¿‘ï¼Œè€Œä¸æ˜¯åœ¨ GPU çš„å…¨å±€å†…å­˜ä¸­ã€‚
- ç”±äºç”¨äºæƒé‡çš„ä½å®½è¾ƒä½ï¼Œå› æ­¤å¯ä»¥èŠ‚çœæ•°æ®é€šä¿¡çš„æ—¶é—´ï¼Œä»è€Œæ½œåœ¨åœ°æå‡äº†æ¨ç†é€Ÿåº¦ã€‚

GPTQ è®ºæ–‡è§£å†³äº†åˆ†å±‚å‹ç¼©çš„é—®é¢˜ï¼š

ç»™å®šä¸€ä¸ªæ‹¥æœ‰æƒé‡çŸ©é˜µ $W_{l}$ å’Œè¾“å…¥ $X_{l}$ çš„ç½‘ç»œå±‚ $l$ï¼Œæˆ‘ä»¬æœŸæœ›è·å¾—ä¸€ä¸ªé‡åŒ–ç‰ˆæœ¬çš„æƒé‡çŸ©é˜µ $\hat{W}_{l}$ ä»¥æœ€å°åŒ–å‡æ–¹è¯¯å·®ï¼ˆMSEï¼‰ï¼š

\\({\hat{W}_{l}}^{*} = argmin_{\hat{W_{l}}} \|W_{l}X-\hat{W}_{l}X\|^{2}_{2})

ä¸€æ—¦æ¯å±‚å®ç°äº†ä¸Šè¿°ç›®æ ‡ï¼Œå°±å¯ä»¥é€šè¿‡ç»„åˆå„ç½‘ç»œå±‚é‡åŒ–æ–¹æ¡ˆçš„æ–¹å¼æ¥è·å¾—å…¨æ¨¡å‹çš„é‡åŒ–æ–¹æ¡ˆã€‚

ä¸ºè§£å†³è¿™ä¸€åˆ†å±‚å‹ç¼©é—®é¢˜ï¼Œè®ºæ–‡ä½œè€…é‡‡ç”¨äº†æœ€ä¼˜è„‘é‡åŒ– (Optimal Brain Quantization, OBQ) æ¡†æ¶ ([Frantar et al 2022](https://arxiv.org/abs/2208.11580)) ã€‚OBQ æ–¹æ³•çš„å‡ºå‘ç‚¹åœ¨äºå…¶è§‚å¯Ÿåˆ°ï¼šä»¥ä¸Šç­‰å¼å¯ä»¥æ”¹å†™æˆæƒé‡çŸ©é˜µ $W_{l}$ æ¯ä¸€è¡Œçš„å¹³æ–¹è¯¯å·®ä¹‹å’Œ

\\( \sum_{i=0}^{d_{row}} \|W_{l[i,:]}X-\hat{W}_{l[i,:]}X\|^{2}_{2} )

è¿™æ„å‘³ç€æˆ‘ä»¬å¯ä»¥ç‹¬ç«‹åœ°å¯¹æ¯ä¸€è¡Œæ‰§è¡Œé‡åŒ–ã€‚å³æ‰€è°“çš„ per-channel quantizationã€‚å¯¹æ¯ä¸€è¡Œ $W_{l[i,:]}$ï¼ŒOBQ åœ¨æ¯ä¸€æ—¶åˆ»åªé‡åŒ–ä¸€ä¸ªæƒé‡ï¼ŒåŒæ—¶æ›´æ–°æ‰€æœ‰æœªè¢«é‡åŒ–çš„æƒé‡ï¼Œä»¥è¡¥å¿é‡åŒ–å•ä¸ªæƒé‡æ‰€å¸¦æ¥çš„è¯¯å·®ã€‚æ‰€é€‰æƒé‡çš„æ›´æ–°é‡‡ç”¨ä¸€ä¸ªé—­ç¯å…¬å¼ï¼Œå¹¶åˆ©ç”¨äº†æµ·æ£®çŸ©é˜µ (Hessian Matrices)ã€‚

GPTQ è®ºæ–‡é€šè¿‡å¼•å…¥ä¸€ç³»åˆ—ä¼˜åŒ–æªæ–½æ¥æ”¹è¿›ä¸Šè¿°é‡åŒ–æ¡†æ¶ï¼Œåœ¨é™ä½é‡åŒ–ç®—æ³•å¤æ‚åº¦çš„åŒæ—¶ä¿ç•™äº†æ¨¡å‹çš„ç²¾åº¦ã€‚

ç›¸è¾ƒäº OBQï¼ŒGPTQ çš„é‡åŒ–æ­¥éª¤æœ¬èº«ä¹Ÿæ›´å¿«ï¼šOBQ éœ€è¦èŠ±è´¹2ä¸ª GPU æ—¶æ¥å®Œæˆ BERT æ¨¡å‹ (336M) çš„é‡åŒ–ï¼Œè€Œä½¿ç”¨ GPTQï¼Œé‡åŒ–ä¸€ä¸ª Bloom æ¨¡å‹ (176B) åˆ™åªéœ€ä¸åˆ°4ä¸ª GPU æ—¶ã€‚

ä¸ºäº†è§£ç®—æ³•çš„æ›´å¤šç»†èŠ‚ä»¥åŠåœ¨å›°æƒ‘åº¦ (perplexity, PPL) æŒ‡æ ‡å’Œæ¨ç†é€Ÿåº¦ä¸Šçš„ä¸åŒæµ‹è¯„æ•°æ®ï¼Œå¯æŸ¥é˜…åŸå§‹[è®ºæ–‡](https://arxiv.org/pdf/2210.17323.pdf) ã€‚

## AutoGPTQ ä»£ç åº“â€”â€”ä¸€ç«™å¼åœ°å°† GPTQ æ–¹æ³•åº”ç”¨äºå¤§è¯­è¨€æ¨¡å‹

AutoGPTQ ä»£ç åº“ä½¿å¾—ç”¨æˆ·èƒ½å¤Ÿä½¿ç”¨ GPTQ æ–¹æ³•é‡åŒ– ğŸ¤— Transformers ä¸­æ”¯æŒçš„å¤§é‡æ¨¡å‹ï¼Œè€Œç¤¾åŒºä¸­çš„å…¶ä»–å¹³è¡Œå·¥ä½œå¦‚ [GPTQ-for-LLaMa](https://github.com/qwopqwop200/GPTQ-for-LLaMa) ã€[Exllama](https://github.com/turboderp/exllama) å’Œ [llama.cpp](https://github.com/ggerganov/llama.cpp/) åˆ™ä¸»è¦é’ˆå¯¹ Llama æ¨¡å‹æ¶æ„å®ç°é‡åŒ–ç­–ç•¥ã€‚ç›¸è¾ƒä¹‹ä¸‹ï¼ŒAutoGPTQ å› å…¶å¯¹ä¸°å¯Œçš„ transformers æ¶æ„çš„å¹³æ»‘è¦†ç›–è€Œå¹¿å—æ¬¢è¿ã€‚

æ­£å› ä¸º AutoGPTQ ä»£ç åº“è¦†ç›–äº†å¤§é‡çš„ transformers æ¨¡å‹ï¼Œæˆ‘ä»¬å†³å®šæä¾›ä¸€ä¸ª ğŸ¤— Transformers çš„ API é›†æˆï¼Œè®©æ¯ä¸ªäººéƒ½èƒ½å¤Ÿæ›´å®¹æ˜“åœ°æ¥è§¦åˆ°å¤§è¯­è¨€æ¨¡å‹é‡åŒ–ã€‚æˆªæ­¢ç›®å‰ï¼Œæˆ‘ä»¬å·²ç»é›†æˆäº†åŒ…æ‹¬ CUDA ç®—å­åœ¨å†…çš„æœ€å¸¸ç”¨çš„ä¼˜åŒ–é€‰é¡¹ã€‚å¯¹äºæ›´å¤šé«˜çº§é€‰é¡¹å¦‚ Triton ç®—å­å’Œï¼ˆæˆ–ï¼‰å…¼å®¹æ³¨æ„åŠ›çš„ç®—å­èåˆï¼Œè¯·æŸ¥çœ‹ [AutoGPTQ](https://github.com/PanQiWei/AutoGPTQ) ä»£ç åº“ã€‚

## ğŸ¤— Transformers å¯¹ GPTQ æ¨¡å‹çš„æœ¬åœ°åŒ–æ”¯æŒ

åœ¨[å®‰è£… AutoGPTQ ä»£ç åº“](https://github.com/PanQiWei/AutoGPTQ#quick-installation) å’Œ `optimum` (`pip install optimum`) ä¹‹åï¼Œåœ¨ Transformers ä¸­è¿è¡Œ GPTQ æ¨¡å‹å°†éå¸¸ç®€å•ï¼š

```python
from transformers import AutoModelForCausalLM

model = AutoModelForCausalLM.from_pretrained("TheBloke/Llama-2-7b-Chat-GPTQ", torch_dtype=torch.float16, device_map="auto")
```

è¯·æŸ¥é˜… Transformers çš„[è¯´æ˜æ–‡æ¡£](https://huggingface.co/docs/transformers/main/en/main_classes/quantization) ä»¥äº†è§£æœ‰å…³æ‰€æœ‰ç‰¹æ€§çš„æ›´å¤šä¿¡æ¯ã€‚

æˆ‘ä»¬çš„ AutoGPTQ é›†æˆæœ‰ä»¥ä¸‹è¯¸å¤šä¼˜ç‚¹ï¼š

- é‡åŒ–æ¨¡å‹å¯è¢«åºåˆ—åŒ–å¹¶åœ¨ Hugging Face Hub ä¸Šåˆ†äº«ã€‚
- GPTQ æ–¹æ³•å¤§å¤§é™ä½è¿è¡Œå¤§è¯­è¨€æ¨¡å‹æ‰€éœ€çš„å†…å­˜ï¼ŒåŒæ—¶ä¿æŒç€ä¸ FP16 ç›¸å½“çš„æ¨ç†å»¶è¿Ÿã€‚
- AutoGPTQ åœ¨æ›´å¹¿æ³›çš„ transformers æ¶æ„ä¸Šæ”¯æŒ Exllama ç®—å­ã€‚
- è¯¥é›†æˆå¸¦æœ‰åŸºäº RoCm çš„ AMD GPU çš„æœ¬åœ°åŒ–æ”¯æŒã€‚
- èƒ½å¤Ÿ[**ä½¿ç”¨ PEFT å¾®è°ƒé‡åŒ–åçš„æ¨¡å‹**](#--ä½¿ç”¨-peft-å¾®è°ƒé‡åŒ–åçš„æ¨¡å‹--) ã€‚

ä½ å¯ä»¥åœ¨ Hugging Face Hub ä¸ŠæŸ¥æ‰¾ä½ æ‰€å–œçˆ±çš„æ¨¡å‹æ˜¯å¦å·²ç»æ‹¥æœ‰ GPTQ é‡åŒ–ç‰ˆæœ¬ã€‚TheBlokeï¼ŒHugging Face çš„é¡¶çº§è´¡çŒ®è€…ä¹‹ä¸€ï¼Œå·²ç»ä½¿ç”¨ AutoGPTQ é‡åŒ–äº†å¤§é‡çš„æ¨¡å‹å¹¶åˆ†äº«åœ¨ Hugging Face Hub ä¸Šã€‚åœ¨æˆ‘ä»¬çš„å…±åŒåŠªåŠ›ä¸‹ï¼Œè¿™äº›æ¨¡å‹ä»“åº“éƒ½å°†å¯ä»¥ä¸æˆ‘ä»¬çš„é›†æˆä¸€èµ·å¼€ç®±å³ç”¨ã€‚


ä»¥ä¸‹æ˜¯ä¸€ä¸ªä½¿ç”¨ batch size = 1 çš„æµ‹è¯„ç»“æœç¤ºä¾‹ã€‚è¯¥æµ‹è¯„ç»“æœé€šè¿‡åœ¨è‹±ä¼Ÿè¾¾ A100-SXM4-80GB GPU ä¸Šè¿è¡Œå¾—åˆ°ã€‚æˆ‘ä»¬ä½¿ç”¨é•¿åº¦ä¸º512ä¸ªè¯å…ƒçš„æç¤ºæ–‡æœ¬ï¼Œå¹¶ç²¾ç¡®åœ°ç”Ÿæˆ512ä¸ªæ–°è¯å…ƒã€‚è¡¨æ ¼çš„ç¬¬ä¸€è¡Œå±•ç¤ºçš„æ˜¯æœªé‡åŒ–çš„ `fp16` åŸºçº¿ï¼Œå¦å¤–ä¸¤è¡Œåˆ™å±•ç¤ºä½¿ç”¨ AutoGPTQ ä¸åŒç®—å­çš„å†…å­˜å¼€é”€å’Œæ€§èƒ½ã€‚

| gptq  | act_order | bits | group_size | kernel            | Load time (s) | Per-token latency (ms) | Throughput (tokens/s) | Peak memory (MB) |
|-------|-----------|------|------------|-------------------|---------------|------------------------|-----------------------|------------------|
| False | None      | None | None       | None              | 26.0          | 36.958                 | 27.058                | 29152.98         |
| True  | False     | 4    | 128        | exllama           | 36.2          | 33.711                 | 29.663                | 10484.34         |
| True  | False     | 4    | 128        | autogptq-cuda-old | 36.2          | 46.44                  | 21.53                 | 10344.62         |

ä¸€ä¸ªæ›´å…¨é¢çš„ã€å¯å¤ç°çš„æµ‹è¯„ç»“æœå¯ä»¥åœ¨[è¿™é‡Œ](https://github.com/huggingface/optimum/tree/main/tests/benchmark#gptq-benchmark) å–å¾—ã€‚


## ä½¿ç”¨ **Optimum ä»£ç åº“** é‡åŒ–æ¨¡å‹

To seamlessly integrate AutoGPTQ into Transformers, we used a minimalist version of the AutoGPTQ API that is available 
in [Optimum](https://github.com/huggingface/optimum), Hugging Face's toolkit for training and inference optimization. 
By following this approach, we achieved easy integration with Transformers, while allowing people to use the Optimum API 
if they want to quantize their own models! Check out the Optimum [documentation](https://huggingface.co/docs/optimum/llm_quantization/usage_guides/quantization) 
if you want to quantize your own LLMs. 

Quantizing ğŸ¤— Transformers models with the GPTQ method can be done in a few lines:

```python
from transformers import AutoModelForCausalLM, AutoTokenizer, GPTQConfig

model_id = "facebook/opt-125m"
tokenizer = AutoTokenizer.from_pretrained(model_id)
quantization_config = GPTQConfig(bits=4, dataset = "c4", tokenizer=tokenizer)

model = AutoModelForCausalLM.from_pretrained(model_id, device_map="auto", quantization_config=quantization_config)
```

Quantizing a model may take a long time. Note that for a 175B model, at least 4 GPU-hours are required if one uses a large dataset (e.g. `"c4"``). As mentioned above, many GPTQ models are already available on the Hugging Face Hub, which bypasses the need to quantize a model yourself in most use cases. Nevertheless, you can also quantize a model using your own dataset appropriate for the particular domain you are working on.

## é€šè¿‡ ***Text-Generation-Inference*** ä½¿ç”¨ GPTQ æ¨¡å‹

In parallel to the integration of GPTQ in Transformers, GPTQ support was added to the [Text-Generation-Inference library](https://github.com/huggingface/text-generation-inference) (TGI), aimed at serving large language models in production. GPTQ can now be used alongside features such as dynamic batching, paged attention and flash attention for a [wide range of architectures](https://huggingface.co/docs/text-generation-inference/main/en/supported_models).

As an example, this integration allows to serve a 70B model on a single A100-80GB GPU! This is not possible using a fp16 checkpoint as it exceeds the available GPU memory.

You can find out more about the usage of GPTQ in TGI in [the documentation](https://huggingface.co/docs/text-generation-inference/main/en/basic_tutorials/preparing_model#quantization).

Note that the kernel integrated in TGI does not scale very well with larger batch sizes. Although this approach saves memory, slowdowns are expected at larger batch sizes.

## **ä½¿ç”¨ PEFT å¾®è°ƒé‡åŒ–åçš„æ¨¡å‹**

You can not further train a quantized model using the regular methods. However, by leveraging the PEFT library, you can train adapters on top! To do that, we freeze all the layers of the quantized model and add the trainable adapters. Here are some examples on how to use PEFT with a GPTQ model: [colab notebook](https://colab.research.google.com/drive/1VoYNfYDKcKRQRor98Zbf2-9VQTtGJ24k?usp=sharing) and [finetuning](https://gist.github.com/SunMarc/dcdb499ac16d355a8f265aa497645996) script. 

## æ”¹è¿›ç©ºé—´

Our AutoGPTQ integration already brings impressive benefits at a small cost in the quality of prediction. There is still room for improvement, both in the quantization techniques and the kernel implementations.

First, while AutoGPTQ integrates (to the best of our knowledge) with the most performant W4A16 kernel (weights as int4, activations as fp16) from the [exllama implementation](https://github.com/turboderp/exllama), there is a good chance that the kernel can still be improved. There have been other promising implementations [from Kim et al.](https://arxiv.org/pdf/2211.10017.pdf) and from [MIT Han Lab](https://github.com/mit-han-lab/llm-awq) that appear to be promising. Moreover, from internal benchmarks, there appears to still be no open-source performant W4A16 kernel written in Triton, which could be a direction to explore.

On the quantization side, letâ€™s emphasize again that this method only quantizes the weights. There have been other approaches proposed for LLM quantization that can quantize both weights and activations at a small cost in prediction quality, such as [LLM-QAT](https://arxiv.org/pdf/2305.17888.pdf) where a mixed int4/int8 scheme can be used, as well as quantization of the key-value cache. One of the strong advantages of this technique is the ability to use actual integer arithmetic for the compute, with e.g. [Nvidia Tensor Cores supporting int8 compute](https://www.nvidia.com/content/dam/en-zz/Solutions/Data-Center/a100/pdf/nvidia-a100-datasheet-us-nvidia-1758950-r4-web.pdf). However, to the best of our knowledge, there are no open-source W4A8 quantization kernels available, but this may well be [an interesting direction to explore](https://www.qualcomm.com/news/onq/2023/04/floating-point-arithmetic-for-ai-inference-hit-or-miss).

On the kernel side as well, designing performant W4A16 kernels for larger batch sizes remains an open challenge.

### å·²æ”¯æŒçš„æ¨¡å‹

In this initial implementation, only large language models with a decoder or encoder only architecture are supported. This may sound a bit restrictive, but it encompasses most state of the art LLMs such as Llama, OPT, GPT-Neo, GPT-NeoX.

Very large vision, audio, and multi-modal models are currently not supported.

## ç»“è®ºå’Œç»“è¯­

In this blogpost we have presented the integration of the [AutoGPTQ library](https://github.com/PanQiWei/AutoGPTQ) in Transformers, making it possible to quantize LLMs with the GPTQ method to make them more accessible for anyone in the community and empower them to build exciting tools and applications with LLMs. 

This integration is available both for Nvidia GPUs, and RoCm-powered AMD GPUs, which is a huge step towards democratizing quantized models for broader GPU architectures.

The collaboration with the AutoGPTQ team has been very fruitful, and we are very grateful for their support and their work on this library.

We hope that this integration will make it easier for everyone to use LLMs in their applications, and we are looking forward to seeing what you will build with it!

Do not miss the useful resources shared above for better understanding the integration and how to quickly get started with GPTQ quantization.

- [Original Paper](https://arxiv.org/pdf/2210.17323.pdf)
- [Basic usage Google Colab notebook](https://colab.research.google.com/drive/1_TIrmuKOFhuRRiTWN94iLKUFu6ZX4ceb?usp=sharing) -  This notebook shows how to quantize your transformers model with GPTQ method, how to do inference, and how to do fine-tuning with the quantized model.
- Transformers integration [documentation](https://huggingface.co/docs/transformers/main/en/main_classes/quantization)
- Optimum integration [documentation](https://huggingface.co/docs/optimum/llm_quantization/usage_guides/quantization)
- The Bloke [repositories](https://huggingface.co/TheBloke?sort_models=likes#models) with compatible GPTQ models.


## è‡´è°¢

We would like to thank [William](https://github.com/PanQiWei) for his support and his work on the amazing AutoGPTQ library and for his help in the integration. 
We would also like to thank [TheBloke](https://huggingface.co/TheBloke) for his work on quantizing many models with AutoGPTQ and sharing them on the Hub and for his help with the integration. 
We would also like to aknowledge [qwopqwop200](https://github.com/qwopqwop200) for his continuous contributions on AutoGPTQ library and his work on extending the library for CPU that is going to be released in the next versions of AutoGPTQ. 

Finally, we would like to thank [Pedro Cuenca](https://github.com/pcuenca) for his help with the writing of this blogpost.
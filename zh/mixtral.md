---
title: "æ¬¢è¿ Mixtral - å½“å‰ Hugging Face ä¸Šæœ€å…ˆè¿›çš„ MoE æ¨¡å‹"
thumbnail: /blog/assets/mixtral/thumbnail.jpg
authors:
- user: lewtun
- user: philschmid
- user: osanseviero
- user: pcuenq
- user: olivierdehaene
- user: lvwerra
- user: ybelkada
translators:
- user: MatrixYao
- user: zhongdongy
  proofreader: true
---

# æ¬¢è¿ Mixtral - å½“å‰ Hugging Face ä¸Šæœ€å…ˆè¿›çš„ MoE æ¨¡å‹

æœ€è¿‘ï¼ŒMistral å‘å¸ƒäº†ä¸€ä¸ªæ¿€åŠ¨äººå¿ƒçš„å¤§è¯­è¨€æ¨¡å‹: Mixtral 8x7bï¼Œè¯¥æ¨¡å‹æŠŠå¼€æ”¾æ¨¡å‹çš„æ€§èƒ½å¸¦åˆ°äº†ä¸€ä¸ªæ–°é«˜åº¦ï¼Œå¹¶åœ¨è®¸å¤šåŸºå‡†æµ‹è¯•ä¸Šè¡¨ç°ä¼˜äº GPT-3.5ã€‚æˆ‘ä»¬å¾ˆé«˜å…´èƒ½å¤Ÿåœ¨ Hugging Face ç”Ÿæ€ç³»ç»Ÿä¸­å…¨é¢é›†æˆ Mixtral ä»¥å¯¹å…¶æä¾›å…¨æ–¹ä½çš„æ”¯æŒ ğŸ”¥ï¼

Hugging Face å¯¹ Mixtral çš„å…¨æ–¹ä½æ”¯æŒåŒ…æ‹¬:

- [Hub ä¸Šçš„æ¨¡å‹](https://huggingface.co/models?search=mistralai/Mixtral)ï¼ŒåŒ…æ‹¬æ¨¡å‹å¡ä»¥åŠç›¸åº”çš„è®¸å¯è¯ (Apache 2.0)
- [ğŸ¤— transformers çš„é›†æˆ](https://github.com/huggingface/transformers/releases/tag/v4.36.0)
- æ¨ç†ç»ˆç«¯çš„é›†æˆ
- [TGI](https://github.com/huggingface/text-generation-inference)Â çš„é›†æˆï¼Œä»¥æ”¯æŒå¿«é€Ÿé«˜æ•ˆçš„ç”Ÿäº§çº§æ¨ç†
- ä½¿ç”¨ ğŸ¤—Â TRL åœ¨å•å¡ä¸Šå¯¹ Mixtral è¿›è¡Œå¾®è°ƒçš„ç¤ºä¾‹

## ç›®å½•


- [æ¬¢è¿ Mixtral - å½“å‰ Hugging Face ä¸Šæœ€å…ˆè¿›çš„ MoE æ¨¡å‹](#æ¬¢è¿-mixtral---å½“å‰-hugging-face-ä¸Šæœ€å…ˆè¿›çš„-moe-æ¨¡å‹)
	- [ç›®å½•](#ç›®å½•)
	- [Mixtral 8x7b æ˜¯ä»€ä¹ˆï¼Ÿ](#mixtral-8x7b-æ˜¯ä»€ä¹ˆ)
		- [å…³äºå‘½å](#å…³äºå‘½å)
		- [æç¤ºæ ¼å¼](#æç¤ºæ ¼å¼)
		- [æˆ‘ä»¬ä¸çŸ¥é“çš„äº‹](#æˆ‘ä»¬ä¸çŸ¥é“çš„äº‹)
	- [æ¼”ç¤º](#æ¼”ç¤º)
	- [æ¨ç†](#æ¨ç†)
		- [ä½¿ç”¨ ğŸ¤—Â transformers](#ä½¿ç”¨-transformers)
		- [ä½¿ç”¨ TGI](#ä½¿ç”¨-tgi)
	- [ç”¨ ğŸ¤—Â TRL å¾®è°ƒ](#ç”¨-trl-å¾®è°ƒ)
	- [é‡åŒ– Mixtral](#é‡åŒ–-mixtral)
		- [ä½¿ç”¨ 4 æ¯”ç‰¹é‡åŒ–åŠ è½½ Mixtral](#ä½¿ç”¨-4-æ¯”ç‰¹é‡åŒ–åŠ è½½-mixtral)
		- [ä½¿ç”¨ GPTQ åŠ è½½ Mixtral](#ä½¿ç”¨-gptq-åŠ è½½-mixtral)
	- [å…è´£å£°æ˜åŠæ­£åœ¨åšçš„å·¥ä½œ](#å…è´£å£°æ˜åŠæ­£åœ¨åšçš„å·¥ä½œ)
	- [æ›´å¤šèµ„æº](#æ›´å¤šèµ„æº)
	- [æ€»ç»“](#æ€»ç»“)

## Mixtral 8x7b æ˜¯ä»€ä¹ˆï¼Ÿ

Mixtral çš„æ¶æ„ä¸ Mistral 7B ç±»ä¼¼ï¼Œä½†æœ‰ä¸€ç‚¹ä¸åŒ: å®ƒå®é™…ä¸Šå†…å«äº† 8 ä¸ªâ€œä¸“å®¶â€æ¨¡å‹ï¼Œè¿™è¦å½’åŠŸäºä¸€ç§ç§°ä¸ºâ€œæ··åˆä¸“å®¶â€(Mixture of Expertsï¼ŒMoE) çš„æŠ€æœ¯ã€‚å½“ MoE ä¸ transformer æ¨¡å‹ç›¸ç»“åˆæ—¶ï¼Œæˆ‘ä»¬ä¼šç”¨ç¨€ç– MoE å±‚æ›¿æ¢æ‰æŸäº›å‰é¦ˆå±‚ã€‚MoE å±‚åŒ…å«ä¸€ä¸ªè·¯ç”±ç½‘ç»œï¼Œç”¨äºé€‰æ‹©å°†è¾“å…¥è¯å…ƒåˆ†æ´¾ç»™å“ªäº›ä¸“å®¶å¤„ç†ã€‚Mixtral æ¨¡å‹ä¸ºæ¯ä¸ªè¯å…ƒé€‰æ‹©ä¸¤åä¸“å®¶ï¼Œå› æ­¤ï¼Œå°½ç®¡å…¶æœ‰æ•ˆå‚æ•°é‡æ˜¯ 12B ç¨ å¯†æ¨¡å‹çš„ 4 å€ï¼Œä½†å…¶è§£ç é€Ÿåº¦å´èƒ½åšåˆ°ä¸ 12B çš„ç¨ å¯†æ¨¡å‹ç›¸å½“ï¼

æ¬²äº†è§£æ›´å¤šæœ‰å…³ MoE çš„çŸ¥è¯†ï¼Œè¯·å‚é˜…æˆ‘ä»¬ä¹‹å‰çš„åšæ–‡: [hf.co/blog/zh/moe](https://huggingface.co/blog/zh/moe)ã€‚

**æœ¬æ¬¡å‘å¸ƒçš„ Mixtral æ¨¡å‹çš„ä¸»è¦ç‰¹ç‚¹:**

- æ¨¡å‹åŒ…æ‹¬åŸºç¡€ç‰ˆå’ŒæŒ‡ä»¤ç‰ˆ
- æ”¯æŒé«˜è¾¾ 32k è¯å…ƒçš„ä¸Šä¸‹æ–‡
- æ€§èƒ½ä¼˜äº Llama 2 70Bï¼Œåœ¨å¤§å¤šæ•°åŸºå‡†æµ‹è¯•ä¸Šè¡¨ç°ä¸é€Šäº GPT3.5
- æ”¯æŒè‹±è¯­ã€æ³•è¯­ã€å¾·è¯­ã€è¥¿ç­ç‰™è¯­åŠæ„å¤§åˆ©è¯­
- æ“…é•¿ç¼–ç ï¼ŒHumanEval å¾—åˆ†ä¸º 40.2%
- å¯å•†ç”¨ï¼ŒApache 2.0 è®¸å¯è¯

é‚£ä¹ˆï¼ŒMixtral æ¨¡å‹æ•ˆæœåˆ°åº•æœ‰å¤šå¥½å‘¢ï¼Ÿä¸‹é¢åˆ—å‡ºäº† Mixtral åŸºç¡€æ¨¡å‹ä¸å…¶ä»–å…ˆè¿›çš„å¼€æ”¾æ¨¡å‹åœ¨ [LLM æ’è¡Œæ¦œ](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard) ä¸Šè¡¨ç° (åˆ†æ•°è¶Šé«˜è¶Šå¥½):

| æ¨¡å‹                                                                             | è®¸å¯è¯         | æ˜¯å¦å¯å•†ç”¨ | é¢„è®­ç»ƒè¯å…ƒæ•° | æ’è¡Œæ¦œå¾—åˆ† â¬‡ï¸ |
| --------------------------------------------------------------------------------- | --------------- | --------------- | ------------------------- | -------------------- |
| [mistralai/Mixtral-8x7B-v0.1](https://huggingface.co/mistralai/Mixtral-8x7B-v0.1) | Apache 2.0      | âœ…               | ä¸è¯¦                   | 68.42                |
| [meta-llama/Llama-2-70b-hf](https://huggingface.co/meta-llama/Llama-2-70b-hf)     | Llama 2 è®¸å¯è¯ | âœ…               | 2,000B                    | 67.87                |
| [tiiuae/falcon-40b](https://huggingface.co/tiiuae/falcon-40b)                     | Apache 2.0      | âœ…               | 1,000B                    | 61.5                 |
| [mistralai/Mistral-7B-v0.1](https://huggingface.co/mistralai/Mistral-7B-v0.1)     | Apache 2.0      | âœ…               | ä¸è¯¦                   | 60.97                |
| [meta-llama/Llama-2-7b-hf](https://huggingface.co/meta-llama/Llama-2-7b-hf)       | Llama 2 è®¸å¯è¯ | âœ…               | 2,000B                    | 54.32                |

æˆ‘ä»¬è¿˜ç”¨ MT-Bench åŠ AlpacaEval ç­‰åŸºå‡†å¯¹æŒ‡ä»¤ç‰ˆå’Œå…¶å®ƒèŠå¤©æ¨¡å‹è¿›è¡Œäº†å¯¹æ¯”ã€‚ä¸‹è¡¨åˆ—å‡ºäº† [Mixtral Instruct](https://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1) ä¸é¡¶çº§é—­æºæˆ–å¼€æ”¾æ¨¡å‹ç›¸æ¯”çš„è¡¨ç° (åˆ†æ•°è¶Šé«˜è¶Šå¥½):

| æ¨¡å‹                                                                                               | å¯å¾—æ€§    | ä¸Šä¸‹æ–‡çª—å£ï¼ˆè¯å…ƒæ•°ï¼‰ | MT-Bench å¾—åˆ† â¬‡ï¸ |
| --------------------------------------------------------------------------------------------------- | --------------- | ----------------------- | ---------------- |
| [GPT-4 Turbo](https://openai.com/blog/new-models-and-developer-products-announced-at-devday)        | ç§æœ‰     | 128k                    | 9.32             |
| [GPT-3.5-turbo-0613](https://platform.openai.com/docs/models/gpt-3-5)                               | ç§æœ‰     | 16k                     | 8.32             |
| [mistralai/Mixtral-8x7B-Instruct-v0.1](https://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1) | Apache 2.0      | 32k                     | 8.30             |
| [Claude 2.1](https://www.anthropic.com/index/claude-2-1)                                            | ç§æœ‰     | 200k                    | 8.18             |
| [openchat/openchat_3.5](https://huggingface.co/openchat/openchat_3.5)                               | Apache 2.0      | 8k                      | 7.81             |
| [HuggingFaceH4/zephyr-7b-beta](https://huggingface.co/HuggingFaceH4/zephyr-7b-beta)                 | MIT             | 8k                      | 7.34             |
| [meta-llama/Llama-2-70b-chat-hf](https://huggingface.co/meta-llama/Llama-2-70b-chat-hf)             | Llama 2 è®¸å¯è¯ | 4k                      | 6.86             |

ä»¤äººå°è±¡æ·±åˆ»çš„æ˜¯ï¼ŒMixtral Instruct çš„æ€§èƒ½ä¼˜äº MT-Bench ä¸Šçš„æ‰€æœ‰å…¶ä»–å¼€æ”¾æ¨¡å‹ï¼Œä¸”æ˜¯ç¬¬ä¸€ä¸ªä¸ GPT-3.5 æ€§èƒ½ç›¸å½“çš„å¼€æ”¾æ¨¡å‹ï¼

### å…³äºå‘½å

Mixtral MoE æ¨¡å‹è™½ç„¶åå­—æ˜¯ **Mixtral-8x7B**ï¼Œä½†å®ƒå…¶å®å¹¶æ²¡æœ‰ 56B å‚æ•°ã€‚å‘å¸ƒåä¸ä¹…ï¼Œæˆ‘ä»¬å°±å‘ç°ä¸å°‘äººè¢«åå­—è¯¯å¯¼äº†ï¼Œè®¤ä¸ºè¯¥æ¨¡å‹çš„è¡Œä¸ºç±»ä¼¼äº 8 ä¸ªæ¨¡å‹çš„é›†åˆï¼Œå…¶ä¸­æ¯ä¸ªæ¨¡å‹æœ‰ 7B ä¸ªå‚æ•°ï¼Œä½†è¿™ç§æƒ³æ³•å…¶å®ä¸ MoE æ¨¡å‹çš„å·¥ä½œåŸç†ä¸ç¬¦ã€‚å®æƒ…æ˜¯ï¼Œè¯¥æ¨¡å‹ä¸­åªæœ‰æŸäº›å±‚ (å‰é¦ˆå±‚) æ˜¯å„ä¸“å®¶ç‹¬æœ‰çš„ï¼Œå…¶ä½™å‚æ•°ä¸ç¨ å¯† 7B æ¨¡å‹æƒ…å†µç›¸åŒï¼Œæ˜¯å„ä¸“å®¶å…±äº«çš„ã€‚æ‰€ä»¥ï¼Œå‚æ•°æ€»é‡å¹¶ä¸æ˜¯ 56Bï¼Œè€Œæ˜¯ 45B å·¦å³ã€‚æ‰€ä»¥å¯èƒ½å«å®ƒ [`Mixtral-45-8e`](https://twitter.com/osanseviero/status/1734248798749159874) æ›´è´´åˆ‡ï¼Œæ›´èƒ½ç¬¦åˆå…¶æ¶æ„ã€‚æ›´å¤šæœ‰å…³ MoE å¦‚ä½•è¿è¡Œçš„è¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…æˆ‘ä»¬ä¹‹å‰å‘è¡¨çš„ [ã€ŠMoE è¯¦è§£ã€‹](https://huggingface.co/blog/zh/moe) ä¸€æ–‡ã€‚

### æç¤ºæ ¼å¼

[åŸºç¡€æ¨¡å‹](https://huggingface.co/mistralai/Mixtral-8x7B-v0.1) æ²¡æœ‰æç¤ºæ ¼å¼ï¼Œä¸å…¶ä»–åŸºç¡€æ¨¡å‹ä¸€æ ·ï¼Œå®ƒå¯ç”¨äºåºåˆ—è¡¥å…¨æˆ–é›¶æ ·æœ¬/å°‘æ ·æœ¬æ¨ç†ã€‚ä½ å¯ä»¥å¯¹åŸºç¡€æ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œå°†å…¶é€‚é…è‡³è‡ªå·±çš„åº”ç”¨åœºæ™¯ã€‚[æŒ‡ä»¤æ¨¡å‹](https://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1) æœ‰ä¸€ä¸ªéå¸¸ç®€å•çš„å¯¹è¯æ ¼å¼ã€‚

```bash
<s> [INST] User Instruction 1 [/INST] Model answer 1</s> [INST] User instruction 2[/INST]
```

ä½ å¿…é¡»å‡†ç¡®éµå¾ªæ­¤æ ¼å¼æ‰èƒ½æœ‰æ•ˆä½¿ç”¨æŒ‡ä»¤æ¨¡å‹ã€‚ç¨åæˆ‘ä»¬å°†å±•ç¤ºï¼Œä½¿ç”¨ `transformers` çš„èŠå¤©æ¨¡æ¿èƒ½å¾ˆè½»æ˜“åœ°æ”¯æŒè¿™ç±»è‡ªå®šä¹‰æŒ‡ä»¤æç¤ºæ ¼å¼ã€‚

### æˆ‘ä»¬ä¸çŸ¥é“çš„äº‹

ä¸ä¹‹å‰çš„ Mistral 7B ç‰ˆæœ¬ä¸€æ ·ï¼Œå¯¹è¿™ä¸€æ–°çš„æ¨¡å‹å®¶æ—ï¼Œæˆ‘ä»¬ä¹Ÿæœ‰å‡ ä¸ªå¾…æ¾„æ¸…çš„é—®é¢˜ã€‚æ¯”å¦‚ï¼Œæˆ‘ä»¬ä¸çŸ¥é“ç”¨äºé¢„è®­ç»ƒçš„æ•°æ®é›†å¤§å°ï¼Œä¹Ÿä¸çŸ¥é“å®ƒçš„ç»„æˆä¿¡æ¯ä»¥åŠé¢„å¤„ç†æ–¹å¼ä¿¡æ¯ã€‚

åŒæ ·ï¼Œå¯¹äº Mixtral æŒ‡ä»¤æ¨¡å‹ï¼Œæˆ‘ä»¬å¯¹å¾®è°ƒæ•°æ®é›†æˆ– SFT å’Œ DPO ä½¿ç”¨çš„è¶…å‚ä¹ŸçŸ¥ä¹‹ç”šå°‘ã€‚

## æ¼”ç¤º

ä½ å¯ä»¥åœ¨ Hugging Face Chat ä¸Šä¸ Mixtral Instruct æ¨¡å‹èŠå¤©ï¼ç‚¹å‡» [æ­¤å¤„](https://huggingface.co/chat/?model=mistralai/Mixtral-8x7B-Instruct-v0.1) å¼€å§‹ä½“éªŒå§ã€‚

## æ¨ç†

æˆ‘ä»¬ä¸»è¦æä¾›ä¸¤ç§å¯¹ Mixtral æ¨¡å‹è¿›è¡Œæ¨ç†çš„æ–¹æ³•:

- é€šè¿‡ ğŸ¤— transformers çš„ `pipeline()` æ¥å£ã€‚
- é€šè¿‡ TGIï¼Œå…¶æ”¯æŒè¿ç»­ç»„æ‰¹ã€å¼ é‡å¹¶è¡Œç­‰é«˜çº§åŠŸèƒ½ï¼Œæ¨ç†é€Ÿåº¦æå¿«ã€‚

ä»¥ä¸Šä¸¤ç§æ–¹æ³•å‡æ”¯æŒåŠç²¾åº¦ (float16) åŠé‡åŒ–æƒé‡ã€‚ç”±äº Mixtral æ¨¡å‹çš„å‚æ•°é‡å¤§è‡´ç›¸å½“äº 45B å‚æ•°çš„ç¨ å¯†æ¨¡å‹ï¼Œå› æ­¤æˆ‘ä»¬å¯ä»¥å¯¹æ‰€éœ€çš„æœ€ä½æ˜¾å­˜é‡ä½œä¸€ä¸ªä¼°è®¡ï¼Œå¦‚ä¸‹:

| ç²¾åº¦ | æ˜¾å­˜éœ€æ±‚ |
| --------- | ------------- |
| float16   | >90 GB        |
| 8-bit     | >45 GB        |
| 4-bit     | >23 GB        |

### ä½¿ç”¨ ğŸ¤—Â transformers

ä» transformers [4.36 ç‰ˆ](https://github.com/huggingface/transformers/releases/tag/v4.36.0) å¼€å§‹ï¼Œç”¨æˆ·å°±å¯ä»¥ç”¨ Hugging Face ç”Ÿæ€ç³»ç»Ÿä¸­çš„æ‰€æœ‰å·¥å…·å¤„ç† Mixtral æ¨¡å‹ï¼Œå¦‚:

- è®­ç»ƒå’Œæ¨ç†è„šæœ¬åŠç¤ºä¾‹
- å®‰å…¨æ–‡ä»¶æ ¼å¼ (`safetensors` )
- ä¸ bitsandbytes (4 æ¯”ç‰¹é‡åŒ–) ã€PEFT (å‚æ•°é«˜æ•ˆå¾®è°ƒ) å’Œ Flash Attention 2 ç­‰å·¥å…·çš„é›†æˆ
- ä½¿ç”¨æ–‡æœ¬ç”Ÿæˆä»»åŠ¡æ‰€æä¾›çš„å·¥å…·åŠè¾…åŠ©æ–¹æ³•
- å¯¼å‡ºæ¨¡å‹ä»¥è¿›è¡Œéƒ¨ç½²

ç”¨æˆ·å”¯ä¸€éœ€è¦åšçš„æ˜¯ç¡®ä¿ `transformers` çš„ç‰ˆæœ¬æ˜¯æœ€æ–°çš„:

```bash
pip install -U "transformers==4.36.0" --upgrade
```

ä¸‹é¢çš„ä»£ç ç‰‡æ®µå±•ç¤ºäº†å¦‚ä½•ä½¿ç”¨ ğŸ¤— transformers åŠ 4 æ¯”ç‰¹é‡åŒ–æ¥è¿è¡Œæ¨ç†ã€‚ç”±äºæ¨¡å‹å°ºå¯¸è¾ƒå¤§ï¼Œä½ éœ€è¦ä¸€å¼ æ˜¾å­˜è‡³å°‘ä¸º 30GB çš„å¡æ‰èƒ½è¿è¡Œï¼Œç¬¦åˆè¦æ±‚çš„å¡æœ‰ A100 (80 æˆ– 40GB ç‰ˆæœ¬) ã€A6000 (48GB) ç­‰ã€‚

```python
from transformers import pipeline
import torch

model = "mistralai/Mixtral-8x7B-Instruct-v0.1"

pipe = pipeline(
    "text-generation",
    model=model,
    model_kwargs={"torch_dtype": torch.float16, "load_in_4bit": True},
)

messages = [{"role": "user", "content": "Explain what a Mixture of Experts is in less than 100 words."}]
outputs = pipe(messages, max_new_tokens=256, do_sample=True, temperature=0.7, top_k=50, top_p=0.95)
print(outputs[0]["generated_text"][-1]["content"])
```

> \<s>[INST] Explain what a Mixture of Experts is in less than 100 words. [/INST] A
Mixture of Experts is an ensemble learning method that combines multiple models,
or "experts," to make more accurate predictions. Each expert specializes in a
different subset of the data, and a gating network determines the appropriate
expert to use for a given input. This approach allows the model to adapt to
complex, non-linear relationships in the data and improve overall performance.
> 

### ä½¿ç”¨ TGI

**[TGI](https://github.com/huggingface/text-generation-inference)**  Â æ˜¯ Hugging Face å¼€å‘çš„ç”Ÿäº§çº§æ¨ç†å®¹å™¨ï¼Œå¯ç”¨äºè½»æ¾éƒ¨ç½²å¤§è¯­è¨€æ¨¡å‹ã€‚å…¶åŠŸèƒ½ä¸»è¦æœ‰: è¿ç»­ç»„æ‰¹ã€æµå¼è¯å…ƒè¾“å‡ºã€å¤š GPU å¼ é‡å¹¶è¡Œä»¥åŠç”Ÿäº§çº§çš„æ—¥å¿—è®°å½•å’Œè·Ÿè¸ªç­‰ã€‚

ä½ å¯åœ¨ Hugging Face çš„ [æ¨ç†ç»ˆç«¯](https://ui.endpoints.huggingface.co/new?repository=mistralai%2FMixtral-8x7B-Instruct-v0.1&vendor=aws&region=us-east-1&accelerator=gpu&instance_size=2xlarge&task=text-generation&no_suggested_compute=true&tgi=true&tgi_max_batch_total_tokens=1024000&tgi_max_total_tokens=32000) ä¸Šéƒ¨ç½² Mixtralï¼Œå…¶ä½¿ç”¨ TGI ä½œä¸ºåç«¯ã€‚è¦éƒ¨ç½² Mixtral æ¨¡å‹ï¼Œå¯è‡³ [æ¨¡å‹é¡µé¢](https://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1)ï¼Œç„¶åå•å‡» [Deploy -> Inference Endpoints](https://ui.endpoints.huggingface.co/new?repository=meta-llama/Llama-2-7b-hf) æŒ‰é’®å³å¯ã€‚

_æ³¨æ„: å¦‚ä½ çš„è´¦å· A100 é…é¢ä¸è¶³ï¼Œå¯å‘é€é‚®ä»¶è‡³ **[api-enterprise@huggingface.co](mailto:api-enterprise@huggingface.co)** ç”³è¯·å‡çº§ã€‚_

ä½ è¿˜å¯ä»¥é˜…è¯»æˆ‘ä»¬çš„åšæ–‡ **[ç”¨ Hugging Face æ¨ç†ç»ˆç«¯éƒ¨ç½² LLM](https://huggingface.co/blog/inference-endpoints-llm)** ä»¥æ·±å…¥äº†è§£å¦‚ä½•éƒ¨ç½² LLMï¼Œè¯¥æ–‡åŒ…å«äº†æ¨ç†ç»ˆç«¯æ”¯æŒçš„è¶…å‚ä»¥åŠå¦‚ä½•ä½¿ç”¨ Python å’Œ Javascript æ¥å£æ¥æµå¼ç”Ÿæˆæ–‡æœ¬ç­‰ä¿¡æ¯ã€‚

ä½ è¿˜å¯ä»¥ä½¿ç”¨ Docker åœ¨ 2 å¼  A100 (80GB) ä¸Šæœ¬åœ°è¿è¡Œ TGIï¼Œå¦‚ä¸‹æ‰€ç¤º:

```bash
docker run --gpus all --shm-size 1g -p 3000:80 -v /data:/data ghcr.io/huggingface/text-generation-inference:1.3.0 \
	--model-id mistralai/Mixtral-8x7B-Instruct-v0.1 \
	--num-shard 2 \
	--max-batch-total-tokens 1024000 \
	--max-total-tokens 32000
```

## ç”¨  ğŸ¤—Â TRL å¾®è°ƒ

è®­ç»ƒ LLM åœ¨æŠ€æœ¯å’Œç®—åŠ›ä¸Šéƒ½æœ‰è¾ƒå¤§æŒ‘æˆ˜ã€‚æœ¬èŠ‚æˆ‘ä»¬å°†äº†è§£åœ¨ Hugging Face ç”Ÿæ€ç³»ç»Ÿä¸­å¦‚ä½•åœ¨å•å¼  A100 GPU ä¸Šé«˜æ•ˆè®­ç»ƒ Mixtralã€‚

ä¸‹é¢æ˜¯åœ¨ OpenAssistant çš„ [èŠå¤©æ•°æ®é›†](https://huggingface.co/datasets/OpenAssistant/oasst_top1_2023-08-25) ä¸Šå¾®è°ƒ Mixtral çš„ç¤ºä¾‹å‘½ä»¤ã€‚ä¸ºäº†èŠ‚çœå†…å­˜ï¼Œæˆ‘ä»¬å¯¹æ³¨æ„åŠ›å—ä¸­çš„æ‰€æœ‰çº¿æ€§å±‚æ‰§è¡Œ 4 æ¯”ç‰¹é‡åŒ–å’Œ [QLoRA](https://arxiv.org/abs/2305.14314)ã€‚è¯·æ³¨æ„ï¼Œä¸ç¨ å¯† transformer æ¨¡å‹ä¸åŒï¼Œæˆ‘ä»¬ä¸å¯¹ä¸“å®¶ç½‘ç»œä¸­çš„ MLP å±‚è¿›è¡Œé‡åŒ–ï¼Œå› ä¸ºå®ƒä»¬å¾ˆç¨€ç–å¹¶ä¸”é‡åŒ–å PEFT æ•ˆæœä¸å¥½ã€‚

é¦–å…ˆï¼Œå®‰è£… ğŸ¤— TRL çš„æ¯æ—¥æ„å»ºç‰ˆå¹¶ä¸‹è½½ä»£ç åº“ä»¥è·å– [è®­ç»ƒè„šæœ¬](https://github.com/huggingface/trl/blob/main/examples/scripts/sft.py):

```bash
pip install -U transformers
pip install git+https://github.com/huggingface/trl
git clone https://github.com/huggingface/trl
cd trl
```

ç„¶åï¼Œè¿è¡Œè„šæœ¬:

```bash
accelerate launch --config_file examples/accelerate_configs/multi_gpu.yaml --num_processes=1 \
	examples/scripts/sft.py \
	--model_name mistralai/Mixtral-8x7B-v0.1 \
	--dataset_name trl-lib/ultrachat_200k_chatml \
	--batch_size 2 \
	--gradient_accumulation_steps 1 \
	--learning_rate 2e-4 \
	--save_steps 200_000 \
	--use_peft \
	--peft_lora_r 16 --peft_lora_alpha 32 \
	--target_modules q_proj k_proj v_proj o_proj \
	--load_in_4bit
```

åœ¨å•å¼  A100 ä¸Šè®­ç»ƒå¤§çº¦éœ€è¦ 48 å°æ—¶ï¼Œä½†æˆ‘ä»¬å¯ä»¥é€šè¿‡ `--num_processes` æ¥è°ƒæ•´ GPU çš„æ•°é‡ä»¥å®ç°å¹¶è¡Œã€‚

## é‡åŒ– Mixtral

å¦‚ä¸Šæ‰€è§ï¼Œè¯¥æ¨¡å‹æœ€å¤§çš„æŒ‘æˆ˜æ˜¯å¦‚ä½•å®ç°æ™®æƒ ï¼Œå³å¦‚ä½•è®©å®ƒèƒ½å¤Ÿåœ¨æ¶ˆè´¹çº§ç¡¬ä»¶ä¸Šè¿è¡Œã€‚å› ä¸ºå³ä½¿ä»¥åŠç²¾åº¦ ( `torch.float16` ) åŠ è½½ï¼Œå®ƒä¹Ÿéœ€è¦ 90GB æ˜¾å­˜ã€‚

å€ŸåŠ© ğŸ¤— transformers åº“ï¼Œæˆ‘ä»¬æ”¯æŒç”¨æˆ·å¼€ç®±å³ç”¨åœ°ä½¿ç”¨ QLoRA å’Œ GPTQ ç­‰æœ€å…ˆè¿›çš„é‡åŒ–æ–¹æ³•è¿›è¡Œæ¨ç†ã€‚ä½ å¯ä»¥é˜…è¯» [ç›¸åº”çš„æ–‡æ¡£](https://huggingface.co/docs/transformers/quantization) ä»¥è·å–æœ‰å…³æˆ‘ä»¬æ”¯æŒçš„é‡åŒ–æ–¹æ³•çš„æ›´å¤šä¿¡æ¯ã€‚

### ä½¿ç”¨ 4 æ¯”ç‰¹é‡åŒ–åŠ è½½ Mixtral

ç”¨æˆ·è¿˜å¯ä»¥é€šè¿‡å®‰è£… `bitsandbytes` åº“ ( `pip install -U bitsandbytes` ) å¹¶å°†å‚æ•° `load_in_4bit=True` ä¼ ç»™ `from_pretrained` æ–¹æ³•æ¥åŠ è½½ 4 æ¯”ç‰¹é‡åŒ–çš„ Mixtralã€‚ä¸ºäº†è·å¾—æ›´å¥½çš„æ€§èƒ½ï¼Œæˆ‘ä»¬å»ºè®®ç”¨æˆ·ä½¿ç”¨ `bnb_4bit_compute_dtype=torch.float16` æ¥åŠ è½½æ¨¡å‹ã€‚è¯·æ³¨æ„ï¼Œä½ çš„ GPU æ˜¾å­˜è‡³å°‘å¾—æœ‰ 30GB æ‰èƒ½æ­£ç¡®è¿è¡Œä¸‹é¢çš„ä»£ç ç‰‡æ®µã€‚

```python
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig

model_id = "mistralai/Mixtral-8x7B-Instruct-v0.1"
tokenizer = AutoTokenizer.from_pretrained(model_id)

quantization_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_compute_dtype=torch.float16
)
model = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=quantization_config)

prompt = "[INST] Explain what a Mixture of Experts is in less than 100 words. [/INST]"
inputs = tokenizer(prompt, return_tensors="pt").to(0)

output = model.generate(**inputs, max_new_tokens=50)
print(tokenizer.decode(output[0], skip_special_tokens=True))
```

è¯¥ 4 æ¯”ç‰¹é‡åŒ–æŠ€æœ¯ç”± [QLoRA è®ºæ–‡](https://huggingface.co/papers/2305.14314) æå‡ºï¼Œä½ å¯ä»¥é€šè¿‡ [ç›¸åº”çš„ Hugging Face æ–‡æ¡£](https://huggingface.co/docs/transformers/quantization#4-bit) æˆ– [è¿™ç¯‡åšæ–‡](https://huggingface.co/blog/zh/4bit-transformers-bitsandbytes) è·å–æ›´å¤šç›¸å…³ä¿¡æ¯ã€‚

### ä½¿ç”¨ GPTQ åŠ è½½ Mixtral

GPTQ ç®—æ³•æ˜¯ä¸€ç§è®­åé‡åŒ–æŠ€æœ¯ï¼Œå…¶ä¸­æƒé‡çŸ©é˜µçš„æ¯ä¸€è¡Œéƒ½æ˜¯ç‹¬ç«‹é‡åŒ–çš„ï¼Œä»¥è·å–è¯¯å·®æœ€å°çš„é‡åŒ–æƒé‡ã€‚è¿™äº›æƒé‡è¢«é‡åŒ–ä¸º int4ï¼Œä½†åœ¨æ¨ç†è¿‡ç¨‹ä¸­ä¼šå³æ—¶æ¢å¤ä¸º fp16ã€‚ä¸ 4 æ¯”ç‰¹ QLoRA ç›¸æ¯”ï¼ŒGPTQ çš„é‡åŒ–æ¨¡å‹æ˜¯é€šè¿‡å¯¹æŸä¸ªæ•°æ®é›†è¿›è¡Œæ ¡å‡†è€Œå¾—çš„ã€‚[TheBloke](https://huggingface.co/TheBloke) åœ¨  ğŸ¤— Hub ä¸Šåˆ†äº«äº†å¾ˆå¤šé‡åŒ–åçš„ GPTQ æ¨¡å‹ï¼Œè¿™æ ·å¤§å®¶æ— éœ€äº²è‡ªæ‰§è¡Œæ ¡å‡†å°±å¯ç›´æ¥ä½¿ç”¨é‡åŒ–æ¨¡å‹ã€‚

å¯¹äº Mixtralï¼Œä¸ºäº†è·å¾—æ›´å¥½çš„æ€§èƒ½ï¼Œæˆ‘ä»¬å¿…é¡»è°ƒæ•´ä¸€ä¸‹æ ¡å‡†æ–¹æ³•ï¼Œä»¥ç¡®ä¿æˆ‘ä»¬ **ä¸ä¼š** é‡åŒ–é‚£äº›ä¸“å®¶é—¨æ§å±‚ã€‚é‡åŒ–æ¨¡å‹çš„æœ€ç»ˆå›°æƒ‘åº¦ (è¶Šä½è¶Šå¥½) ä¸º `4.40` ï¼Œè€ŒåŠç²¾åº¦æ¨¡å‹ä¸º `4.25` ã€‚ä½ å¯åœ¨ [æ­¤å¤„](https://huggingface.co/TheBloke/Mixtral-8x7B-v0.1-GPTQ) æ‰¾åˆ°é‡åŒ–æ¨¡å‹ï¼Œè¦ä½¿ç”¨ ğŸ¤— transformers è¿è¡Œå®ƒï¼Œä½ é¦–å…ˆéœ€è¦æ›´æ–° `auto-gptq` å’Œ `optimum` åº“:

```bash
pip install -U optimum auto-gptq
```

ç„¶åæ˜¯ä»æºä»£ç å®‰è£… transformers:

```bash
pip install -U git+https://github.com/huggingface/transformers.git
```

å®‰è£…å¥½åï¼Œåªéœ€ä½¿ç”¨ `from_pretrained` æ–¹æ³•åŠ è½½ GPTQ æ¨¡å‹å³å¯:

```python
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig

model_id = "TheBloke/Mixtral-8x7B-v0.1-GPTQ"
tokenizer = AutoTokenizer.from_pretrained(model_id)

model = AutoModelForCausalLM.from_pretrained(model_id, device_map="auto")

prompt = "[INST] Explain what a Mixture of Experts is in less than 100 words. [/INST]"
inputs = tokenizer(prompt, return_tensors="pt").to(0)

output = model.generate(**inputs, max_new_tokens=50)
print(tokenizer.decode(output[0], skip_special_tokens=True))
```

è¯·æ³¨æ„ï¼Œä½ çš„ GPU æ˜¾å­˜è‡³å°‘å¾—æœ‰ 30GB æ‰èƒ½è¿è¡Œ Mixtral æ¨¡å‹çš„ QLoRA å’Œ GPTQ ç‰ˆæœ¬ã€‚å¦‚æœä½ å¦‚ä¸Šä¾‹ä¸€æ ·ä½¿ç”¨äº† `device_map="auto"` ï¼Œåˆ™å…¶åœ¨ 24GB æ˜¾å­˜æ—¶ä¹Ÿå¯ä»¥è¿è¡Œï¼Œå› æ­¤ä¼šæœ‰ä¸€äº›å±‚è¢«è‡ªåŠ¨å¸è½½åˆ° CPUã€‚

## å…è´£å£°æ˜åŠæ­£åœ¨åšçš„å·¥ä½œ

- **é‡åŒ–**: å›´ç»• MoE çš„é‡åŒ–è¿˜æœ‰è®¸å¤šç ”ç©¶æ­£å¦‚ç«å¦‚è¼åœ°å±•å¼€ã€‚ä¸Šæ–‡å±•ç¤ºäº†æˆ‘ä»¬åŸºäº TheBloke æ‰€åšçš„ä¸€äº›åˆæ­¥å®éªŒï¼Œä½†æˆ‘ä»¬é¢„è®¡éšç€å¯¹è¯¥æ¶æ„ç ”ç©¶çš„æ·±å…¥ï¼Œä¼šæ¶Œç°å‡ºæ›´å¤šè¿›å±•ï¼è¿™ä¸€é¢†åŸŸçš„è¿›å±•å°†ä¼šæ˜¯æ—¥æ–°æœˆå¼‚çš„ï¼Œæˆ‘ä»¬ç¿˜é¦–ä»¥ç›¼ã€‚æ­¤å¤–ï¼Œæœ€è¿‘çš„å·¥ä½œï¼Œå¦‚ [QMoE](https://arxiv.org/abs/2310.16795)ï¼Œå®ç°äº† MoE çš„äºš 1 æ¯”ç‰¹é‡åŒ–ï¼Œä¹Ÿæ˜¯å€¼å¾—å°è¯•çš„æ–¹æ¡ˆã€‚
- **é«˜æ˜¾å­˜å ç”¨**: MoE è¿è¡Œæ¨ç†é€Ÿåº¦è¾ƒå¿«ï¼Œä½†å¯¹æ˜¾å­˜çš„è¦æ±‚ä¹Ÿç›¸å¯¹è¾ƒé«˜ (å› æ­¤éœ€è¦æ˜‚è´µçš„ GPU)ã€‚è¿™å¯¹æœ¬åœ°æ¨ç†æå‡ºäº†æŒ‘æˆ˜ï¼Œå› ä¸ºæœ¬åœ°æ¨ç†æ‰€æ‹¥æœ‰çš„è®¾å¤‡æ˜¾å­˜ä¸€èˆ¬è¾ƒå°ã€‚MoE éå¸¸é€‚åˆå¤šè®¾å¤‡å¤§æ˜¾å­˜çš„åŸºç¡€è®¾æ–½ã€‚å¯¹ Mixtral è¿›è¡ŒåŠç²¾åº¦æ¨ç†éœ€è¦ 90GB æ˜¾å­˜ ğŸ¤¯ã€‚

## æ›´å¤šèµ„æº

- [MoE è¯¦è§£](https://huggingface.co/blog/zh/moe)
- [Mistral çš„ Mixtral åšæ–‡](https://mistral.ai/news/mixtral-of-experts/)
- [Hub ä¸Šçš„æ¨¡å‹](https://huggingface.co/models?other=mixtral)
- [å¼€æ”¾ LLM æ’è¡Œæ¦œ](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard)
- [åŸºäº Mixtral çš„ Hugging Chat èŠå¤©æ¼”ç¤ºåº”ç”¨](https://huggingface.co/chat/?model=mistralai/Mixtral-8x7B-Instruct-v0.1)

## æ€»ç»“

æˆ‘ä»¬å¯¹ Mixtral çš„å‘å¸ƒæ„Ÿåˆ°æ¬¢æ¬£é¼“èˆï¼æˆ‘ä»¬æ­£å›´ç»• Mixtral å‡†å¤‡æ›´å¤šå…³äºå¾®è°ƒå’Œéƒ¨ç½²æ–‡ç« ï¼Œå°½è¯·æœŸå¾…ã€‚
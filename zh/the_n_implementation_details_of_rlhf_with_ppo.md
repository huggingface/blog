---
title: "ä½¿ç”¨ PPO ç®—æ³•è¿›è¡Œ RLHF çš„ N æ­¥å®ç°ç»†èŠ‚"
thumbnail: /blog/assets/167_the_n_implementation_details_of_rlhf_with_ppo/thumbnail.png
authors:
- user: vwxyzjn
- user: tianlinliu0121
  guest: true
- user: lvwerra
translators:
- user: innovation64
- user: zhongdongy
  proofreader: true
---

# ä½¿ç”¨ PPO ç®—æ³•è¿›è¡Œ RLHF çš„ N æ­¥å®ç°ç»†èŠ‚

å½“ä¸‹ï¼ŒRLHF/ChatGPT å·²ç»å˜æˆäº†ä¸€ä¸ªéå¸¸æµè¡Œçš„è¯é¢˜ã€‚æˆ‘ä»¬æ­£åœ¨è‡´åŠ›äºæ›´å¤šæœ‰å…³ RLHF çš„ç ”ç©¶ï¼Œè¿™ç¯‡åšå®¢å°è¯•å¤ç° OpenAI åœ¨ 2019 å¹´å¼€æºçš„åŸå§‹ RLHF ä»£ç åº“ï¼Œå…¶ä»“åº“ä½ç½®ä½äº [_openai/lm-human-preferences_](https://github.com/openai/lm-human-preferences)ã€‚å°½ç®¡å®ƒå…·æœ‰ â€œtensorflow-1.xâ€ çš„ç‰¹æ€§ï¼Œä½† OpenAI çš„åŸå§‹ä»£ç åº“è¯„ä¼°å’ŒåŸºå‡†æµ‹è¯•éå¸¸å®Œå–„ï¼Œä½¿å…¶æˆä¸ºç ”ç©¶ RLHF å®ç°å·¥ç¨‹ç»†èŠ‚çš„å¥½åœ°æ–¹ã€‚

æˆ‘ä»¬çš„ç›®æ ‡æ˜¯:

1. å¤ç° OAI åœ¨é£æ ¼åŒ–ä»»åŠ¡ä¸­çš„ç»“æœï¼Œå¹¶åŒ¹é… [_openai/lm-human-preferences_](https://github.com/openai/lm-human-preferences) çš„å­¦ä¹ æ›²çº¿ã€‚
2. æä¾›ä¸€ä¸ªå®ç°ç»†èŠ‚çš„æ¸…å•ï¼Œç±»ä¼¼äº [è¿‘ç«¯ä¼˜åŒ–ç­–ç•¥çš„ 37 ä¸ªå®æ–½ç»†èŠ‚ (_The 37 Implementation Details of Proximal Policy Optimization_)](https://iclr-blog-track.github.io/2022/03/25/ppo-implementation-details/) å’Œ [æ²¡æœ‰ç—›è‹¦æŠ˜ç£¨çš„è°ƒè¯• RL (_Debugging RL, Without the Agonizing Pain_)](https://andyljones.com/posts/rl-debugging.html) çš„é£æ ¼;
3. æä¾›ä¸€ä¸ªæ˜“äºé˜…è¯»ä¸”ç®€æ´çš„ RLHF å‚è€ƒå®ç°;

è¿™é¡¹å·¥ä½œä»…é€‚ç”¨äºä»¥æ•™è‚²/å­¦ä¹ ä¸ºç›®çš„çš„ã€‚å¯¹äºéœ€è¦æ›´å¤šåŠŸèƒ½çš„é«˜çº§ç”¨æˆ·ï¼Œä¾‹å¦‚ä½¿ç”¨ PEFT è¿è¡Œæ›´å¤§çš„æ¨¡å‹ï¼Œ [_huggingface/trl_](https://github.com/huggingface/trl) å°†æ˜¯ä¸€ä¸ªä¸é”™çš„é€‰æ‹©ã€‚

- åœ¨ [åŒ¹é…å­¦ä¹ æ›²çº¿](#åŒ¹é…å­¦ä¹ æ›²çº¿) ä¸­ï¼Œæˆ‘ä»¬å±•ç¤ºäº†æˆ‘ä»¬çš„ä¸»è¦è´¡çŒ®: åˆ›å»ºä¸€ä¸ªä»£ç åº“ï¼Œèƒ½å¤Ÿåœ¨é£æ ¼åŒ–ä»»åŠ¡ä¸­å¤ç° OAI çš„ç»“æœï¼Œå¹¶ä¸”ä¸ [_openai/lm-human-preferences_](https://github.com/openai/lm-human-preferences) çš„å­¦ä¹ æ›²çº¿éå¸¸æ¥è¿‘åœ°åŒ¹é…ã€‚
- ç„¶åæˆ‘ä»¬æ·±å…¥æ¢è®¨äº†ä¸å¤ç° OAI çš„å·¥ä½œç›¸å…³çš„å®ç°ç»†èŠ‚ã€‚åœ¨ [æ€»ä½“å®ç°ç»†èŠ‚](#æ€»ä½“å®ç°ç»†èŠ‚) ä¸­ï¼Œæˆ‘ä»¬è®¨è®ºäº†åŸºæœ¬ç»†èŠ‚ï¼Œåƒå¦‚ä½•ç”Ÿæˆå¥–åŠ±/å€¼å’Œå¦‚ä½•ç”Ÿæˆå“åº”ã€‚åœ¨ [å¥–åŠ±æ¨¡å‹å®ç°ç»†èŠ‚](#å¥–åŠ±æ¨¡å‹å®ç°ç»†èŠ‚) ä¸­ï¼Œæˆ‘ä»¬è®¨è®ºäº†è¯¸å¦‚å¥–åŠ±æ ‡å‡†åŒ–ä¹‹ç±»çš„ç»†èŠ‚ã€‚åœ¨ [ç­–ç•¥è®­ç»ƒå®ç°ç»†èŠ‚](#ç­–ç•¥è®­ç»ƒå®ç°ç»†èŠ‚) ä¸­ï¼Œæˆ‘ä»¬è®¨è®ºäº†æ‹’ç»é‡‡æ ·å’Œå¥–åŠ±â€œç™½åŒ–â€ç­‰ç»†èŠ‚ã€‚

    - åœ¨ [**PyTorch Adam ä¼˜åŒ–å™¨åœ¨å¤„ç† RLHF æ—¶çš„æ•°å€¼é—®é¢˜**](https://www.notion.so/PyTorch-Adam-optimizer-numerical-issues-w-r-t-RLHF-c48b1335349941c6992a04a2c8069f2b?pvs=21) ä¸­ï¼Œæˆ‘ä»¬å¼ºè°ƒäº† TensorFlow å’Œ PyTorch ä¹‹é—´ Adam çš„ä¸€ä¸ªéå¸¸æœ‰è¶£çš„å®ç°åŒºåˆ«ï¼Œå…¶å¯¼è‡´äº†æ¨¡å‹è®­ç»ƒä¸­çš„æ¿€è¿›æ›´æ–°ã€‚

- æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬æ£€æŸ¥äº†åœ¨å¥–åŠ±æ ‡ç­¾ç”± `gpt2-large` ç”Ÿæˆçš„æƒ…å†µä¸‹ï¼Œè®­ç»ƒä¸åŒåŸºç¡€æ¨¡å‹ (ä¾‹å¦‚ gpt2-xl, falcon-1b) çš„æ•ˆæœã€‚
- æœ€åï¼Œæˆ‘ä»¬é€šè¿‡è®¨è®ºä¸€äº›é™åˆ¶æ¥æ€»ç»“æˆ‘ä»¬çš„ç ”ç©¶å·¥ä½œã€‚

**ä»¥ä¸‹æ˜¯ä¸€äº›é‡è¦é“¾æ¥:**

- ğŸ’¾ æˆ‘ä»¬çš„å¤ç°ä»£ç åº“ [_https://github.com/vwxyzjn/lm-human-preference-details_](https://github.com/vwxyzjn/lm-human-preference-details)
- ğŸ¤— RLHF æ¨¡å‹æ¯”è¾ƒç¤ºä¾‹: [_https://huggingface.co/spaces/lm-human-preference-details/rlhf-demo_](https://huggingface.co/spaces/lm-human-preference-details/rlhf-demo)
- ğŸ æ‰€æœ‰çš„ w&b è®­ç»ƒæ—¥å¿— [_https://wandb.ai/openrlbenchmark/lm_human_preference_details_](https://wandb.ai/openrlbenchmark/lm_human_preference_details)

# åŒ¹é…å­¦ä¹ æ›²çº¿

æˆ‘ä»¬çš„ä¸»è¦è´¡çŒ®æ˜¯åœ¨é£æ ¼åŒ–ä»»åŠ¡ä¸­å¤ç° OAI çš„ç»“æœï¼Œä¾‹å¦‚æƒ…æ„Ÿå’Œæè¿°æ€§ã€‚å¦‚ä¸‹å›¾æ‰€ç¤ºï¼Œæˆ‘ä»¬çš„ä»£ç åº“ (æ©™è‰²æ›²çº¿) èƒ½å¤Ÿäº§ç”Ÿä¸ OAI çš„ä»£ç åº“ (è“è‰²æ›²çº¿) å‡ ä¹ç›¸åŒçš„å­¦ä¹ æ›²çº¿ã€‚

![](https://huggingface.co/datasets/trl-internal-testing/example-images/resolve/main/rlhf_implementation_details/Untitled.png)

## å…³äºè¿è¡Œ openai/lm-human-preferences çš„è¯´æ˜

ä¸ºäº†ç›´è§‚æ¯”è¾ƒï¼Œæˆ‘ä»¬è¿è¡Œäº†åŸå§‹çš„ RLHF ä»£ç ï¼Œå…¶ä»“åº“ä½ç½®ä½äº [_openai/lm-human-preferences_](https://github.com/openai/lm-human-preferences)ï¼Œå®ƒå°†æä¾›å®è´µçš„æŒ‡æ ‡ï¼Œä»¥å¸®åŠ©éªŒè¯å’Œè¯Šæ–­æˆ‘ä»¬çš„å¤ç°ã€‚æˆ‘ä»¬èƒ½å¤Ÿè®¾ç½®åŸå§‹çš„ TensorFlow 1.x ä»£ç ï¼Œä½†å®ƒéœ€è¦ä¸€ä¸ªéå¸¸ç‰¹å®šçš„è®¾ç½®:

- OAI çš„æ•°æ®é›†éƒ¨åˆ†æŸå/ä¸¢å¤± (æ‰€ä»¥æˆ‘ä»¬ç”¨ç±»ä¼¼çš„ HF æ•°æ®é›†æ›¿æ¢äº†å®ƒä»¬ï¼Œè¿™å¯èƒ½ä¼šæˆ–å¯èƒ½ä¸ä¼šå¯¼è‡´æ€§èƒ½å·®å¼‚)
  - å…·ä½“æ¥è¯´ï¼Œå®ƒçš„ä¹¦ç±æ•°æ®é›†åœ¨ OpenAI çš„ GCP - Azure è¿ç§»è¿‡ç¨‹ä¸­ä¸¢å¤±äº† ([https://github.com/openai/lm-human-preferences/issues/17#issuecomment-1044051496](https://github.com/openai/lm-human-preferences/issues/17#issuecomment-1044051496))ã€‚æˆ‘ç”¨ Hugging Face çš„ `bookcorpus` æ•°æ®é›†æ›¿æ¢äº†ä¹¦ç±æ•°æ®é›†ï¼ŒåŸåˆ™ä¸Šï¼Œè¿™æ˜¯ç±»ä¼¼ OAI ä½¿ç”¨çš„æ•°æ®é›†ã€‚

- å®ƒä¸èƒ½åœ¨ 1 ä¸ª V100 ä¸Šè¿è¡Œï¼Œå› ä¸ºå®ƒæ²¡æœ‰å®ç°æ¢¯åº¦ç´¯ç§¯ã€‚ç›¸åï¼Œå®ƒä½¿ç”¨ä¸€ä¸ªå¤§çš„ BS (æ‰¹é‡å¤§å°)ï¼Œå¹¶åœ¨ 8 ä¸ª GPU ä¸Šåˆ†å‰² batch (æ‰¹é‡)ï¼Œä»…åœ¨ 1 ä¸ª GPU ä¸Šå°±ä¼šå‡ºç° OOM (å†…å­˜æº¢å‡º)ã€‚
- å®ƒä¸èƒ½åœ¨ 8 ä¸ª A100 ä¸Šè¿è¡Œï¼Œå› ä¸ºå®ƒä½¿ç”¨çš„æ˜¯ TensorFlow 1.xï¼Œä¸ Cuda 8+ ä¸å…¼å®¹ã€‚
- å®ƒä¸èƒ½åœ¨ 8 ä¸ª V100 (16GB) ä¸Šè¿è¡Œï¼Œå› ä¸ºå®ƒä¼š OOM (å†…å­˜æº¢å‡º)ã€‚
- å®ƒåªèƒ½åœ¨ 8 ä¸ª V100 (32GB) ä¸Šè¿è¡Œï¼Œè¿™ç§é…ç½®ä»…ç”± AWS ä»¥ `p3dn.24xlarge` å®ä¾‹çš„å½¢å¼æä¾›ã€‚

# æ€»ä½“å®ç°ç»†èŠ‚

æˆ‘ä»¬ç°åœ¨æ·±å…¥æ¢è®¨ä¸å¤ç° OAI å·¥ä½œç›¸å…³çš„æŠ€æœ¯å®ç°ç»†èŠ‚ã€‚åœ¨è¿™ä¸ªéƒ¨åˆ†ï¼Œæˆ‘ä»¬è®¨è®ºäº†ä¸€äº›åŸºæœ¬ç»†èŠ‚ï¼Œä¾‹å¦‚å¥–åŠ±/å€¼æ˜¯å¦‚ä½•ç”Ÿæˆçš„ï¼Œä»¥åŠå“åº”æ˜¯å¦‚ä½•ç”Ÿæˆçš„ã€‚ä»¥ä¸‹æ˜¯è¿™äº›ç»†èŠ‚ï¼Œä¸æŒ‰ç‰¹å®šé¡ºåºåˆ—å‡º:

1. **å¥–åŠ±æ¨¡å‹å’Œç­–ç•¥çš„ä»·å€¼å¤´å°† `query` å’Œ `response` çš„è¿æ¥ä½œä¸ºè¾“å…¥**

    1. å¥–åŠ±æ¨¡å‹å’Œç­–ç•¥çš„ä»·å€¼å¤´ _ä¸_ ä»…ä»…æŸ¥çœ‹å“åº”ã€‚ç›¸åï¼Œå®ƒå°† `query` å’Œ `response` è¿æ¥åœ¨ä¸€èµ·ï¼Œä½œä¸º `query_response` ([lm_human_preferences/rewards.py#L105-L107](https://github.com/openai/lm-human-preferences/blob/cbfd210bb8b08f6bc5c26878c10984b90f516c66/lm_human_preferences/rewards.py#L105-L107))ã€‚
    2. ä¸¾ä¾‹æ¥è¯´ï¼Œå¦‚æœ `query = "ä»–åœ¨æƒ³æŸäº‹ï¼Œä½†ä»–çš„çœ¼ç¥å¾ˆéš¾è¯»æ‡‚"ã€‚` ï¼Œå’Œ `response = "ä»–çœ‹ç€ä»–çš„å·¦æ‰‹ï¼Œæ‰‹è‡‚ä¼¸åœ¨ä»–çš„å‰é¢ã€‚"` ï¼Œé‚£ä¹ˆå¥–åŠ±æ¨¡å‹å’Œç­–ç•¥çš„ä»·å€¼ä¼šå¯¹`query_response = "ä»–åœ¨æƒ³æŸäº‹ï¼Œä½†ä»–çš„çœ¼ç¥å¾ˆéš¾è¯»æ‡‚ã€‚ä»–çœ‹ç€ä»–çš„å·¦æ‰‹ï¼Œæ‰‹è‡‚ä¼¸åœ¨ä»–çš„å‰é¢ã€‚"` è¿›è¡Œå‰å‘ä¼ é€’ï¼Œå¹¶äº§ç”Ÿå½¢çŠ¶ä¸º `(B, T, 1)` çš„å¥–åŠ±å’Œä»·å€¼ï¼Œå…¶ä¸­ `B` æ˜¯ BS (æ‰¹é‡å¤§å°)ï¼Œ`T` æ˜¯åºåˆ—é•¿åº¦ï¼Œè€Œ `1` ä»£è¡¨å¥–åŠ±å¤´çš„è¾“å‡ºç»“æ„çš„ç»´åº¦ä¸º 1 ([lm_human_preferences/rewards.py#L105-L107](https://github.com/openai/lm-human-preferences/blob/cbfd210bb8b08f6bc5c26878c10984b90f516c66/lm_human_preferences/rewards.py#L105-L107), [lm_human_preferences/policy.py#L111](https://github.com/openai/lm-human-preferences/blob/cbfd210bb8b08f6bc5c26878c10984b90f516c66/lm_human_preferences/policy.py#L111))ã€‚
    3. `T` æ„å‘³ç€æ¯ä¸ª token éƒ½æœ‰ä¸å…¶å’Œå‰æ–‡å…³è”çš„å¥–åŠ±ã€‚ä¾‹å¦‚ï¼Œ`eyes` token å°†æœ‰ä¸€ä¸ªä¸`ä»–åœ¨æƒ³æŸäº‹ï¼Œä½†ä»–çš„çœ¼ç¥å¾ˆéš¾è¯»æ‡‚` ç›¸å¯¹åº”çš„å¥–åŠ±ã€‚

2. **ä½¿ç”¨ç‰¹æ®Šçš„å¡«å…… token æ¥å¡«å……å’Œæˆªæ–­è¾“å…¥ã€‚**

    1. OAI ä¸ºæŸ¥è¯¢ `query_length` è®¾ç½®äº†å›ºå®šçš„è¾“å…¥é•¿åº¦; å®ƒä½¿ç”¨ `pad_token` **å¡«å……** è¿‡çŸ­çš„åºåˆ— ([lm_human_preferences/language/datasets.py#L66-L67](https://github.com/openai/lm-human-preferences/blob/cbfd210bb8b08f6bc5c26878c10984b90f516c66/lm_human_preferences/language/datasets.py#L66-L67))ï¼Œå¹¶ **æˆªæ–­** è¿‡é•¿çš„åºåˆ— ([lm_human_preferences/language/datasets.py#L57](https://github.com/openai/lm-human-preferences/blob/cbfd210bb8b08f6bc5c26878c10984b90f516c66/lm_human_preferences/language/datasets.py#L57))ã€‚è¯¦è§ [æ­¤å¤„](https://huggingface.co/docs/transformers/pad_truncation) ä»¥è·å–è¯¥æ¦‚å¿µçš„é€šç”¨ä»‹ç»ã€‚åœ¨å¡«å……è¾“å…¥æ—¶ï¼ŒOAI ä½¿ç”¨äº†è¯æ±‡è¡¨ä¹‹å¤–çš„ token ([lm_human_preferences/language/encodings.py#L56](https://github.com/openai/lm-human-preferences/blob/cbfd210bb8b08f6bc5c26878c10984b90f516c66/lm_human_preferences/language/encodings.py#L56))ã€‚

        1. **å…³äº HF çš„ transformers â€” å¡«å…… token çš„æ³¨è§£ã€‚** æ ¹æ® ([transformers#2630#issuecomment-578159876](https://github.com/huggingface/transformers/issues/2630#issuecomment-578159876))ï¼Œåœ¨ GPT å’Œ GPT-2 çš„é¢„è®­ç»ƒæœŸé—´æ²¡æœ‰ä½¿ç”¨å¡«å…… token; å› æ­¤ï¼Œtransformer çš„ gpt2 æ¨¡å‹ä¸å…¶åˆ†è¯å™¨æ²¡æœ‰å…³è”çš„å®˜æ–¹å¡«å…… tokenã€‚é€šå¸¸çš„åšæ³•æ˜¯è®¾ç½® `tokenizer.pad_token = tokenizer.eos_token` ï¼Œä½†åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å°†åŒºåˆ†è¿™ä¸¤ä¸ªç‰¹æ®Š token ä»¥åŒ¹é… OAI çš„åŸå§‹è®¾ç½®ï¼Œæ‰€ä»¥æˆ‘ä»¬å°†ä½¿ç”¨ `tokenizer.add_special_tokens({"pad_token": "[PAD]"})` ã€‚

        æ³¨æ„ï¼Œæ²¡æœ‰å¡«å…… token æ˜¯è§£ç å™¨æ¨¡å‹çš„é»˜è®¤è®¾ç½®ï¼Œå› ä¸ºå®ƒä»¬åœ¨é¢„è®­ç»ƒæœŸé—´ä½¿ç”¨â€œæ‰“åŒ…â€è®­ç»ƒï¼Œè¿™æ„å‘³ç€è®¸å¤šåºåˆ—è¢«è¿æ¥å¹¶ç”± EOS token åˆ†éš”ï¼Œè¿™äº›åºåˆ—çš„å—åœ¨é¢„è®­ç»ƒæœŸé—´å§‹ç»ˆå…·æœ‰æœ€å¤§é•¿åº¦å¹¶è¢«é¦ˆé€åˆ°æ¨¡å‹ä¸­ã€‚

    2. å½“æŠŠæ‰€æœ‰äº‹ç‰©æ”¾åœ¨ä¸€èµ·æ—¶ï¼Œè¿™é‡Œæœ‰ä¸€ä¸ªä¾‹å­

        ```python
        import transformers
        tokenizer = transformers.AutoTokenizer.from_pretrained("gpt2", padding_side="right")
        tokenizer.add_special_tokens({"pad_token": "[PAD]"})
        query_length = 5
        texts = [
            "usually, he would",
            "she thought about it",
        ]
        tokens = []
        for text in texts:
            tokens.append(tokenizer.encode(text)[:query_length])

        print("tokens", tokens)
        inputs = tokenizer.pad(
            {"input_ids": tokens},
            padding="max_length",
            max_length=query_length,
            return_tensors="pt",
            return_attention_mask=True,
        )
        print("inputs", inputs)

        """prints are
        tokens [[23073, 11, 339, 561], [7091, 1807, 546, 340]]
        inputs {'input_ids': tensor([[23073, 11, 339, 561, 50257],
                [ 7091, 1807, 546, 340, 50257]]), 'attention_mask': tensor([[1, 1, 1, 1, 0],
                [1, 1, 1, 1, 0]])}
        """
        ```

3. **ç›¸åº”åœ°è°ƒæ•´å¡«å…… token çš„ä½ç½®ç´¢å¼•**

    1. åœ¨è®¡ç®— logits æ—¶ï¼ŒOAI çš„ä»£ç é€šè¿‡é€‚å½“åœ°å±è”½å¡«å…… token æ¥å·¥ä½œã€‚è¿™æ˜¯é€šè¿‡æ‰¾å‡ºä¸å¡«å…… token ç›¸å¯¹åº”çš„ token ç´¢å¼•æ¥å®ç°çš„ ([lm_human_preferences/language/model.py#L296-L297](https://github.com/openai/lm-human-preferences/blob/cbfd210bb8b08f6bc5c26878c10984b90f516c66/lm_human_preferences/language/model.py#L296-L297))ï¼Œç„¶åç›¸åº”åœ°è°ƒæ•´å®ƒä»¬çš„ä½ç½®ç´¢å¼• ([lm_human_preferences/language/model.py#L320](https://github.com/openai/lm-human-preferences/blob/cbfd210bb8b08f6bc5c26878c10984b90f516c66/lm_human_preferences/language/model.py#L320))ã€‚
    2. ä¾‹å¦‚ï¼Œå¦‚æœ `query=[23073, 50259, 50259]` å’Œ `response=[11, 339, 561]` ï¼Œå…¶ä¸­ ( `50259` æ˜¯ OAI çš„å¡«å…… token)ï¼Œå®ƒä¼šåˆ›å»ºä½ç½®ç´¢å¼•ä¸º `[[0 1 1 1 2 3]]` å¹¶ä¸”å¦‚ä¸‹çš„ logitsã€‚æ³¨æ„å¡«å…… token å¯¹åº”çš„ logits å¦‚ä½•ä¿æŒä¸å˜ï¼è¿™æ˜¯æˆ‘ä»¬åœ¨å¤åˆ¶è¿‡ç¨‹ä¸­åº”è¯¥è¿½æ±‚çš„æ•ˆæœã€‚

        ```python
        all_logits [[[ -35.28693 -34.2875 -38.16074 ... -41.595802 -41.082108
            -35.36577 ]
        [ -35.28693 -34.2875 -38.16074 ... -41.595802 -41.082108
            -35.36577 ]
        [ -35.28693 -34.2875 -38.16074 ... -41.595802 -41.082108
            -35.36577 ]
        [-111.303955 -110.94471 -112.90624 ... -113.13064 -113.7788
        -109.17345 ]
        [-111.51512 -109.61077 -114.90231 ... -118.43514 -111.56671
        -112.12478 ]
        [-122.69775 -121.84468 -128.27417 ... -132.28055 -130.39604
        -125.707756]]] (1, 6, 50257)
        ```


    3. **å…³äº HF çš„ transformers â€” `position_ids` å’Œ `padding_side` çš„æ³¨è§£ã€‚** æˆ‘ä»¬å¯ä»¥é€šè¿‡ 1) å·¦å¡«å……å’Œ 2) ä¼ å…¥é€‚å½“çš„ `position_ids` ï¼Œä½¿ç”¨ Hugging Face çš„ transformer å¤åˆ¶ç²¾ç¡®çš„ logits:

        ```python
        import torch
        import transformers
        tokenizer = transformers.AutoTokenizer.from_pretrained("gpt2", padding_side="right")
        tokenizer.add_special_tokens({"pad_token": "[PAD]"})
        pad_id = tokenizer.pad_token_id
        query = torch.tensor([
            [pad_id, pad_id, 23073],
        ])
        response = torch.tensor([
            [11, 339, 561],
        ])
        temperature = 1.0

        query = torch.tensor(query)
        response = torch.tensor(response).long()
        context_length = query.shape[1]
        query_response = torch.cat((query, response), 1)
        pretrained_model = transformers.AutoModelForCausalLM.from_pretrained("gpt2")
        def forward(policy, query_responses, tokenizer):
            attention_mask = query_responses != tokenizer.pad_token_id
            position_ids = attention_mask.cumsum(1) - attention_mask.long() # exclusive cumsum
            input_ids = query_responses.clone()
            input_ids[~attention_mask] = 0
            return policy(
                input_ids=input_ids,
                attention_mask=attention_mask,
                position_ids=position_ids,
                return_dict=True,
                output_hidden_states=True,
            )
        output = forward(pretrained_model, query_response, tokenizer)
        logits = output.logits
        logits /= temperature
        print(logits)

        """
        tensor([[[ -26.9395, -26.4709, -30.0456, ..., -33.2208, -33.2884,
                -27.4360],
                [ -27.1677, -26.7330, -30.2386, ..., -33.6813, -33.6931,
                -27.5928],
                [ -35.2869, -34.2875, -38.1608, ..., -41.5958, -41.0821,
                -35.3658],
                [-111.3040, -110.9447, -112.9062, ..., -113.1306, -113.7788,
                -109.1734],
                [-111.5152, -109.6108, -114.9024, ..., -118.4352, -111.5668,
                -112.1248],
                [-122.6978, -121.8447, -128.2742, ..., -132.2805, -130.3961,
                -125.7078]]], grad_fn=<DivBackward0>)
        """
        ```


    4. **å…³äº HF çš„ transformers â€”â€”åœ¨ `ç”Ÿæˆ` è¿‡ç¨‹ä¸­çš„ `position_ids` çš„æ³¨è§£:** åœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬ä¸åº”ä¼ å…¥ `position_ids` ï¼Œå› ä¸ºåœ¨ `transformers` ä¸­ï¼Œ `position_ids` å·²ç»ä»¥æŸç§æ–¹å¼è¢«è°ƒæ•´äº†ã€‚å½“æˆ‘åœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­ä¹Ÿä¼ å…¥ `position_ids` æ—¶ï¼Œæ€§èƒ½ä¼šç¾éš¾æ€§åœ°æ¶åŒ–ã€‚

    é€šå¸¸æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬å‡ ä¹ä»ä¸åœ¨ transformers ä¸­ä¼ é€’ `position_ids` ã€‚æ‰€æœ‰çš„é®è”½ (masking) å’Œç§»ä½ (shifting) logic å·²ç»å®ç°ï¼Œä¾‹å¦‚ï¼Œåœ¨ `generate` å‡½æ•°ä¸­ (éœ€è¦æ°¸ä¹…çš„ä»£ç é“¾æ¥)ã€‚
4. **ç”Ÿæˆå›ºå®šé•¿åº¦å“åº”çš„å“åº”ç”Ÿæˆä¸éœ€è¦å¡«å……ã€‚**

    1. åœ¨å“åº”ç”ŸæˆæœŸé—´ï¼ŒOAI ä½¿ç”¨ `top_k=0, top_p=1.0` å¹¶ä»…åœ¨è¯æ±‡è¡¨ä¸Šåšåˆ†ç±»æ ·æœ¬ ([lm_human_preferences/language/sample.py#L43](https://github.com/openai/lm-human-preferences/blob/cbfd210bb8b08f6bc5c26878c10984b90f516c66/lm_human_preferences/language/sample.py#L43))ï¼Œä»£ç ä¼šä¸€ç›´é‡‡æ ·ï¼Œç›´åˆ°ç”Ÿæˆå›ºå®šé•¿åº¦çš„å“åº” ([lm_human_preferences/policy.py#L103](https://github.com/openai/lm-human-preferences/blob/cbfd210bb8b08f6bc5c26878c10984b90f516c66/lm_human_preferences/policy.py#L103))ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œå³ä½¿é‡åˆ° EOS (åºåˆ—ç»“æŸ) token ï¼Œå®ƒä¹Ÿä¼šç»§ç»­é‡‡æ ·ã€‚
    2. **å…³äº HF çš„ transformers çš„æ³¨è§£ â€” åœ¨ `eos_token` å¤„é‡‡æ ·å¯èƒ½ä¼šåœæ­¢:** åœ¨ `transformers` ä¸­ï¼Œç”Ÿæˆå¯èƒ½ä¼šåœ¨ `eos_token` å¤„åœæ­¢ ([src/transformers/generation/utils.py#L2248-L2256](https://github.com/huggingface/transformers/blob/67b85f24def79962ce075353c2627f78e0e53e9f/src/transformers/generation/utils.py#L2248-L2256))ï¼Œè¿™ä¸ OAI çš„è®¾ç½®ä¸åŒã€‚ä¸ºäº†å¯¹é½è®¾ç½®ï¼Œæˆ‘ä»¬éœ€è¦è®¾ç½® `pretrained_model.generation_config.eos_token_id = None, pretrained_model.generation_config.pad_token_id = None` ã€‚è¯·æ³¨æ„ï¼Œ `transformers.GenerationConfig(eos_token_id=None, pad_token_id=None, ...)` ä¸èµ·ä½œç”¨ï¼Œå› ä¸º `pretrained_model.generation_config` ä¼šè¦†ç›–å¹¶è®¾ç½®ä¸€ä¸ª `eos_token` ã€‚

        ```python
        import torch
        import transformers
        tokenizer = transformers.AutoTokenizer.from_pretrained("gpt2", padding_side="right")
        tokenizer.add_special_tokens({"pad_token": "[PAD]"})
        pad_id = tokenizer.pad_token_id
        query = torch.tensor([
            [pad_id, pad_id, 23073],
        ])
        response = torch.tensor([
            [11, 339, 561],
        ])
        response_length = 4
        temperature = 0.7
        pretrained_model = transformers.AutoModelForCausalLM.from_pretrained("gpt2")
        pretrained_model.generation_config.eos_token_id = None # disable `pad_token_id` and `eos_token_id` because we just want to
        pretrained_model.generation_config.pad_token_id = None # generate tokens without truncation / padding
        generation_config = transformers.GenerationConfig(
            max_new_tokens=response_length,
            min_new_tokens=response_length,
            temperature=temperature,
            top_k=0.0,
            top_p=1.0,
            do_sample=True,
        )
        context_length = query.shape[1]
        attention_mask = query != tokenizer.pad_token_id
        input_ids = query.clone()
        input_ids[~attention_mask] = 0 # set padding tokens to 0
        output = pretrained_model.generate(
            input_ids=input_ids,
            attention_mask=attention_mask,
            # position_ids=attention_mask.cumsum(1) - attention_mask.long(), # generation collapsed if this was turned on. TODO: why does generation collapse with this?
            generation_config=generation_config,
            return_dict_in_generate=True,
        )
        print(output.sequences)

        """
        tensor([[ 0, 0, 23073, 16851, 11, 475, 991]])
        """
        ```


    3. è¯·æ³¨æ„ï¼Œåœ¨è¾ƒæ–°çš„ä»£ç åº“ https://github.com/openai/summarize-from-feedback ä¸­ï¼Œå½“é‡åˆ° EOS token æ—¶ï¼ŒOAI ç¡®å®ä¼šåœæ­¢é‡‡æ · ([summarize_from_feedback/utils/experiment_helpers.py#L19](https://github.com/openai/summarize-from-feedback/blob/8af822a428c93432aa80ffbe5b065a8f93895669/summarize_from_feedback/utils/experiment_helpers.py#L19))ã€‚ç„¶è€Œï¼Œåœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬çš„ç›®æ ‡æ˜¯è¿›è¡Œ 1:1 çš„å¤åˆ»ï¼Œæ‰€ä»¥æˆ‘ä»¬è°ƒæ•´äº†è®¾ç½®ï¼Œå³ä½¿é‡åˆ° eos_token ä¹Ÿå¯ä»¥ç»§ç»­é‡‡æ ·ã€‚

5. **å¥–åŠ±æ¨¡å‹å’Œç­–ç•¥è®­ç»ƒçš„å­¦ä¹ ç‡é€€ç«ã€‚**

    1. æ­£å¦‚ Ziegler ç­‰äºº (2019) å»ºè®®çš„ï¼Œå¥–åŠ±æ¨¡å‹åªè®­ç»ƒä¸€ä¸ª epchoï¼Œä»¥é¿å…è¿‡åº¦æ‹Ÿåˆæœ‰é™é‡çš„äººç±»æ³¨é‡Šæ•°æ® (ä¾‹å¦‚ï¼Œ`descriptiveness` ä»»åŠ¡åªæœ‰å¤§çº¦ 5000 ä¸ªæ ‡ç­¾)ã€‚åœ¨è¿™ä¸ªå•ä¸€çš„ epcho ä¸­ï¼Œå­¦ä¹ ç‡ä¼šé€€ç«è‡³é›¶ ([lm_human_preferences/train_reward.py#L249](https://github.com/openai/lm-human-preferences/blob/cbfd210bb8b08f6bc5c26878c10984b90f516c66/lm_human_preferences/train_reward.py#L249))ã€‚
    2. ç±»ä¼¼äºå¥–åŠ±æ¨¡å‹è®­ç»ƒï¼Œç­–ç•¥è®­ç»ƒçš„å­¦ä¹ ç‡ä¹Ÿä¼šé€€ç«è‡³é›¶ ([lm_human_preferences/train_policy.py#L172-L173](https://github.com/openai/lm-human-preferences/blob/cbfd210bb8b08f6bc5c26878c10984b90f516c66/lm_human_preferences/train_policy.py#L172-L173))ã€‚

6. **ä¸ºä¸åŒçš„è¿›ç¨‹ä½¿ç”¨ä¸åŒçš„ç§å­**

    1. åœ¨ç”Ÿæˆ 8 ä¸ª GPU è¿›ç¨‹è¿›è¡Œæ•°æ®å¹¶è¡Œæ—¶ï¼ŒOAI ä¸ºæ¯ä¸ªè¿›ç¨‹è®¾ç½®äº†ä¸åŒçš„éšæœºç§å­ ([lm_human_preferences/utils/core.py#L108-L111](https://github.com/openai/lm-human-preferences/blob/cbfd210bb8b08f6bc5c26878c10984b90f516c66/lm_human_preferences/utils/core.py#L108-L111))ã€‚åœ¨å®ç°ä¸Šï¼Œè¿™æ˜¯é€šè¿‡ `local_seed = args.seed + process_rank * 100003` å®Œæˆçš„ã€‚ç§å­ä¼šè®©æ¨¡å‹äº§ç”Ÿä¸åŒçš„å“åº”å¹¶å¾—åˆ°ä¸åŒçš„åˆ†æ•°ï¼Œä¾‹å¦‚ã€‚

        1. æ³¨: æˆ‘è®¤ä¸ºæ•°æ®é›†çš„æ´—ç‰Œ (shuffling) å­˜åœ¨ä¸€ä¸ªé”™è¯¯â€”â€”ç”±äºæŸç§åŸå› ï¼Œæ•°æ®é›†æ˜¯ä½¿ç”¨ç›¸åŒçš„ç§å­è¿›è¡Œæ´—ç‰Œçš„ ([lm_human_preferences/lm_tasks.py#L94-L97](https://github.com/openai/lm-human-preferences/blob/cbfd210bb8b08f6bc5c26878c10984b90f516c66/lm_human_preferences/lm_tasks.py#L94-L97))ã€‚

# å¥–åŠ±æ¨¡å‹å®ç°ç»†èŠ‚

åœ¨æœ¬èŠ‚ä¸­ï¼Œæˆ‘ä»¬è®¨è®ºäº†å¥–åŠ±æ¨¡å‹ç‰¹å®šçš„å®ç°ç»†èŠ‚ã€‚æˆ‘ä»¬è®¨è®ºäº†è¯¸å¦‚å¥–åŠ±å½’ä¸€åŒ–å’Œå±‚åˆå§‹åŒ–ç­‰ç»†èŠ‚ã€‚ä»¥ä¸‹æ˜¯è¿™äº›ç»†èŠ‚ï¼Œä¸æŒ‰ç‰¹å®šé¡ºåºæ’åˆ—:

1. **å¥–åŠ±æ¨¡å‹åªè¾“å‡ºæœ€åä¸€ä¸ª token çš„å€¼ã€‚**

    1. è¯·æ³¨æ„ï¼Œåœ¨å¯¹ `query` å’Œ `response` çš„è¿æ¥è¿›è¡Œå‰å‘ä¼ é€’åè·å¾—çš„å¥–åŠ±å°†å…·æœ‰å½¢çŠ¶ `(B, T, 1)` ï¼Œå…¶ä¸­ `B` æ˜¯ BS(æ‰¹é‡å¤§å°)ï¼Œ`T` æ˜¯åºåˆ—é•¿åº¦ (å§‹ç»ˆç›¸åŒ; åœ¨ OAI çš„è®¾ç½®ä¸­ï¼Œå®ƒæ˜¯ `query_length + response_length = 64 + 24 = 88` ï¼Œç”¨äºé£æ ¼ä»»åŠ¡ï¼Œå‚è§ [launch.py#L9-L11](https://github.com/openai/lm-human-preferences/blob/cbfd210bb8b08f6bc5c26878c10984b90f516c66/launch.py#L9-L11))ï¼Œ`1` æ˜¯å¥–åŠ±å¤´å…¶ç»´åº¦ä¸º 1ã€‚å¯¹äº RLHF (Reinforcement Learning from Human Feedbackï¼Œé€šè¿‡äººç±»åé¦ˆè¿›è¡Œå¼ºåŒ–å­¦ä¹ ) çš„ç›®çš„ï¼ŒåŸå§‹ä»£ç åº“æå–æœ€åä¸€ä¸ª token çš„å¥–åŠ± ([lm_human_preferences/rewards.py#L132](https://github.com/openai/lm-human-preferences/blob/cbfd210bb8b08f6bc5c26878c10984b90f516c66/lm_human_preferences/rewards.py#L132))ï¼Œå› æ­¤å¥–åŠ±å°†åªå…·æœ‰å½¢çŠ¶ `(B, 1)` ã€‚
    2. è¯·æ³¨æ„ï¼Œåœ¨è¾ƒæ–°çš„ä»£ç åº“ [_openai/summarize-from-feedback_](https://github.com/openai/summarize-from-feedback) ä¸­ï¼ŒOAI åœ¨é‡åˆ° EOS token æ—¶åœæ­¢é‡‡æ · ([summarize_from_feedback/utils/experiment_helpers.py#L19](https://github.com/openai/summarize-from-feedback/blob/8af822a428c93432aa80ffbe5b065a8f93895669/summarize_from_feedback/utils/experiment_helpers.py#L19))ã€‚åœ¨æå–å¥–åŠ±æ—¶ï¼Œå®ƒå°†ç¡®å®š `last_response_index` ï¼Œå³ EOS token ä¹‹å‰çš„ç´¢å¼• ([#L11-L13](https://github.com/openai/summarize-from-feedback/blob/8af822a428c93432aa80ffbe5b065a8f93895669/summarize_from_feedback/reward_model.py#L11-L13))ï¼Œå¹¶åœ¨è¯¥ç´¢å¼•å¤„æå–å¥–åŠ± ([summarize_from_feedback/reward_model.py#L59](https://github.com/openai/summarize-from-feedback/blob/8af822a428c93432aa80ffbe5b065a8f93895669/summarize_from_feedback/reward_model.py#L59))ã€‚ä½†åœ¨æ­¤å·¥ä½œä¸­ï¼Œæˆ‘ä»¬åªæ˜¯åšæŒåŸå§‹è®¾ç½®ã€‚

2. **å¥–åŠ±å¤´å±‚åˆå§‹åŒ–**

    1. å¥–åŠ±å¤´çš„æƒé‡æ˜¯æ ¹æ® \( \mathcal{N}\left(0,1 /\left(\sqrt{d_{\text {model }}+1}\right)\right) \) åˆå§‹åŒ–çš„ ([lm_human_preferences/language/model.py#L368,](https://github.com/openai/lm-human-preferences/blob/cbfd210bb8b08f6bc5c26878c10984b90f516c66/lm_human_preferences/language/model.py#L368) [lm_human_preferences/language/model.py#L251-L252](https://github.com/openai/lm-human-preferences/blob/cbfd210bb8b08f6bc5c26878c10984b90f516c66/lm_human_preferences/language/model.py#L251-L252))ã€‚è¿™ä¸ Stiennon ç­‰äººçš„è®¾ç½®ç›¸ç¬¦ï¼Œ2020 å¹´ ([summarize_from_feedback/query_response_model.py#L106-L107](https://github.com/openai/summarize-from-feedback/blob/8af822a428c93432aa80ffbe5b065a8f93895669/summarize_from_feedback/query_response_model.py#L106-L107)) (é™„æ³¨ï¼ŒStiennon ç­‰äººï¼Œ2020 å¹´åœ¨ç¬¬ 17 é¡µä¸Šæœ‰ä¸€ä¸ªé”™å­—ï¼Œè¡¨ç¤ºåˆ†å¸ƒæ˜¯ \( \mathcal{N}\left(0,1 /\left(d_{\text {model }}+1\right)\right) \) æ²¡æœ‰å¹³æ–¹æ ¹)
    2. å¥–åŠ±å¤´çš„ bias (åç½®) è®¾ä¸º 0 ([lm_human_preferences/language/model.py#L254](https://github.com/openai/lm-human-preferences/blob/cbfd210bb8b08f6bc5c26878c10984b90f516c66/lm_human_preferences/language/model.py#L254))ã€‚

3. **å¥–åŠ±æ¨¡å‹çš„å‰åå½’ä¸€åŒ–**

    1. åœ¨è®ºæ–‡ä¸­ï¼ŒZiegler ç­‰äºº (2019) æåˆ°â€œä¸ºäº†ä¿æŒè®­ç»ƒè¿‡ç¨‹ä¸­å¥–åŠ±æ¨¡å‹çš„è§„æ¨¡ä¸€è‡´ï¼Œæˆ‘ä»¬å°†å…¶å½’ä¸€åŒ–ï¼Œä½¿å…¶åœ¨ \( x \sim \mathcal{D}, y \sim \rho(Â·|x) \) çš„æƒ…å†µä¸‹ï¼Œå‡å€¼ä¸º 0ï¼Œæ–¹å·®ä¸º 1â€ã€‚ä¸ºäº†æ‰§è¡Œå½’ä¸€åŒ–è¿‡ç¨‹ï¼Œä»£ç é¦–å…ˆåˆ›å»ºäº† `reward_gain` å’Œ `reward_bias` ï¼Œä»¥ä¾¿å¯ä»¥é€šè¿‡ `reward = reward * reward_gain + reward_bias` æ¥è®¡ç®—å¥–åŠ±å€¼ ([lm_human_preferences/rewards.py#L50-L51](https://github.com/openai/lm-human-preferences/blob/cbfd210bb8b08f6bc5c26878c10984b90f516c66/lm_human_preferences/rewards.py#L50-L51))ã€‚
    2. åœ¨æ‰§è¡Œå½’ä¸€åŒ–è¿‡ç¨‹æ—¶ï¼Œä»£ç é¦–å…ˆè®¾ç½® `reward_gain=1, reward_bias=0` ([lm_human_preferences/train_reward.py#L211](https://github.com/openai/lm-human-preferences/blob/cbfd210bb8b08f6bc5c26878c10984b90f516c66/lm_human_preferences/train_reward.py#L211))ï¼Œç„¶åä»ç›®æ ‡æ•°æ®é›† (ä¾‹å¦‚ï¼Œ`bookcorpus, tldr, cnndm` ) ä¸­æ”¶é›†é‡‡æ ·æŸ¥è¯¢ã€å®Œæˆçš„å“åº”å’Œè¯„ä¼°çš„å¥–åŠ±ã€‚æ¥ç€ï¼Œå®ƒå¾—åˆ°è¯„ä¼°å¥–åŠ±çš„ **å®è¯å‡å€¼å’Œæ ‡å‡†å·®** ([lm_human_preferences/train_reward.py#L162-L167](https://github.com/openai/lm-human-preferences/blob/cbfd210bb8b08f6bc5c26878c10984b90f516c66/lm_human_preferences/train_reward.py#L162-L167))ï¼Œå¹¶å°è¯•è®¡ç®— `reward_gain` å’Œ `reward_bias` åº”è¯¥æ˜¯ä»€ä¹ˆã€‚
    3. æˆ‘ä»¬ç”¨\( \mu_{\mathcal{D}} \) æ¥è¡¨ç¤ºå®è¯å‡å€¼ï¼Œç”¨\( \sigma_{\mathcal{D}} \) è¡¨ç¤ºå®è¯æ ‡å‡†å·®ï¼Œç”¨\(g\) è¡¨ç¤º `reward_gain` ï¼Œç”¨\(b\) è¡¨ç¤º `reward_bias` ï¼Œç”¨\( \mu_{\mathcal{T}} = 0\) è¡¨ç¤º **ç›®æ ‡å‡å€¼**ï¼Œç”¨\( \sigma_{\mathcal{T}}=1\) è¡¨ç¤º **ç›®æ ‡æ ‡å‡†å·®**ã€‚ç„¶åæˆ‘ä»¬æœ‰ä»¥ä¸‹å…¬å¼ã€‚

        $$
        \begin{aligned}g*\mathcal{N}(\mu_{\mathcal{D}}, \sigma_{\mathcal{D}}) + b &= \mathcal{N}(g*\mu_{\mathcal{D}}, g*\sigma_{\mathcal{D}}) + b\\&= \mathcal{N}(g*\mu_{\mathcal{D}} + b, g*\sigma_{\mathcal{D}}) \\&= \mathcal{N}(\mu_{\mathcal{T}}, \sigma_{\mathcal{T}}) \\g &= \frac{\sigma_{\mathcal{T}}}{\sigma_{\mathcal{D}}} \\b &= \mu_{\mathcal{T}} - g*\mu_{\mathcal{D}}\end{aligned}
        $$

    4. ç„¶ååœ¨å¥–åŠ±æ¨¡å‹è®­ç»ƒçš„ **å‰** å’Œ **å** åº”ç”¨å½’ä¸€åŒ–è¿‡ç¨‹ ([lm_human_preferences/train_reward.py#L232-L234](https://github.com/openai/lm-human-preferences/blob/cbfd210bb8b08f6bc5c26878c10984b90f516c66/lm_human_preferences/train_reward.py#L232-L234)ï¼Œ[lm_human_preferences/train_reward.py#L252-L254](https://github.com/openai/lm-human-preferences/blob/cbfd210bb8b08f6bc5c26878c10984b90f516c66/lm_human_preferences/train_reward.py#L252-L254))ã€‚
    5. è¯·æ³¨æ„ï¼Œæˆ‘ä»¬ä¸ºå½’ä¸€åŒ–ç›®çš„ç”Ÿæˆçš„å“åº” \( y \sim \rho(Â·|x) \) æ¥è‡ªé¢„è®­ç»ƒçš„è¯­è¨€æ¨¡å‹ \(\rho \)ã€‚æ¨¡å‹ \(\rho \) è¢«å›ºå®šä¸ºå‚è€ƒï¼Œå¹¶ä¸”åœ¨å¥–åŠ±å­¦ä¹ ä¸­ä¸ä¼šæ›´æ–° ([lm_human_preferences/train_reward.py#L286C1-L286C31](https://github.com/openai/lm-human-preferences/blob/master/lm_human_preferences/train_reward.py#L286C1-L286C31))ã€‚

# ç­–ç•¥è®­ç»ƒå®ç°ç»†èŠ‚

åœ¨æœ¬èŠ‚ä¸­ï¼Œæˆ‘ä»¬å°†æ·±å…¥æ¢è®¨è¯¸å¦‚å±‚åˆå§‹åŒ–ã€æ•°æ®åå¤„ç†å’Œ dropout è®¾ç½®ç­‰ç»†èŠ‚ã€‚æˆ‘ä»¬è¿˜å°†æ¢è®¨ä¸€äº›æŠ€æœ¯ï¼Œå¦‚æ‹’ç»é‡‡æ ·å’Œå¥–åŠ± â€œç™½åŒ–â€ï¼Œä»¥åŠè‡ªé€‚åº” KLã€‚ä»¥ä¸‹æ˜¯è¿™äº›ç»†èŠ‚ï¼Œæ’åˆ—ä¸åˆ†å…ˆå:

1. **é€šè¿‡é‡‡æ ·æ¸©åº¦æ¥ç¼©æ”¾ logits**

    1. åœ¨è®¡ç®—å“åº”çš„å¯¹æ•°æ¦‚ç‡æ—¶ï¼Œæ¨¡å‹é¦–å…ˆè¾“å‡ºå“åº”ä¸­ token çš„ logitsï¼Œç„¶åç”¨é‡‡æ ·æ¸©åº¦é™¤ä»¥è¿™äº› logits ([lm_human_preferences/policy.py#L121](https://github.com/openai/lm-human-preferences/blob/cbfd210bb8b08f6bc5c26878c10984b90f516c66/lm_human_preferences/policy.py#L121))ã€‚å³ `logits /= self.temperature`
    2. åœ¨ä¸€ä¸ªéæ­£å¼çš„æµ‹è¯•ä¸­ï¼Œæˆ‘ä»¬å‘ç°å¦‚æœä¸è¿›è¡Œæ­¤ç¼©æ”¾ï¼ŒKL æ•£åº¦ä¼šæ¯”é¢„æœŸæ›´å¿«åœ°ä¸Šå‡ï¼Œæ€§èƒ½ä¼šä¸‹é™ã€‚

2. **ä»·å€¼å¤´å±‚çš„åˆå§‹åŒ–**

    1. ä»·å€¼å¤´çš„æƒé‡æ˜¯æ ¹æ® \(\mathcal{N}(0,0)\) è¿›è¡Œåˆå§‹åŒ–çš„ ([lm_human_preferences/language/model.py#L368](https://github.com/openai/lm-human-preferences/blob/cbfd210bb8b08f6bc5c26878c10984b90f516c66/lm_human_preferences/language/model.py#L368)ã€[lm_human_preferences/language/model.py#L251-L252](https://github.com/openai/lm-human-preferences/blob/cbfd210bb8b08f6bc5c26878c10984b90f516c66/lm_human_preferences/language/model.py#L251-L252))ã€‚
    2. å¥–åŠ±å¤´çš„ bias (åç½®) è®¾ç½®ä¸º 0 ([lm_human_preferences/language/model.py#L254](https://github.com/openai/lm-human-preferences/blob/cbfd210bb8b08f6bc5c26878c10984b90f516c66/lm_human_preferences/language/model.py#L254))ã€‚

3. **é€‰æ‹©ä»¥å¥å·å¼€å§‹å’Œç»“æŸçš„æŸ¥è¯¢æ–‡æœ¬**

    1. è¿™æ˜¯æ•°æ®é¢„å¤„ç†çš„ä¸€éƒ¨åˆ†:
        1. å°è¯•ä»…åœ¨ `start_text="."` ä¹‹åé€‰æ‹©æ–‡æœ¬ ([lm_human_preferences/language/datasets.py#L51](https://github.com/openai/lm-human-preferences/blob/cbfd210bb8b08f6bc5c26878c10984b90f516c66/lm_human_preferences/language/datasets.py#L51))
        2. å°è¯•åœ¨ `end_text="."` ä¹‹å‰é€‰æ‹©æ–‡æœ¬ ([lm_human_preferences/language/datasets.py#L61](https://github.com/openai/lm-human-preferences/blob/cbfd210bb8b08f6bc5c26878c10984b90f516c66/lm_human_preferences/language/datasets.py#L61))
        3. ç„¶åå¡«å……æ–‡æœ¬ ([lm_human_preferences/language/datasets.py#L66-L67](https://github.com/openai/lm-human-preferences/blob/cbfd210bb8b08f6bc5c26878c10984b90f516c66/lm_human_preferences/language/datasets.py#L66-L67))


    2. åœ¨è¿è¡Œ `openai/lm-human-preferences` æ—¶ï¼ŒOAI çš„æ•°æ®é›†éƒ¨åˆ†æŸå/ä¸¢å¤± ([openai/lm-human-preferences/issues/17#issuecomment-104405149](https://github.com/openai/lm-human-preferences/issues/17#issuecomment-1044051496))ï¼Œå› æ­¤æˆ‘ä»¬ä¸å¾—ä¸ç”¨ç±»ä¼¼çš„ HF æ•°æ®é›†æ›¿æ¢å®ƒä»¬ï¼Œè¿™å¯èƒ½ä¼šæˆ–å¯èƒ½ä¸ä¼šå¯¼è‡´æ€§èƒ½å·®å¼‚ã€‚
    3. å¯¹äºä¹¦ç±æ•°æ®é›†ï¼Œæˆ‘ä»¬ä½¿ç”¨ [https://huggingface.co/datasets/bookcorpus](https://huggingface.co/datasets/bookcorpus)ï¼Œæˆ‘ä»¬å‘ç°æ²¡æœ‰å¿…è¦æå–ä»¥å¥å·å¼€å§‹å’Œç»“æŸçš„å¥å­ï¼Œå› ä¸ºæ•°æ®é›†å·²ç»æ˜¯è¿™æ ·é¢„å¤„ç†è¿‡çš„ (ä¾‹å¦‚ï¼Œ`"usually , he would be tearing around the living room , playing with his toys."` ) ä¸ºæ­¤ï¼Œæˆ‘ä»¬ä¸º `sentiment` å’Œ `descriptiveness` ä»»åŠ¡è®¾ç½® `start_text=None, end_text=None` ã€‚

4. **ç¦ç”¨ dropout**

    1. Ziegler ç­‰äºº (2019) å»ºè®®ï¼Œâ€œæˆ‘ä»¬åœ¨ç­–ç•¥è®­ç»ƒä¸­ä¸ä½¿ç”¨ dropoutã€‚â€ è¿™ä¹Ÿåœ¨ä»£ç ä¸­å®ç°äº† ([lm_human_preferences/policy.py#L48](https://github.com/openai/lm-human-preferences/blob/cbfd210bb8b08f6bc5c26878c10984b90f516c66/lm_human_preferences/policy.py#L48))ã€‚

5. **æ‹’ç»é‡‡æ ·**

    1. Ziegler ç­‰äºº (2019) å»ºè®®: â€œæˆ‘ä»¬ä½¿ç”¨æ‹’ç»é‡‡æ ·æ¥ç¡®ä¿åœ¨ç¬¬ 16 å’Œ 24 ä¸ª token ä¹‹é—´æœ‰ä¸€ä¸ªå¥å·ï¼Œç„¶ååœ¨é‚£ä¸ªå¥å·å¤„æˆªæ–­ (è¿™æ˜¯â€˜å¥å­ç»“æŸâ€™çš„ç²—ç•¥è¿‘ä¼¼ã€‚æˆ‘ä»¬é€‰æ‹©å®ƒæ˜¯å› ä¸ºå®ƒå¾ˆå®¹æ˜“é›†æˆåˆ° RL å¾ªç¯ä¸­ï¼Œå³ä½¿æ˜¯ç²—ç•¥çš„è¿‘ä¼¼ä¹Ÿè¶³ä»¥ä½¿äººç±»è¯„ä¼°ä»»åŠ¡å˜å¾—ç¨å¾®å®¹æ˜“ä¸€äº›)ã€‚åœ¨ RL å¾®è°ƒæœŸé—´ï¼Œæˆ‘ä»¬å¯¹æ²¡æœ‰è¿™æ ·çš„å¥å·çš„å»¶ç»­ç»™äºˆå›ºå®šå¥–åŠ± -1ã€‚â€
    2. å…·ä½“æ¥è¯´ï¼Œé€šè¿‡ä»¥ä¸‹æ­¥éª¤å®ç°æ­¤ç›®çš„:
        1. **token æˆªæ–­**: æˆ‘ä»¬æƒ³è¦åœ¨ç¬¬ä¸€ä¸ªå‡ºç°åœ¨å“åº”çš„ `truncate_after` ä½ç½®ä¹‹åçš„ `truncate_token` å¤„æˆªæ–­ ([lm_human_preferences/train_policy.py#L378](https://github.com/openai/lm-human-preferences/blob/cbfd210bb8b08f6bc5c26878c10984b90f516c66/lm_human_preferences/train_policy.py#L378))ã€‚

            1. ä»£ç æ³¨é‡Š: â€œä¸­å¿ƒç¤ºä¾‹: å°†æˆªæ–­ token åçš„æ‰€æœ‰ token æ›¿æ¢ä¸ºå¡«å…… tokenâ€


        2. **åœ¨æˆªæ–­å“åº”ä¸Šè¿è¡Œå¥–åŠ±æ¨¡å‹**: åœ¨ token æˆªæ–­è¿‡ç¨‹å°†å“åº”æˆªæ–­åï¼Œä»£ç ç„¶ååœ¨ **æˆªæ–­çš„å“åº”** ä¸Šè¿è¡Œå¥–åŠ±æ¨¡å‹ã€‚
        3. **æ‹’ç»é‡‡æ ·**: å¦‚æœåœ¨ç¬¬ 16 å’Œ 24 ä¸ª token ä¹‹é—´æ²¡æœ‰å¥å·ï¼Œé‚£ä¹ˆå°†å“åº”çš„åˆ†æ•°æ›¿æ¢ä¸ºå›ºå®šçš„ä½å€¼ (ä¾‹å¦‚ -1) ([lm_human_preferences/train_policy.py#L384](https://github.com/openai/lm-human-preferences/blob/cbfd210bb8b08f6bc5c26878c10984b90f516c66/lm_human_preferences/train_policy.py#L384)ã€[lm_human_preferences/train_policy.py#L384-L402](https://github.com/openai/lm-human-preferences/blob/cbfd210bb8b08f6bc5c26878c10984b90f516c66/lm_human_preferences/train_policy.py#L384-L402))ã€‚

            1. ä»£ç æ³¨é‡Š: â€œä¸­å¿ƒç¤ºä¾‹: ç¡®ä¿æ ·æœ¬åŒ…å« `truncate_token` â€œ
            2. ä»£ç æ³¨é‡Š: â€œåªå¯¹é€šè¿‡è¯¥åŠŸèƒ½çš„å“åº”è¿›è¡Œäººç±»æŸ¥è¯¢â€


        4. åœ¨ `descriptiveness` ä¸­ä¸¾ä¸€äº›ä¾‹å­:

            ![](https://huggingface.co/datasets/trl-internal-testing/example-images/resolve/main/rlhf_implementation_details/Untitled%201.png)

            ä»æˆ‘ä»¬çš„å¤åˆ¶ä¸­æå–çš„æ ·æœ¬ [https://wandb.ai/openrlbenchmark/lm_human_preference_details/runs/djf8yymv/logs](https://wandb.ai/openrlbenchmark/lm_human_preference_details/runs/djf8yymv/logs?workspace=user-costa-huang)ã€‚è¯·æ³¨æ„ï¼Œç¬¬ 1 å’Œç¬¬ 3 ä¸ªç¤ºä¾‹åœ¨å¥å·åæœ‰å¤ªå¤š tokenï¼Œå› æ­¤å…¶åˆ†æ•°è¢«æ›¿æ¢ä¸º -1ã€‚

6. **æŠ˜ç°å› å­ (discount factor) = 1**

    1. æŠ˜ç°å› å­ \(\gamma\) è®¾ç½®ä¸º 1 ([lm_human_preferences/train_policy.py#L56](https://github.com/openai/lm-human-preferences/blob/cbfd210bb8b08f6bc5c26878c10984b90f516c66/lm_human_preferences/train_policy.py#L56))ï¼Œè¿™æ„å‘³ç€æœªæ¥çš„å¥–åŠ±ä¸å³æ—¶å¥–åŠ±å…·æœ‰ç›¸åŒçš„æƒé‡ã€‚

7. **è®­ç»ƒå¾ªç¯çš„æœ¯è¯­: PPO ä¸­çš„æ‰¹æ¬¡å’Œå°æ‰¹æ¬¡**

    1. OAI ä½¿ç”¨ä»¥ä¸‹è®­ç»ƒå¾ªç¯ ([lm_human_preferences/train_policy.py#L184-L192](https://github.com/openai/lm-human-preferences/blob/cbfd210bb8b08f6bc5c26878c10984b90f516c66/lm_human_preferences/train_policy.py#L184-L192))ã€‚æ³¨æ„: æˆ‘ä»¬é¢å¤–æ·»åŠ äº† `micro_batch_size` æ¥å¸®åŠ©å¤„ç†æ¢¯åº¦ç´¯ç§¯çš„æƒ…å†µã€‚åœ¨æ¯ä¸ªæ—¶æœŸï¼Œå®ƒéƒ½ä¼šæ´—ç‰Œæ‰¹æ¬¡ç´¢å¼•ã€‚

        ```python
        import numpy as np
        batch_size = 8
        nminibatches = 2
        gradient_accumulation_steps = 2
        mini_batch_size = batch_size // nminibatches
        micro_batch_size = mini_batch_size // gradient_accumulation_steps
        data = np.arange(batch_size).astype(np.float32)
        print("data:", data)
        print("batch_size:", batch_size)
        print("mini_batch_size:", mini_batch_size)
        print("micro_batch_size:", micro_batch_size)
        for epoch in range(4):
            batch_inds = np.random.permutation(batch_size)
            print("epoch:", epoch, "batch_inds:", batch_inds)
            for mini_batch_start in range(0, batch_size, mini_batch_size):
                mini_batch_end = mini_batch_start + mini_batch_size
                mini_batch_inds = batch_inds[mini_batch_start:mini_batch_end]
                
                # `optimizer.zero_grad()` set optimizer to zero for gradient accumulation
                for micro_batch_start in range(0, mini_batch_size, micro_batch_size):
                    micro_batch_end = micro_batch_start + micro_batch_size
                    micro_batch_inds = mini_batch_inds[micro_batch_start:micro_batch_end]
                    print("____â© a forward pass on", data[micro_batch_inds])
                # `optimizer.step()`
                print("âª a backward pass on", data[mini_batch_inds])

        # data: [0. 1. 2. 3. 4. 5. 6. 7.]
        # batch_size: 8
        # mini_batch_size: 4
        # micro_batch_size: 2
        # epoch: 0 batch_inds: [6 4 0 7 3 5 1 2]
        # ____â© a forward pass on [6. 4.]
        # ____â© a forward pass on [0. 7.]
        # âª a backward pass on [6. 4. 0. 7.]
        # ____â© a forward pass on [3. 5.]
        # ____â© a forward pass on [1. 2.]
        # âª a backward pass on [3. 5. 1. 2.]
        # epoch: 1 batch_inds: [6 7 3 2 0 4 5 1]
        # ____â© a forward pass on [6. 7.]
        # ____â© a forward pass on [3. 2.]
        # âª a backward pass on [6. 7. 3. 2.]
        # ____â© a forward pass on [0. 4.]
        # ____â© a forward pass on [5. 1.]
        # âª a backward pass on [0. 4. 5. 1.]
        # epoch: 2 batch_inds: [1 4 5 6 0 7 3 2]
        # ____â© a forward pass on [1. 4.]
        # ____â© a forward pass on [5. 6.]
        # âª a backward pass on [1. 4. 5. 6.]
        # ____â© a forward pass on [0. 7.]
        # ____â© a forward pass on [3. 2.]
        # âª a backward pass on [0. 7. 3. 2.]
        # epoch: 3 batch_inds: [7 2 4 1 3 0 6 5]
        # ____â© a forward pass on [7. 2.]
        # ____â© a forward pass on [4. 1.]
        # âª a backward pass on [7. 2. 4. 1.]
        # ____â© a forward pass on [3. 0.]
        # ____â© a forward pass on [6. 5.]
        # âª a backward pass on [3. 0. 6. 5.]
        ```

8. **åŸºäºæ¯ä¸ªæ ‡è®°çš„ KL æƒ©ç½š**

    - ä»£ç ä¸ºå¥–åŠ±æ·»åŠ äº†æ¯ä¸ªæ ‡è®°çš„ KL æƒ©ç½š ([lm_human_preferences/train_policy.py#L150-L153](https://github.com/openai/lm-human-preferences/blob/cbfd210bb8b08f6bc5c26878c10984b90f516c66/lm_human_preferences/train_policy.py#L150-L153))ï¼Œä»¥é˜»æ­¢ç­–ç•¥ä¸åŸå§‹ç­–ç•¥å·®å¼‚è¿‡å¤§ã€‚
    - ä»¥ â€œusually, he wouldâ€ ä¸ºä¾‹ï¼Œå®ƒè¢«æ ‡è®°åŒ–ä¸º `[23073, 11, 339, 561]` ã€‚å‡è®¾æˆ‘ä»¬ä½¿ç”¨ `[23073]` ä½œä¸ºæŸ¥è¯¢ï¼Œ`[11, 339, 561]` ä½œä¸ºå“åº”ã€‚ç„¶ååœ¨é»˜è®¤çš„ `gpt2` å‚æ•°ä¸‹ï¼Œå“åº”æ ‡è®°å°†å…·æœ‰å‚è€ƒç­–ç•¥çš„å¯¹æ•°æ¦‚ç‡ `logprobs=[-3.3213, -4.9980, -3.8690]` ã€‚

        - åœ¨ç¬¬ä¸€ä¸ª PPO æ›´æ–°æ—¶æœŸå’Œå°æ‰¹æ¬¡æ›´æ–°æ—¶ï¼Œæ¿€æ´»ç­–ç•¥å°†å…·æœ‰ç›¸åŒçš„å¯¹æ•°æ¦‚ç‡`new_logprobs=[-3.3213, -4.9980, -3.8690]` ã€‚å› æ­¤ï¼Œæ¯ä¸ªæ ‡è®°çš„ KL æƒ©ç½šå°†ä¸º `kl = new_logprobs - logprobs = [0., 0., 0.]` ã€‚
        - ä½†æ˜¯ï¼Œåœ¨ç¬¬ä¸€ä¸ªæ¢¯åº¦åå‘ä¼ æ’­åï¼Œæˆ‘ä»¬å¯èƒ½ä¼šå¾—åˆ° `new_logprob=[3.3213, -4.9980, -3.8690]` ï¼Œå› æ­¤æ¯ä¸ªæ ‡è®°çš„ KL æƒ©ç½šå˜ä¸º `kl = new_logprobs - logprobs = [-0.3315, -0.0426, 0.6351]` ã€‚
        - éšåï¼Œ`non_score_reward = beta * kl` ï¼Œå…¶ä¸­ `beta` æ˜¯ KL æƒ©ç½šç³»æ•° \(\beta\)ï¼Œå®ƒè¢«æ·»åŠ åˆ°ä»å¥–åŠ±æ¨¡å‹è·å¾—çš„ `score` ä¸­ï¼Œä»¥åˆ›å»ºç”¨äºè®­ç»ƒçš„ `rewards` ã€‚`score` ä»…åœ¨æ¯ä¸ªå›åˆ ( episode ) ç»“æŸæ—¶ç»™å‡ºï¼Œå¯èƒ½ç±»ä¼¼äº `[0.4]` ï¼Œç„¶åæˆ‘ä»¬æœ‰ `rewards = [beta * -0.3315, beta * -0.0426, beta * 0.6351 + 0.4]` ã€‚

9. **æ¯ä¸ªå°æ‰¹æ¬¡çš„å¥–åŠ±å’Œä¼˜åŠ¿ç™½åŒ–ï¼Œå¯é€‰æ‹©å‡å€¼å¹³ç§»**

    1. OAI å®ç°äº†ä¸€ä¸ªåä¸º `whiten` çš„å‡½æ•°ï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼ŒåŸºæœ¬ä¸Šé€šè¿‡å‡å»å…¶å‡å€¼ç„¶åé™¤ä»¥å…¶æ ‡å‡†å·®æ¥å¯¹ `values` è¿›è¡Œå½’ä¸€åŒ–ã€‚å¯é€‰åœ°ï¼Œ`whiten` å¯ä»¥é€šè¿‡ `shift_mean=True` å°†ç™½åŒ–åçš„ `values` å¹³ç§»åˆ°å‡å€¼ã€‚

        ```python
        def whiten(values, shift_mean=True):
            mean, var = torch.mean(values), torch.var(values, unbiased=False)
            whitened = (values - mean)* torch.rsqrt(var + 1e-8)
            if not shift_mean:
                whitened += mean
            return whitened
        ```

    2. åœ¨æ¯ä¸ªå°æ‰¹æ¬¡ä¸­ï¼ŒOAI ä½¿ç”¨ `whiten(rewards, shift_mean=False)` å¯¹å¥–åŠ±è¿›è¡Œç™½åŒ–ï¼Œä¸å¯¹å‡å€¼è¿›è¡Œå¹³ç§»å¤„ç† ([lm_human_preferences/train_policy.py#L325](https://github.com/openai/lm-human-preferences/blob/cbfd210bb8b08f6bc5c26878c10984b90f516c66/lm_human_preferences/train_policy.py#L325))ï¼Œå¹¶ä½¿ç”¨å¹³ç§»åçš„å‡å€¼å¯¹ä¼˜åŠ¿è¿›è¡Œç™½åŒ– `whiten(advantages)` ([lm_human_preferences/train_policy.py#L338](https://github.com/openai/lm-human-preferences/blob/cbfd210bb8b08f6bc5c26878c10984b90f516c66/lm_human_preferences/train_policy.py#L338))ã€‚
    3. **ä¼˜åŒ–æ³¨æ„äº‹é¡¹:** å¦‚æœå°æ‰¹æ¬¡çš„æ•°é‡ä¸ºä¸€ (åœ¨æ­¤å¤ç°ä¸­æ˜¯è¿™ç§æƒ…å†µ)ï¼Œæˆ‘ä»¬åªéœ€è¦å¯¹å¥–åŠ±è¿›è¡Œç™½åŒ–ã€è®¡ç®—å¹¶å¯¹ä¼˜åŠ¿è¿›è¡Œä¸€æ¬¡ç™½åŒ–ï¼Œå› ä¸ºå®ƒä»¬çš„å€¼ä¸ä¼šæ”¹å˜ã€‚
    4. **TensorFlow vs PyTorch æ³¨æ„äº‹é¡¹:** `tf.moments` ä¸ `torch.var` çš„ä¸åŒè¡Œä¸º: ç”±äºæ–¹å·®è®¡ç®—æ–¹å¼ä¸åŒï¼ŒTorch å’Œ TensorFlow ä¸­çš„ç™½åŒ–è¡Œä¸ºä¸åŒ:

        ```jsx
        import numpy as np
        import tensorflow as tf
        import torch

        def whiten_tf(values, shift_mean=True):
            mean, var = tf.nn.moments(values, axes=list(range(values.shape.rank)))
            mean = tf.Print(mean, [mean], 'mean', summarize=100)
            var = tf.Print(var, [var], 'var', summarize=100)
            whitened = (values - mean)* tf.rsqrt(var + 1e-8)
            if not shift_mean:
                whitened += mean
            return whitened

        def whiten_pt(values, shift_mean=True, unbiased=True):
            mean, var = torch.mean(values), torch.var(values, unbiased=unbiased)
            print("mean", mean)
            print("var", var)
            whitened = (values - mean)* torch.rsqrt(var + 1e-8)
            if not shift_mean:
                whitened += mean
            return whitened

        rewards = np.array([
            [1.2, 1.3, 1.4],
            [1.5, 1.6, 1.7],
            [1.8, 1.9, 2.0],
        ])

        with tf.Session() as sess:
            print(sess.run(whiten_tf(tf.constant(rewards, dtype=tf.float32), shift_mean=False)))
            print(whiten_pt(torch.tensor(rewards), shift_mean=False, unbiased=True))
            print(whiten_pt(torch.tensor(rewards), shift_mean=False, unbiased=False))
        ```

        ```jsx
        mean[1.5999999]
        var[0.0666666627]
        [[0.05080712 0.4381051 0.8254035 ]
        [1.2127019 1.6000004 1.9872988 ]
        [2.3745968 2.7618952 3.1491938 ]]
        mean tensor(1.6000, dtype=torch.float64)
        var tensor(0.0750, dtype=torch.float64)
        tensor([[0.1394, 0.5046, 0.8697],
                [1.2349, 1.6000, 1.9651],
                [2.3303, 2.6954, 3.0606]], dtype=torch.float64)
        mean tensor(1.6000, dtype=torch.float64)
        var tensor(0.0667, dtype=torch.float64)
        tensor([[0.0508, 0.4381, 0.8254],
                [1.2127, 1.6000, 1.9873],
                [2.3746, 2.7619, 3.1492]], dtype=torch.float64)

        ```

10. **è£å‰ªå€¼å‡½æ•°**

    1. ä¸åŸå§‹çš„ PPO ä¸€æ · ([baselines/ppo2/model.py#L68-L75](https://github.com/openai/baselines/blob/ea25b9e8b234e6ee1bca43083f8f3cf974143998/baselines/ppo2/model.py#L68-L75))ï¼Œå€¼å‡½æ•°è¢«è£å‰ª ([lm_human_preferences/train_policy.py#L343-L348](https://github.com/openai/lm-human-preferences/blob/cbfd210bb8b08f6bc5c26878c10984b90f516c66/lm_human_preferences/train_policy.py#L343-L348))ï¼Œæ–¹å¼ä¸ç­–ç•¥ç›®æ ‡ç±»ä¼¼ã€‚

11. **è‡ªé€‚åº” KL æ•£åº¦**

    - KL æ•£åº¦æƒ©ç½šç³»æ•° \(\beta\) æ ¹æ®å½“å‰ç­–ç•¥ä¸å…ˆå‰ç­–ç•¥ä¹‹é—´çš„ KL æ•£åº¦è‡ªé€‚åº”ä¿®æ”¹ã€‚å¦‚æœ KL æ•£åº¦è¶…å‡ºé¢„å®šçš„ç›®æ ‡èŒƒå›´ï¼Œåˆ™è°ƒæ•´æƒ©ç½šç³»æ•°ä»¥ä½¿å…¶æ›´æ¥è¿‘ç›®æ ‡èŒƒå›´ ([lm_human_preferences/train_policy.py#L115-L124](https://github.com/openai/lm-human-preferences/blob/cbfd210bb8b08f6bc5c26878c10984b90f516c66/lm_human_preferences/train_policy.py#L115-L124))ã€‚å®ƒçš„å®ç°å¦‚ä¸‹:

        ```python
        class AdaptiveKLController:
            def __init__(self, init_kl_coef, hparams):
                self.value = init_kl_coef
                self.hparams = hparams

            def update(self, current, n_steps):
                target = self.hparams.target
                proportional_error = np.clip(current / target - 1, -0.2, 0.2)
                mult = 1 + proportional_error * n_steps / self.hparams.horizon
                self.value *= mult
        ```


    - å¯¹äºæœ¬å·¥ä½œä¸­ç ”ç©¶çš„ `sentiment` å’Œ `descriptiveness` ä»»åŠ¡ï¼Œæˆ‘ä»¬ä½¿ç”¨äº† `init_kl_coef=0.15, hparams.target=6, hparams.horizon=10000` ã€‚

## **PyTorch Adam ä¼˜åŒ–å™¨ä¸ RLHF ç›¸å…³çš„æ•°å€¼é—®é¢˜**

- è¿™ä¸ªå®ç°ç»†èŠ‚éå¸¸æœ‰è¶£ï¼Œå€¼å¾—ä¸“é—¨ä¸€èŠ‚æ¥è®¨è®ºã€‚
- PyTorch çš„ Adam ä¼˜åŒ–å™¨ ([torch.optim.Adam.html](https://pytorch.org/docs/stable/generated/torch.optim.Adam.html)) ä¸ TensorFlow çš„ Adam ä¼˜åŒ–å™¨ (TF1 Adam åœ¨ [tensorflow/v1.15.2/adam.py](https://github.com/tensorflow/tensorflow/blob/v1.15.2/tensorflow/python/training/adam.py)ï¼ŒTF2 Adam åœ¨ [keras/adam.py#L26-L220](https://github.com/keras-team/keras/blob/v2.13.1/keras/optimizers/adam.py#L26-L220)) æœ‰ä¸åŒçš„å®ç°æ–¹å¼ã€‚å…·ä½“æ¥è¯´ï¼Œ **PyTorch éµå¾ªäº† Kingma å’Œ Ba çš„ Adam è®ºæ–‡ä¸­çš„ç®—æ³• 1** ([arxiv/1412.6980](https://arxiv.org/pdf/1412.6980.pdf))ï¼Œè€Œ **TensorFlow ä½¿ç”¨äº†è¯¥è®ºæ–‡ç¬¬ 2.1 èŠ‚å‰çš„å…¬å¼**ï¼Œè¿™é‡Œæåˆ°çš„ `epsilon` åœ¨è®ºæ–‡ä¸­ç§°ä¸º `epsilon hat` ã€‚åœ¨ä¼ªä»£ç æ¯”è¾ƒä¸­ï¼Œæˆ‘ä»¬æœ‰ä»¥ä¸‹å†…å®¹:

    ```python
    ### pytorch adam implementation:
    bias_correction1 = 1 - beta1 ** step
    bias_correction2 = 1 - beta2 ** step
    step_size = lr / bias_correction1
    bias_correction2_sqrt = _dispatch_sqrt(bias_correction2)
    denom = (exp_avg_sq.sqrt() / bias_correction2_sqrt).add_(eps)
    param.addcdiv_(exp_avg, denom, value=-step_size)

    ### tensorflow adam implementation:
    lr_t = lr * _dispatch_sqrt((1 - beta2 ** step)) / (1 - beta1 ** step)
    denom = exp_avg_sq.sqrt().add_(eps)
    param.addcdiv_(exp_avg, denom, value=-lr_t)
    ```

- è®©æˆ‘ä»¬æ¯”è¾ƒä¸€ä¸‹ PyTorch é£æ ¼å’Œ TensorFlow é£æ ¼ Adam çš„æ›´æ–°æ–¹ç¨‹ã€‚æŒ‰ç…§ Adam è®ºæ–‡ [(Kingma å’Œ Baï¼Œ2014)](https://arxiv.org/abs/1412.6980) çš„ç¬¦å·è¡¨ç¤ºï¼Œæˆ‘ä»¬å¯ä»¥å¾—åˆ° PyTorch Adam (Kingma å’Œ Ba è®ºæ–‡çš„ç®—æ³• 1) å’Œ TensorFlow é£æ ¼ Adam (Kingma å’Œ Ba è®ºæ–‡ç¬¬ 2.1 èŠ‚å‰çš„å…¬å¼) çš„æ¢¯åº¦æ›´æ–°è§„åˆ™å¦‚ä¸‹:

    $$\begin{aligned}\text{pytorch adam :}\quad \theta_t & =\theta_{t-1}-\alpha \cdot \hat{m} _t /\left(\sqrt{\hat{v} _t}+\varepsilon\right) \& =\theta_ {t-1}- \alpha \underbrace{\left[m_t /\left(1-\beta_1^t\right)\right]}_ {=\hat{m} _t} /\left[\sqrt{\underbrace{v_t /\left(1-\beta_2^t\right)}_ {=\hat{v} _t} }+\varepsilon\right]\& =\theta_ {t-1}- \alpha\left[m_t /\left(1-\beta_1^t\right)\right]\frac{\sqrt{1-\beta_2^t}}{\sqrt{v_t}+\color{green}{\varepsilon \sqrt{1-\beta_2^t}}}\end{aligned}$$

    $$\begin{aligned}\text{tensorflow adam:}\quad \theta_t & =\theta_{t-1}-\alpha_t m_t /\left(\sqrt{v_t}+\hat{\varepsilon}\right) \& =\theta_{t-1}-\underbrace{\left[\alpha \sqrt{1-\beta_2^t} /\left(1-\beta_1^t\right)\right]} _{=\alpha_t} m_t /\left(\sqrt{v_t}+\hat{\varepsilon}\right) \& =\theta_ {t-1}- \alpha\left[m_t /\left(1-\beta_1^t\right)\right] \frac{\sqrt{1-\beta_2^t}}{\sqrt{v_t}+\color{green}{\hat{\varepsilon}}} \end{aligned}$$

- ä¸Šé¢çš„æ–¹ç¨‹å¼ºè°ƒäº† PyTorch å’Œ TensorFlow å®ç°ä¹‹é—´çš„åŒºåˆ«åœ¨äºå®ƒä»¬çš„ **å½’ä¸€åŒ–é¡¹**ï¼Œå³ \(\color{green}{\varepsilon \sqrt{1-\beta_2^t}}\) å’Œ  \(\color{green}{\hat{\varepsilon}}\)ã€‚å¦‚æœæˆ‘ä»¬è®¾ç½® \(\hat{\varepsilon} = \varepsilon \sqrt{1-\beta_2^t}\)ï¼Œåˆ™è¿™ä¸¤ä¸ªç‰ˆæœ¬æ˜¯ç­‰ä»·çš„ã€‚ç„¶è€Œï¼Œåœ¨ PyTorch å’Œ TensorFlow çš„ API ä¸­ï¼Œæˆ‘ä»¬åªèƒ½é€šè¿‡ `eps` å‚æ•°è®¾ç½® \(\varepsilon\) (PyTorch) å’Œ  \(\hat{\varepsilon}\) (TensorFlow)ï¼Œä»è€Œå¯¼è‡´å®ƒä»¬çš„æ›´æ–°æ–¹ç¨‹å­˜åœ¨å·®å¼‚ã€‚å¦‚æœæˆ‘ä»¬å°† \(\varepsilon\) å’Œ  \(\hat{\varepsilon}\) éƒ½è®¾ç½®ä¸ºç›¸åŒçš„å€¼ï¼Œæ¯”å¦‚ 1e-5 ä¼šå‘ç”Ÿä»€ä¹ˆï¼Ÿé‚£ä¹ˆå¯¹äº TensorFlow Adamï¼Œå½’ä¸€åŒ–é¡¹ \(\hat{\varepsilon} = \text{1e-5}\) å°±æ˜¯ä¸€ä¸ªå¸¸æ•°ã€‚ä½†å¯¹äº PyTorch Adamï¼Œå½’ä¸€åŒ–é¡¹ \({\varepsilon \sqrt{1-\beta_2^t}}\) éšç€æ—¶é—´çš„æ¨ç§»è€Œå˜åŒ–ã€‚é‡è¦çš„æ˜¯ï¼Œå½“æ—¶é—´æ­¥ \(t\) è¾ƒå°æ—¶ï¼Œè¯¥é¡¹ \({\varepsilon \sqrt{1-\beta_2^t}}\) æ˜æ˜¾å°äº 1e-5ï¼Œéšç€æ—¶é—´æ­¥å¢åŠ ï¼Œé€æ¸æ¥è¿‘ 1e-5ã€‚ä¸‹é¢çš„å›¾è¡¨æ¯”è¾ƒäº†è¿™ä¸¤ä¸ªå½’ä¸€åŒ–é¡¹éšç€æ—¶é—´æ­¥çš„å˜åŒ–æƒ…å†µ:


    ![norma_const_comparison.png](https://huggingface.co/datasets/trl-internal-testing/example-images/resolve/main/rlhf_implementation_details/norma_const_comparison.png)

- ä¸Šå›¾æ˜¾ç¤ºï¼Œå¦‚æœæˆ‘ä»¬åœ¨ PyTorch Adam å’Œ TensorFlow Adam ä¸­è®¾ç½®ç›¸åŒçš„ `eps` ï¼Œé‚£ä¹ˆåœ¨è®­ç»ƒçš„æ—©æœŸé˜¶æ®µï¼ŒPyTorch Adam ä½¿ç”¨çš„å½’ä¸€åŒ–é¡¹è¦æ¯” TensorFlow Adam å°å¾—å¤šã€‚æ¢å¥è¯è¯´ï¼ŒPyTorch Adam åœ¨è®­ç»ƒçš„æ—©æœŸé‡‡ç”¨äº† **æ›´æ¿€è¿›çš„æ¢¯åº¦æ›´æ–°**ã€‚æˆ‘ä»¬çš„å®éªŒè¯æ˜äº†è¿™ä¸€å‘ç°ï¼Œå¦‚ä¸‹æ‰€ç¤ºã€‚
- è¿™å¯¹å¤ç°æ€§å’Œæ€§èƒ½æœ‰ä½•å½±å“ï¼Ÿä¸ºäº†ä¿æŒè®¾ç½®ä¸€è‡´ï¼Œæˆ‘ä»¬è®°å½•äº†æ¥è‡ª [https://github.com/openai/lm-human-preferences](https://github.com/openai/lm-human-preferences) çš„åŸå§‹æŸ¥è¯¢ã€å“åº”å’Œå¥–åŠ±ï¼Œå¹¶å°†å®ƒä»¬ä¿å­˜åœ¨ [https://huggingface.co/datasets/vwxyzjn/lm-human-preferences-debug/tree/main](https://huggingface.co/datasets/vwxyzjn/lm-human-preferences-debug/tree/main) ä¸­ã€‚æˆ‘è¿˜è®°å½•äº†ä½¿ç”¨ TF1 çš„ `AdamOptimizer` ä¼˜åŒ–å™¨çš„å‰ä¸¤ä¸ªè®­ç»ƒå‘¨æœŸçš„æŒ‡æ ‡ä½œä¸ºåŸºå‡†ã€‚ä»¥ä¸‹æ˜¯ä¸€äº›å…³é”®æŒ‡æ ‡:

    |  | OAI çš„ TF1 Adam | PyTorch çš„ Adam | æˆ‘ä»¬è‡ªå®šä¹‰çš„ç±»ä¼¼ TensorFlow é£æ ¼çš„ Adam|
    | --- | --- | --- | --- |
    | policy/approxkl | 0.00037167023 | 0.0023672834504395723 | 0.000374998344341293 |
    | policy/clipfrac | 0.0045572915 | 0.02018229104578495 | 0.0052083334885537624 |
    | ratio_mean | 1.0051285 | 1.0105520486831665 | 1.0044583082199097 |
    | ratio_var | 0.0007716546 | 0.005374275613576174 | 0.0007942612282931805 |
    | ratio_max | 1.227216 | 1.8121057748794556 | 1.250215768814087 |
    | ratio_min | 0.7400441 | 0.4011387825012207 | 0.7299948930740356 |
    | logprob_diff_mean | 0.0047487603 | 0.008101251907646656 | 0.004073789343237877 |
    | logprob_diff_var | 0.0007207897 | 0.004668936599045992 | 0.0007334011606872082 |
    | logprob_diff_max | 0.20474821 | 0.594489574432373 | 0.22331619262695312 |
    | logprob_diff_min | -0.30104542 | -0.9134478569030762 | -0.31471776962280273 |

- ç”±äºæŸç§åŸå› ï¼Œ **PyTorch çš„ Adam ç”Ÿæˆäº†æ›´æ¿€è¿›çš„æ›´æ–°**ã€‚ä»¥ä¸‹æ˜¯ä¸€äº›è¯æ®:

    - **PyTorch çš„ Adam çš„ logprob_diff_var é«˜å‡º 6 å€**ã€‚è¿™é‡Œçš„ `logprobs_diff = new_logprobs - logprobs` æ˜¯ç»è¿‡ä¸¤ä¸ªè®­ç»ƒå‘¨æœŸåï¼Œåˆå§‹ç­–ç•¥å’Œå½“å‰ç­–ç•¥ä¹‹é—´çš„æ ‡è®°å¯¹æ•°æ¦‚ç‡å·®å¼‚ã€‚å…·æœ‰æ›´å¤§çš„ `logprob_diff_var` æ„å‘³ç€å¯¹æ•°æ¦‚ç‡å˜åŒ–çš„å¹…åº¦æ¯” OAI çš„ TF1 Adam å¤§ã€‚
    - **PyTorch çš„ Adam å‘ˆç°æ›´æç«¯çš„æœ€å¤§å’Œæœ€å°æ¯”ç‡**ã€‚è¿™é‡Œçš„ `ratio = torch.exp(logprobs_diff)` ã€‚å…·æœ‰ `ratio_max=1.8121057748794556` æ„å‘³ç€å¯¹äºæŸäº›æ ‡è®°ï¼Œåœ¨å½“å‰ç­–ç•¥ä¸‹æŠ½å–è¯¥æ ‡è®°çš„æ¦‚ç‡è¦æ¯” OAI çš„ TF1 Adam é«˜ 1.8 å€ï¼Œè€Œåè€…ä»…ä¸º 1.2 å€ã€‚
    - **æ›´å¤§çš„ `policy/approxkl` å’Œ `policy/clipfrac`**ã€‚ç”±äºæ¿€è¿›çš„æ›´æ–°ï¼Œæ¯”ç‡è¢«å‰ªåˆ‡çš„æ¬¡æ•° **å¤š 4.4 å€ï¼Œè¿‘ä¼¼çš„ KL æ•£åº¦å¤§ 6 å€**ã€‚
    - è¿™ç§æ¿€è¿›çš„æ›´æ–°å¯èƒ½ä¼šå¯¼è‡´è¿›ä¸€æ­¥çš„é—®é¢˜ã€‚ä¾‹å¦‚ï¼ŒPyTorch çš„ `Adam` ä¸­çš„`logprob_diff_mean` è¦å¤§ 1.7 å€ï¼Œè¿™å°†å¯¹ä¸‹ä¸€ä¸ªå¥–åŠ±è®¡ç®—ä¸­çš„ KL æƒ©ç½šäº§ç”Ÿ 1.7 å€å¤§çš„å½±å“; è¿™å¯èƒ½ä¼šè¢«ç´¯ç§¯ã€‚å®é™…ä¸Šï¼Œè¿™å¯èƒ½ä¸è‘—åçš„ KL æ•£åº¦é—®é¢˜æœ‰å…³â€”â€” KL æƒ©ç½šè¿œå¤§äºå®ƒåº”è¯¥çš„å€¼ï¼Œæ¨¡å‹å¯èƒ½ä¼šæ›´å¤šåœ°å…³æ³¨å®ƒå¹¶è¿›è¡Œæ›´å¤šä¼˜åŒ–ï¼Œä»è€Œå¯¼è‡´è´Ÿçš„ KL æ•£åº¦ã€‚

- **æ›´å¤§çš„æ¨¡å‹å—åˆ°æ›´å¤šå½±å“**ã€‚æˆ‘ä»¬è¿›è¡Œäº†ä¸€äº›å®éªŒï¼Œæ¯”è¾ƒäº† PyTorch çš„ `Adam` (ä»£å· `pt_adam` ) å’Œæˆ‘ä»¬è‡ªå®šä¹‰çš„ç±»ä¼¼ TensorFlow é£æ ¼çš„ Adam (ä»£å· `tf_adam` ) åœ¨ `gpt2` å’Œ `gpt2-xl` ä¸Šçš„æ€§èƒ½ã€‚æˆ‘ä»¬å‘ç°åœ¨ `gpt2` ä¸‹æ€§èƒ½å¤§è‡´ç›¸ä¼¼; ä½†æ˜¯åœ¨ `gpt2-xl` ä¸‹ï¼Œæˆ‘ä»¬è§‚å¯Ÿåˆ°äº†æ›´æ¿€è¿›çš„æ›´æ–°ï¼Œè¿™æ„å‘³ç€æ›´å¤§çš„æ¨¡å‹å—åˆ°äº†æ›´å¤šçš„å½±å“ã€‚

    - å½“åœ¨ `gpt2-xl` ä¸­åˆå§‹ç­–ç•¥æ›´æ–°æ›´ä¸ºæ¿€è¿›æ—¶ï¼Œè®­ç»ƒåŠ¨æ€ä¼šå—åˆ°å½±å“ã€‚ä¾‹å¦‚ï¼Œæˆ‘ä»¬å‘ç°ä½¿ç”¨ `pt_adam` æ—¶ï¼Œ`sentiment` çš„ `objective/kl` å’Œ `objective/scores` å³°å€¼è¦å¤§å¾—å¤šï¼Œ _åœ¨å…¶ä¸­ä¸€ä¸ªéšæœºç§å­ä¸­ï¼Œæœ€å¤§çš„ KL å€¼è¾¾åˆ°äº† 17.5_ ï¼Œè¿™è¡¨æ˜äº†ä¸å¸Œæœ›çš„è¿‡åº¦ä¼˜åŒ–ã€‚
    - æ­¤å¤–ï¼Œç”±äº KL æ›´å¤§ï¼Œè®¸å¤šå…¶ä»–è®­ç»ƒæŒ‡æ ‡ä¹Ÿå—åˆ°å½±å“ã€‚ä¾‹å¦‚ï¼Œæˆ‘ä»¬è§‚å¯Ÿåˆ°æ›´å¤§çš„ `clipfrac` (`ratio` è¢« PPO çš„ç›®æ ‡è£å‰ªç³»æ•° 0.2 è£å‰ªçš„æ—¶é—´æ¯”ä¾‹) å’Œ `approxkl` ã€‚

    ![adam_gpt2.png](https://huggingface.co/datasets/trl-internal-testing/example-images/resolve/main/rlhf_implementation_details/adam_gpt2.png)

    ![adam_gpt2_xl.png](https://huggingface.co/datasets/trl-internal-testing/example-images/resolve/main/rlhf_implementation_details/adam_gpt2_xl.png)

# å±€é™æ€§

æ³¨æ„åˆ°è¿™é¡¹å·¥ä½œæ²¡æœ‰å°è¯•å¤ç° CNN DM ä¸­çš„æ‘˜è¦å·¥ä½œã€‚è¿™æ˜¯å› ä¸ºæˆ‘ä»¬å‘ç°è®­ç»ƒè€—æ—¶ä¸”ä¸ç¨³å®šã€‚

æˆ‘ä»¬çš„ç‰¹å®šè®­ç»ƒè¿è¡Œæ˜¾ç¤º GPU åˆ©ç”¨ç‡è¾ƒä½ (çº¦ 30%)ï¼Œå› æ­¤ä¸€ä¸ªè®­ç»ƒè¿è¡Œéœ€è¦è¿‘ 4 å¤©çš„æ—¶é—´ï¼Œè¿™éå¸¸æ˜‚è´µ (åªæœ‰ AWS é”€å”® p3dn.24xlargeï¼Œæ¯å°æ—¶è´¹ç”¨ä¸º 31.212 ç¾å…ƒ)ã€‚

æ­¤å¤–ï¼Œè®­ç»ƒä¹Ÿå¾ˆä¸ç¨³å®šã€‚è™½ç„¶å¥–åŠ±å€¼ä¸Šå‡ï¼Œä½†æˆ‘ä»¬å‘ç°éš¾ä»¥å¤ç° Ziegler ç­‰äºº (2019 å¹´) æŠ¥å‘Šçš„â€œæ™ºèƒ½å¤åˆ¶â€è¡Œä¸ºã€‚ä»¥ä¸‹æ˜¯ä¸€äº›æ ·æœ¬è¾“å‡º â€” æ˜¾ç„¶ï¼Œæ™ºèƒ½ä½“å‡ºç°äº†æŸç§ç¨‹åº¦çš„è¿‡æ‹Ÿåˆã€‚è¯·æŸ¥çœ‹ [https://wandb.ai/openrlbenchmark/lm-human-preferences/runs/1ab47rqi/logs](https://wandb.ai/openrlbenchmark/lm-human-preferences/runs/1ab47rqi/logs?workspace=user-costa-huang) ä»¥è·å–æ›´å®Œæ•´çš„æ—¥å¿—ã€‚

![tldr1.png](https://huggingface.co/datasets/trl-internal-testing/example-images/resolve/main/rlhf_implementation_details/tldr1.png)

![tldr2.png](https://huggingface.co/datasets/trl-internal-testing/example-images/resolve/main/rlhf_implementation_details/tldr2.png)

# æ€»ç»“

åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æ·±å…¥ç ”ç©¶äº† OpenAI çš„åŸå§‹ RLHF (Reinforcement Learning from Human Feedback) ä»£ç åº“ï¼Œå¹¶ç¼–åˆ¶äº†å…¶å®æ–½ç»†èŠ‚çš„åˆ—è¡¨ã€‚æˆ‘ä»¬è¿˜åˆ›å»ºäº†ä¸€ä¸ªæœ€å°çš„åŸºç¡€ç‰ˆæœ¬ï¼Œå½“æ•°æ®é›†å’Œè¶…å‚æ•°å—æ§åˆ¶æ—¶ï¼Œå¯ä»¥å¤ç°ä¸ OpenAI åŸå§‹ RLHF ä»£ç åº“ç›¸åŒçš„å­¦ä¹ æ›²çº¿ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜è¯†åˆ«äº†ä¸€äº›ä»¤äººæƒŠè®¶çš„å®æ–½ç»†èŠ‚ï¼Œæ¯”å¦‚ Adam ä¼˜åŒ–å™¨çš„è®¾ç½®ï¼Œå®ƒä¼šå¯¼è‡´åœ¨ RLHF è®­ç»ƒçš„æ—©æœŸå‡ºç°æ¿€è¿›çš„æ›´æ–°ã€‚

# è‡´è°¢

è¿™é¡¹å·¥ä½œå¾—åˆ°äº† Hugging Face çš„ Big Science é›†ç¾¤çš„æ”¯æŒ ğŸ¤—ã€‚æˆ‘ä»¬è¿˜æ„Ÿè°¢ @lewtun å’Œ @natolambert çš„å»ºè®¾æ€§è®¨è®ºã€‚

# Bibtex

```bibtex
@article{Huang2023implementation,
  author = {Huang, Shengyi and Liu, Tianlin and von Werra, Leandro},
  title = {The N Implementation Details of RLHF with PPO},
  journal = {Hugging Face Blog},
  year = {2023},
  note = {https://huggingface.co/blog/the_n_implementation_details_of_rlhf_with_ppo},
}
```

---
title: "Reformer æ¨¡å‹ - çªç ´è¯­è¨€å»ºæ¨¡çš„æé™"
thumbnail: /blog/assets/03_reformer/thumbnail.png
authors:
- user: patrickvonplaten
translators:
- user: MatrixYao
- user: zhongdongy
  proofreader: true
---

# Reformer æ¨¡å‹ - çªç ´è¯­è¨€å»ºæ¨¡çš„æé™

<a href="https://colab.research.google.com/github/patrickvonplaten/blog/blob/main/notebooks/03_reformer.ipynb" target="_parent"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt=" åœ¨ Colab ä¸­æ‰“å¼€ "/></a>

## Reformer å¦‚ä½•åœ¨ä¸åˆ° 8GB çš„â€‹â€‹å†…å­˜ä¸Šè®­ç»ƒ 50 ä¸‡ä¸ªè¯å…ƒ

[Kitaevã€Kaiser ç­‰äººäº 20202 å¹´å¼•å…¥çš„ Reformer æ¨¡å‹](https://arxiv.org/pdf/2001.04451.pdf) æ˜¯è¿„ä»Šä¸ºæ­¢é•¿åºåˆ—å»ºæ¨¡é¢†åŸŸå†…å­˜æ•ˆç‡æœ€é«˜çš„ transformer æ¨¡å‹ä¹‹ä¸€ã€‚

æœ€è¿‘ï¼Œäººä»¬å¯¹é•¿åºåˆ—å»ºæ¨¡çš„å…´è¶£æ¿€å¢ï¼Œä»…ä»Šå¹´ä¸€å¹´ï¼Œå°±æ¶Œç°å‡ºäº†å¤§é‡çš„å·¥ä½œï¼Œå¦‚ [Beltagy ç­‰äººçš„å·¥ä½œ (2020) ](https://arxiv.org/abs/2004.05150)ã€[Roy ç­‰äººçš„å·¥ä½œ (2020) ](https://arxiv.org/abs/2003.05997)ã€[Tay ç­‰äººçš„å·¥ä½œ](https://arxiv.org/abs/2002.11296) ä»¥åŠ [Wang ç­‰äººçš„å·¥ä½œ](https://arxiv.org/abs/2006.04768) ç­‰ç­‰ã€‚é•¿åºåˆ—å»ºæ¨¡èƒŒåçš„åŠ¨æœºæ˜¯ï¼ŒNâ€‹â€‹LP ä¸­çš„è®¸å¤šä»»åŠ¡ (ä¾‹å¦‚ _æ‘˜è¦ã€é—®ç­”_ ) è¦æ±‚æ¨¡å‹å¤„ç†æ›´é•¿çš„åºåˆ—ï¼Œè¿™äº›åºåˆ—é•¿åº¦è¶…å‡ºäº† BERT ç­‰æ¨¡å‹çš„å¤„ç†èƒ½åŠ›ã€‚åœ¨éœ€è¦æ¨¡å‹å¤„ç†é•¿è¾“å…¥åºåˆ—çš„ä»»åŠ¡ä¸­ï¼Œé•¿åºåˆ—æ¨¡å‹æ— éœ€å¯¹è¾“å…¥åºåˆ—è¿›è¡Œè£å‰ªä»¥é¿å…å†…å­˜æº¢å‡ºï¼Œå› æ­¤å·²è¢«è¯æ˜ä¼˜äºæ ‡å‡†çš„ **BERT ç±»æ¨¡å‹** ( _è§_ [Beltagy ç­‰äºº 2020 å¹´çš„å·¥ä½œ](https://arxiv.org/abs/2004.05150))ã€‚

Reformer èƒ½å¤Ÿä¸€æ¬¡å¤„ç†å¤šè¾¾ 50 ä¸‡ä¸ªè¯å…ƒï¼Œä»è€Œçªç ´äº†é•¿åºåˆ—å»ºæ¨¡çš„æé™ (å…·ä½“å¯å‚è§æœ¬ [ç¬”è®°æœ¬](https://github.com/patrickvonplaten/notebooks/blob/master/PyTorch_Reformer.ipynb))ã€‚ç›¸å½¢ä¹‹ä¸‹ï¼Œä¼ ç»Ÿçš„ `bert-base-uncased` æ¨¡å‹æœ€é•¿ä»…æ”¯æŒ 512 ä¸ªè¯å…ƒã€‚åœ¨ Reformer ä¸­ï¼Œæ ‡å‡† transformer æ¶æ„çš„æ¯ä¸ªéƒ¨åˆ†éƒ½ç»è¿‡é‡æ–°è®¾è®¡ï¼Œä»¥æœ€å°åŒ–å†…å­˜éœ€æ±‚ï¼Œå¹¶é¿å…æ˜¾è‘—é™ä½æ€§èƒ½ã€‚

å†…å­˜çš„æ”¹è¿›æ¥è‡ªäº Reformer ä½œè€…å‘ transformer ä¸–ç•Œå¼•å…¥çš„ **4** å¤§ç‰¹æ€§:

1. **Reformer è‡ªæ³¨æ„åŠ›å±‚** - _å¦‚ä½•åœ¨ä¸å—é™äºæœ¬åœ°ä¸Šä¸‹æ–‡çš„æƒ…å†µä¸‹é«˜æ•ˆåœ°å®ç°è‡ªæ³¨æ„åŠ›æœºåˆ¶ï¼Ÿ_
2. **åˆ†å—å‰é¦ˆå±‚** - _å¦‚ä½•æ›´å¥½åœ°å¯¹å¤§å‹å‰é¦ˆå±‚çš„æ—¶é—´å’Œå†…å­˜è¿›è¡Œæƒè¡¡ï¼Ÿ_
3. **å¯é€†æ®‹å·®å±‚** - _å¦‚ä½•èªæ˜åœ°è®¾è®¡æ®‹å·®æ¶æ„ä»¥å¤§å¹…å‡å°‘è®­ç»ƒä¸­çš„å†…å­˜æ¶ˆè€—ï¼Ÿ_
4. **è½´å‘ä½ç½®ç¼–ç  (Axial Positional Encodings)** - _å¦‚ä½•ä½¿ä½ç½®ç¼–ç å¯ç”¨äºè¶…é•¿è¾“å…¥åºåˆ—ï¼Ÿ_

æœ¬æ–‡çš„ç›®çš„æ˜¯ **æ·±å…¥** é˜è¿° Reformer çš„ä¸Šè¿°å››å¤§ç‰¹æ€§ã€‚è™½ç„¶è¿™å››ä¸ªç‰¹æ€§ç›®å‰æ˜¯ç”¨åœ¨ Reformer ä¸Šçš„ï¼Œä½†å…¶æ–¹æ³•æ˜¯é€šç”¨çš„ã€‚å› æ­¤ï¼Œè¯»è€…ä¸åº”è¢«æ­¤æŸç¼šï¼Œè€Œåº”è¯¥å¤šæ€è€ƒåœ¨å“ªäº›æƒ…å†µä¸‹å¯ä»¥æŠŠè¿™å››ä¸ªç‰¹æ€§ä¸­çš„æŸä¸€ä¸ªæˆ–æŸå‡ ä¸ªåº”ç”¨äºå…¶ä»–çš„ transformer æ¨¡å‹ï¼Œä»¥è§£å†³å…¶é—®é¢˜ã€‚

ä¸‹æ–‡å››ä¸ªéƒ¨åˆ†ä¹‹é—´çš„è”ç³»å¾ˆæ¾æ•£ï¼Œå› æ­¤å¯ä»¥å•ç‹¬é˜…è¯»ã€‚

Reformer å·²é›†æˆå…¥ ğŸ¤—Transformers åº“ã€‚å¯¹äºæƒ³ä½¿ç”¨ Reformer çš„ç”¨æˆ·ï¼Œå»ºè®®å¤§å®¶é˜…è¯»æœ¬æ–‡ï¼Œä»¥æ›´å¥½åœ°äº†è§£è¯¥æ¨¡å‹çš„å·¥ä½œåŸç†ä»¥åŠå¦‚ä½•æ­£ç¡®é…ç½®å®ƒã€‚æ–‡ä¸­æ‰€æœ‰å…¬å¼éƒ½é™„æœ‰å…¶åœ¨ transformers ä¸­å¯¹åº”çš„ Reformer é…ç½®é¡¹ ( _ä¾‹å¦‚_ `config.<param_name>` )ï¼Œä»¥ä¾¿è¯»è€…å¯ä»¥å¿«é€Ÿå…³è”åˆ°å®˜æ–¹æ–‡æ¡£å’Œé…ç½®æ–‡ä»¶ã€‚

**æ³¨æ„**: _è½´å‘ä½ç½®ç¼–ç _ åœ¨å®˜æ–¹ Reformer è®ºæ–‡ä¸­æ²¡æœ‰è§£é‡Šï¼Œä½†åœ¨å®˜æ–¹ä»£ç åº“ä¸­å¹¿æ³›ä½¿ç”¨ã€‚æœ¬æ–‡é¦–æ¬¡æ·±å…¥é˜é‡Šäº†è½´å‘ä½ç½®ç¼–ç ã€‚

## 1. Reformer è‡ªæ³¨æ„åŠ›å±‚

Reformer ä½¿ç”¨äº†ä¸¤ç§ç‰¹æ®Šçš„è‡ªæ³¨æ„åŠ›å±‚: _å±€éƒ¨_ è‡ªæ³¨æ„åŠ›å±‚å’Œ LSH (Locality Sensitive Hashingï¼Œå±€éƒ¨æ•æ„Ÿå“ˆå¸Œï¼Œ _LSH_ ) è‡ªæ³¨æ„åŠ›å±‚ã€‚

åœ¨ä»‹ç»æ–°çš„è‡ªæ³¨æ„åŠ›å±‚ä¹‹å‰ï¼Œæˆ‘ä»¬å…ˆç®€è¦å›é¡¾ä¸€ä¸‹ä¼ ç»Ÿçš„è‡ªæ³¨æ„åŠ›ï¼Œå…¶ç”± Vaswani ç­‰äººåœ¨å…¶ [2017 å¹´çš„è®ºæ–‡](https://arxiv.org/abs/1706.03762) ä¸­å¼•å…¥ã€‚

æœ¬æ–‡çš„ç¬¦å·åŠé…è‰²ä¸ [ã€Šå›¾è§£ transformerã€‹](https://jalammar.github.io/illustrated-transformer/) ä¸€æ–‡ä¸€è‡´ï¼Œå› æ­¤å¼ºçƒˆå»ºè®®è¯»è€…åœ¨é˜…è¯»æœ¬æ–‡ä¹‹å‰ï¼Œå…ˆé˜…è¯»ã€Šå›¾è§£ transformerã€‹ä¸€æ–‡ã€‚

**é‡è¦**: è™½ç„¶ Reformer æœ€åˆæ˜¯ä¸ºäº†å› æœè‡ªæ³¨æ„åŠ›è€Œå¼•å…¥çš„ï¼Œä½†å®ƒä¹Ÿå¯ä»¥å¾ˆå¥½åœ°ç”¨äºåŒå‘è‡ªæ³¨æ„åŠ›ã€‚æœ¬æ–‡åœ¨è§£é‡Š Reformer çš„è‡ªæ³¨æ„åŠ›æ—¶ï¼Œå°†å…¶ç”¨äº _åŒå‘_ è‡ªæ³¨æ„åŠ›ã€‚

### å…¨å±€è‡ªæ³¨æ„åŠ›å›é¡¾

Transformer æ¨¡å‹çš„æ ¸å¿ƒæ˜¯ **è‡ªæ³¨æ„åŠ›** å±‚ã€‚ç°åœ¨ï¼Œæˆ‘ä»¬å›é¡¾ä¸€ä¸‹ä¼ ç»Ÿçš„è‡ªæ³¨æ„åŠ›å±‚ï¼Œè¿™é‡Œç§°ä¸º **å…¨å±€è‡ªæ³¨æ„åŠ›** å±‚ã€‚é¦–å…ˆæˆ‘ä»¬å‡è®¾å¯¹åµŒå…¥å‘é‡åºåˆ— $\mathbf{X} = \mathbf{x}_1, \ldots, \mathbf{x}_n$ æ‰§è¡Œä¸€ä¸ª transformer å±‚ï¼Œè¯¥åºåˆ—ä¸­çš„æ¯ä¸ªå‘é‡ $\mathbf{x}_{i}$ çš„ç»´åº¦ä¸º `config.hidden_â€‹â€‹size` ï¼Œ _å³_ $d_h$ã€‚

ç®€è€Œè¨€ä¹‹ï¼Œå…¨å±€è‡ªæ³¨æ„åŠ›å±‚å°† $\mathbf{X}$ æŠ•å½±åˆ°æŸ¥è¯¢çŸ©é˜µã€é”®çŸ©é˜µå’Œå€¼çŸ©é˜µ: $\mathbf{Q}$ã€$\mathbf{K}$ã€$\mathbf{V}$ å¹¶ä½¿ç”¨ _softmax_ è®¡ç®—æœ€ç»ˆè¾“å‡º $\mathbf{Z}$ï¼Œå¦‚ä¸‹æ‰€ç¤º:

$\mathbf{Z} = \text{SelfAttn}(\mathbf{X}) = \text{softmax}(\mathbf{Q}\mathbf{K}^T) \mathbf{V}$ï¼Œå…¶ä¸­ $\mathbf{Z}$ çš„ç»´åº¦ä¸º $d_h \times n$ (ä¸ºç®€å•èµ·è§ï¼Œæ­¤å¤„çœç•¥äº†é”®å½’ä¸€åŒ–å› å­å’Œè¾“å‡ºæ˜ å°„æƒé‡ $\mathbf{W}^{O}$)ã€‚æœ‰å…³å®Œæ•´ transformer æ“ä½œçš„æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜… [ã€Šå›¾è§£ transformerã€‹](https://jalammar.github.io/illustrated-transformer/) ä¸€æ–‡ã€‚

ä¸‹å›¾ç»™å‡ºäº† $n=16ï¼Œd_h=3$ æƒ…å†µä¸‹çš„æ“ä½œ:

![](https://raw.githubusercontent.com/patrickvonplaten/scientific_images/master/reformer_benchmark/conventional_attention.png)

è¯·æ³¨æ„ï¼Œæœ¬æ–‡æ‰€æœ‰ç¤ºæ„å›¾éƒ½å‡è®¾ `batch_size` å’Œ `config.num_attention_heads` ä¸º 1ã€‚ä¸ºäº†ä¾¿äºç¨åæ›´å¥½åœ°è§£é‡Š _LSH è‡ªæ³¨æ„åŠ›_ ï¼Œæˆ‘ä»¬è¿˜åœ¨å›¾ä¸­æ ‡è®°å‡ºäº†ä¸€äº›å‘é‡ï¼Œ _å¦‚_ $\mathbf{x_3}$ åŠå…¶ç›¸åº”çš„è¾“å‡ºå‘é‡ $\mathbf{z_3}$ã€‚å›¾ä¸­çš„é€»è¾‘å¯ä»¥è½»æ˜“æ‰©å±•è‡³å¤šå¤´è‡ªæ³¨æ„åŠ› ( `config.num_attention_heads` > 1)ã€‚å¦‚éœ€äº†è§£å¤šå¤´æ³¨æ„åŠ›ï¼Œå»ºè®®è¯»è€…å‚é˜… [ã€Šå›¾è§£ transformerã€‹](https://jalammar.github.io/illustrated-transformer/)ã€‚

æ•²ä¸ªé‡ç‚¹ï¼Œå¯¹äºæ¯ä¸ªè¾“å‡ºå‘é‡ $\mathbf{z}_{i}$ï¼Œæ•´ä¸ªè¾“å…¥åºåˆ— $\mathbf{X}$ éƒ½éœ€è¦å‚ä¸å…¶è®¡ç®—ã€‚å†…ç§¯å¼ é‡ $\mathbf{Q}\mathbf{K}^T$ çš„å†…å­˜å¤æ‚åº¦ä¸º $\mathcal{O}(n^2)$ï¼Œè¿™äº‹å®ä¸Šä½¿å¾— transformer æ¨¡å‹çš„ç“¶é¢ˆåœ¨å†…å­˜ã€‚

è¿™ä¹Ÿæ˜¯ä¸ºä»€ä¹ˆ `bert-base-cased` çš„ `config.max_position_embedding_size` åªæœ‰ 512 çš„åŸå› ã€‚

### å±€éƒ¨è‡ªæ³¨æ„åŠ›

**å±€éƒ¨è‡ªæ³¨æ„åŠ›** æ˜¯ç¼“è§£ $\mathcal{O}(n^2)$ å†…å­˜ç“¶é¢ˆçš„ä¸€ä¸ªæ˜¾ç„¶çš„è§£å†³æ–¹æ¡ˆï¼Œå®ƒä½¿æˆ‘ä»¬èƒ½å¤Ÿä»¥æ›´ä½çš„è®¡ç®—æˆæœ¬å»ºæ¨¡æ›´é•¿çš„åºåˆ—ã€‚åœ¨å±€éƒ¨è‡ªæ³¨æ„åŠ›ä¸­ï¼Œè¾“å…¥ $\mathbf{X} = \mathbf{X}_{1:n} = \mathbf{x}_{1}, \ldots, \mathbf{x}_{n}$ è¢«åˆ‡æˆ $n_{c}$ ä¸ªå—: $\mathbf{X} = \left[\mathbf{X}_{1:l_{c}}, \ldots, \mathbf{X} _{(n_{c} - 1) * l_{c} : n_{c} * l_{c}}\right]$ï¼Œæ¯å—é•¿åº¦ä¸º `config.local_chunk_length` ï¼Œ _å³_ $l_{c}$ï¼Œéšåï¼Œå¯¹æ¯ä¸ªå—åˆ†åˆ«åº”ç”¨å…¨å±€è‡ªæ³¨æ„åŠ›ã€‚

ç»§ç»­ä»¥ $n=16ï¼Œd_h=3$ ä¸ºä¾‹:

![](https://raw.githubusercontent.com/patrickvonplaten/scientific_images/master/reformer_benchmark/input.png)

å‡è®¾ $l_{c} = 4ï¼Œn_{c} = 4$ï¼Œæ­¤æ—¶ï¼Œæˆ‘ä»¬å°†åˆ†å—æ³¨æ„åŠ›å›¾ç¤ºå¦‚ä¸‹:

![](https://raw.githubusercontent.com/patrickvonplaten/scientific_images/master/reformer_benchmark/chunked_attention_1.png)

å¯ä»¥çœ‹å‡ºï¼Œæˆ‘ä»¬å¯¹æ¯ä¸ªå—åˆ†åˆ«æ‰§è¡Œäº†æ³¨æ„åŠ›æ“ä½œ $\mathbf{X} _{1:4}ï¼Œ\mathbf{X}_ {5:8}ï¼Œ\mathbf{X} _{9:12 }ï¼Œ\mathbf{X}_ {13:16}$ã€‚
è¯¥æ¶æ„çš„ä¸€ä¸ªæ˜æ˜¾çš„ç¼ºç‚¹æ˜¯: ä¸€äº›è¾“å…¥å‘é‡æ— æ³•è®¿é—®å…¶ç›´æ¥ä¸Šä¸‹æ–‡ï¼Œ _ä¾‹å¦‚_ ï¼Œæˆ‘ä»¬çš„ä¾‹å­ä¸­çš„ $\mathbf{x} _9$ æ— æ³•è®¿é—® $\mathbf{x}_ {8}$ï¼Œåä¹‹äº¦ç„¶ã€‚è¿™æ˜¯æœ‰é—®é¢˜çš„ï¼Œå› ä¸ºè¿™äº›è¯å…ƒæ— æ³•åœ¨å­¦ä¹ å…¶å‘é‡è¡¨å¾æ—¶å°†å…¶ç›´æ¥ä¸Šä¸‹æ–‡çš„çº³å…¥è€ƒé‡ã€‚

ä¸€ä¸ªç®€å•çš„è¡¥æ•‘æªæ–½æ˜¯ç”¨ `config.local_num_chunks_before` ( _å³_ $n_{p}$) ä»¥åŠ `config.local_num_chunks_after` ( _å³_ $n_{a}$) æ¥æ‰©å……æ¯ä¸ªå—ï¼Œä»¥ä¾¿æ¯ä¸ªè¾“å…¥å‘é‡è‡³å°‘å¯ä»¥è®¿é—® $n_{p}$ ä¸ªå…ˆå‰è¾“å…¥å—åŠ $n_{a}$ ä¸ªåç»­è¾“å…¥å—ã€‚æˆ‘ä»¬å¯å°†å…¶ç†è§£ä¸ºé‡å åˆ†å—ï¼Œå…¶ä¸­ $n_{p}$ å’Œ  $n_{a}$ å®šä¹‰äº†æ¯ä¸ªå—ä¸å…¶å…ˆå‰å—å’Œåç»­å—çš„é‡å é‡ã€‚æˆ‘ä»¬å°†è¿™ç§æ‰©å±•çš„å±€éƒ¨è‡ªæ³¨æ„åŠ›è¡¨ç¤ºå¦‚ä¸‹:

$$\mathbf{Z}^{\text{loc}} = \left[\mathbf{Z}_{0:l_{c}}^{\text{loc}}, \ldots, \mathbf{Z}_{(n_{c} - 1) * l_{c} + 1 : n_{c} * l_{c}}^{\text{loc}}\right]ï¼Œ$$ 

å…¶ä¸­

$$\mathbf{Z}_{l_{c} * (i - 1) + 1 : l_{c} * i}^{\text{loc}} = \text{SelfAttn}(\mathbf{X}_ {l_{c} * (i - 1 - n_{p}) + 1: l_{c} * (i + n_{a})})\left[n_{p} * l_{c}: -n_{ a} * l_{c}\right], \forall i \in \{1, \ldots, n_{c} \}$$

å¥½å§ï¼Œè¿™ä¸ªå…¬å¼çœ‹èµ·æ¥æœ‰ç‚¹å¤æ‚ï¼Œæˆ‘ä»¬ç¨å¾®åˆ†æä¸€ä¸‹ã€‚åœ¨ Reformer çš„è‡ªæ³¨æ„åŠ›å±‚ä¸­ï¼Œ$n_{a}$ é€šå¸¸è®¾ä¸º 0ï¼Œ$n_{p}$ è®¾ä¸º 1ï¼Œæˆ‘ä»¬æ®æ­¤é‡å†™ $i = 1$ æ—¶çš„å…¬å¼:

$$\mathbf{Z}_{1:l_{c}}^{\text{loc}} = \text{SelfAttn}(\mathbf{X}_{-l_{c} + 1: l_{c}})\left[l_{c}:\right]$$

æˆ‘ä»¬æ³¨æ„åˆ°è¿™é‡Œæœ‰ä¸€ä¸ªå¾ªç¯å…³ç³»ï¼Œå› æ­¤ç¬¬ä¸€ä¸ªå—ä¹Ÿå¯ä»¥å…³æ³¨æœ€åä¸€ä¸ªå—ã€‚æˆ‘ä»¬å†æ¬¡å›¾è§£ä¸€ä¸‹è¿™ç§å¢å¼ºçš„å±€éƒ¨å…³æ³¨ç®—æ³•ã€‚æˆ‘ä»¬å…ˆæŒ‰å—æ‰¾åˆ°å…¶å¯¹åº”çš„çª—å£ï¼Œå¹¶åœ¨å…¶ä¸Šåº”ç”¨è‡ªæ³¨æ„åŠ›ï¼Œç„¶åä»…ä¿ç•™ä¸­å¿ƒè¾“å‡ºæ®µä½œä¸ºæœ¬å—çš„è¾“å‡ºã€‚

![](https://raw.githubusercontent.com/patrickvonplaten/scientific_images/master/reformer_benchmark/local_attention_2.png)

æœ€åï¼Œå°†ç›¸åº”çš„è¾“å‡ºä¸²æ¥åˆ° $\mathbf{Z}^{\text{loc}}$ ä¸­ï¼Œå¦‚ä¸‹æ‰€ç¤º:

![](https://raw.githubusercontent.com/patrickvonplaten/scientific_images/master/reformer_benchmark/local_attention_3.png)

è¯·æ³¨æ„ï¼Œåœ¨å®ç°å±€éƒ¨è‡ªæ³¨æ„åŠ›æ—¶ï¼Œä¸ºäº†è®¡ç®—æ•ˆç‡ï¼Œæˆ‘ä»¬å¹¶ä¸ä¼šåƒå›¾ä¸­ä¸€æ ·å…ˆè®¡ç®—å…¨éƒ¨è¾“å‡ºå¹¶éšå _ä¸¢å¼ƒ_ ä¸€éƒ¨åˆ†ã€‚å›¾ä¸­çº¢å‰æ‰€ç¤ºçš„åœ°æ–¹ä»…ç”¨äºè¯´æ˜ï¼Œå®é™…å¹¶ä¸ä¼šäº§ç”Ÿè®¡ç®—è¡Œä¸ºã€‚

è¿™é‡Œéœ€è¦æ³¨æ„çš„æ˜¯ï¼Œæ‰©å±•æ¯ä¸ªåˆ†å—è‡ªæ³¨æ„åŠ›å‡½æ•°çš„è¾“å…¥å‘é‡å¯ä»¥ä½¿å¾— _æ¯ä¸ª_ è¾“å‡ºå‘é‡ $\mathbf{z}_{i}$ éƒ½èƒ½å¤Ÿå­¦åˆ°æ›´å¥½çš„å‘é‡è¡¨å¾ã€‚ä»¥å›¾ä¸­çš„å‘é‡ä¸ºä¾‹ï¼Œæ¯ä¸ªè¾“å‡ºå‘é‡ $\mathbf{z}_{5}^{\text{loc}}ï¼Œ\mathbf{z}_{6}^{\text{loc}}ï¼Œ\mathbf{z}_{7}^{\text{loc}}ï¼Œ\mathbf{z}_{8}^{\text{loc}}$ éƒ½å¯ä»¥å°† $\mathbf{X}_{1:8}$ çš„æ‰€æœ‰è¾“å…¥å‘é‡çº³å…¥è€ƒé‡ä»¥å­¦åˆ°æ›´å¥½çš„è¡¨å¾ã€‚

å†…å­˜æ¶ˆè€—ä¸Šçš„é™ä½ä¹Ÿæ˜¯æ˜¾è€Œæ˜“è§çš„: $\mathcal{O}(n^2)$ çš„å†…å­˜å¤æ‚åº¦è¢«åˆ†è§£åˆ°æ®µï¼Œå› æ­¤æ€»å†…å­˜å¤æ‚åº¦å‡å°‘ä¸º $\mathcal{O}(n_{c} * l_{c}^2) = \mathcal{O}(n * l_{c})$ã€‚

è¿™ç§å¢å¼ºçš„å±€éƒ¨è‡ªæ³¨æ„åŠ›æ¯”æ™®é€šçš„å±€éƒ¨è‡ªæ³¨æ„åŠ›æ¶æ„æ›´å¥½ï¼Œä½†ä»ç„¶å­˜åœ¨ä¸€ä¸ªä¸»è¦ç¼ºé™·ï¼Œå› ä¸ºæ¯ä¸ªè¾“å…¥å‘é‡åªèƒ½å…³æ³¨é¢„å®šä¹‰å¤§å°çš„å±€éƒ¨ä¸Šä¸‹æ–‡ã€‚å¯¹äºä¸éœ€è¦ transformer æ¨¡å‹å­¦ä¹ è¾“å…¥å‘é‡ä¹‹é—´çš„è¿œç¨‹ä¾èµ–å…³ç³»çš„ NLP ä»»åŠ¡ ( _ä¾‹å¦‚_ è¯­éŸ³è¯†åˆ«ã€å‘½åå®ä½“è¯†åˆ«ä»¥åŠçŸ­å¥å­çš„å› æœè¯­è¨€å»ºæ¨¡) è€Œè¨€ï¼Œå¯èƒ½ä¸æ˜¯ä¸€ä¸ªå¤§é—®é¢˜ã€‚ä½†è¿˜æœ‰è®¸å¤š NLP ä»»åŠ¡éœ€è¦æ¨¡å‹å­¦ä¹ è¿œç¨‹ä¾èµ–å…³ç³»ï¼Œå› æ­¤å±€éƒ¨è‡ªæ³¨æ„åŠ›åœ¨è¿™äº›ä»»åŠ¡ä¸‹å¯èƒ½ä¼šå¯¼è‡´æ˜¾è‘—çš„æ€§èƒ½ä¸‹é™ï¼Œ _å¦‚_ :

- _é—®ç­”_ : æ¨¡å‹å¿…é¡»å­¦ä¹ é—®é¢˜è¯å…ƒå’Œç›¸å…³ç­”æ¡ˆè¯å…ƒä¹‹é—´çš„å…³ç³»ï¼Œè¿™äº›è¯å…ƒå¾ˆå¯èƒ½å¹¶ä¸ç›¸é‚»;
- _å¤šé¡¹é€‰æ‹©_ : æ¨¡å‹å¿…é¡»å°†å¤šä¸ªç­”æ¡ˆè¯å…ƒæ®µç›¸äº’æ¯”è¾ƒï¼Œè¿™äº›ç­”æ¡ˆè¯å…ƒæ®µé€šå¸¸éš”å¾—æ¯”è¾ƒè¿œ;
- _æ‘˜è¦_ : æ¨¡å‹å¿…é¡»å­¦ä¹ é•¿åºåˆ—çš„ä¸Šä¸‹æ–‡è¯å…ƒå’Œè¾ƒçŸ­çš„æ‘˜è¦è¯å…ƒåºåˆ—ä¹‹é—´çš„å…³ç³»ï¼Œè€Œä¸Šä¸‹æ–‡å’Œæ‘˜è¦ä¹‹é—´çš„ç›¸å…³å…³ç³»å¾ˆå¯èƒ½æ— æ³•é€šè¿‡å±€éƒ¨è‡ªæ³¨æ„åŠ›æ¥æ•è·ã€‚
- â€¦â€¦

å±€éƒ¨è‡ªæ³¨æ„åŠ›æœ¬èº«å¾ˆå¯èƒ½ä¸è¶³ä»¥è®© transformer æ¨¡å‹å­¦ä¹ è¾“å…¥å‘é‡ (è¯å…ƒ) å½¼æ­¤ä¹‹é—´çš„ç›¸å…³å…³ç³»ã€‚

å› æ­¤ï¼ŒReformer é¢å¤–é‡‡ç”¨äº†ä¸€ä¸ªè¿‘ä¼¼å…¨å±€è‡ªæ³¨æ„åŠ›çš„é«˜æ•ˆè‡ªæ³¨æ„åŠ›å±‚ï¼Œç§°ä¸º _LSH è‡ªæ³¨æ„åŠ›_ ã€‚

### LSH è‡ªæ³¨æ„åŠ›

é‰´äºæˆ‘ä»¬å·²ç»äº†è§£äº†å±€éƒ¨è‡ªæ³¨æ„åŠ›çš„å·¥ä½œåŸç†ï¼Œä¸‹é¢æˆ‘ä»¬ç»§ç»­å°è¯•ä¸€ä¸‹å¯èƒ½æ˜¯ Reformer ä¸­æœ€å…·â€‹â€‹åˆ›æ–°æ€§çš„ç®—æ³•æ”¹è¿›: **LSH è‡ªæ³¨æ„åŠ›**ã€‚

LSH è‡ªæ³¨æ„åŠ›çš„è®¾è®¡ç›®æ ‡æ˜¯åœ¨æ•ˆæœä¸Šæ¥è¿‘å…¨å±€è‡ªæ³¨æ„åŠ›ï¼Œè€Œåœ¨é€Ÿåº¦ä¸èµ„æºæ¶ˆè€—ä¸Šä¸å±€éƒ¨è‡ªæ³¨æ„åŠ›ä¸€æ ·é«˜æ•ˆã€‚

LSH è‡ªæ³¨æ„åŠ›å› ä¾èµ–äº Andoni ç­‰äººäº 2015 å¹´æå‡ºçš„ [LSH ç®—æ³•](https://arxiv.org/abs/1509.02897) è€Œå¾—åã€‚

LSH è‡ªæ³¨æ„åŠ›æºäºä»¥ä¸‹æ´è§: å¦‚æœ $n$ å¾ˆå¤§ï¼Œåˆ™å¯¹æ¯ä¸ªæŸ¥è¯¢å‘é‡è€Œè¨€ï¼Œå…¶å¯¹åº”çš„è¾“å‡ºå‘é‡ $\mathbf{z}_{i}$ ä½œä¸ºæ‰€æœ‰ $\mathbf{V}$ çš„çº¿æ€§ç»„åˆï¼Œå…¶ä¸­åº”åªæœ‰æå°‘æ•°å‡ ä¸ª $\mathbf{v}_{i}$ çš„æƒé‡æ¯”å…¶ä»–å¤§å¾—å¤šã€‚ä¹Ÿå°±æ˜¯è¯´å¯¹ $\mathbf{Q}\mathbf{K}^T$ æ³¨æ„åŠ›ç‚¹ç§¯ä½œ softmax äº§ç”Ÿçš„æƒé‡çŸ©é˜µçš„æ¯ä¸€è¡Œåº”ä»…æœ‰æå°‘æ•°çš„å€¼è¿œå¤§äº 0ã€‚

æˆ‘ä»¬å±•å¼€è®²è®²: è®¾  $\mathbf{k}_{i} \in \mathbf{K} = \left[\mathbf{k}_1, \ldots, \mathbf{k}_n \right]^T$ å’Œ  $\mathbf{q}_{i} \in \mathbf{Q} = \left[\mathbf{q}_1, \ldots, \mathbf{q}_n\right]^T$ åˆ†åˆ«ä¸ºé”®å‘é‡å’ŒæŸ¥è¯¢å‘é‡ã€‚å¯¹äºæ¯ä¸ª $\mathbf{q}_{i}$ï¼Œå¯ä»¥ä»…ç”¨é‚£äº›ä¸ $\mathbf{q}_{i}$ å…·æœ‰é«˜ä½™å¼¦ç›¸ä¼¼åº¦çš„ $\mathbf{k}_{j}$ çš„é”®å‘é‡æ¥è¿‘ä¼¼è®¡ç®— $\text{softmax}(\mathbf{q}_{i}^T \mathbf{K}^T)$ ã€‚è¿™æ˜¯å› ä¸º softmax å‡½æ•°å¯¹è¾ƒå¤§è¾“å…¥å€¼çš„è¾“å‡ºä¼šå‘ˆæŒ‡æ•°çº§å¢åŠ ã€‚å¬èµ·æ¥æ²¡æ¯›ç—…ï¼Œé‚£ä¹ˆä¸‹ä¸€ä¸ªé—®é¢˜å°±å˜æˆäº†å¦‚ä½•é«˜æ•ˆåœ°æ‰¾åˆ°æ¯ä¸ª $\mathbf{q}_{i}$ çš„é«˜ä½™å¼¦ç›¸ä¼¼åº¦é”®å‘é‡é›†åˆã€‚

é¦–å…ˆï¼ŒReformer çš„ä½œè€…æ³¨æ„åˆ°å…±äº«æŸ¥è¯¢æŠ•å½±å’Œé”®æŠ•å½±: $\mathbf{Q} = \mathbf{K}$ å¹¶ä¸ä¼šå½±å“ transformer æ¨¡å‹ ${}^1$ã€‚ç°åœ¨ï¼Œä¸å¿…ä¸ºæ¯ä¸ªæŸ¥è¯¢å‘é‡ $q_i$ æ‰¾åˆ°å…¶é«˜ä½™å¼¦ç›¸ä¼¼åº¦çš„é”®å‘é‡ï¼Œè€Œåªéœ€è®¡ç®—æŸ¥è¯¢å‘é‡å½¼æ­¤ä¹‹é—´çš„ä½™å¼¦ç›¸ä¼¼åº¦ã€‚è¿™ä¸€ç®€åŒ–å¾ˆé‡è¦ï¼Œå› ä¸ºæŸ¥è¯¢å‘é‡ä¹‹é—´çš„ä½™å¼¦ç›¸ä¼¼åº¦æ»¡è¶³ä¼ é€’æ€§: å¦‚æœ $\mathbf{q}_{i}$ ä¸  $\mathbf{q}_{j}$ å’Œ  $\mathbf{q}_{k}$ éƒ½å…·æœ‰è¾ƒé«˜çš„ä½™å¼¦ç›¸ä¼¼åº¦ï¼Œåˆ™ $\mathbf{q}_{j}$ ä¸  $\mathbf{q}_{k}$ ä¹Ÿå…·æœ‰è¾ƒé«˜çš„ä½™å¼¦ç›¸ä¼¼åº¦ã€‚å› æ­¤ï¼Œå¯ä»¥å°†æŸ¥è¯¢å‘é‡èšç±»è‡³ä¸åŒçš„æ¡¶ä¸­ï¼Œä½¿å¾—åŒä¸€æ¡¶ä¸­çš„æ‰€æœ‰æŸ¥è¯¢å‘é‡å½¼æ­¤çš„ä½™å¼¦ç›¸ä¼¼åº¦è¾ƒé«˜ã€‚æˆ‘ä»¬å°† $C_{m}$ å®šä¹‰ä¸ºç¬¬ _m_ ç»„ä½ç½®ç´¢å¼•ï¼Œå…¶ä¸­è£…çš„æ˜¯å±äºåŒä¸€ä¸ªæ¡¶çš„æ‰€æœ‰æŸ¥è¯¢å‘é‡: $C_{m} = { i | \mathbf{q}_{i} \in \text{ç¬¬ m ç°‡}}$ï¼ŒåŒæ—¶æˆ‘ä»¬å®šä¹‰æ¡¶çš„æ•°é‡ `config.num_buckets` ï¼Œ _å³_ $n_{b}$ã€‚

å¯¹æ¯ä¸ªç´¢å¼• $C_{m}$ å¯¹åº”çš„æŸ¥è¯¢å‘é‡æ¡¶å†…çš„æŸ¥è¯¢å‘é‡ $\mathbf{q}_{i}$ï¼Œæˆ‘ä»¬å¯ä»¥ç”¨ softmax å‡½æ•° $\text{softmax}(\mathbf{Q}_{i \in C_{m}} \mathbf{Q}^T_{i \in C_{m}})$ é€šè¿‡å…±äº«æŸ¥è¯¢å’Œé”®æŠ•å½±æ¥è¿‘ä¼¼å…¨å±€è‡ªæ³¨æ„åŠ›çš„ softmax å‡½æ•° $\text{softmax}(\mathbf{q}_{i}^T \mathbf{Q}^T)$ã€‚

å…¶æ¬¡ï¼Œä½œè€…åˆ©ç”¨ **LSH** ç®—æ³•å°†æŸ¥è¯¢å‘é‡èšç±»åˆ°é¢„å®šä¹‰çš„ $n_{b}$ ä¸ªæ¡¶ ä¸­ã€‚è¿™é‡Œï¼ŒLSH ç®—æ³•æ˜¯ç†æƒ³ä¹‹é€‰ï¼Œå› ä¸ºå®ƒéå¸¸é«˜æ•ˆï¼Œä¸”å¯ç”¨äºè¿‘ä¼¼åŸºäºä½™å¼¦ç›¸ä¼¼åº¦çš„æœ€è¿‘é‚»ç®—æ³•ã€‚å¯¹ LSH è¿›è¡Œè§£é‡Šè¶…å‡ºäº†æœ¬æ–‡çš„èŒƒå›´ï¼Œæˆ‘ä»¬åªè¦è®°ä½ï¼Œå¯¹å‘é‡ $\mathbf{q}_{i}$ï¼ŒLSH ç®—æ³•å°†å…¶ç´¢å¼•è‡³ $n_{b}$ ä¸ªé¢„å®šä¹‰æ¡¶ä¸­çš„æŸä¸ªæ¡¶ï¼Œ _å³_ $\text{LSH}(\mathbf{q}_{i}) = m$ å…¶ä¸­ $i \in {1, \ldots, n}$ï¼Œ$m \in {1, \ldots, n_{b}}$ã€‚

è¿˜ç”¨å‰é¢çš„ä¾‹å­ï¼Œæˆ‘ä»¬æœ‰:

![](https://raw.githubusercontent.com/patrickvonplaten/scientific_images/master/reformer_benchmark/lsh_hashing.png)

æ¥ç€ï¼Œå¯ä»¥æ³¨æ„åˆ°ï¼Œå°†æ‰€æœ‰æŸ¥è¯¢å‘é‡èšç±»è‡³ $n_{b}$ ä¸ªæ¡¶ä¸­åï¼Œæˆ‘ä»¬å¯ä»¥å°†è¾“å…¥å‘é‡ $\mathbf{x}_1, \ldots, \mathbf{x}_n$ æŒ‰å…¶å¯¹åº”çš„ç´¢å¼• $C_{m}$ è¿›è¡Œé‡æ’ ${}^2$ï¼Œä»¥ä¾¿å…±äº«æŸ¥è¯¢ - é”®è‡ªæ³¨æ„åŠ›å¯ä»¥åƒå±€éƒ¨æ³¨æ„åŠ›ä¸€æ ·åˆ†æ®µåº”ç”¨ã€‚

æˆ‘ä»¬ç”¨ä¾‹å­å†è§£é‡Šä¸€ä¸‹ï¼Œå‡è®¾åœ¨ `config.num_buckets=4` ï¼Œ ` config.lsh_chunk_length=4` æ—¶é‡æ’è¾“å…¥å‘é‡ $\mathbf{X} = \mathbf{x}_1, â€¦, \mathbf{x}_{16}$ã€‚ä¸Šå›¾å·²å°†æ¯ä¸ªæŸ¥è¯¢å‘é‡ $\mathbf{q}_1, \ldots, \mathbf{q}_{16}$ åˆ†é…ç»™ç°‡ $\mathcal{C}_{1}ã€\mathcal{C}_{2}ã€\mathcal{C}_{3}ã€\mathcal{C}_{4}$ ä¸­çš„æŸä¸€ä¸ªã€‚ç°åœ¨ï¼Œå¯¹å…¶å¯¹åº”çš„è¾“å…¥å‘é‡ $\mathbf{x}_1, \ldots, \mathbf{x}_{16}$ è¿›è¡Œé‡æ’ï¼Œå¹¶å°†é‡æ’åçš„è¾“å…¥è®°ä¸º $\mathbf{X'}$:

![](https://raw.githubusercontent.com/patrickvonplaten/scientific_images/master/reformer_benchmark/lsh_perm.png)

å¯¹æ¯ä¸ªè¾“å…¥å‘é‡ï¼Œä»…éœ€åœ¨ç°‡å†…è¿›è¡Œè‡ªæ³¨æ„åŠ›è®¡ç®—å³å¯ï¼Œå› æ­¤æ¯ä¸ªè¾“å…¥å‘é‡å¯¹åº”çš„è¾“å‡ºå‘é‡å¯è®¡ç®—å¦‚ä¸‹: $\mathbf{Z}^{\text{LSH}}_{i \in \mathcal{C}_m} = \text{SelfAttn}_{\mathbf{Q}=\mathbf{K}}(\mathbf{X}_{i \in \mathcal{C}_m})$ã€‚

æˆ‘ä»¬å†æ¬¡å›¾è§£ä¸€ä¸‹è¯¥è¿‡ç¨‹:

![](https://raw.githubusercontent.com/patrickvonplaten/scientific_images/master/reformer_benchmark/lsh_cluster_attn.png)

å¯ä»¥çœ‹å‡ºï¼Œè‡ªæ³¨æ„åŠ›å‡½æ•°çš„è¿ç®—çŸ©é˜µå¤§å°å„ä¸ç›¸åŒï¼Œè¿™ç§æƒ…å†µæ¯”è¾ƒéº»çƒ¦ï¼Œå› ä¸º GPU å’Œ TPU æ— æ³•é«˜æ•ˆå¹¶è¡Œå¤„ç†ä¸åŒå°ºå¯¸çš„çŸ©é˜µè¿ç®—ã€‚

ä¸ºäº†è¿›ä¸€æ­¥è§£å†³é«˜æ•ˆè®¡ç®—çš„é—®é¢˜ï¼Œå¯ä»¥å€Ÿé‰´å±€éƒ¨æ³¨æ„åŠ›çš„æ–¹æ³•ï¼Œå¯¹é‡æ’åçš„è¾“å…¥è¿›è¡Œåˆ†å—ï¼Œä»¥ä½¿æ¯ä¸ªå—çš„å¤§å°å‡ä¸º `config.lsh_chunk_length` ã€‚é€šè¿‡å¯¹é‡æ’åçš„è¾“å…¥è¿›è¡Œåˆ†å—ï¼Œä¸€ä¸ªæ¡¶å¯èƒ½ä¼šè¢«åˆ†æˆä¸¤ä¸ªä¸åŒçš„å—ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œä¸å±€éƒ¨è‡ªæ³¨æ„åŠ›ä¸€æ ·ï¼Œåœ¨ LSH è‡ªæ³¨æ„åŠ›ä¸­ï¼Œæ¯ä¸ªå—é™¤äº†è‡ªèº«ä¹‹å¤–è¿˜å…³æ³¨å…¶å‰ä¸€ä¸ªå— `config.lsh_num_chunks_before=1` ( `config.lsh_num_chunks_after` é€šå¸¸è®¾ç½®ä¸º 0)ã€‚è¿™æ ·ï¼Œæˆ‘ä»¬å°±å¯ä»¥å¤§æ¦‚ç‡ç¡®ä¿æ¡¶ä¸­çš„æ‰€æœ‰å‘é‡ç›¸äº’å…³æ³¨ ${}^3$ã€‚

æ€»è€Œè¨€ä¹‹ï¼Œå¯¹äºæ‰€æœ‰å— $k \in {1, \ldots, n_{c}}$ï¼ŒLSH è‡ªæ³¨æ„åŠ›å¯ä»¥å¦‚ä¸‹è¡¨ç¤º:

$$ \mathbf{Zâ€™}_{l_ {c} * k + 1:l_{c} *(k + 1)}^{\text{LSH}} = \text{SelfAttn}_{\mathbf{Q} = \mathbf{K}}(\mathbf{Xâ€™}_{l_{c} * (k + 1): l_{c} *(k + 1)})\left[l_{c}:\right] $$

å…¶ä¸­ $\mathbf{X'}$ å’Œ  $\mathbf{Z'}$ æ˜¯æŒ‰ç…§ LSH åˆ†æ¡¶è¿›è¡Œé‡æ’åçš„è¾“å…¥å’Œè¾“å‡ºå‘é‡ã€‚å…¬å¼æœ‰ç‚¹å¤æ‚ï¼Œæˆ‘ä»¬è¿˜æ˜¯ç”»ä¸ªå›¾ä»¥å¸®åŠ©å¤§å®¶ç†è§£ã€‚

è¿™é‡Œï¼Œæˆ‘ä»¬å¯¹ä¸Šå›¾ä¸­çš„é‡æ’å‘é‡ $\mathbf{X'}$ è¿›è¡Œåˆ†å—ï¼Œå¹¶åˆ†åˆ«è®¡ç®—æ¯å—çš„å…±äº«æŸ¥è¯¢ - é”®è‡ªæ³¨æ„åŠ›ã€‚

![](https://raw.githubusercontent.com/patrickvonplaten/scientific_images/master/reformer_benchmark/lsh_attention_2.png)

æœ€åï¼Œå°†è¾“å‡º $\mathbf{Z'}^{\text{LSH}}$ é‡æ’å›åŸé¡ºåºã€‚

![](https://raw.githubusercontent.com/patrickvonplaten/scientific_images/master/reformer_benchmark/lsh_attention_3.png)

è¿™é‡Œè¿˜è¦æåˆ°çš„ä¸€ä¸ªé‡è¦ç‰¹å¾æ˜¯ï¼Œå¯ä»¥é€šè¿‡å¹¶è¡Œè¿è¡Œ LSH è‡ªæ³¨æ„åŠ› `config.num_hashes` (å³ $n_{h}$) æ¬¡æ¥æé«˜ LSH è‡ªæ³¨æ„åŠ›çš„å‡†ç¡®æ€§ï¼Œå…¶ä¸­æ¯æ¬¡ä½¿ç”¨ä¸åŒçš„éšæœº LSH å“ˆå¸Œã€‚é€šè¿‡è®¾ç½® `config.num_hashes > 1` ï¼Œå¯¹äºæ¯ä¸ª $i$ï¼Œä¼šè®¡ç®—å¤šä¸ªè¾“å‡ºå‘é‡ $\mathbf{z}^{\text{LSH}, 1}_{i}, \ldots , \mathbf{z}^{\text{LSH}, n_{h}}_{i}$ã€‚éšåï¼Œå¯ä»¥å¯¹å®ƒä»¬è¿›è¡ŒåŠ æƒæ±‚å’Œ: $\mathbf{z}^{\text{LSH}}_{i} = \sum_k^{n_{h}} \mathbf{Z}^{\text{LSH}, k}_{i} * \text{weight}^k_i$ï¼Œè¿™é‡Œ $\text{weight}^k_i$ è¡¨ç¤ºç¬¬ $k$ è½®å“ˆå¸Œçš„è¾“å‡ºå‘é‡ $\mathbf{z}^{\text{LSH}, k}_{i}$ ä¸å…¶ä»–å“ˆå¸Œè½®æ¬¡ç›¸æ¯”çš„é‡è¦åº¦ï¼Œå…¶åº”ä¸å…¶å¯¹åº”è¾“å‡ºçš„ softmax å½’ä¸€åŒ–ç³»æ•°å‘ˆæŒ‡æ•°æ­£æ¯”å…³ç³»ã€‚è¿™ä¸€è®¾è®¡èƒŒåçš„ç›´è§‰æ˜¯ï¼Œå¦‚æœæŸ¥è¯¢å‘é‡ $\mathbf{q}_{i}^{k}$ ä¸å…¶å¯¹åº”å—ä¸­çš„æ‰€æœ‰å…¶ä»–æŸ¥è¯¢å‘é‡å…·æœ‰è¾ƒé«˜çš„ä½™å¼¦ç›¸ä¼¼åº¦ï¼Œåˆ™è¯¥å—çš„ softmax å½’ä¸€åŒ–ç³»æ•°å¾€å¾€å¾ˆå¤§ï¼Œå› æ­¤ç›¸åº”çš„è¾“å‡ºå‘é‡ $\mathbf{q}_{i}^{k}$ åº”è¯¥èƒ½æ›´å¥½åœ°è¿‘ä¼¼å…¨å±€æ³¨æ„åŠ›ï¼Œå› æ­¤å…¶ç†åº”æ¯” softmax å½’ä¸€åŒ–ç³»æ•°è¾ƒå°çš„å“ˆå¸Œè½®æ¬¡æ‰€äº§ç”Ÿçš„è¾“å‡ºå‘é‡è·å¾—æ›´é«˜çš„æƒé‡ã€‚æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜… [è¯¥è®ºæ–‡](https://arxiv.org/pdf/2001.04451.pdf) çš„é™„å½• Aã€‚åœ¨æˆ‘ä»¬çš„ä¾‹å­ä¸­ï¼Œå¤šè½® LSH è‡ªæ³¨æ„åŠ›ç¤ºæ„å›¾å¦‚ä¸‹ã€‚

![](https://raw.githubusercontent.com/patrickvonplaten/scientific_images/master/reformer_benchmark/lsh_attention_4.png)

æ‰“å®Œæ”¶å·¥ï¼è‡³æ­¤ï¼Œæˆ‘ä»¬äº†è§£äº† LSH è‡ªæ³¨æ„åŠ›åœ¨ Reformer ä¸­æ˜¯å¦‚ä½•å·¥ä½œçš„ã€‚

è¯´å›å†…å­˜å¤æ‚åº¦ï¼Œè¯¥æ–¹æ³•æœ‰ä¸¤ä¸ªå¯èƒ½çš„ç“¶é¢ˆç‚¹: ç‚¹ç§¯æ‰€éœ€çš„å†…å­˜: $\mathcal{O}(n_{h} * n_{c} * l_{c}^2) = \mathcal{O}(n * n_{h} * l_{c})$ ä»¥åŠ LSH åˆ†æ¡¶æ‰€éœ€çš„å†…å­˜: $\mathcal{O}(n * n_{h} * \frac{n_{b}}{2})$ å…¶ä¸­ $l_{c}$ æ˜¯å—é•¿åº¦ã€‚å› ä¸ºå¯¹äºå¤§çš„ $n$ è€Œè¨€ï¼Œæ¡¶çš„æ•°é‡ $\frac{n_{b}}{2}$ çš„å¢é•¿é€Ÿåº¦è¿œè¿œå¿«äºå—é•¿åº¦ $l_{c}$ï¼Œå› æ­¤ç”¨æˆ·å¯ä»¥ç»§ç»­å¯¹å­˜å‚¨æ¡¶çš„æ•°é‡ `config.num_buckets` è¿›è¡Œåˆ†è§£ï¼Œè¯¦è§ [æ­¤å¤„](https://huggingface.co/transformers/model_doc/reformer.html#lsh-self-attention)ã€‚

æˆ‘ä»¬å¿«é€Ÿæ€»ç»“ä¸€ä¸‹:

1. æˆ‘ä»¬å¸Œæœ›åˆ©ç”¨ softmax è¿ç®—ä»…å¯¹æå°‘æ•°é”®å‘é‡èµ‹äºˆé‡è¦æƒé‡çš„å…ˆéªŒçŸ¥è¯†æ¥å¯¹å…¨å±€æ³¨æ„åŠ›è¿›è¡Œè¿‘ä¼¼ã€‚
2. å¦‚æœé”®å‘é‡ç­‰äºæŸ¥è¯¢å‘é‡ï¼Œè¿™æ„å‘³ç€ _å¯¹äºæ¯ä¸ª_ æŸ¥è¯¢å‘é‡ $\mathbf{q}_{i}$ï¼Œsoftmax åªéœ€ç»™ä¸å…¶ä½™å¼¦ç›¸ä¼¼åº¦é«˜çš„å…¶ä»–æŸ¥è¯¢å‘é‡èµ‹äºˆé‡è¦æƒé‡å°±è¡Œäº†ã€‚
3. è¿™ç§å…³ç³»æ˜¯å¯¹ç§°çš„ï¼Œä¹Ÿå°±æ˜¯è¯´ï¼Œå¦‚æœ $\mathbf{q}_{j}$ ä¸  $\mathbf{q}_{i}$ ç›¸ä¼¼ï¼Œåˆ™ $\mathbf{q}_{j}$ ä¹Ÿä¸ $\mathbf{q}_{i}$ ç›¸ä¼¼ï¼Œå› æ­¤æˆ‘ä»¬å¯ä»¥åœ¨è®¡ç®—è‡ªæ³¨æ„åŠ›ä¹‹å‰å¯¹è¾“å…¥è¿›è¡Œå…¨å±€èšç±»ã€‚
4. æˆ‘ä»¬å¯¹è¾“å…¥æŒ‰ç°‡è¿›è¡Œé‡æ’ï¼Œå¹¶å¯¹é‡æ’åçš„è¾“å…¥è®¡ç®—å±€éƒ¨è‡ªæ³¨æ„åŠ›ï¼Œæœ€åå°†è¾“å‡ºé‡æ–°æ¢å¤ä¸ºåŸé¡ºåºã€‚

---

${}^{1}$ ä½œè€…è¿›è¡Œäº†ä¸€äº›åˆæ­¥å®éªŒï¼Œç¡®è®¤å…±äº«æŸ¥è¯¢ - é”®è‡ªæ³¨æ„åŠ›çš„è¡¨ç°ä¸æ ‡å‡†è‡ªæ³¨æ„åŠ›å¤§ä½“ä¸€è‡´ã€‚

${}^{2}$ æ›´å‡†ç¡®åœ°è¯´ï¼Œå¯¹å­˜å‚¨æ¡¶ä¸­çš„æŸ¥è¯¢å‘é‡æ ¹æ®å…¶åŸå§‹é¡ºåºè¿›è¡Œæ’åºã€‚ä¸¾ä¸ªä¾‹å­ï¼Œ _å‡å¦‚_ å‘é‡ $\mathbf{q}_1, \mathbf{q}_3, \mathbf{q}_7$ å…¨éƒ¨æ•£åˆ—åˆ°å­˜å‚¨æ¡¶ 2ï¼Œåˆ™å­˜å‚¨æ¡¶ 2 ä¸­å‘é‡çš„é¡ºåºä»åº”æ˜¯å…ˆ $\mathbf{q}_1$ï¼Œåè·Ÿ $\mathbf{q}_3$ å’Œ  $\mathbf{q}_7$ã€‚

${}^3$ é¡ºå¸¦è¯´æ˜ä¸€ä¸‹ï¼Œä½œè€…åœ¨æŸ¥è¯¢å‘é‡ $\mathbf{q}_{i}$ ä¸Šæ”¾äº†ä¸€ä¸ªæ©ç ï¼Œä»¥é˜²æ­¢å‘é‡å…³æ³¨æœ¬èº«ã€‚å› ä¸ºå‘é‡ä¸å…¶è‡ªèº«çš„ä½™å¼¦ç›¸ä¼¼åº¦æ€»æ˜¯å¤§äºç­‰äºå…¶ä¸å…¶ä»–å‘é‡çš„ä½™å¼¦ç›¸ä¼¼åº¦ï¼Œæ‰€ä»¥å¼ºçƒˆä¸å»ºè®®å…±äº«æŸ¥è¯¢ - é”®è‡ªæ³¨æ„åŠ›ä¸­çš„æŸ¥è¯¢å‘é‡å…³æ³¨è‡ªèº«ã€‚

### åŸºå‡†æµ‹è¯•

Transformers æœ€è¿‘å¢åŠ äº†åŸºå‡†æµ‹è¯•ç›¸å…³çš„ä»£ç ï¼Œä½ å¯å‚é˜… [æ­¤å¤„](https://github.com/huggingface/notebooks/blob/main/examples/benchmark.ipynb) ä»¥è·å–æ›´è¯¦ç»†çš„è¯´æ˜ã€‚

ä¸ºäº†å±•ç¤ºå±€éƒ¨ LSH è‡ªæ³¨æ„åŠ›å¯ä»¥èŠ‚çœå¤šå°‘å†…å­˜ï¼Œæˆ‘ä»¬åœ¨ä¸åŒçš„ `local_attn_chunk_length` å’Œ `lsh_attn_chunk_length` ä¸Šå¯¹ Reformer æ¨¡å‹ `google/reformer-enwik8` ä¸Šè¿›è¡Œäº†åŸºå‡†æµ‹è¯•ã€‚ä½ å¯ä»¥ä» [æ­¤å¤„](https://huggingface.co/google/reformer-enwik8) æ‰¾åˆ°æ›´è¯¦ç»†çš„æœ‰å…³ `google/reformer-enwik8` æ¨¡å‹çš„é»˜è®¤é…ç½®å’Œç”¨æ³•ä¿¡æ¯ã€‚

æˆ‘ä»¬å…ˆè¿›è¡Œä¸€äº›å¿…è¦çš„å¯¼å…¥å’Œå®‰è£…ã€‚

```
#@title Installs and Imports
# pip installs
!pip -qq install git+https://github.com/huggingface/transformers.git
!pip install -qq py3nvml

from transformers import ReformerConfig, PyTorchBenchmark, PyTorchBenchmarkArguments
```

é¦–å…ˆï¼Œæˆ‘ä»¬æµ‹è¯•ä¸€ä¸‹åœ¨ Reformer æ¨¡å‹ä¸Šä½¿ç”¨ _å…¨å±€_ è‡ªæ³¨æ„åŠ›çš„å†…å­˜ä½¿ç”¨æƒ…å†µã€‚è¿™å¯ä»¥é€šè¿‡è®¾ç½® `lsh_attn_chunk_length` = `local_attn_chunk_length` = 8192 æ¥è¾¾æˆï¼Œæ­¤æ—¶ï¼Œå¯¹äºæ‰€æœ‰å°äºæˆ–ç­‰äº 8192 çš„è¾“å…¥åºåˆ—ï¼Œæ¨¡å‹äº‹å®ä¸Šå°±å›é€€æˆå…¨å±€è‡ªæ³¨æ„åŠ›äº†ã€‚

```
config = ReformerConfig.from_pretrained("google/reformer-enwik8", lsh_attn_chunk_length=16386, local_attn_chunk_length=16386, lsh_num_chunks_before=0, local_num_chunks_before=0)
benchmark_args = PyTorchBenchmarkArguments(sequence_lengths=[2048, 4096, 8192, 16386], batch_sizes=[1], models=["Reformer"], no_speed=True, no_env_print=True)
benchmark = PyTorchBenchmark(configs=[config], args=benchmark_args)
result = benchmark.run()
```


    HBox(children=(FloatProgress(value=0.0, description='Downloading', max=1279.0, style=ProgressStyle(descriptionâ€¦


    
    1 / 1
    Doesn't fit on GPU. CUDA out of memory. Tried to allocate 2.00 GiB (GPU 0; 11.17 GiB total capacity; 8.87 GiB already allocated; 1.92 GiB free; 8.88 GiB reserved in total by PyTorch)
    
    ====================      INFERENCE - MEMORY - RESULT       ====================
    --------------------------------------------------------------------------------
              Model Name             Batch Size     Seq Length    Memory in MB 
    --------------------------------------------------------------------------------
               Reformer                  1              2048            1465     
               Reformer                  1              4096            2757     
               Reformer                  1              8192            7893     
               Reformer                  1             16386            N/A      
    --------------------------------------------------------------------------------

è¾“å…¥åºåˆ—è¶Šé•¿ï¼Œè¾“å…¥åºåˆ—å’Œå³°å€¼å†…å­˜ä½¿ç”¨ä¹‹é—´çš„å¹³æ–¹å…³ç³» $\mathcal{O}(n^2)$ è¶Šæ˜æ˜¾ã€‚å¯ä»¥çœ‹å‡ºï¼Œå®é™…ä¸Šï¼Œéœ€è¦æ›´é•¿çš„è¾“å…¥åºåˆ—æ‰èƒ½æ¸…æ¥šåœ°è§‚å¯Ÿåˆ°è¾“å…¥åºåˆ—ç¿»å€ä¼šå¯¼è‡´å³°å€¼å†…å­˜ä½¿ç”¨é‡å¢åŠ å››å€ã€‚

å¯¹ä½¿ç”¨å…¨å±€æ³¨æ„åŠ›çš„ `google/reformer-enwik8` æ¨¡å‹è€Œè¨€ï¼Œåºåˆ—é•¿åº¦è¶…è¿‡ 16K å†…å­˜å°±æº¢å‡ºäº†ã€‚

ç°åœ¨ï¼Œæˆ‘ä»¬ä½¿ç”¨æ¨¡å‹çš„é»˜è®¤å‚æ•°ä»¥ä½¿èƒ½ _å±€éƒ¨ LSH_ è‡ªæ³¨æ„åŠ›ã€‚

```
  config = ReformerConfig.from_pretrained("google/reformer-enwik8")
  benchmark_args = PyTorchBenchmarkArguments(sequence_lengths=[2048, 4096, 8192, 16384, 32768, 65436], batch_sizes=[1], models=["Reformer"], no_speed=True, no_env_print=True)
  benchmark = PyTorchBenchmark(configs=[config], args=benchmark_args)
  result = benchmark.run()
```


    1 / 1
    Doesn't fit on GPU. CUDA out of memory. Tried to allocate 2.00 GiB (GPU 0; 11.17 GiB total capacity; 7.85 GiB already allocated; 1.74 GiB free; 9.06 GiB reserved in total by PyTorch)
    Doesn't fit on GPU. CUDA out of memory. Tried to allocate 4.00 GiB (GPU 0; 11.17 GiB total capacity; 6.56 GiB already allocated; 3.99 GiB free; 6.81 GiB reserved in total by PyTorch)
    
    ====================      INFERENCE - MEMORY - RESULT       ====================
    --------------------------------------------------------------------------------
              Model Name             Batch Size     Seq Length    Memory in MB 
    --------------------------------------------------------------------------------
               Reformer                  1              2048            1785     
               Reformer                  1              4096            2621     
               Reformer                  1              8192            4281     
               Reformer                  1             16384            7607     
               Reformer                  1             32768            N/A      
               Reformer                  1             65436            N/A      
    --------------------------------------------------------------------------------

ä¸å‡ºæ‰€æ–™ï¼Œå¯¹äºè¾ƒé•¿çš„è¾“å…¥åºåˆ—ï¼Œä½¿ç”¨å±€éƒ¨ LSH è‡ªæ³¨æ„åŠ›æœºåˆ¶çš„å†…å­˜æ•ˆç‡æ›´é«˜ï¼Œå¯¹äºæœ¬æ–‡ä½¿ç”¨çš„ 11GB æ˜¾å­˜ GPU è€Œè¨€ï¼Œæ¨¡å‹ç›´åˆ°åºåˆ—é•¿åº¦ä¸º 32K æ—¶ï¼Œå†…å­˜æ‰è€—å°½ã€‚

## 2. åˆ†å—å‰é¦ˆå±‚

åŸºäº transformer çš„æ¨¡å‹é€šå¸¸åœ¨è‡ªæ³¨æ„åŠ›å±‚ä¹‹åä¼šæœ‰ä¸€ä¸ªéå¸¸å¤§çš„å‰é¦ˆå±‚ã€‚è¯¥å±‚å¯èƒ½ä¼šå ç”¨å¤§é‡å†…å­˜ï¼Œæœ‰æ—¶ç”šè‡³æˆä¸ºæ¨¡å‹ä¸»è¦çš„å†…å­˜ç“¶é¢ˆã€‚Reformer è®ºæ–‡ä¸­é¦–æ¬¡å¼•å…¥äº†å‰é¦ˆåˆ†å—æŠ€æœ¯ï¼Œä»¥ç”¨æ—¶é—´æ¢å–å†…å­˜ã€‚

### Reformer ä¸­çš„åˆ†å—å‰é¦ˆå±‚

åœ¨ Reformer ä¸­ï¼Œ _LSH_ è‡ªæ³¨æ„åŠ›å±‚æˆ–å±€éƒ¨è‡ªæ³¨æ„åŠ›å±‚é€šå¸¸åé¢è·Ÿç€ä¸€ä¸ªæ®‹å·®è¿æ¥ï¼Œæˆ‘ä»¬å¯å°†å…¶å®šä¹‰ä¸º _transformer å—_ çš„ç¬¬ä¸€éƒ¨åˆ†ã€‚æ›´å¤šç›¸å…³çŸ¥è¯†ï¼Œå¯å‚é˜…æ­¤ [åšæ–‡](http://jalammar.github.io/illusterated-transformer/)ã€‚

_Transformer å—_ ç¬¬ä¸€éƒ¨åˆ†çš„è¾“å‡ºï¼Œç§°ä¸º _å½’èŒƒåŒ–è‡ªæ³¨æ„åŠ›_ è¾“å‡ºï¼Œå¯ä»¥è®°ä¸º $\mathbf{\overline{Z}} = \mathbf{Z} + \mathbf{X}$ã€‚åœ¨ Reformer æ¨¡å‹ä¸­ï¼Œ$\mathbf{Z}$ ä¸º  $\mathbf{Z}^{\text{LSH}}$ æˆ–  $\mathbf{Z}^\text{loc}$ã€‚

åœ¨æˆ‘ä»¬çš„ä¾‹å­ä¸­ï¼Œè¾“å…¥ $\mathbf{x}_1, \ldots, \mathbf{x}_{16}$ çš„è§„èŒƒåŒ–è‡ªæ³¨æ„åŠ›è¾“å‡ºå›¾ç¤ºå¦‚ä¸‹:

![](https://raw.githubusercontent.com/patrickvonplaten/scientific_images/master/reformer_benchmark/layer_normed_output.png)

_Transformer å—_ çš„ç¬¬äºŒéƒ¨åˆ†é€šå¸¸ç”±ä¸¤ä¸ªå‰é¦ˆå±‚ $^{1}$ ç»„æˆï¼Œå…¶ä¸­ $\text{Linear}_{\text{int}}(\ldots)$ ç”¨äºå°† $\mathbf{\overline{Z}}$ æ˜ å°„åˆ°ä¸­é—´è¾“å‡º $\mathbf{Y}_{\text{int}}$ï¼Œ$\text{Linear}_{\text{out}}(\ldots)$ ç”¨äºå°†ä¸­é—´è¾“å‡ºæ˜ å°„ä¸ºæœ€ç»ˆè¾“å‡º $\mathbf{Y}_{\text{out}}$ã€‚æˆ‘ä»¬å°†ä¸¤ä¸ªå‰é¦ˆå±‚å®šä¹‰å¦‚ä¸‹:

$$\mathbf{Y}_{\text{out}} = \text{Linear}_{\text{out}}(\mathbf{Y} _\text{int}) = \text{Linear}_{\text{out}}(\text{Linear}_{\text{int}}(\mathbf{\overline{Z}}))$$

æ•²é‡ç‚¹ï¼åœ¨æ•°å­¦ä¸Šï¼Œå‰é¦ˆå±‚åœ¨ä½ç½® $i$ å¤„çš„è¾“å‡º $\mathbf{y}_{\text{out}, i}$ ä»…å–å†³äºè¯¥ä½ç½®çš„è¾“å…¥ $\mathbf{\overline{y}}_{i}$ã€‚ä¸è‡ªæ³¨æ„åŠ›å±‚ç›¸åï¼Œæ¯ä¸ªè¾“å‡º $\mathbf{y}_{\text{out}, i}$ ä¸å…¶ä»–ä½ç½®çš„è¾“å…¥ $\mathbf{\overline{y}}_{j \ne i}$ å®Œå…¨ç‹¬ç«‹ã€‚

$\mathbf{\overline{z}}_1, \ldots, \mathbf{\overline{z}}_{16}$ çš„å‰é¦ˆå±‚å›¾ç¤ºå¦‚ä¸‹:

![](https://raw.githubusercontent.com/patrickvonplaten/scientific_images/master/reformer_benchmark/feed_forward.png)

ä»å›¾ä¸­å¯ä»¥çœ‹å‡ºï¼Œæ‰€æœ‰è¾“å…¥å‘é‡ $\mathbf{\overline{z}}_{i}$ å‡ç”±åŒä¸€å‰é¦ˆå±‚å¹¶è¡Œå¤„ç†ã€‚

æˆ‘ä»¬å†è§‚å¯Ÿä¸€ä¸‹å‰é¦ˆå±‚çš„è¾“å‡ºç»´åº¦ï¼Œçœ‹çœ‹æœ‰æ²¡æœ‰å•¥æœ‰æ„æ€çš„äº‹æƒ…ã€‚åœ¨ Reformer ä¸­ï¼Œ$\text{Linear}_{\text{int}}$ çš„è¾“å‡ºç»´åº¦ä¸º `config.feed_forward_size` ï¼Œ _å³_ $d_ {f}$; è€Œ  $\text{Linear}_{\text{out}}$ çš„è¾“å‡ºç»´åº¦ä¸º `config.hidden_â€‹â€‹size` ï¼Œ _å³_ $d_ {h}$ã€‚

Reformer ä½œè€…è§‚å¯Ÿåˆ° $^{2}$ï¼Œåœ¨ transformer æ¨¡å‹ä¸­ï¼Œä¸­é—´ç»´åº¦ $d_{f}$ é€šå¸¸å¾€å¾€æ¯”è¾“å‡ºç»´åº¦ $d_{h}$ å¤§è®¸å¤šã€‚è¿™æ„å‘³ç€å°ºå¯¸ä¸º $d_{f} \times n$ çš„å¼ é‡ $\mathbf{\mathbf{Y}}_\text{int}$ å æ®äº†å¤§é‡çš„å†…å­˜ï¼Œç”šè‡³å¯èƒ½æˆä¸ºå†…å­˜ç“¶é¢ˆã€‚

ä¸ºäº†æ›´å¥½åœ°æ„Ÿå—ç»´åº¦çš„å·®å¼‚ï¼Œæˆ‘ä»¬å°†æœ¬æ–‡ä¾‹å­ä¸­çš„çŸ©é˜µ $\mathbf{Y}_\text{int}$ å’Œ  $\mathbf{Y}_\text{out}$ å›¾ç¤ºå¦‚ä¸‹:

![](https://raw.githubusercontent.com/patrickvonplaten/scientific_images/master/reformer_benchmark/feed_forward_matrix.png)

å¾ˆæ˜æ˜¾ï¼Œå¼ é‡ $\mathbf{Y} _\text{int}$ æ¯”  $\mathbf{Y}_{\text{out}}$ å ç”¨äº†æ›´å¤šçš„å†…å­˜ (å‡†ç¡®åœ°è¯´ï¼Œå¤šå  $\frac{d_{f}}{d_{h}} \times n$ å­—èŠ‚çš„å†…å­˜)ã€‚ä½†æ˜¯ï¼Œæ˜¯å¦æœ‰å¿…è¦å­˜å‚¨å®Œæ•´çš„ä¸­é—´çŸ©é˜µ $\mathbf{Y}_\text{int}$ ï¼Ÿå¹¶éå¦‚æ­¤ï¼Œå› ä¸ºæˆ‘ä»¬å…³å¿ƒçš„å®é™…ä¸Šåªæœ‰è¾“å‡ºçŸ©é˜µ $\mathbf{Y}_ \text{out}$ã€‚ä¸ºäº†ä»¥é€Ÿåº¦æ¢å†…å­˜ï¼Œæˆ‘ä»¬å¯ä»¥å¯¹çº¿æ€§å±‚è®¡ç®—è¿›è¡Œåˆ†å—ï¼Œä¸€æ¬¡åªå¤„ç†ä¸€ä¸ªå—ã€‚å®šä¹‰ `config.chunk_size_feed_forward` ä¸º  $c_{f}$ï¼Œåˆ™åˆ†å—çº¿æ€§å±‚å®šä¹‰ä¸º $\mathbf{Y}_{\text{out}} = \left[\mathbf{Y}_{\text{out}, 1: c_{f}}, \ldots, \mathbf{Y}_{\text{out}, (n - c_{f}): n}\right]$ å³  $\mathbf{Y}_{\text{out}, (c_{f} * i):(i * c_{f} + i)} = \text{Linear}_{\text{out}}( \text{Linear}_{\text{int}}(\mathbf{\overline{Z}}_{(c_{f} * i):(i * c_{f} + i)}))$ã€‚è¿™ä¹ˆåšæ„å‘³ç€æˆ‘ä»¬å¯ä»¥å¢é‡è®¡ç®—è¾“å‡ºæœ€åå†ä¸²æ¥åœ¨ä¸€èµ·ï¼Œè¿™æ ·å¯ä»¥é¿å…å°†æ•´ä¸ªä¸­é—´å¼ é‡ $\mathbf{Y}_{\text{int}}$ å­˜å‚¨åœ¨å†…å­˜ä¸­ã€‚

å‡è®¾ $c_{f}=1$ï¼Œæˆ‘ä»¬æŠŠå¢é‡è®¡ç®— $i=9$ çš„è¿‡ç¨‹å›¾ç¤ºå¦‚ä¸‹:

![](https://raw.githubusercontent.com/patrickvonplaten/scientific_images/master/reformer_benchmark/chunked_feed_forward.png)

å½“å—å¤§å°ä¸º 1 æ—¶ï¼Œå¿…é¡»å®Œæ•´å­˜å‚¨åœ¨å†…å­˜ä¸­çš„å”¯ä¸€å¼ é‡æ˜¯å¤§å°ä¸º $16 \times d_{h}$ çš„è¾“å…¥å¼ é‡ $\mathbf{\overline{Z}}$ï¼Œå…¶ä¸­ $d_{h}$ ä¸º `config.hidden_â€‹â€‹size` ã€‚è€Œä¸­é—´å¼ é‡åªéœ€è¦å­˜å‚¨å¤§å°ä¸º $d_{f}$ çš„  $\mathbf{y}_{\text{int}, i}$ å°±å¯ä»¥äº† $^{3}$ã€‚

æœ€åï¼Œé‡è¦çš„æ˜¯è¦è®°ä½ï¼Œ _åˆ†å—çº¿æ€§å±‚_ ä¸ä¼ ç»Ÿçš„å®Œæ•´çº¿æ€§å±‚ç›¸æ¯”ï¼Œå…¶è¾“å‡ºåœ¨æ•°å­¦ä¸Šæ˜¯ç­‰æ•ˆçš„ï¼Œå› æ­¤å¯ä»¥åº”ç”¨äºæ‰€æœ‰ transformer çº¿æ€§å±‚ã€‚å› æ­¤ï¼Œåœ¨æŸäº›åœºæ™¯ä¸‹ï¼Œå¯ä»¥è€ƒè™‘ä½¿ç”¨ `config.chunk_size_feed_forward` åœ¨å†…å­˜å’Œé€Ÿåº¦ä¹‹é—´è¿›è¡Œæ›´å¥½çš„æƒè¡¡ã€‚

---

${}^1$ ä¸ºäº†ç®€å•èµ·è§ï¼Œæˆ‘ä»¬çœç•¥äº†å‰é¦ˆå±‚ä¹‹å‰çš„å±‚å½’ä¸€åŒ–æ“ä½œã€‚

${}^2$ ä»¥ `bert-base-uncased` ä¸ºä¾‹ï¼Œå…¶ä¸­é—´ç»´åº¦ $d_{f}$ æ˜¯ 3072ï¼Œä¸ºè¾“å‡ºç»´åº¦ $d_{h}$ çš„ 4 å€ã€‚

${}^3$ æé†’ä¸€ä¸‹ï¼Œä¸ºæ¸…æ™°è¯´æ˜èµ·è§ï¼Œæœ¬æ–‡å‡è®¾è¾“å‡º `config.num_attention_heads` ä¸º 1ï¼Œå› æ­¤å‡è®¾è‡ªæ³¨æ„åŠ›å±‚çš„è¾“å‡ºå¤§å°ä¸º `config.hidden_â€‹â€‹size` ã€‚

è¯»è€…ä¹Ÿå¯ä»¥åœ¨ ğŸ¤—Transformers çš„ [ç›¸åº”æ–‡æ¡£](https://huggingface.co/transformers/glossary.html#feed-forward-chunking) ä¸­æ‰¾åˆ°æœ‰å…³åˆ†å—çº¿æ€§/å‰é¦ˆå±‚çš„æ›´å¤šä¿¡æ¯ã€‚

### åŸºå‡†æµ‹è¯•

æˆ‘ä»¬æµ‹è¯•ä¸€ä¸‹ä½¿ç”¨åˆ†å—å‰é¦ˆå±‚å¯ä»¥èŠ‚çœå¤šå°‘å†…å­˜ã€‚

```
#@title Installs and Imports
# pip installs
!pip -qq install git+https://github.com/huggingface/transformers.git
!pip install -qq py3nvml

from transformers import ReformerConfig, PyTorchBenchmark, PyTorchBenchmarkArguments
```

      Building wheel for transformers (setup.py) ... [?25l[?25hdone

é¦–å…ˆï¼Œæˆ‘ä»¬å°†æ²¡æœ‰åˆ†å—å‰é¦ˆå±‚çš„é»˜è®¤ `google/reformer-enwik8` æ¨¡å‹ä¸æœ‰åˆ†å—å‰é¦ˆå±‚çš„æ¨¡å‹è¿›è¡Œæ¯”è¾ƒã€‚

```
config_no_chunk = ReformerConfig.from_pretrained("google/reformer-enwik8") # no chunk
config_chunk = ReformerConfig.from_pretrained("google/reformer-enwik8", chunk_size_feed_forward=1) # feed forward chunk
benchmark_args = PyTorchBenchmarkArguments(sequence_lengths=[1024, 2048, 4096], batch_sizes=[8], models=["Reformer-No-Chunk", "Reformer-Chunk"], no_speed=True, no_env_print=True)
benchmark = PyTorchBenchmark(configs=[config_no_chunk, config_chunk], args=benchmark_args)
result = benchmark.run()
```

    1 / 2
    Doesn't fit on GPU. CUDA out of memory. Tried to allocate 2.00 GiB (GPU 0; 11.17 GiB total capacity; 7.85 GiB already allocated; 1.74 GiB free; 9.06 GiB reserved in total by PyTorch)
    2 / 2
    Doesn't fit on GPU. CUDA out of memory. Tried to allocate 2.00 GiB (GPU 0; 11.17 GiB total capacity; 7.85 GiB already allocated; 1.24 GiB free; 9.56 GiB reserved in total by PyTorch)
    
    ====================      INFERENCE - MEMORY - RESULT       ====================
    --------------------------------------------------------------------------------
              Model Name             Batch Size     Seq Length    Memory in MB 
    --------------------------------------------------------------------------------
          Reformer-No-Chunk              8              1024            4281     
          Reformer-No-Chunk              8              2048            7607     
          Reformer-No-Chunk              8              4096            N/A      
            Reformer-Chunk               8              1024            4309     
            Reformer-Chunk               8              2048            7669     
            Reformer-Chunk               8              4096            N/A      
    --------------------------------------------------------------------------------

æœ‰è¶£çš„æ˜¯ï¼Œåˆ†å—å‰é¦ˆå±‚ä¼¼ä¹åœ¨è¿™é‡Œæ ¹æœ¬æ²¡æœ‰å¸®åŠ©ã€‚åŸå› æ˜¯ `config.feed_forward_size` ä¸å¤Ÿå¤§ï¼Œæ‰€ä»¥æ•ˆæœä¸æ˜æ˜¾ã€‚ä»…å½“åºåˆ—é•¿åº¦è¾ƒé•¿ (4096) æ—¶ï¼Œæ‰èƒ½çœ‹åˆ°å†…å­˜ä½¿ç”¨é‡ç•¥æœ‰ä¸‹é™ã€‚

æˆ‘ä»¬å†çœ‹çœ‹å¦‚æœå°†å‰é¦ˆå±‚çš„å¤§å°å¢åŠ  4 å€ï¼Œå¹¶å°†æ³¨æ„åŠ›å¤´çš„æ•°é‡åŒæ—¶å‡å°‘ 4 å€ï¼Œä»è€Œä½¿å‰é¦ˆå±‚æˆä¸ºå†…å­˜ç“¶é¢ˆï¼Œæ­¤æ—¶å³°å€¼å†…å­˜æƒ…å½¢å¦‚ä½•ã€‚

```
config_no_chunk = ReformerConfig.from_pretrained("google/reformer-enwik8", chunk_size_feed_forward=0, num_attention_{h}eads=2, feed_forward_size=16384) # no chuck
config_chunk = ReformerConfig.from_pretrained("google/reformer-enwik8", chunk_size_feed_forward=1, num_attention_{h}eads=2, feed_forward_size=16384) # feed forward chunk
benchmark_args = PyTorchBenchmarkArguments(sequence_lengths=[1024, 2048, 4096], batch_sizes=[8], models=["Reformer-No-Chunk", "Reformer-Chunk"], no_speed=True, no_env_print=True)
benchmark = PyTorchBenchmark(configs=[config_no_chunk, config_chunk], args=benchmark_args)
result = benchmark.run()
```

    1 / 2
    2 / 2
    
    ====================      INFERENCE - MEMORY - RESULT       ====================
    --------------------------------------------------------------------------------
              Model Name             Batch Size     Seq Length    Memory in MB 
    --------------------------------------------------------------------------------
          Reformer-No-Chunk              8              1024            3743     
          Reformer-No-Chunk              8              2048            5539     
          Reformer-No-Chunk              8              4096            9087     
            Reformer-Chunk               8              1024            2973     
            Reformer-Chunk               8              2048            3999     
            Reformer-Chunk               8              4096            6011     
    --------------------------------------------------------------------------------

ç°åœ¨ï¼Œå¯¹äºè¾ƒé•¿çš„è¾“å…¥åºåˆ—ï¼Œå¯ä»¥çœ‹åˆ°å³°å€¼å†…å­˜ä½¿ç”¨é‡æ˜æ˜¾å‡å°‘ã€‚æ€»ä¹‹ï¼Œåº”è¯¥æ³¨æ„çš„æ˜¯ï¼Œåˆ†å—å‰é¦ˆå±‚ä»…å¯¹äºå…·æœ‰å¾ˆå°‘æ³¨æ„åŠ›å¤´å’Œè¾ƒå¤§å‰é¦ˆå±‚çš„æ¨¡å‹æ‰æœ‰æ„ä¹‰ã€‚

## 3. å¯é€†æ®‹å·®å±‚

å¯é€†æ®‹å·®å±‚ç”± [N. Gomez ç­‰äºº](https://arxiv.org/abs/1707.04585) é¦–å…ˆæå‡ºå¹¶åº”ç”¨åœ¨ _ResNet_ æ¨¡å‹çš„è®­ç»ƒä¸Šä»¥å‡å°‘å†…å­˜æ¶ˆè€—ã€‚ä»æ•°å­¦ä¸Šè®²ï¼Œå¯é€†æ®‹å·®å±‚ä¸ _çœŸæ­£çš„_ æ®‹å·®å±‚ç•¥æœ‰ä¸åŒï¼Œå…¶ä¸éœ€è¦åœ¨å‰å‘ä¼ æ’­æœŸé—´ä¿å­˜æ¿€æ´»ï¼Œå› æ­¤å¯ä»¥å¤§å¤§å‡å°‘è®­ç»ƒçš„å†…å­˜æ¶ˆè€—ã€‚

### Reformer ä¸­çš„å¯é€†æ®‹å·®å±‚

æˆ‘ä»¬é¦–å…ˆç ”ç©¶ä¸ºä»€ä¹ˆæ¨¡å‹è®­ç»ƒæ¯”æ¨ç†éœ€è¦æ›´å¤šçš„å†…å­˜ã€‚

åœ¨æ¨¡å‹æ¨ç†æ—¶ï¼Œæ‰€éœ€çš„å†…å­˜å·®ä¸å¤šç­‰äºè®¡ç®—æ¨¡å‹ä¸­ **å•ä¸ª** æœ€å¤§å¼ é‡æ‰€éœ€çš„å†…å­˜ã€‚è€Œåœ¨è®­ç»ƒæ¨¡å‹æ—¶ï¼Œæ‰€éœ€çš„å†…å­˜å·®ä¸å¤šç­‰äºæ‰€æœ‰å¯å¾®å¼ é‡çš„ **æ€»å’Œ**ã€‚

å¦‚æœè¯»è€…å·²ç»ç†è§£äº†æ·±åº¦å­¦ä¹ æ¡†æ¶ä¸­çš„è‡ªåŠ¨å¾®åˆ†çš„å·¥ä½œåŸç†ï¼Œå¯¹æ­¤å°±æ¯”è¾ƒå®¹æ˜“ç†è§£äº†ã€‚å¤šä¼¦å¤šå¤§å­¦ Roger Grosse çš„è¿™äº› [å¹»ç¯ç‰‡](https://www.cs.toronto.edu/~rgrosse/courses/csc321_2018/slides/lec10.pdf) å¯¹å¤§å®¶ç†è§£è‡ªåŠ¨å¾®åˆ†å¾ˆæœ‰å¸®åŠ©ã€‚

ç®€è€Œè¨€ä¹‹ï¼Œä¸ºäº†è®¡ç®—å¯å¾®å‡½æ•° ( _å¦‚_ ä¸€å±‚) çš„æ¢¯åº¦ï¼Œè‡ªåŠ¨å¾®åˆ†éœ€è¦å‡½æ•°è¾“å‡ºçš„æ¢¯åº¦ä»¥åŠå‡½æ•°çš„è¾“å…¥ã€è¾“å‡ºå¼ é‡ã€‚è™½ç„¶æ¢¯åº¦æ˜¯å¯ä»¥åŠ¨æ€è®¡ç®—å¹¶éšåä¸¢å¼ƒçš„ï¼Œä½†å‡½æ•°çš„è¾“å…¥å’Œè¾“å‡ºå¼ é‡ ( _åˆå_ æ¿€æ´») éœ€è¦åœ¨å‰å‘ä¼ æ’­è¿‡ç¨‹ä¸­è¢«ä¿å­˜ä¸‹æ¥ï¼Œä»¥ä¾›åå‘ä¼ æ’­æ—¶ä½¿ç”¨ã€‚

æˆ‘ä»¬å…·ä½“çœ‹ä¸‹ transformer æ¨¡å‹ä¸­çš„æƒ…å†µã€‚Transformer æ¨¡å‹æ˜¯ç”±å¤šä¸ª transformer å±‚å †å èµ·æ¥çš„ã€‚æ¯å¤šä¸€ä¸ª transformer å±‚éƒ½ä¼šè¿«ä½¿æ¨¡å‹åœ¨å‰å‘ä¼ æ’­è¿‡ç¨‹ä¸­ä¿å­˜æ›´å¤šçš„æ¿€æ´»ï¼Œä»è€Œå¢åŠ è®­ç»ƒæ‰€éœ€çš„å†…å­˜ã€‚
æˆ‘ä»¬ç»†çœ‹ä¸€ä¸‹ transformer å±‚ã€‚Transformer å±‚æœ¬è´¨ä¸Šç”±ä¸¤ä¸ªæ®‹å·®å±‚ç»„æˆã€‚ç¬¬ä¸€ä¸ªæ®‹å·®å±‚æ˜¯ç¬¬ 1) èŠ‚ä¸­è§£é‡Šçš„ _è‡ªæ³¨æ„åŠ›_ æœºåˆ¶ï¼Œç¬¬äºŒä¸ªæ®‹å·®å±‚æ˜¯ç¬¬ 2) èŠ‚ä¸­è§£é‡Šçš„ _çº¿æ€§å±‚_ (æˆ–å‰é¦ˆå±‚)ã€‚

ä½¿ç”¨ä¸ä¹‹å‰ç›¸åŒçš„ç¬¦å·ï¼Œtransformer å±‚çš„è¾“å…¥ _å³_ $\mathbf{X}$ é¦–å…ˆè¢«å½’ä¸€åŒ– $^{1}$ï¼Œç„¶åç»è¿‡è‡ªæ³¨æ„åŠ›å±‚è·å¾—è¾“å‡º $\mathbf{Z} = \text{SelfAttn}(\text{LayerNorm}(\mathbf{X}))$ã€‚ä¸ºæ–¹ä¾¿è®¨è®ºï¼Œæˆ‘ä»¬å°†è¿™ä¸¤å±‚ç¼©å†™ä¸º $G$ï¼Œå³ $\mathbf{Z} = G(\mathbf{X})$ã€‚
æ¥ä¸‹æ¥ï¼Œå°†æ®‹å·® $\mathbf{Z}$ ä¸è¾“å…¥ç›¸åŠ  $\mathbf{\overline{Z}} = \mathbf{Z} + \mathbf{X}$ï¼Œå¾—åˆ°å¼ é‡è¾“å…¥åˆ°ç¬¬äºŒä¸ªæ®‹å·®å±‚ â€”â€” ä¸¤ä¸ªçº¿æ€§å±‚ã€‚$\mathbf{\overline{Z}}$ ç»è¿‡ç¬¬äºŒä¸ªå½’ä¸€åŒ–å±‚å¤„ç†åï¼Œå†ç»è¿‡ä¸¤ä¸ªçº¿æ€§å±‚ï¼Œå¾—åˆ° $\mathbf{Y} = \text{Linear}(\text{LayerNorm}(\mathbf{Z} + \mathbf{X}))$ã€‚æˆ‘ä»¬å°†ç¬¬äºŒä¸ªå½’ä¸€åŒ–å±‚å’Œä¸¤ä¸ªçº¿æ€§å±‚ç¼©å†™ä¸º $F$ ï¼Œå¾—åˆ° $\mathbf{Y} = F(\mathbf{\overline{Z}})$ã€‚æœ€åï¼Œå°†æ®‹å·® $\mathbf{Y}$ åŠ åˆ° $\mathbf{\overline{Z}}$ ä¸Šå¾—åˆ° transformer å±‚çš„è¾“å‡º $\mathbf{\overline{Y}} = \mathbf{Y} + \mathbf{\overline{Z}}$ã€‚

æˆ‘ä»¬ä»ä»¥ $\mathbf{x}_1, \ldots, \mathbf{x}_{16}$ ä¸ºä¾‹å¯¹å®Œæ•´çš„ transformer å±‚è¿›è¡Œå›¾è§£ã€‚

![](https://raw.githubusercontent.com/patrickvonplaten/scientific_images/master/reformer_benchmark/normal_trans_resnet.png)

_æ¯”å¦‚_ ï¼Œè¦è®¡ç®—è‡ªæ³¨æ„åŠ›å— $G$ çš„æ¢¯åº¦ï¼Œå¿…é¡»äº‹å…ˆçŸ¥é“ä¸‰ä¸ªå¼ é‡: æ¢¯åº¦ $\partial \mathbf{Z}$ã€è¾“å‡º $\mathbf{Z}$ ä»¥åŠè¾“å…¥ $\mathbf{X}$ã€‚è™½ç„¶ $\partial \mathbf{Z}$ å¯ä»¥å³æ—¶è®¡ç®—å¹¶éšåä¸¢å¼ƒï¼Œä½† $\mathbf{Z}$ å’Œ $\mathbf{X}$ å¿…é¡»åœ¨å‰å‘ä¼ æ’­æœŸé—´è®¡ç®—å¹¶ä¿å­˜ä¸‹æ¥ï¼Œå› ä¸ºåœ¨åå‘ä¼ æ’­æœŸé—´æ¯”è¾ƒéš¾è½»æ¾åœ°å³æ—¶é‡æ–°è®¡ç®—å®ƒä»¬ã€‚å› æ­¤ï¼Œåœ¨å‰å‘ä¼ æ’­è¿‡ç¨‹ä¸­ï¼Œå¤§å¼ é‡è¾“å‡º (å¦‚æŸ¥è¯¢ - é”®ç‚¹ç§¯çŸ©é˜µ $\mathbf{Q}\mathbf{K}^T$ æˆ–çº¿æ€§å±‚çš„ä¸­é—´è¾“å‡º $\mathbf{Y}^{\text{int}}$) å¿…é¡»ä¿å­˜åœ¨å†…å­˜ä¸­ $^{2}$ã€‚

æ­¤æ—¶ï¼Œå¯é€†æ®‹å·®å±‚å°±æœ‰ç”¨äº†ã€‚å®ƒçš„æƒ³æ³•ç›¸å¯¹ç®€å•: æ®‹å·®å—çš„è®¾è®¡æ–¹å¼ä½¿å¾—ä¸å¿…ä¿å­˜å‡½æ•°çš„è¾“å…¥å’Œè¾“å‡ºå¼ é‡ï¼Œè€Œåœ¨åå‘ä¼ æ’­æœŸé—´å°±è½»æ¾åœ°å¯¹äºŒè€…è¿›è¡Œé‡æ–°è®¡ç®—ï¼Œè¿™æ ·çš„è¯åœ¨å‰å‘ä¼ æ’­æœŸé—´å°±æ— éœ€å°†è¿™äº›å¼ é‡ä¿å­˜åœ¨å†…å­˜ä¸­äº†ã€‚

è¿™æ˜¯é€šè¿‡ä¸¤ä¸ªè¾“å…¥æµ $\mathbf{X}^{(1)}ã€\mathbf{X}^{(2)}$ åŠä¸¤ä¸ªè¾“å‡ºæµ $\mathbf{\overline {Y}}^{(1)}ã€\mathbf{\overline{Y}}^{(2)}$ æ¥å®ç°çš„ã€‚ç¬¬ä¸€ä¸ªæ®‹å·® $\mathbf{Z}$ ç”±ç¬¬ä¸€ä¸ªè¾“å‡ºæµ $\mathbf{Z} = G(\mathbf{X}^{(1)})$ ç®—å¾—ï¼Œç„¶åå…¶åŠ åˆ°ç¬¬äºŒä¸ªè¾“å…¥æµçš„è¾“å…¥ä¸Šï¼Œå³ $\mathbf{\overline{Z}} = \mathbf{Z} + \mathbf{X}^{(2)}$ã€‚ç±»ä¼¼åœ°ï¼Œå†å°†æ®‹å·® $\mathbf{Y} = F(\mathbf{\overline{Z}})$ ä¸ç¬¬ä¸€ä¸ªè¾“å…¥æµç›¸åŠ ã€‚æœ€ç»ˆï¼Œä¸¤ä¸ªè¾“å‡ºæµå³ä¸º $\mathbf{Y}^{(1)} = \mathbf{Y} + \mathbf{X}^{(1)}$ã€$\mathbf{Y}^{(2)} = \mathbf{ X}^{(2)} + \mathbf{Z} = \mathbf{\overline{Z}}$ã€‚

ä»¥ $\mathbf{x}_1, \ldots, \mathbf{x}_{16}$ ä¸ºä¾‹æ¥å›¾ç¤ºå¯é€† transformer å±‚ï¼Œå¦‚ä¸‹:

![](https://raw.githubusercontent.com/patrickvonplaten/scientific_images/master/reformer_benchmark/rev_trans_resnet.png)

å¯ä»¥çœ‹å‡ºï¼Œè¾“å‡º $\mathbf{\overline{Y}}^{(1)}ã€\mathbf{\overline{Y}}^{(2)}$ çš„è®¡ç®—æ–¹å¼ä¸ä¸å¯é€†å±‚ $\mathbf{\overline{Y}}$ çš„è®¡ç®—æ–¹å¼éå¸¸ç›¸ä¼¼ï¼Œä½†åœ¨æ•°å­¦ä¸Šåˆä¸åŒã€‚Reformer çš„ä½œè€…åœ¨ä¸€äº›åˆæ­¥å®éªŒä¸­è§‚å¯Ÿåˆ°ï¼Œå¯é€† transformer æ¨¡å‹çš„æ€§èƒ½ä¸æ ‡å‡† transformer æ¨¡å‹çš„æ€§èƒ½ç›¸å½“ã€‚ä¸æ ‡å‡† transformer å±‚çš„ä¸€ä¸ªæ˜æ˜¾åŒºåˆ«æ˜¯æœ‰ä¸¤ä¸ªè¾“å…¥æµå’Œè¾“å‡ºæµ $^{3}$ï¼Œè¿™ä¸€å¼€å§‹åè€Œç¨å¾®å¢åŠ äº†å‰å‘ä¼ æ’­æ‰€éœ€çš„å†…å­˜ã€‚ä½†å³ä½¿å¦‚æ­¤ï¼Œæˆ‘ä»¬è¿˜æ˜¯å¼ºè°ƒåŒæµæ¶æ„è‡³å…³é‡è¦ï¼Œå› ä¸ºå…¶åœ¨å‰å‘ä¼ æ’­è¿‡ç¨‹ä¸­æ— éœ€ä¿å­˜ä»»ä½•æ¿€æ´»ã€‚æˆ‘ä»¬è§£é‡Šä¸€ä¸‹: å¯¹äºåå‘ä¼ æ’­ï¼Œå¯é€† treansformer å±‚å¿…é¡»è®¡ç®—æ¢¯åº¦ $\partial G$ å’Œ  $\partial F$ã€‚é™¤äº†å¯å³æ—¶è®¡ç®—çš„æ¢¯åº¦ $\partial \mathbf{Y}$ å’Œ  $\partial \mathbf{Z}$ ä¹‹å¤–ï¼Œä¸ºäº†è®¡ç®— $\partial F$ å¿…é¡»å·²çŸ¥å¼ é‡å€¼ $\mathbf{Y}$ã€$\mathbf{\overline{Z}}$ï¼Œä¸ºäº†è®¡ç®— $\partial G$ å¿…é¡»å·²çŸ¥ $\mathbf{Z}$ å’Œ  $\mathbf{X}^{(1)}$ã€‚

å‡è®¾æˆ‘ä»¬çŸ¥é“ $\mathbf{\overline{Y}}^{(1)}ï¼Œ\mathbf{\overline{Y}}^{(2)}$ï¼Œåˆ™ä»å›¾ä¸­å¯ä»¥å¾ˆå®¹æ˜“çœ‹å‡ºï¼Œæˆ‘ä»¬å¯ä»¥å¦‚ä¸‹è®¡ç®—å‡º $\mathbf{X}^{(1)}ï¼Œ\mathbf{X}^{(2)}$ ã€‚$\mathbf{X}^{(1)} = F(\mathbf{\overline{Y}}^{(1)}) - \mathbf{\overline{Y}}^{(1)}$ã€‚$\mathbf{X}^{(1)}$ è®¡ç®—å‡ºæ¥äº†ï¼ç„¶åï¼Œ$\mathbf{X}^{(2)}$ å¯ä»¥é€šè¿‡ $\mathbf {X}^{(2)} = \mathbf{\overline{Y}}^{(1)} - G(\mathbf{X}^{(1)})$ ç®—å‡ºã€‚ä¹‹åï¼Œ$\mathbf{Z}$ å’Œ  $\mathbf{Y}$ çš„è®¡ç®—å°±ç®€å•äº†ï¼Œå¯ä»¥é€šè¿‡ $\mathbf{Y} = \mathbf{\overline{Y}}^{(1)} - \mathbf{X}^{(1)}$ å’Œ  $\mathbf{Z} = \mathbf{\overline{Y}}^{(2)} - \mathbf{X }^{(2)} ç®—å‡º$ã€‚æ€»ç»“ä¸€ä¸‹ï¼Œä»…éœ€åœ¨å‰å‘ä¼ æ’­æœŸé—´å­˜å‚¨ **æœ€åä¸€ä¸ª** å¯é€† transformer å±‚çš„è¾“å‡º $\mathbf{\overline{Y}}^{(1)}ï¼Œ\mathbf{\overline{Y}}^{(2)}$ï¼Œæ‰€æœ‰å…¶ä»–å±‚çš„æ¿€æ´»å°±å¯ä»¥é€šè¿‡åœ¨åå‘ä¼ æ’­æœŸé—´ä½¿ç”¨ $G$ å’Œ  $F$ ä»¥åŠ $\mathbf {X}^{(1)}$ å’Œ  $\mathbf{X}^{(2)}$ æ¨å¯¼è€Œå¾—ã€‚åœ¨åå‘ä¼ æ’­æœŸé—´ï¼Œæ¯ä¸ªå¯é€† transformer å±‚ç”¨ä¸¤æ¬¡å‰å‘ä¼ æ’­ $G$ å’Œ  $F$ çš„è®¡ç®—å¼€é”€æ¢å–å‰å‘ä¼ æ’­æ—¶ä¸å¿…ä¿å­˜ä»»ä½•æ¿€æ´»ã€‚å¥½ä¹°å–ï¼

**æ³¨æ„**: æœ€è¿‘ï¼Œä¸»è¦çš„æ·±åº¦å­¦ä¹ æ¡†æ¶éƒ½æ”¯æŒäº†æ¢¯åº¦æ£€æŸ¥ç‚¹æŠ€æœ¯ï¼Œä»¥å…è®¸ä»…ä¿å­˜æŸäº›æ¿€æ´»å¹¶åœ¨åå‘ä¼ æ’­æœŸé—´é‡è®¡ç®—å°ºå¯¸è¾ƒå¤§çš„æ¿€æ´» (Tensoflow ä»£ç è§ [æ­¤å¤„](https://www.tensorflow.org/api_docs/python/tf/recompute_grad)ï¼ŒPyTorch ä»£ç è§ [æ­¤å¤„](https://pytorch.org/docs/stable/checkpoint.html))ã€‚å¯¹äºæ ‡å‡†å¯é€†å±‚ï¼Œè¿™ä»ç„¶æ„å‘³ç€å¿…é¡»ä¸ºæ¯ä¸ª transformer å±‚ä¿å­˜è‡³å°‘ä¸€ä¸ªæ¿€æ´»ï¼Œä½†é€šè¿‡å®šä¹‰å“ªäº›æ¿€æ´»å¯ä»¥åŠ¨æ€é‡æ–°è®¡ç®—ï¼Œèƒ½å¤ŸèŠ‚çœå¤§é‡å†…å­˜ã€‚

---

$^{1}$ åœ¨å‰ä¸¤èŠ‚ä¸­ï¼Œæˆ‘ä»¬çœç•¥äº†è‡ªæ³¨æ„åŠ›å±‚å’Œçº¿æ€§å±‚ä¹‹å‰çš„å±‚å½’ä¸€åŒ–æ“ä½œã€‚è¯»è€…åº”è¯¥çŸ¥é“ $\mathbf{X}$ å’Œ  $\mathbf{\overline{Z}}$ åœ¨è¾“å…¥è‡ªæ³¨æ„åŠ›å±‚å’Œçº¿æ€§å±‚ä¹‹å‰éƒ½åˆ†åˆ«ç»è¿‡å±‚å½’ä¸€åŒ–å¤„ç†ã€‚

$^{2}$ åœ¨åŸå§‹è‡ªæ³¨æ„åŠ›ä¸­ï¼Œ$\mathbf{Q}\mathbf{K}$ çš„ç»´åº¦ä¸º $n \times n$; è€Œåœ¨ _LSH è‡ªæ³¨æ„åŠ›_ æˆ– _å±€éƒ¨è‡ªæ³¨æ„åŠ›_ å±‚çš„ç»´åº¦ä¸º $n \times l_{c} \times n_{h}$ æˆ–  $n \times l_{c}$ å…¶ä¸­ $l_{c}$ ä¸ºå—é•¿åº¦ï¼Œ$n_{h}$ ä¸ºå“ˆå¸Œæ•°ã€‚

$^{3}$ ç¬¬ä¸€ä¸ªå¯é€† transformer å±‚çš„ $\mathbf{X}^{(2)}$ ç­‰äº $\mathbf{X}^{(1)}$ã€‚

### æµ‹è¯•åŸºå‡†

ä¸ºäº†æµ‹é‡å¯é€†æ®‹å·®å±‚çš„æ•ˆæœï¼Œæˆ‘ä»¬å°†å¢åŠ æ¨¡å‹å±‚æ•°çš„åŒæ—¶æ¯”è¾ƒ BERT å’Œ Reformer çš„å†…å­˜æ¶ˆè€—ã€‚

```
#@title Installs and Imports
# pip installs
!pip -qq install git+https://github.com/huggingface/transformers.git
!pip install -qq py3nvml

from transformers import ReformerConfig, BertConfig, PyTorchBenchmark, PyTorchBenchmarkArguments
```

æˆ‘ä»¬æŠŠæ ‡å‡† `bert-base-uncased` BERT æ¨¡å‹çš„å±‚æ•°ä» 4 å¢åŠ åˆ° 12 ï¼ŒåŒæ—¶æµ‹é‡å…¶æ‰€éœ€å†…å­˜ã€‚

```
config_4_layers_bert = BertConfig.from_pretrained("bert-base-uncased", num_hidden_layers=4)
config_8_layers_bert = BertConfig.from_pretrained("bert-base-uncased", num_hidden_layers=8)
config_12_layers_bert = BertConfig.from_pretrained("bert-base-uncased", num_hidden_layers=12)
benchmark_args = PyTorchBenchmarkArguments(sequence_lengths=[512], batch_sizes=[8], models=["Bert-4-Layers", "Bert-8-Layers", "Bert-12-Layers"], training=True, no_inference=True, no_speed=True, no_env_print=True)
benchmark = PyTorchBenchmark(configs=[config_4_layers_bert, config_8_layers_bert, config_12_layers_bert], args=benchmark_args)
result = benchmark.run()
```


    HBox(children=(FloatProgress(value=0.0, description='Downloading', max=433.0, style=ProgressStyle(description_â€¦


    
    1 / 3
    2 / 3
    3 / 3
    
    ====================        TRAIN - MEMORY - RESULTS        ====================
    --------------------------------------------------------------------------------
              Model Name             Batch Size     Seq Length    Memory in MB 
    --------------------------------------------------------------------------------
            Bert-4-Layers                8              512             4103     
            Bert-8-Layers                8              512             5759     
            Bert-12-Layers               8              512             7415     
    --------------------------------------------------------------------------------

å¯ä»¥çœ‹å‡ºï¼ŒBERT å±‚æ•°æ¯å¢åŠ  1ï¼Œå…¶æ‰€éœ€å†…å­˜å°±ä¼šæœ‰è¶… 400MB çš„çº¿æ€§å¢é•¿ã€‚

```
config_4_layers_reformer = ReformerConfig.from_pretrained("google/reformer-enwik8", num_hidden_layers=4, num_hashes=1)
config_8_layers_reformer = ReformerConfig.from_pretrained("google/reformer-enwik8", num_hidden_layers=8, num_hashes=1)
config_12_layers_reformer = ReformerConfig.from_pretrained("google/reformer-enwik8", num_hidden_layers=12, num_hashes=1)
benchmark_args = PyTorchBenchmarkArguments(sequence_lengths=[512], batch_sizes=[8], models=["Reformer-4-Layers", "Reformer-8-Layers", "Reformer-12-Layers"], training=True, no_inference=True, no_speed=True, no_env_print=True)
benchmark = PyTorchBenchmark(configs=[config_4_layers_reformer, config_8_layers_reformer, config_12_layers_reformer], args=benchmark_args)
result = benchmark.run()
```


    1 / 3
    2 / 3
    3 / 3
    
    ====================        TRAIN - MEMORY - RESULTS        ====================
    --------------------------------------------------------------------------------
              Model Name             Batch Size     Seq Length    Memory in MB 
    --------------------------------------------------------------------------------
          Reformer-4-Layers              8              512             4607     
          Reformer-8-Layers              8              512             4987     
          Reformer-12-Layers             8              512             5367     
    --------------------------------------------------------------------------------

å¦ä¸€æ–¹é¢ï¼Œå¯¹äº Reformer è€Œè¨€ï¼Œæ¯å¢åŠ ä¸€å±‚æ‰€å¸¦æ¥çš„å†…å­˜å¢é‡ä¼šæ˜¾è‘—å‡å°‘ï¼Œå¹³å‡ä¸åˆ° 100MBã€‚å› æ­¤ 12 å±‚çš„ `reformer-enwik8` æ¨¡å‹æ¯” 12 å±‚çš„ `bert-base-uncased` æ¨¡å‹çš„å†…å­˜éœ€æ±‚æ›´å°‘ã€‚

## 4. è½´å‘ä½ç½®ç¼–ç 

Reformer ä½¿å¾—å¤„ç†è¶…é•¿è¾“å…¥åºåˆ—æˆä¸ºå¯èƒ½ã€‚ç„¶è€Œï¼Œå¯¹äºå¦‚æ­¤é•¿çš„è¾“å…¥åºåˆ—ï¼Œä»…å­˜å‚¨æ ‡å‡†ä½ç½®ç¼–ç æƒé‡çŸ©é˜µå°±éœ€è¦è¶…è¿‡ 1GB å†…å­˜ã€‚ä¸ºäº†é¿å…å¦‚æ­¤å¤§çš„ä½ç½®ç¼–ç çŸ©é˜µï¼Œå®˜æ–¹ Reformer ä»£ç å¼•å…¥äº† _è½´å‘ä½ç½®ç¼–ç _ ã€‚

**é‡è¦:** _å®˜æ–¹è®ºæ–‡ä¸­æ²¡æœ‰è§£é‡Šè½´å‘ä½ç½®ç¼–ç ï¼Œä½†é€šè¿‡é˜…è¯»ä»£ç ä»¥åŠä¸ä½œè€…è®¨è®ºæˆ‘ä»¬å¾ˆå¥½åœ°ç†è§£äº†å®ƒã€‚_

### Reformer ä¸­çš„è½´å‘ä½ç½®ç¼–ç 

Transformer éœ€è¦ä½ç½®ç¼–ç æ¥å¯¹è¾“å…¥åºåˆ—ä¸­çš„å•è¯é¡ºåºè¿›è¡Œç¼–ç ï¼Œå› ä¸ºè‡ªæ³¨æ„åŠ›å±‚ _æ²¡æœ‰é¡ºåºçš„æ¦‚å¿µ_ ã€‚ä½ç½®ç¼–ç é€šå¸¸ç”±ä¸€ä¸ªç®€å•çš„æŸ¥æ‰¾çŸ©é˜µ $\mathbf{E} = \left[\mathbf{e}_1, \ldots, \mathbf{e}_{n_\text{max}}\right]$ æ¥å®šä¹‰ï¼Œç„¶åå°†ä½ç½®ç¼–ç å‘é‡ $\mathbf{e}_{i}$ ç®€å•åœ°åŠ åˆ° _ç¬¬ i ä¸ª_ è¾“å…¥å‘é‡ä¸Šï¼Œå³ $\mathbf{x}_{i} + \mathbf{e}_{i}$ï¼Œä»¥ä¾¿æ¨¡å‹å¯ä»¥åŒºåˆ†è¾“å…¥å‘é‡ ( _å³_ è¯å…ƒ) ä½äºä½ç½® $i$ è¿˜æ˜¯ä½ç½®$j$ã€‚å¯¹äºæ¯ä¸ªè¾“å…¥ä½ç½®ï¼Œæ¨¡å‹éœ€è¦èƒ½å¤ŸæŸ¥æ‰¾åˆ°ç›¸åº”çš„ä½ç½®ç¼–ç å‘é‡ï¼Œå› æ­¤ $\mathbf{E}$ çš„ç»´åº¦ç”±æ¨¡å‹å¯ä»¥å¤„ç†çš„æœ€å¤§è¾“å…¥åºåˆ—é•¿åº¦ `config.max_position_embeddings` ( _å³_ $n_\text{max}$) ä»¥åŠè¾“å…¥å‘é‡çš„ç»´åº¦ `config.hidden_â€‹â€‹size` ( _å³_ $d_{h}$) å…±åŒå†³å®šã€‚

å‡è®¾ $d_{h}=4$ï¼Œ$n_\text{max}=49$ï¼Œå…¶ä½ç½®ç¼–ç çŸ©é˜µå¦‚ä¸‹å›¾æ‰€ç¤º:

![](https://raw.githubusercontent.com/patrickvonplaten/scientific_images/master/reformer_benchmark/positional_encodings_default.png)

æ­¤å¤„ï¼Œæˆ‘ä»¬ä»…å±•ç¤ºä½ç½®ç¼–ç  $\mathbf{e}_{1}$ã€$\mathbf{e}_{2}$ åŠ $\mathbf{e}_{49}$ï¼Œå…¶ç»´åº¦ ( _å³_ é«˜åº¦) ä¸º 4ã€‚

æƒ³è±¡ä¸€ä¸‹ï¼Œæˆ‘ä»¬æƒ³è¦åœ¨é•¿åº¦æœ€é•¿ä¸º 0.5M ä¸ªè¯å…ƒï¼Œè¾“å…¥å‘é‡ç»´åº¦ `config.hidden_â€‹â€‹size` ä¸º 1024 çš„åºåˆ—ä¸Šè®­ç»ƒ Reformer æ¨¡å‹ (è¯·å‚é˜… [æ­¤ç¬”è®°æœ¬](https://github.com/patrickvonplaten/notebooks/blob/master/PyTorch_Reformer.ipynb))ã€‚å…¶å¯¹åº”çš„ä½ç½®åµŒå…¥çš„å‚æ•°é‡ä¸º $0.5M \times 1024 \sim 512M$ï¼Œå¤§å°ä¸º 2GBã€‚

åœ¨å°†æ¨¡å‹åŠ è½½åˆ°å†…å­˜ä¸­æˆ–å°†å…¶ä¿å­˜åœ¨ç¡¬ç›˜ä¸Šæ—¶ï¼Œæ‰€éœ€è¦çš„å†…å­˜æ˜¯å¾ˆå¤§ä¸”å¾ˆæ²¡å¿…è¦çš„ã€‚

Reformer ä½œè€…é€šè¿‡å°† `config.hidden_â€‹â€‹size` ç»´åº¦ä¸€åˆ†ä¸ºäºŒï¼Œå¹¶å·§å¦™åœ°å¯¹ $n_\text{max}$ ç»´è¿›è¡Œåˆ†è§£ï¼Œä»è€ŒæˆåŠŸåœ°å¤§å¹…ç¼©å°äº†ä½ç½®ç¼–ç çš„å¤§å°ã€‚åœ¨ transformers ä¸­ï¼Œç”¨æˆ·å¯ä»¥å°† `config.axis_pos_shape` è®¾ç½®ä¸ºä¸€ä¸ªå«æœ‰ä¸¤ä¸ªå€¼çš„åˆ—è¡¨: $n_\text{max}^ 1$ã€$n_\text{max}^2$ï¼Œå…¶ä¸­ $n_\text{max}^1 \times n_\text{max}^2 = n_\text{max}$ï¼Œä»è€Œå¯¹ $n_\text{max}$ ç»´åº¦è¿›è¡Œåˆ†è§£ã€‚åŒæ—¶ï¼Œç”¨æˆ·å¯ä»¥æŠŠ `config.axis_pos_embds_dim` è®¾ç½®ä¸ºä¸€ä¸ªå«æœ‰ä¸¤ä¸ªå€¼ $d_{h}^{1}$ å’Œ $d_{h}^2$ çš„åˆ—è¡¨ï¼Œå…¶ä¸­ $d_{h} ^1 + d_{h}^2 = d_{h}$ï¼Œä»è€Œå†³å®šéšè—ç»´åº¦åº”è¯¥å¦‚ä½•åˆ‡å‰²ã€‚ä¸‹é¢ç”¨å›¾ç¤ºæ¥ç›´è§‚è§£é‡Šä¸€ä¸‹ã€‚

å¤§å®¶å¯ä»¥å°†å¯¹ $n_{\text{max}}$ çš„åˆ†è§£è§†ä¸ºå°†å…¶ç»´åº¦æŠ˜å åˆ°ç¬¬ä¸‰ä¸ªè½´ï¼Œä¸‹å›¾æ‰€ç¤ºä¸º `config.axis_pos_shape = [7, 7]` åˆ†è§£:

![](https://raw.githubusercontent.com/patrickvonplaten/scientific_images/master/reformer_benchmark/3d_positional_encoding.png)

ä¸‰ä¸ªç›´ç«‹çŸ©å½¢æ£±æŸ±åˆ†åˆ«å¯¹åº”äºç¼–ç å‘é‡ $\mathbf{e}_{1}, \mathbf{e}_{2}, \mathbf{e}_{49}$ï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ° 49 ä¸ªç¼–ç å‘é‡è¢«åˆ†ä¸º 7 è¡Œï¼Œæ¯è¡Œ 7 ä¸ªå‘é‡ã€‚ç°åœ¨çš„æƒ³æ³•æ˜¯ä»…ä½¿ç”¨ 7 ä¸ªç¼–ç å‘é‡ä¸­çš„ä¸€è¡Œï¼Œå¹¶å°†è¿™äº›å‘é‡æ‰©å±•åˆ°å…¶ä»– 6 è¡Œã€‚æœ¬è´¨ä¸Šæ˜¯æƒ³è®©ä¸ƒè¡Œé‡ç”¨ä¸€è¡Œçš„å€¼ï¼Œä½†æ˜¯åˆä¸èƒ½è®©ä¸åŒä½ç½®çš„ç¼–ç å‘é‡çš„å€¼ç›¸åŒï¼Œæ‰€ä»¥è¦å°†æ¯ä¸ªç»´åº¦ ( _æˆ–ç§°_ é«˜åº¦) ä¸º `config.hidden_â€‹â€‹size=4` çš„å‘é‡åˆ‡å‰²æˆä¸¤ä¸ªéƒ¨åˆ†: å¤§å°ä¸º $1$ çš„ä½åŒºç¼–ç å‘é‡ $\mathbf{e}_\text{down}$ ä»¥åŠå¤§å°ä¸º $3$ çš„é«˜åŒºç¼–ç å‘é‡ $\mathbf{e}_\text{up}$ï¼Œè¿™æ ·ä½åŒºå°±å¯ä»¥æ²¿è¡Œæ‰©å±•è€Œé«˜åŒºå¯ä»¥æ²¿åˆ—æ‰©å±•ã€‚ä¸ºäº†è®²æ¸…æ¥šï¼Œæˆ‘ä»¬è¿˜æ˜¯ç”»ä¸ªå›¾ã€‚

![](https://raw.githubusercontent.com/patrickvonplaten/scientific_images/master/reformer_benchmark/3d_positional_encoding_cut.png)

å¯ä»¥çœ‹åˆ°ï¼Œæˆ‘ä»¬å·²å°†åµŒå…¥å‘é‡åˆ‡ä¸º $\mathbf{e}_\text{down}$ ( _è“è‰²_ ) å’Œ $\mathbf{e}_\text{up}$ ( _é»„è‰²_ ) ä¸¤ä¸ªéƒ¨åˆ†ã€‚ç°åœ¨å¯¹ _å­_ å‘é‡ $\mathbf{E} _\text{down} = \left[\mathbf{e}_ {\text{down},1}, \ldots, \mathbf{e} _{\text{down},49}\right]$ ä»…ä¿ç•™ç¬¬ä¸€è¡Œçš„ 7 ä¸ªå­å‘é‡ï¼Œ _å³_ å›¾ä¸­å®½åº¦ï¼Œå¹¶å°†å…¶æ²¿åˆ— ( _åˆå_ æ·±åº¦) æ‰©å±•ã€‚ç›¸åï¼Œå¯¹ _å­_ å‘é‡ $\mathbf{E}_\text{up} = \left[\mathbf{e}_{\text{up},1}, \ldots, \mathbf{e }_{\text{up},49}\right]$ ä»…ä¿ç•™ç¬¬ä¸€åˆ—çš„ $7$ ä¸ªå­å‘é‡å¹¶æ²¿è¡Œæ‰©å±•ã€‚æ­¤æ—¶ï¼Œå¾—åˆ°çš„åµŒå…¥å‘é‡ $\mathbf{e'}_{i}$ å¦‚ä¸‹:

$$\mathbf{e'}_{i} = \left[ \left[\mathbf{e}_{\text{down, } i \% n_\text{max}^1}\right]^T, \left[\mathbf{e}_{\text{up, } \left \lfloor{\frac{i}{{n}^2_{\text{max}}}}\right \rfloor} \right]^T \right]^T $$

æœ¬ä¾‹ä¸­ï¼Œ$n_\text{max}^1 = 7$ï¼Œ$n_\text{max}^2 = 7$ ã€‚è¿™äº›æ–°ç¼–ç  $\mathbf{E'} = \left[\mathbf{e'}_{1}, \ldots, \mathbf{e'}_{n_\text{max}}\right]$ ç§°ä¸º **è½´å‘ä½ç½®ç¼–ç **ã€‚

ä¸‹å›¾é’ˆå¯¹æˆ‘ä»¬çš„ä¾‹å­å¯¹è½´å‘ä½ç½®ç¼–ç è¿›è¡Œäº†æ›´è¯¦ç»†çš„è¯´æ˜ã€‚

![](https://raw.githubusercontent.com/patrickvonplaten/scientific_images/master/reformer_benchmark/axial_pos_encoding.png)

ç°åœ¨åº”è¯¥å¾ˆæ¸…æ¥šå¦‚ä½•ä»…æ ¹æ®ç»´åº¦ä¸º $d_{h}^1 \times n_{\text{max}^1}$ çš„  $\mathbf{E}_{\text{down}}$ åŠç»´åº¦ä¸º $d_{h}^2 \times n_{\text{max}}^2$ çš„  $\mathbf{E}_{\text{up}}$ è®¡ç®—æœ€ç»ˆä½ç½®ç¼–ç å‘é‡ $\mathbf{E'}$ äº†ã€‚

è¿™é‡Œçš„å…³é”®æ˜¯ï¼Œè½´å‘ä½ç½®ç¼–ç èƒ½å¤Ÿä»è®¾è®¡ä¸Šç¡®ä¿å‘é‡ $\left[\mathbf{e'}_1, \ldots, \mathbf{e'}_{n_{\text{max} }}\right]$ ä¹‹é—´å„ä¸ç›¸ç­‰ï¼Œå¹¶ä¸”ä½¿ç¼–ç çŸ©é˜µçš„å¤§å°ä» $n_{\text{max}} \times d_{h}$ å‡å°åˆ° $n_{\text{max}}^1 \times d_{h}^1 + n_\text{max}^2 \times d_{h}^2$ã€‚å› ä¸ºè®¾è®¡ä¸Šå…è®¸æ¯ä¸ªè½´å‘ä½ç½®ç¼–ç å‘é‡ä¸åŒï¼Œæ‰€ä»¥ä¸€æ—¦æ¨¡å‹ä¸­çš„è½´å‘ä½ç½®ç¼–ç è®­å‡ºæ¥åï¼Œæ¨¡å‹å°±å¯ä»¥çµæ´»é«˜æ•ˆåœ°è·å–ä½ç½®ç¼–ç ã€‚

ä¸ºäº†è¯æ˜ä½ç½®ç¼–ç çŸ©é˜µçš„å°ºå¯¸å¾—åˆ°äº†å¤§å¹…å‡å°ï¼Œå‡è®¾æˆ‘ä»¬ä¸º Reformer æ¨¡å‹è®¾ç½®äº†å‚æ•° `config.axis_pos_shape = [1024, 512]` ä»¥åŠ `config.axis_pos_embds_dim = [512, 512]` ï¼Œä¸”è¯¥æ¨¡å‹æ”¯æŒçš„æœ€é•¿è¾“å…¥åºåˆ—é•¿åº¦ä¸º 0.5M è¯å…ƒã€‚æ­¤æ—¶ï¼Œç”Ÿæˆçš„è½´å‘ä½ç½®ç¼–ç çŸ©é˜µçš„å‚æ•°é‡ä»…ä¸º $1024 \times 512 + 512 \times 512 \sim 800K$ï¼Œå³å¤§çº¦ 3MBã€‚è¿™ä¸ªæ•°å­—ä¸æ ‡å‡†ä½ç½®ç¼–ç çŸ©é˜µæ‰€éœ€çš„ 2GB ç›¸æ¯”ï¼Œç®€ç›´æ˜¯å°å·«è§å¤§å·«ã€‚

å¦‚éœ€æ›´ç®€æ´ã€æ›´æ•°å­¦åŒ–çš„è§£é‡Šï¼Œè¯·å‚é˜… [æ­¤å¤„](https://huggingface.co/transformers/model_doc/reformer.html#axis-positional-encodings) çš„  ğŸ¤—Transformers æ–‡æ¡£ã€‚

### åŸºå‡†æµ‹è¯•

æœ€åï¼Œæˆ‘ä»¬å¯¹ä¼ ç»Ÿä½ç½®åµŒå…¥ä¸ _è½´å‘ä½ç½®åµŒå…¥_ çš„å³°å€¼å†…å­˜æ¶ˆè€—è¿›è¡Œæ¯”è¾ƒã€‚

```
#@title Installs and Imports
# pip installs
!pip -qq install git+https://github.com/huggingface/transformers.git
!pip install -qq py3nvml

from transformers import ReformerConfig, PyTorchBenchmark, PyTorchBenchmarkArguments, ReformerModel
```

ä½ç½®åµŒå…¥ä»…å–å†³äºä¸¤ä¸ªé…ç½®å‚æ•°: è¾“å…¥åºåˆ—å…è®¸çš„æœ€å¤§é•¿åº¦ `config.max_position_embeddings` ä»¥åŠ `config.hidden_â€‹â€‹size` ã€‚æˆ‘ä»¬ä½¿ç”¨ä¸€ä¸ªæ¨¡å‹ï¼Œå…¶æ”¯æŒçš„è¾“å…¥åºåˆ—çš„æœ€å¤§å…è®¸é•¿åº¦ä¸º 50 ä¸‡ä¸ªè¯å…ƒï¼Œå³ `google/reformer-crime-and-punishment` ï¼Œæ¥çœ‹çœ‹ä½¿ç”¨è½´å‘ä½ç½®åµŒå…¥åçš„æ•ˆæœã€‚

é¦–å…ˆï¼Œæˆ‘ä»¬æ¯”è¾ƒè½´å‘ä½ç½®ç¼–ç ä¸æ ‡å‡†ä½ç½®ç¼–ç çš„å‚æ•°å½¢çŠ¶ï¼ŒåŠå…¶ç›¸åº”æ¨¡å‹çš„æ€»å‚æ•°é‡ã€‚

```
config_no_pos_axial_embeds = ReformerConfig.from_pretrained("google/reformer-crime-and-punishment", axial_pos_embds=False) # disable axial positional embeddings
config_pos_axial_embeds = ReformerConfig.from_pretrained("google/reformer-crime-and-punishment", axial_pos_embds=True, axial_pos_embds_dim=(64, 192), axial_pos_shape=(512, 1024)) # enable axial positional embeddings

print("Default Positional Encodings")
print(20 *'-')
model = ReformerModel(config_no_pos_axial_embeds)
print(f"Positional embeddings shape: {model.embeddings.position_embeddings}")
print(f"Num parameters of model: {model.num_parameters()}")
print(20 *'-' + '\n\n')

print("Axial Positional Encodings")
print(20 *'-')
model = ReformerModel(config_pos_axial_embeds)
print(f"Positional embeddings shape: {model.embeddings.position_embeddings}")
print(f"Num parameters of model: {model.num_parameters()}")
print(20 *'-' + '\n\n')
```


    HBox(children=(FloatProgress(value=0.0, description='Downloading', max=1151.0, style=ProgressStyle(descriptionâ€¦


    
    Default Positional Encodings
    --------------------
    Positional embeddings shape: PositionEmbeddings(
      (embedding): Embedding(524288, 256)
    )
    Num parameters of model: 136572416
    --------------------
    
    
    Axial Positional Encodings
    --------------------
    Positional embeddings shape: AxialPositionEmbeddings(
      (weights): ParameterList(
          (0): Parameter containing: [torch.FloatTensor of size 512x1x64]
          (1): Parameter containing: [torch.FloatTensor of size 1x1024x192]
      )
    )
    Num parameters of model: 2584064
    --------------------

ç†è§£äº†ç›¸åº”çš„ç†è®ºåï¼Œè¯»è€…åº”è¯¥ä¸ä¼šå¯¹è½´å‘ä½ç½®ç¼–ç æƒé‡çš„å½¢çŠ¶æ„Ÿåˆ°æƒŠè®¶ã€‚

ä»ç»“æœä¸­å¯ä»¥çœ‹å‡ºï¼Œå¯¹äºéœ€è¦å¤„ç†å¦‚æ­¤é•¿è¾“å…¥åºåˆ—çš„æ¨¡å‹ï¼Œä½¿ç”¨æ ‡å‡†ä½ç½®ç¼–ç æ˜¯ä¸åˆ‡å®é™…çš„ã€‚ä»¥ `google/reformer-crime-and-punishment` ä¸ºä¾‹ï¼Œä»…æ ‡å‡†ä½ç½®ç¼–ç è‡ªèº«å‚æ•°é‡å°±è¶…è¿‡ 100Mã€‚è½´å‘ä½ç½®ç¼–ç å¯ä»¥å°†è¿™ä¸ªæ•°å­—å‡å°‘åˆ°ç•¥é«˜äº 200Kã€‚

æœ€åï¼Œæˆ‘ä»¬æ¯”è¾ƒä¸€ä¸‹æ¨ç†æ‰€éœ€å†…å­˜ã€‚

```
benchmark_args = PyTorchBenchmarkArguments(sequence_lengths=[512], batch_sizes=[8], models=["Reformer-No-Axial-Pos-Embeddings", "Reformer-Axial-Pos-Embeddings"], no_speed=True, no_env_print=True)
benchmark = PyTorchBenchmark(configs=[config_no_pos_axial_embeds, config_pos_axial_embeds], args=benchmark_args)
result = benchmark.run()
```

    1 / 2
    2 / 2
    
    ====================      INFERENCE - MEMORY - RESULT       ====================
    --------------------------------------------------------------------------------
              Model Name             Batch Size     Seq Length    Memory in MB 
    --------------------------------------------------------------------------------
    Reformer-No-Axial-Pos-Embeddin       8              512             959      
    Reformer-Axial-Pos-Embeddings        8              512             447      
    --------------------------------------------------------------------------------

å¯ä»¥çœ‹å‡ºï¼Œåœ¨ `google/reformer-crime-and-punishment` æ¨¡å‹ä¸Šï¼Œä½¿ç”¨è½´å‘ä½ç½®åµŒå…¥å¯å‡å°‘å¤§çº¦ä¸€åŠçš„å†…å­˜éœ€æ±‚ã€‚

---
title: "ä½¿ç”¨ PyTorch FSDP å¾®è°ƒ Llama 2 70B" 
thumbnail: /blog/assets/160_fsdp_llama/thumbnail.jpg
authors:
- user: smangrul
- user: sgugger
- user: lewtun
- user: philschmid
translators:
- user: MatrixYao
- user: zhongdongy
  proofreader: true
---

# ä½¿ç”¨ PyTorch FSDP å¾®è°ƒ Llama 2 70B

## å¼•è¨€

é€šè¿‡æœ¬æ–‡ï¼Œä½ å°†äº†è§£å¦‚ä½•ä½¿ç”¨ PyTorch FSDP åŠç›¸å…³æœ€ä½³å®è·µå¾®è°ƒ Llama 2 70Bã€‚åœ¨æ­¤è¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬ä¸»è¦ä¼šç”¨åˆ° Hugging Face Transformersã€Accelerate å’Œ TRL åº“ã€‚æˆ‘ä»¬è¿˜å°†å±•ç¤ºå¦‚ä½•åœ¨ SLURM ä¸­ä½¿ç”¨ Accelerateã€‚

å®Œå…¨åˆ†ç‰‡æ•°æ®å¹¶è¡Œ (Fully Sharded Data Parallelismï¼ŒFSDP) æ˜¯ä¸€ç§è®­ç»ƒèŒƒå¼ï¼Œåœ¨è¯¥èŒƒå¼ä¸­ä¼˜åŒ–å™¨çŠ¶æ€ã€æ¢¯åº¦å’Œæ¨¡å‹å‚æ•°éƒ½ä¼šè¢«è·¨è®¾å¤‡åˆ†ç‰‡ã€‚å‰å‘ä¼ æ’­æ—¶ï¼Œæ¯ä¸ª FSDP å•å…ƒæ‰§è¡Œ _all gather_ ä»¥è·å–å®Œæ•´çš„æƒé‡ï¼Œç„¶åç”¨å®ƒä»¬è¿›è¡Œè®¡ç®—å¹¶åœ¨è®¡ç®—åä¸¢å¼ƒæ‰å…¶ä»–è®¾å¤‡çš„åˆ†ç‰‡ã€‚éšåæ˜¯åå‘ä¼ æ’­ï¼Œç„¶åå°±æ˜¯æŸå¤±è®¡ç®—ã€‚åå‘ä¼ æ’­æ—¶ï¼Œæ¯ä¸ª FSDP å•å…ƒæ‰§è¡Œ _all gather_ æ“ä½œä»¥è·å–å®Œæ•´çš„æƒé‡ï¼Œå¹¶æ‰§è¡Œè®¡ç®—ä»¥è·å¾—æœ¬åœ° batch çš„æ¢¯åº¦ã€‚è¿™äº›æ¢¯åº¦é€šè¿‡ _reduce scatter_ åœ¨è®¾å¤‡ä¸Šè¿›è¡Œå‡å€¼è®¡ç®—å¹¶åˆ†ç‰‡ï¼Œè¿™æ ·æ¯ä¸ªè®¾å¤‡éƒ½å¯ä»¥æ›´æ–°å…¶å¯¹åº”åˆ†ç‰‡çš„å‚æ•°ã€‚æœ‰å…³ PyTorch FSDP çš„æ›´å¤šä¿¡æ¯ï¼Œè¯·å‚é˜…æ­¤åšæ–‡: [ä½¿ç”¨ PyTorch å®Œå…¨åˆ†ç‰‡æ•°æ®å¹¶è¡ŒæŠ€æœ¯åŠ é€Ÿå¤§æ¨¡å‹è®­ç»ƒ](https://huggingface.co/blog/zh/pytorch-fsdp)ã€‚

![FSDP å·¥ä½œæµ](https://huggingface.co/blog/assets/62_pytorch_fsdp/FSDP_workflow.png)

(å›¾æº: [é“¾æ¥](https://pytorch.org/blog/introducing-pytorch-fully-sharded-data-parallel-api/))

## ä½¿ç”¨çš„ç¡¬ä»¶

èŠ‚ç‚¹æ•°: 2ï¼Œè‡³å°‘ 1 ä¸ªèŠ‚ç‚¹  
æ¯èŠ‚ç‚¹ GPU æ•°: 8  
GPU ç±»å‹: A100  
GPU æ˜¾å­˜: 80GB  
èŠ‚ç‚¹å†…äº’è”: NVLink  
æ¯èŠ‚ç‚¹å†…å­˜: 1TB  
æ¯èŠ‚ç‚¹ CPU æ ¸æ•°: 96  
èŠ‚ç‚¹é—´äº’è”: AWS çš„ Elastic Fabric Adapter (EFA)

## å¾®è°ƒ LLaMa 2 70B é¢ä¸´çš„æŒ‘æˆ˜

åœ¨å°è¯•ä½¿ç”¨ FSDP å¾®è°ƒ LLaMa 2 70B æ—¶ï¼Œæˆ‘ä»¬ä¸»è¦é‡åˆ°äº†ä¸‰ä¸ªæŒ‘æˆ˜:

1. FSDP ä¼šå…ˆåŠ è½½æ•´ä¸ªé¢„è®­ç»ƒæ¨¡å‹ï¼Œç„¶åå†å¯¹æ¨¡å‹è¿›è¡Œåˆ†ç‰‡ã€‚è¿™æ ·å°±æ„å‘³ç€èŠ‚ç‚¹å†…çš„æ¯ä¸ªè¿›ç¨‹ (å³ rank) éƒ½ä¼šåŠ è½½æ•´ä¸ª Llama-70B æ¨¡å‹ï¼Œå› æ­¤éœ€è¦ 70*4*8 GB ~ 2TB çš„ CPU å†…å­˜ï¼Œè¿™ä¸ªç®—å¼ä¸­ 4 æ˜¯æ¯ä¸ªå‚æ•°æ‰€éœ€å­—èŠ‚æ•°ï¼Œ8 æ˜¯æ¯ä¸ªèŠ‚ç‚¹çš„ GPU æ•°ã€‚è¿™ä¼šå¯¼è‡´ CPU å†…å­˜ä¸è¶³ï¼Œè¿›è€Œå¯¼è‡´è¿›ç¨‹ç»ˆæ­¢ã€‚
2. ä½¿ç”¨ `FULL_STATE_DICT` æ¥ä¿å­˜å®Œæ•´ä¸­é—´æ£€æŸ¥ç‚¹å¹¶å°†å…¶å¸è½½è‡³ rank 0 çš„ CPU å†…å­˜ä¸­éœ€è¦èŠ±è´¹å¤§é‡æ—¶é—´ï¼Œä¸”ç”±äºåœ¨æ­¤æœŸé—´é€šä¿¡åº“éœ€è¦æ— é™æœŸæŒ‚èµ·ç­‰å¾…ä¿å­˜å®Œæˆï¼Œå› æ­¤ç»å¸¸ä¼šå¯¼è‡´ NCCL è¶…æ—¶é”™è¯¯ã€‚ç„¶è€Œï¼Œå®Œå…¨å…³æ‰è¿™ä¸ªé€‰é¡¹ä¹Ÿä¸å¥½ï¼Œå› ä¸ºåœ¨è®­ç»ƒç»“æŸæ—¶æˆ‘ä»¬éœ€è¦ä¿å­˜å®Œæ•´çš„æ¨¡å‹çŠ¶æ€å­—å…¸ï¼Œè€Œä¸æ˜¯ FSDP å¼åˆ†ç‰‡çš„çŠ¶æ€å­—å…¸ã€‚
3. æˆ‘ä»¬éœ€è¦æé«˜é€Ÿåº¦å¹¶å‡å°‘æ˜¾å­˜ä½¿ç”¨ï¼Œä»¥åŠ å¿«è®­ç»ƒå¹¶èŠ‚çº¦è®¡ç®—æˆæœ¬ã€‚

ä¸‹æ–‡ï¼Œæˆ‘ä»¬ä¸»è¦è®¨è®ºå¦‚ä½•ä¸€ä¸€è§£å†³ä¸Šè¿°æŒ‘æˆ˜ï¼Œæœ€ç»ˆå¾®è°ƒå‡ºä¸€ä¸ª 70B çš„æ¨¡å‹ï¼

å…ˆåˆ—å‡ºé‡ç°ç»“æœæ‰€éœ€çš„æ‰€æœ‰èµ„æº:

1. ä»£ç åº“: <url>https://github.com/pacman100/DHS-LLM-Workshop/tree/main/chat_assistant/training</url>ï¼Œä»£ç ä¸­åŒ…å«äº†ä½¿èƒ½ flash æ³¨æ„åŠ› V2 çš„çƒ­è¡¥ä¸
2. FSDP é…ç½®æ–‡ä»¶: <url>https://github.com/pacman100/DHS-LLM-Workshop/blob/main/chat_assistant/training/configs/fsdp_config.yaml</url>
3. SLURM å¯åŠ¨è„šæœ¬ - `launch.slurm` : <url>https://gist.github.com/pacman100/1cb1f17b2f1b3139a63b764263e70b25</url>
4. æ¨¡å‹: `meta-llama/Llama-2-70b-chat-hf` 
5. æ•°æ®é›†: [smangrul/code-chat-assistant-v1](https://huggingface.co/datasets/smangrul/code-chat-assistant-v1) (æ··åˆäº† LIMA å’Œ GUANACO æ•°æ®é›†ï¼Œä¸”å·²è½¬æ¢ä¸ºè®­ç»ƒæ‰€éœ€çš„æ ¼å¼)

### å‡†å¤‡å·¥ä½œ

é¦–å…ˆæŒ‰ç…§ [æ­¤æ­¥éª¤](https://github.com/Dao-AILab/flash-attention) å®‰è£… Flash Attention V2ã€‚ç„¶åï¼Œå®‰è£…æœ€æ–°çš„ PyTorch nightly (CUDA â‰¥11.8)ã€‚æ¥ç€ï¼Œæ ¹æ® [æ­¤æ–‡ä»¶](https://github.com/pacman100/DHS-LLM-Workshop/blob/main/personal_copilot/training/requirements.txt) å®‰è£…å…¶ä½™ä¾èµ–è½¯ä»¶ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æ˜¯ä»ä¸»åˆ†æ”¯å®‰è£… ğŸ¤— Accelerate å’Œ ğŸ¤— Transformers çš„ã€‚

## å¾®è°ƒ

### åº”å¯¹æŒ‘æˆ˜ 1

PR [25107](https://github.com/huggingface/transformers/pull/25107) å’Œ PR [1777](https://github.com/huggingface/accelerate/pull/1777) è§£å†³äº†ç¬¬ä¸€ä¸ªæŒ‘æˆ˜ï¼Œä¸”æ— éœ€ç”¨æˆ·ä¾§æ›´æ”¹ä»»ä½•ä»£ç ã€‚ä¸»è¦åšçš„äº‹æƒ…å¦‚ä¸‹:

1. åœ¨æ‰€æœ‰ rank ä¸Šåˆ›å»ºæ— æƒé‡çš„ç©ºæ¨¡å‹ (ä½¿ç”¨ `meta` è®¾å¤‡)
2. ä»…åœ¨ rank 0 ä¸Šå°†çŠ¶æ€å­—å…¸åŠ è½½è‡³æ¨¡å‹
3. å…¶ä»– rank ä»…å¯¹ `meta` è®¾å¤‡ä¸Šçš„å‚æ•°æ‰§è¡Œ `torch.empty(*param.size(), dtype=dtype)`
4. å› æ­¤ï¼Œåªæœ‰ rank 0 ä¸ŠåŠ è½½äº†å®Œæ•´çš„æ¨¡å‹åŠæƒé‡ï¼Œè€Œæ‰€æœ‰å…¶ä»– rank ä¸Šçš„æƒé‡æ˜¯ç©ºçš„
5. è®¾ç½® `sync_module_states=True` ï¼Œä»¥ä¾¿ FSDP å®ä¾‹åœ¨è®­ç»ƒå¼€å§‹ä¹‹å‰å°†æƒé‡å¹¿æ’­åˆ°å„ rank

ä¸‹é¢æ˜¯åœ¨ 2 ä¸ª GPU ä¸ŠåŠ è½½ 7B æ¨¡å‹çš„è¾“å‡ºæ—¥å¿—ç‰‡æ®µï¼Œå®ƒæµ‹é‡äº†å„ä¸ªé˜¶æ®µå†…å­˜çš„æ¶ˆè€—åŠå…¶åŠ è½½çš„æ¨¡å‹å‚æ•°é‡ã€‚æˆ‘ä»¬å¯ä»¥è§‚å¯Ÿåˆ°ï¼Œåœ¨åŠ è½½é¢„è®­ç»ƒæ¨¡å‹æ—¶ï¼Œrank 0 å’Œ rank 1 çš„ CPU å³°å€¼å†…å­˜åˆ†åˆ«ä¸º `32744 MB` å’Œ `1506 MB` ã€‚å› æ­¤å¯çŸ¥ï¼Œä»…æœ‰ rank 0 åŠ è½½äº†é¢„è®­ç»ƒæ¨¡å‹ï¼Œè¿™å°±å®ç°äº† CPU å†…å­˜çš„æœ‰æ•ˆåˆ©ç”¨ã€‚ä½ å¯åœ¨ [æ­¤å¤„](https://gist.github.com/pacman100/2fbda8eb4526443a73c1455de43e20f9) æ‰¾åˆ°å®Œæ•´æ—¥å¿—ã€‚

```bash
accelerator.process_index=0 GPU Memory before entering the loading : 0
accelerator.process_index=0 GPU Memory consumed at the end of the loading (end-begin): 0
accelerator.process_index=0 GPU Peak Memory consumed during the loading (max-begin): 0
accelerator.process_index=0 GPU Total Peak Memory consumed during the loading (max): 0
accelerator.process_index=0 CPU Memory before entering the loading : 926
accelerator.process_index=0 CPU Memory consumed at the end of the loading (end-begin): 26415
accelerator.process_index=0 CPU Peak Memory consumed during the loading (max-begin): 31818
accelerator.process_index=0 CPU Total Peak Memory consumed during the loading (max): 32744

accelerator.process_index=1 GPU Memory before entering the loading : 0
accelerator.process_index=1 GPU Memory consumed at the end of the loading (end-begin): 0
accelerator.process_index=1 GPU Peak Memory consumed during the loading (max-begin): 0
accelerator.process_index=1 GPU Total Peak Memory consumed during the loading (max): 0
accelerator.process_index=1 CPU Memory before entering the loading : 933
accelerator.process_index=1 CPU Memory consumed at the end of the loading (end-begin): 10
accelerator.process_index=1 CPU Peak Memory consumed during the loading (max-begin): 573
accelerator.process_index=1 CPU Total Peak Memory consumed during the loading (max): 1506
```

### åº”å¯¹æŒ‘æˆ˜ 2

è¯¥æŒ‘æˆ˜å¯ä»¥é€šè¿‡åœ¨é…ç½® FSDP æ—¶å°†çŠ¶æ€å­—å…¸ç±»å‹è®¾ä¸º `SHARDED_STATE_DICT` æ¥è§£å†³ã€‚è®¾ä¸º `SHARDED_STATE_DICT` åï¼Œæ¯ä¸ª rank å„è‡ªä¿å­˜å„è‡ª GPU æ‰€éœ€è¦çš„åˆ†ç‰‡ï¼Œè¿™ä½¿å¾—ç”¨æˆ·å¯ä»¥å¿«é€Ÿä¿å­˜ä¸­é—´æ£€æŸ¥ç‚¹å¹¶å¿«é€Ÿä»å…¶æ¢å¤è®­ç»ƒã€‚è€Œå½“ä½¿ç”¨ `FULL_STATE_DICT` æ—¶ï¼Œç¬¬ä¸€ä¸ªè¿›ç¨‹ (rank 0) ä¼šç”¨ CPU æ”¶é›†æ•´ä¸ªæ¨¡å‹ï¼Œç„¶åå°†å…¶ä¿å­˜ä¸ºæ ‡å‡†æ ¼å¼ã€‚

æˆ‘ä»¬å¯ä»¥ç”¨ä»¥ä¸‹å‘½ä»¤åˆ›å»ºç›¸åº”çš„ accelerte é…ç½®æ–‡ä»¶:

```
accelerate config --config_file "fsdp_config.yaml"
```

![fsdp é…ç½®](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/160_ram_efficient_fsdp/fsdp_config.jpg)

ä½ å¯ä»¥ä»æ­¤å¤„è·å–ç”Ÿæˆçš„é…ç½®æ–‡ä»¶: [fsdp_config.yaml](https://github.com/pacman100/DHS-LLM-Workshop/blob/main/chat_assistant/training/configs/fsdp_config.yaml)ã€‚åœ¨è¯¥é…ç½®æ–‡ä»¶ä¸­ï¼Œåˆ†ç‰‡ç­–ç•¥æ˜¯ `FULL_SHARD` ã€‚æˆ‘ä»¬ä½¿ç”¨ `TRANSFORMER_BASED_WRAP` ä½œä¸ºè‡ªåŠ¨æ¨¡å‹åŒ…è£…ç­–ç•¥ï¼Œå®ƒä½¿ç”¨ `_no_split_module` æ¥æœç´¢ transformer å—åå¹¶è‡ªåŠ¨è¿›è¡ŒåµŒå¥— FSDP åŒ…è£…ã€‚æˆ‘ä»¬ä½¿ç”¨ `SHAARDED_STATE_DICT` æŠŠä¸­é—´æ£€æŸ¥ç‚¹å’Œä¼˜åŒ–å™¨çŠ¶æ€ä¿å­˜ä¸º PyTorch å®˜æ–¹æ¨èçš„æ ¼å¼ã€‚åŒæ—¶ï¼Œå¦‚ä¸Šä¸€èŠ‚ `åº”å¯¹æŒ‘æˆ˜ 1` ä¸­æ‰€è¿°ï¼Œæˆ‘ä»¬è¿˜éœ€è¦ç¡®ä¿è®­ç»ƒå¼€å§‹æ—¶ç”¨ rank 0 æ¥å¹¿æ’­å‚æ•°ã€‚ä»é…ç½®æ–‡ä»¶ä¸­ä½ è¿˜å¯ä»¥çœ‹åˆ°æˆ‘ä»¬ç”¨çš„æ˜¯ `bf16` æ··åˆç²¾åº¦è®­ç»ƒã€‚

é‚£ä¹ˆï¼Œåœ¨ä¿å­˜æœ€ç»ˆæ£€æŸ¥ç‚¹æ—¶ï¼Œå¦‚æœå°†å…¶ä¿å­˜æˆå•ä¸ªæ–‡ä»¶å‘¢ï¼Ÿæˆ‘ä»¬ä½¿ç”¨çš„æ˜¯ä»¥ä¸‹ä»£ç æ®µ:

```python
if trainer.is_fsdp_enabled:
    trainer.accelerator.state.fsdp_plugin.set_state_dict_type("FULL_STATE_DICT")

trainer.save_model(script_args.output_dir) # æˆ–è€… , å¦‚æœæ•´ä¸ªæ¨¡å‹å°äº 50 GB (å³ LFS å•æ–‡ä»¶çš„æœ€å¤§å°ºå¯¸)ï¼Œä½ è¿˜å¯ä»¥ä½¿ç”¨ trainer.push_to_hub() æŠŠæ¨¡å‹æ¨åˆ° hub ä¸Šå»ã€‚
```

### åº”å¯¹æŒ‘æˆ˜ 3

ä¸ºäº†åŠ å¿«è®­ç»ƒé€Ÿåº¦å¹¶å‡å°‘æ˜¾å­˜å ç”¨ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨ flash æ³¨æ„åŠ›å¹¶å¼€å¯æ¢¯åº¦æ£€æŸ¥ç‚¹ä¼˜åŒ–ï¼Œä»è€Œåœ¨å¾®è°ƒçš„åŒæ—¶èŠ‚çœè®¡ç®—æˆæœ¬ã€‚å½“å‰ï¼Œæˆ‘ä»¬ç”¨äº†ä¸€ä¸ªçƒ­è¡¥ä¸æ¥å®ç° flash æ³¨æ„åŠ›ï¼Œå…·ä½“ä»£ç å¯è§ [è¿™å„¿](https://github.com/pacman100/DHS-LLM-Workshop/blob/main/chat_assistant/training/llama_flash_attn_monkey_patch.py)ã€‚

[FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness](https://arxiv.org/pdf/2205.14135.pdf) ä¸€æ–‡åŸºäºå¯¹åº•å±‚ç¡¬ä»¶ (å³ GPU) çš„å†…å­˜å±‚æ¬¡ç»“æ„çš„æ·±åˆ»ç†è§£è€Œå¼•å…¥äº†ä¸€ç§æ›´å¿«ã€æ›´èŠ‚çœå†…å­˜çš„æ— æŸæ³¨æ„åŠ›åŠ é€Ÿç®—æ³•ã€‚åº•å±‚ç¡¬ä»¶åœ¨è®¾è®¡å†…å­˜å±‚æ¬¡ç»“æ„æ—¶ï¼Œéµå¾ªçš„å®è·µåŸåˆ™æ˜¯: å¸¦å®½/é€Ÿåº¦è¶Šé«˜çš„å†…å­˜ï¼Œå…¶å®¹é‡è¶Šå°ï¼Œå› ä¸ºå®ƒæ›´è´µã€‚

æ ¹æ®åšæ–‡ [æ ¹æ®ç¬¬ä¸€æ€§åŸç†è®©æ·±åº¦å­¦ä¹ æ€§èƒ½èµ·é£](https://horace.io/brrr_intro.html)ï¼Œæˆ‘ä»¬å¯ä»¥å‘ç°ï¼Œå½“å‰ç¡¬ä»¶ä¸Šçš„æ³¨æ„åŠ›æ¨¡å—æ˜¯ `å†…å­˜å¸¦å®½å—é™` çš„ã€‚åŸå› æ˜¯æ³¨æ„åŠ›æœºåˆ¶ **ä¸»è¦ç”±é€å…ƒç´ æ“ä½œ** ç»„æˆï¼Œå¦‚ä¸‹å·¦å›¾æ‰€ç¤ºã€‚æˆ‘ä»¬å¯ä»¥è§‚å¯Ÿåˆ°ï¼Œæ©ç ã€softmax å’Œ dropout æ“ä½œå ç”¨äº†å¤§éƒ¨åˆ†æ—¶é—´ï¼Œè€Œééœ€è¦å¤§é‡ FLOP çš„çŸ©é˜µä¹˜æ³•ã€‚

![æ³¨æ„åŠ›æœºåˆ¶çš„æ€§èƒ½ç“¶é¢ˆ](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/160_ram_efficient_fsdp/attention_bottleneck.png)

(å›¾æº: [é“¾æ¥](https://arxiv.org/pdf/2205.14135.pdf))

è¿™æ­£æ˜¯ flash æ³¨æ„åŠ›è§£å†³çš„é—®é¢˜ï¼Œå…¶æƒ³æ³•æ˜¯ **å»é™¤å†—ä½™çš„ HBM è¯»/å†™æ“ä½œ**ã€‚è¯¥ç®—æ³•é€šè¿‡å°†æ‰€æœ‰å†…å®¹ä¿ç•™åœ¨ SRAM ä¸­ï¼Œå¾…æ‰§è¡Œå®Œæ‰€æœ‰ä¸­é—´æ­¥éª¤åå†å°†æœ€ç»ˆç»“æœå†™å›åˆ° HBMï¼Œå³ **ç®—å­èåˆ** æ¥å®ç°è¿™ä¸€ç›®çš„ã€‚ä¸‹å›¾ç®€è¦æè¿°äº†ç®—å­èåˆæ˜¯å¦‚ä½•å…‹æœå†…å­˜ç“¶é¢ˆçš„ã€‚

![ç®—å­èåˆ](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/160_ram_efficient_fsdp/kernel_fusion.webp)

(å›¾æº: [é“¾æ¥](https://gordicaleksa.medium.com/eli5-flash-attention-5c44017022ad))

åœ¨å‰å‘å’Œåå‘ä¼ æ’­è¿‡ç¨‹ä¸­æˆ‘ä»¬è¿˜ä½¿ç”¨äº† **å¹³é“º (Tiling)** ä¼˜åŒ–æŠ€å·§ï¼Œå°† NxN å¤§å°çš„ softmax åˆ†æ•°è®¡ç®—åˆ‡æˆå—ï¼Œä»¥å…‹æœ SRAM å†…å­˜å¤§å°çš„é™åˆ¶ã€‚åœ¨ä½¿ç”¨å¹³é“ºæŠ€å·§æ—¶ï¼Œæˆ‘ä»¬ä¼šä½¿ç”¨åœ¨çº¿ softmax ç®—æ³•ã€‚åŒæ—¶ï¼Œæˆ‘ä»¬è¿˜åœ¨åå‘ä¼ æ’­ä¸­ä½¿ç”¨äº† **é‡è®¡ç®—** æŠ€å·§ï¼Œä»¥å¤§å¤§é™ä½åœ¨å‰å‘ä¼ æ’­è¿‡ç¨‹ä¸­å­˜å‚¨æ•´ä¸ª NxN softmax åˆ†æ•°çŸ©é˜µæ‰€å¸¦æ¥çš„å†…å­˜æ¶ˆè€—ã€‚

å¦‚æ¬²æ·±å…¥ç†è§£ flash æ³¨æ„åŠ›ï¼Œè¯·å‚è€ƒåšæ–‡ [ELI5: FlashAttention](https://gordicaleksa.medium.com/eli5-flash-attention-5c44017022ad)ã€[æ ¹æ®ç¬¬ä¸€æ€§åŸç†è®©æ·±åº¦å­¦ä¹ æ€§èƒ½èµ·é£](https://horace.io/brrr_intro.html) ä»¥åŠåŸå§‹è®ºæ–‡ [FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness](https://arxiv.org/pdf/2205.14135.pdf)ã€‚

## ç»¼åˆè¿ç”¨æ‰€æœ‰æ‰‹æ®µ

ä½ å¯å‚è€ƒ [æ­¤è„šæœ¬]((https://gist.github.com/pacman100/1cb1f17b2f1b3139a63b764263e70b25))ï¼Œä»¥åœ¨ SLURM ä¸­ç”¨ `Accelerate` å¯åŠ¨å™¨è¿è¡Œè®­ç»ƒã€‚ä¸‹é¢è¿˜ç»™å‡ºäº†ä¸€ä¸ªç­‰æ•ˆå‘½ä»¤ï¼Œå±•ç¤ºäº†å¦‚ä½•ä½¿ç”¨ `Accelerate` å¯åŠ¨å™¨æ¥è¿è¡Œè®­ç»ƒã€‚è¯·æ³¨æ„ï¼Œè¯¥å‘½ä»¤ä¼šè¦†ç›– `fsdp_config.yaml` ä¸­çš„ `main_process_ip` ã€ `main_process_port` ã€ `machine_rank` ã€ `num_processes` ä»¥åŠ `num_machines` é…ç½®ã€‚å¦ä¸€ä¸ªéœ€è¦é‡ç‚¹æ³¨æ„çš„æ˜¯ï¼Œè¿™é‡Œçš„å­˜å‚¨æ˜¯æ‰€æœ‰èŠ‚ç‚¹å…±äº«çš„ã€‚

```
accelerate launch \
    --config_file configs/fsdp_config.yaml \
    --main_process_ip $MASTER_ADDR \
    --main_process_port $MASTER_PORT \
    --machine_rank \$MACHINE_RANK \
    --num_processes 16 \
    --num_machines 2 \
    train.py \
    --model_name "meta-llama/Llama-2-70b-chat-hf" \
    --dataset_name "smangrul/code-chat-assistant-v1" \
    --max_seq_len 2048 \
    --max_steps 500 \
    --logging_steps 25 \
    --eval_steps 100 \
    --save_steps 250 \
    --bf16 True \
    --packing True \
    --output_dir "/shared_storage/sourab/experiments/full-finetune-llama-chat-asst" \
    --per_device_train_batch_size 1 \
    --gradient_accumulation_steps 1 \
    --dataset_text_field "content" \
    --use_gradient_checkpointing True \
    --learning_rate 5e-5 \
    --lr_scheduler_type "cosine" \
    --weight_decay 0.01 \
    --warmup_ratio 0.03 \
    --use_flash_attn True
```

æ•´ä¸ªå¾®è°ƒè¿‡ç¨‹éœ€è¦çº¦ 13.5 å°æ—¶ï¼Œä¸‹å›¾ç»™å‡ºäº†è®­ç»ƒæŸå¤±æ›²çº¿ã€‚

![è®­ç»ƒæŸå¤±æ›²çº¿](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/160_ram_efficient_fsdp/train_loss.png)

ä¸‹ä¾‹ç»™å‡ºäº†ä½¿ç”¨ä¸Šè¿°æ¨¡å‹å®Œæˆçš„ä¸€æ®µå¯¹è¯:

```diff
System Prompt: You are a helpful, respectful and honest assistant. Always answer as helpfully \
as possible, while being safe. Your answers should not include any harmful, \
unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that \
your responses are socially unbiased and positive in nature.

If a question does not make any sense, or is not factually coherent, explain why \
instead of answering something not correct. If you donâ€™t know the answer to a \
question, please donâ€™t share false information.

- Human: What is Deep Learning? Explain like a Pirate.

+ Assistant: Arrr Grumete! Are ye lookin' fer a proper explanation o' Deep Learnin'?
+ Alright then, listen up me hearty! Deep Learnin' be a type o' machine learnin' that uses artificial neural networks to analyze data.
+ Aye, ye heard that right, mateys! Artificial neural networks be models o' the human brain, but instead o' using neurons, they use interconnected nodes or "artificial neurons" to process information.
+ These nodes work together to form layers o' processing units that can learn from large amounts o' data.
+ The more data these systems receive, the better they get at recognizing patterns and makin' decisions on their own.
+ So hoist the sails and set course for adventure with deep learnin', me hearties! It's a treasure trove o' knowledge just waitin' to be discovered.
+ Fair winds and following seas!

- Human: Now explain it like a chef.

+ Assistant: Certainly! Here's an explanation of deep learning from a culinary perspective:

+ Think of a recipe as a sequence of steps used to transform raw ingredients into a delicious dish.
+ Similarly, in deep learning, there are multiple layers of "ingredients" (or features) that are combined and transformed through various operations to produce a final output or prediction.
+ Each layer builds upon the previous one, refining and enhancing the representation of the input data until it reaches the desired outcome.
+ Just as different spices enhance the flavors of food, each layer adds depth and complexity to the model's understanding of the input, allowing it to make more accurate predictions.
+ And much like how chefs must carefully balance the amount of salt, sugar, and other seasonings in a recipe, the training process in deep learning involves adjusting the weights and biases of the network to achieve the perfect blend of accuracy and efficiency.
+ With enough practice and experimentation, anyone can become a master chef... I mean, a master of deep learning! Bon appÃ©tit!
```

æ•´ä¸ªå¯¹è¯ä½¿ç”¨çš„æ ¼å¼å¦‚ä¸‹:

```
<|system|> system message <|endoftext|> <|prompter|> Q1 <|endoftext|> <|assistant|> A1 <|endoftext|> ...
```

## æ€»ç»“

æˆ‘ä»¬åœ¨å¤šèŠ‚ç‚¹å¤š GPU ä¸Šä½¿ç”¨ PyTorch FSDP æˆåŠŸå¾®è°ƒäº†ä¸€ä¸ª 70B Llama æ¨¡å‹ï¼Œå¹¶åœ¨æ­¤è¿‡ç¨‹ä¸­è§£å†³äº†å„ç§æŒ‘æˆ˜ã€‚æˆ‘ä»¬çœ‹åˆ°äº†å½“å‰åœ¨ ğŸ¤— Transformers å’Œ ğŸ¤— Accelerates ä¸­åº”å¦‚ä½•åˆå§‹åŒ–å¤§æ¨¡å‹ä»è€Œæœ‰æ•ˆå…‹æœ CPU å†…å­˜ä¸è¶³çš„é—®é¢˜ã€‚æˆ‘ä»¬è¿˜ç»™å‡ºäº†å¦‚ä½•é«˜æ•ˆåœ°ä¿å­˜/åŠ è½½ä¸­é—´æ£€æŸ¥ç‚¹ï¼ŒåŒæ—¶åˆèƒ½ä»¥æ˜“äºä½¿ç”¨çš„æ–¹å¼ä¿å­˜æœ€ç»ˆæ¨¡å‹çš„æœ€ä½³å®è·µã€‚ä¸ºäº†åŠ é€Ÿè®­ç»ƒå¹¶å‡å°‘ GPU æ˜¾å­˜ä½¿ç”¨ï¼Œæˆ‘ä»¬è¿˜å¼ºè°ƒäº† flash æ³¨æ„åŠ›å’Œæ¢¯åº¦æ£€æŸ¥ç‚¹æœºåˆ¶çš„é‡è¦æ€§ã€‚æœ€åï¼Œæˆ‘ä»¬å‘å¤§å®¶å±•ç¤ºäº†åœ¨ ğŸ¤— Accelerate ä¸Šä»…éœ€è¦ç®€å•çš„é…ç½®å°±å¯ä»¥åœ¨å¤šèŠ‚ç‚¹å¤š GPU ä¸Šå¾®è°ƒå¤§æ¨¡å‹ã€‚
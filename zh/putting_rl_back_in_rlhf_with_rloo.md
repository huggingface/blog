---
title: "å°†å¼ºåŒ–å­¦ä¹ é‡æ–°å¼•å…¥RLHF"
thumbnail: /blog/assets/putting_rl_back_in_rlhf_with_rloo/thumbnail.png
authors:
- user: vwxyzjn
- user: ArashAhmadian
  org: CohereForAI
  guest: true
translators:
- user: innovation64
---

# å°†å¼ºåŒ–å­¦ä¹ é‡æ–°å¼•å…¥ RLHF


æˆ‘ä»¬å¾ˆé«˜å…´åœ¨ TRL ä¸­ä»‹ç» RLOOï¼ˆREINFORCE Leave One-Outï¼‰è®­ç»ƒå™¨ã€‚ä½œä¸ºä¸€ç§æ›¿ä»£ PPO çš„æ–¹æ³•ï¼ŒRLOO æ˜¯ä¸€ç§æ–°çš„åœ¨çº¿ RLHF è®­ç»ƒç®—æ³•ï¼Œæ—¨åœ¨ä½¿å…¶æ›´æ˜“äºè®¿é—®å’Œå®æ–½ã€‚ç‰¹åˆ«æ˜¯ï¼Œ**RLOO éœ€è¦çš„ GPU å†…å­˜æ›´å°‘ï¼Œå¹¶ä¸”è¾¾åˆ°æ”¶æ•›æ‰€éœ€çš„æŒ‚é’Ÿæ—¶é—´ä¹Ÿæ›´çŸ­**ã€‚å¦‚ä¸‹é¢çš„å›¾è¡¨æ‰€ç¤ºï¼š


1. ğŸ¤‘æ ¹æ®æ¨¡å‹å¤§å°ï¼ŒRLOO ä½¿ç”¨çš„ vRAM æ¯” PPO **å°‘å¤§çº¦ 50-70%**ï¼›
2. ğŸš€å¯¹äº 1B å‚æ•°æ¨¡å‹ï¼ŒRLOO çš„è¿è¡Œé€Ÿåº¦æ¯” PPO **å¿« 2 å€**ï¼Œå¯¹äº 6.9B å‚æ•°æ¨¡å‹ï¼ŒRLOO çš„è¿è¡Œé€Ÿåº¦æ¯” PPO **å¿« 3 å€**ã€‚
3. ğŸ”¥åœ¨å“åº”èƒœç‡ï¼ˆç”± GPT4 åˆ¤æ–­ï¼‰æ–¹é¢ï¼ŒRLOO **ä¸ PPO ç›¸å½“**ï¼Œå¹¶ä¸”å§‹ç»ˆä¼˜äº DPO ç­‰æµè¡Œçš„ç¦»çº¿æ–¹æ³•ã€‚

é€šè¿‡ RLOOï¼Œæˆ‘ä»¬å°†å¼ºåŒ–å­¦ä¹ é‡æ–°å¼•å…¥ RLHFï¼Œä½¿ç¤¾åŒºèƒ½å¤Ÿæ›´è½»æ¾åœ°æ¢ç´¢åœ¨çº¿ RL æ–¹æ³•ã€‚è¿™ä»¤äººå…´å¥‹ï¼Œå› ä¸ºè¶Šæ¥è¶Šå¤šçš„ç ”ç©¶è¡¨æ˜ï¼Œåœ¨çº¿ RL æ¯” DPO ç­‰ç¦»çº¿æ–¹æ³•æ›´æœ‰æ•ˆ([https://arxiv.org/abs/2402.04792](https://arxiv.org/abs/2402.04792), [https://arxiv.org/abs/2405.08448](https://arxiv.org/abs/2405.08448))ã€‚


<p align="center">
  <img src="https://huggingface.co/datasets/trl-internal-testing/example-images/resolve/main/blog/putting_rl_back_in_rlhf_with_rloo/win_rate_comparison.png?download=true" alt="alt_text" title="image_tooltip" />
</p>
<p align="center">
  <img src="https://huggingface.co/datasets/trl-internal-testing/example-images/resolve/main/blog/putting_rl_back_in_rlhf_with_rloo/memory_runtime_comparison.png?download=true" alt="alt_text" title="image_tooltip" />
</p>


è¿™ç¯‡åšå®¢å°†è§£é‡Š RLOO è®­ç»ƒå™¨çš„èƒŒåçš„åŠ¨æœºï¼Œå®ƒæ˜¯å¦‚ä½•å·¥ä½œçš„ï¼Œä»¥åŠå¦‚ä½•åœ¨ TRL ä¸­ä½¿ç”¨å®ƒã€‚

# åŠ¨æœº

PPO æ˜¯ä¸€ç§æœ‰æ•ˆçš„åœ¨çº¿ RLHF è®­ç»ƒç®—æ³•ï¼Œç”¨äºè®­ç»ƒæœ€å…ˆè¿›çš„æ¨¡å‹ï¼Œå¦‚ GPT-4ã€‚ç„¶è€Œï¼Œç”±äºå…¶å¯¹ GPU å†…å­˜çš„é«˜è¦æ±‚ï¼ŒPPO åœ¨å®é™…ä½¿ç”¨ä¸­å¯èƒ½ç›¸å½“å…·æœ‰æŒ‘æˆ˜æ€§ã€‚ç‰¹åˆ«æ˜¯ï¼ŒPPO éœ€è¦å°†æ¨¡å‹çš„ 4 ä¸ªå‰¯æœ¬åŠ è½½åˆ°å†…å­˜ä¸­ï¼š1ï¼‰ç­–ç•¥æ¨¡å‹ï¼Œ2ï¼‰å‚è€ƒç­–ç•¥æ¨¡å‹ï¼Œ3ï¼‰å¥–åŠ±æ¨¡å‹ï¼Œä»¥åŠ 4ï¼‰ä»·å€¼æ¨¡å‹ï¼Œå¦‚ä¸‹é¢çš„å›¾æ‰€ç¤ºã€‚PPO è¿˜æœ‰è®¸å¤šå¾®å¦™çš„å®ç°ç»†èŠ‚ï¼Œè¿™äº›ç»†èŠ‚å¯èƒ½å¾ˆéš¾æ­£ç¡®æŠŠæ¡ï¼ˆ[Engstromç­‰äººï¼›2020](https://openreview.net/forum?id=r1etN1rtPB)ï¼Œ[Huangç­‰äºº2022](https://iclr-blog-track.github.io/2022/03/25/ppo-implementation-details/)ï¼‰ã€‚


![alt_text](https://huggingface.co/datasets/trl-internal-testing/example-images/resolve/main/blog/putting_rl_back_in_rlhf_with_rloo/image7.png?download=true "image_tooltip")


åœ¨ Cohere çš„ä¸€ç¯‡æ–°è®ºæ–‡ä¸­ï¼Œ[Ahmadian ç­‰äºº(2024)](https://cohere.com/research/papers/back-to-basics-revisiting-reinforce-style-optimization-for-learning-from-human-feedback-in-llms-2024-02-23)é‡æ–°å®¡è§†äº† RLHF è®­ç»ƒçš„åŸºç¡€ï¼Œå¹¶æå‡ºäº†ä¸€ç§æ›´ç®€æ´çš„æ–¹æ³•ï¼Œç§°ä¸º RLOOï¼Œè¿™æ˜¯ä¸€ç§æ–°çš„åœ¨çº¿è®­ç»ƒç®—æ³•ã€‚RLOO åªéœ€è¦å°†æ¨¡å‹çš„ 3 ä¸ªå‰¯æœ¬åŠ è½½åˆ°å†…å­˜ä¸­ï¼š1ï¼‰ç­–ç•¥æ¨¡å‹ï¼Œ2ï¼‰å‚è€ƒç­–ç•¥æ¨¡å‹ï¼Œä»¥åŠ 3ï¼‰å¥–åŠ±æ¨¡å‹ï¼Œå¦‚ä¸Šå›¾æ‰€ç¤ºã€‚

é‡è¦çš„æ˜¯ï¼ŒRLOO éœ€è¦æ›´å°‘çš„å†…å­˜ï¼Œè¿™æ„å‘³ç€å®ƒæ›´å®¹æ˜“ï¼š

1. åœ¨ä¸å‡ºç° OOMsï¼ˆå†…å­˜ä¸è¶³é”™è¯¯ï¼‰çš„æƒ…å†µä¸‹è¿è¡Œ
2. èƒ½å¤ŸåŠ è½½æ›´å¤§çš„æ‰¹é‡å¤§å°
3. è¿è¡Œæ›´é«˜æ•ˆä¸”æ›´å¿«ã€‚

æ­¤å¤–ï¼ŒRLOO å°†æ•´ä¸ªè¡¥å…¨ token ä½œä¸ºå•ä¸€åŠ¨ä½œè¿›è¡Œå»ºæ¨¡ï¼Œå¦‚ä¸‹å›¾æ‰€ç¤ºã€‚åœ¨ä¸‹ä¸€èŠ‚ä¸­ï¼Œæˆ‘ä»¬å°†é€šè¿‡ä»£ç ç‰‡æ®µè¿›ä¸€æ­¥è¯¦ç»†ä»‹ç»ã€‚


![alt_text](https://huggingface.co/datasets/trl-internal-testing/example-images/resolve/main/blog/putting_rl_back_in_rlhf_with_rloo/image4.png?download=true "image_tooltip")

# RLOO æ˜¯å¦‚ä½•å·¥ä½œçš„

RLOO å’Œ PPO æœ‰å‡ ä¸ªå…±åŒçš„æ­¥éª¤ï¼š

1. ç­–ç•¥æ¨¡å‹ä¼šç”Ÿæˆä¸€äº›è¡¥å…¨ token ï¼Œå¹¶è·å–å½“å‰ç­–ç•¥å’Œå‚è€ƒç­–ç•¥ä¸‹çš„æ¯ä¸ª token çš„å¯¹æ•°æ¦‚ç‡ã€‚
2. ç„¶åæˆ‘ä»¬è®¡ç®—æ¯ä¸ª token çš„ KL æƒ©ç½šï¼Œä½œä¸ºå½“å‰ç­–ç•¥å’Œå‚è€ƒç­–ç•¥ä¸‹å¯¹æ•°æ¦‚ç‡çš„å·®å¼‚ã€‚
3. æ¥ç€æˆ‘ä»¬ä»å¥–åŠ±æ¨¡å‹ä¸­è·å¾—æ•´ä¸ªè¡¥å…¨çš„å¾—åˆ†ã€‚

ä»è¿™é‡Œå¼€å§‹ï¼Œå¸¸è§„çš„ PPO å’Œ RLOO åœ¨æ–¹æ³•ä¸Šæœ‰æ‰€ä¸åŒã€‚RLOO æœ‰å‡ ä¸ªå…³é”®æƒ³æ³•ã€‚é¦–å…ˆï¼Œå®ƒå°†**æ•´ä¸ªæ¨¡å‹è¡¥å…¨**è§†ä¸ºå•ä¸€åŠ¨ä½œï¼Œè€Œå¸¸è§„ PPO å°†**æ¯ä¸ªè¡¥å…¨ token** è§†ä¸ºå•ç‹¬çš„åŠ¨ä½œã€‚é€šå¸¸ï¼Œåªæœ‰ EOS token è·å¾—çœŸæ­£çš„å¥–åŠ±ï¼Œè¿™éå¸¸ç¨€ç–ã€‚å¸¸è§„ PPO ä¼šå°†å¥–åŠ±å½’å› äº EOS tokenï¼Œè€Œ RLOO ä¼šå°† EOS å¥–åŠ±å½’å› äºæ•´ä¸ªè¡¥å…¨ï¼Œå¦‚ä¸‹æ‰€ç¤ºã€‚


```python
from torch import Tensor
response = Tensor([4., 5., 6.])
per_token_logprobs = Tensor([-12.3, -8.3, -2.3])
reference_per_token_logprobs = Tensor([-11.3, -8.4, -2.0])
kl = per_token_logprobs - reference_per_token_logprobs
score_from_rm = 1.0
print(f"{kl=}")  # kl=tensor([-1.0000,  0.1000, -0.3000])
per_token_reward = kl.clone()
per_token_reward[-1] += score_from_rm  # assume last token is the EOS token
print(f"{per_token_reward=}")  # per_token_reward=tensor([-1.0000,  0.1000,  0.7000])
print(f"{score_from_rm=}")  # score_from_rm=1.0
print("#### Modeling each token as an action")
for action, reward in zip(response, per_token_reward):
    print(f"{action=}, {reward=}")
# action=tensor(4.), reward=tensor(-1.)
# action=tensor(5.), reward=tensor(0.1000)
# action=tensor(6.), reward=tensor(0.7000)
print("#### Modeling the entire response as an action")
entire_generation_reward = per_token_reward.sum()
print(f"action='entire completion', reward={entire_generation_reward}")
# action='entire completion', reward=-0.2000 (-1 + 0.1 + 0.7)
```

å…¶æ¬¡ï¼ŒRLOO ä½¿ç”¨ REINFORCE æŸå¤±ï¼Œå®ƒåŸºæœ¬ä¸Šå°†ï¼ˆå¥–åŠ± - åŸºçº¿ï¼‰ä¸åŠ¨ä½œçš„å¯¹æ•°æ¦‚ç‡ç›¸ä¹˜ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬çªå‡ºäº†æ¯ä¸ª token çš„ REINFORCE æŸå¤±ä¸æ•´ä¸ªè¡¥å…¨çš„ REINFORCE æŸå¤±ä¹‹é—´çš„åŒºåˆ«ã€‚è¯·æ³¨æ„ï¼Œå¯¹äº PPO çš„æŸå¤±ï¼Œæˆ‘ä»¬è¿˜éœ€è¦åŸºäºä»·å€¼æ¨¡å‹å’Œ[å¹¿ä¹‰ä¼˜åŠ¿ä¼°è®¡(GAE)](https://arxiv.org/abs/1506.02438)æ¥è®¡ç®—ä¼˜åŠ¿ã€‚

```python
from torch import Tensor
response = Tensor([4., 5., 6.])
per_token_logprobs = Tensor([-12.3, -8.3, -2.3])
reference_per_token_logprobs = Tensor([-11.3, -8.4, -2.0])
kl = per_token_logprobs - reference_per_token_logprobs
score_from_rm = 1.0
print(f"{kl=}")  # kl=tensor([-1.0000,  0.1000, -0.3000])
per_token_reward = kl.clone()
per_token_reward[-1] += score_from_rm  # assume last token is the EOS token
print(f"{per_token_reward=}")  # per_token_reward=tensor([-1.0000,  0.1000,  0.7000])
print(f"{score_from_rm=}")  # score_from_rm=1.0
print("#### Modeling each token as an action")
for action, reward in zip(response, per_token_reward):
    print(f"{action=}, {reward=}")
# action=tensor(4.), reward=tensor(-1.)
# action=tensor(5.), reward=tensor(0.1000)
# action=tensor(6.), reward=tensor(0.7000)
print("#### Modeling the entire response as an action")
entire_generation_reward = per_token_reward.sum()
print(f"action='entire completion', reward={entire_generation_reward}")
# action='entire completion', reward=-0.2000 (-1 + 0.1 + 0.7)
baseline = Tensor([0.2, 0.3, 0.4])  # dummy baseline
print("#### Modeling each token as an action")
advantage = per_token_reward - baseline
per_token_reinforce_loss = per_token_logprobs * advantage
print(f"{advantage=}")  # advantage=tensor([-1.2000, -0.2000,  0.3000])
print(f"{per_token_reinforce_loss=}")  # per_token_reinforce_loss=tensor([14.7600,  1.6600, -0.6900])
print(f"{per_token_reinforce_loss.mean()=}")  # per_token_reinforce_loss.mean()=tensor(5.2433)

print("#### Modeling the entire response as an action")
advantage = entire_generation_reward - baseline.sum()
reinforce_loss = per_token_logprobs.sum() * advantage
print(f"{advantage=}")  # advantage=tensor(-1.1000)
print(f"{reinforce_loss=}")  # reinforce_loss=tensor(25.1900)
```

ç¬¬ä¸‰ï¼ŒRLOO èªæ˜åœ°è®¡ç®—åŸºçº¿ã€‚æ³¨æ„æˆ‘ä»¬ä¸Šé¢ä½¿ç”¨äº†ä¸€ä¸ªè™šæ‹ŸåŸºçº¿ã€‚åœ¨å®é™…æ“ä½œä¸­ï¼ŒRLOO ä½¿ç”¨æ‰¹æ¬¡ä¸­æ‰€æœ‰å…¶ä»–æ ·æœ¬çš„å¥–åŠ±ä½œä¸ºåŸºçº¿ã€‚ä¸‹é¢æ˜¯ä¸€ä¸ªæœ‰ 3 ä¸ªæç¤ºå’Œæ¯ä¸ªæç¤º 4 ä¸ªè¡¥å…¨çš„ä¾‹å­ã€‚æˆ‘ä»¬é€šè¿‡å¹³å‡åŒä¸€æç¤ºçš„æ‰€æœ‰å…¶ä»–è¡¥å…¨çš„å¥–åŠ±æ¥è®¡ç®—æ¯ä¸ªè¡¥å…¨çš„åŸºçº¿ã€‚


```python
import torch
local_batch_size = 3
rloo_k = 4

rlhf_reward = torch.tensor([
    1, 2, 3, # first rlhf reward for three prompts
    2, 3, 4, # second rlhf reward for three prompts
    5, 6, 7, # third rlhf reward for three prompts
    8, 9, 10, # fourth rlhf reward for three prompts
]).float() # here we have 3 prompts which have 4 completions each

# slow impl
baseline = (rlhf_reward.sum(0) - rlhf_reward) / (rloo_k - 1)
advantages = torch.zeros_like(rlhf_reward)
for i in range(0, len(advantages), local_batch_size):
    other_response_rlhf_rewards = []
    for j in range(0, len(advantages), local_batch_size):
        if i != j:
            other_response_rlhf_rewards.append(rlhf_reward[j : j + local_batch_size])
    advantages[i : i + local_batch_size] = rlhf_reward[i : i + local_batch_size] - torch.stack(
        other_response_rlhf_rewards
    ).mean(0)
assert (1 - (2 + 5 + 8) / 3 - advantages[0].item()) < 1e-6
assert (6 - (3 + 2 + 9) / 3 - advantages[7].item()) < 1e-6

# vectorized impl
rlhf_reward = rlhf_reward.reshape(rloo_k, local_batch_size)
baseline = (rlhf_reward.sum(0) - rlhf_reward) / (rloo_k - 1)
vec_advantages = rlhf_reward - baseline
torch.testing.assert_close(vec_advantages.flatten(), advantages)
```

å‘ Arash Ahmadian è‡´è°¢ï¼Œä»–æä¾›äº†ä¸Šè¿°ä¼˜åŠ¿è®¡ç®—çš„å‘é‡åŒ–å®ç°ã€‚

# å¼€å§‹ä½¿ç”¨ TRL çš„ RLOO

è¦å¼€å§‹ä½¿ç”¨ RLOOï¼Œä½ å¯ä»¥é€šè¿‡ `pip install --upgrade trl` å®‰è£… TRL çš„æœ€æ–°ç‰ˆæœ¬ï¼Œå¹¶å¯¼å…¥ RLOOTrainerã€‚ä¸‹é¢æ˜¯ä¸€ä¸ªå±•ç¤ºä¸€äº›é«˜çº§ API ä½¿ç”¨çš„ç®€çŸ­ä»£ç ç‰‡æ®µã€‚è‡ªç”±æŸ¥çœ‹æ–‡æ¡£.


* [https://huggingface.co/docs/trl/main/en/rloo_trainer](https://huggingface.co/docs/trl/main/en/rloo_trainer) 
* [https://huggingface.co/docs/trl/main/en/ppov2_trainer](https://huggingface.co/docs/trl/main/en/ppov2_trainer) 

```python
from transformers import (
    AutoModelForCausalLM,
    AutoModelForSequenceClassification,
    AutoTokenizer,
)

from trl.trainer.rloo_trainer import RLOOConfig, RLOOTrainer
from trl.trainer.utils import SIMPLE_QUERY_CHAT_TEMPLATE


base_model_name = "EleutherAI/pythia-1b-deduped"
tokenizer = AutoTokenizer.from_pretrained(base_model_name, padding_side="left")
tokenizer.add_special_tokens({"pad_token": "[PAD]"})
if tokenizer.chat_template is None:
    tokenizer.chat_template = SIMPLE_QUERY_CHAT_TEMPLATE
reward_model = AutoModelForSequenceClassification.from_pretrained(base_model_name, num_labels=1)
ref_policy = AutoModelForCausalLM.from_pretrained(base_model_name)
policy = AutoModelForCausalLM.from_pretrained(base_model_name)

train_dataset = ...  # make sure to have columns "input_ids"
eval_dataset = ...

trainer = RLOOTrainer(
    config=RLOOConfig(
        per_device_train_batch_size=1,
        gradient_accumulation_steps=64,
        total_episodes=30000,
    ),
    tokenizer=tokenizer,
    policy=policy,
    ref_policy=ref_policy,
    reward_model=reward_model,
    train_dataset=train_dataset,
    eval_dataset=eval_dataset,
)
trainer.train()
```

è¿™æ˜¯ä¸€ä¸ªè·Ÿè¸ªæƒé‡å’Œåå·®å®éªŒçš„ä¾‹å­ï¼š[https://wandb.ai/huggingface/trl/runs/dd2o3g35](https://wandb.ai/huggingface/trl/runs/dd2o3g35)


![alt_text](https://huggingface.co/datasets/trl-internal-testing/example-images/resolve/main/blog/putting_rl_back_in_rlhf_with_rloo/image9.png?download=true "image_tooltip")


åœ¨ç¼–å†™ RLOO å’Œ PPOv2 å®ç°æ—¶ï¼Œæˆ‘ä»¬å¼ºè°ƒä½¿æ¨¡å‹å¼€å‘çš„é€æ˜åº¦æ›´å®¹æ˜“æå‡ã€‚ç‰¹åˆ«æ˜¯ï¼Œæˆ‘ä»¬å·²ç»å¢å¼ºäº†æ–‡æ¡£ï¼ŒåŒ…æ‹¬å¯¹è®°å½•æŒ‡æ ‡çš„è§£é‡Šä»¥åŠé˜…è¯»å’Œè°ƒè¯•è¿™äº›æŒ‡æ ‡çš„æ“ä½œæŒ‡å—ã€‚ä¾‹å¦‚ï¼Œæˆ‘ä»¬å»ºè®®åœ¨è®­ç»ƒæœŸé—´å¯†åˆ‡ç›‘æ§ objective/rlhf_rewardï¼Œè¿™æ˜¯ RLHF è®­ç»ƒçš„æœ€ç»ˆç›®æ ‡ã€‚


![alt_text](https://huggingface.co/datasets/trl-internal-testing/example-images/resolve/main/blog/putting_rl_back_in_rlhf_with_rloo/image2.png?download=true "image_tooltip")

![alt_text](https://huggingface.co/datasets/trl-internal-testing/example-images/resolve/main/blog/putting_rl_back_in_rlhf_with_rloo/image6.png?download=true "image_tooltip")




ä¸ºäº†å¸®åŠ©å¯è§†åŒ–è®­ç»ƒè¿›åº¦ï¼Œæˆ‘ä»¬å®šæœŸè®°å½•æ¨¡å‹çš„ä¸€äº›ç¤ºä¾‹è¡¥å…¨ã€‚è¿™é‡Œæ˜¯ä¸€ä¸ªè¡¥å…¨çš„ä¾‹å­ã€‚åœ¨ä¸€ä¸ªæƒé‡å’Œåå·®ï¼ˆ[https://wandb.ai/huggingface/trl/runs/dd2o3g35](https://wandb.ai/huggingface/trl/runs/dd2o3g35)ï¼‰è·Ÿè¸ªè¿è¡Œçš„ç¤ºä¾‹ä¸­ï¼Œå®ƒçœ‹èµ·æ¥åƒä¸‹é¢è¿™æ ·ï¼Œå…è®¸ä½ çœ‹åˆ°æ¨¡å‹åœ¨ä¸åŒè®­ç»ƒé˜¶æ®µçš„å“åº”ã€‚é»˜è®¤æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬åœ¨è®­ç»ƒæœŸé—´ç”Ÿæˆ--num_sample_generations 10ï¼Œä½†ä½ å¯ä»¥è‡ªå®šä¹‰ç”Ÿæˆçš„æ•°é‡ã€‚



![alt_text](https://huggingface.co/datasets/trl-internal-testing/example-images/resolve/main/blog/putting_rl_back_in_rlhf_with_rloo/image5.gif?download=true "image_tooltip")


# æˆ‘ä»¬å¦‚ä½•åœ¨ TRL ä¸­å®ç° RLOO è®­ç»ƒå™¨

æˆ‘ä»¬åŸºäºæ–°çš„å®éªŒæ€§ `PPOv2Trainer` å®ç°äº† RLOO è®­ç»ƒå™¨ï¼Œåè€…åˆæ˜¯åŸºäº https://arxiv.org/abs/2403.17031ã€‚æœ‰è¶£çš„æ˜¯ï¼Œæˆ‘ä»¬å®ç°çš„ RLOO è®­ç»ƒå™¨ä»ç„¶ä½¿ç”¨ PPO æŸå¤±ã€‚è¿™æ˜¯å› ä¸º REINFORCE çš„æŸå¤±æ˜¯ PPO çš„ä¸€ä¸ªç‰¹ä¾‹ï¼ˆhttps://arxiv.org/abs/2205.09123ï¼‰ã€‚è¯·æ³¨æ„ï¼Œå³ä½¿å¯¹æ•°æ¦‚ç‡æ˜ç¡®å‡ºç°åœ¨ REINFORCE æŸå¤±ä¸­ï¼Œå®ƒä¹Ÿéšå«åœ¨ PPO æŸå¤±ä¸­ã€‚çœ¼è§ä¸ºå®ï¼Œæ‰€ä»¥è®©æˆ‘ä»¬ç”¨ä¸€ä¸ªç®€å•çš„ä¾‹å­æ¥è¯æ˜è¿™ä¸€ç‚¹ã€‚


```python
import torch.nn.functional as F
from torch import LongTensor, Tensor, gather, no_grad

action = LongTensor([1])
advantage = Tensor([1.0])
logits = Tensor([[1.0, 2.0, 1.0, 1.0]])
logits.requires_grad = True
all_logprob = F.log_softmax(logits, dim=-1)
with no_grad():
    old_logprob = gather(all_logprob, 1, action.unsqueeze(-1)).squeeze(-1)
logprob = gather(all_logprob, 1, action.unsqueeze(-1)).squeeze(-1)
ratio = (logprob - old_logprob).exp()
ppo_loss = (ratio * advantage).mean() # [Ï€Î¸(at | st) / Ï€Î¸_old(at | st) * At]
# when the Ï€Î¸ and Ï€Î¸_old are the same, the ratio is 1, and PPO's clipping has no effect
ppo_loss.backward()
print(f"{logits.grad=}")  # tensor([[-0.1749,  0.5246, -0.1749, -0.1749]])
logits2 = Tensor([[1.0, 2.0, 1.0, 1.0]])
logits2.requires_grad = True
all_logprob2 = F.log_softmax(logits2, dim=-1)
logprob2 = gather(all_logprob2, 1, action.unsqueeze(-1)).squeeze(-1)
reinforce_loss = logprob2 * advantage  # [log Ï€Î¸(at | st) * At]
reinforce_loss.mean().backward()
print(f"{logits2.grad=}")  # tensor([[-0.1749,  0.5246, -0.1749, -0.1749]])
```


# å®éªŒ

ä¸ºäº†éªŒè¯ RLOO å®ç°çš„æœ‰æ•ˆæ€§ï¼Œæˆ‘ä»¬åœ¨ Pythia 1B å’Œ 6.9B æ¨¡å‹ä¸Šè¿›è¡Œäº†å®éªŒï¼Œå¹¶åœ¨è¿™é‡Œå‘å¸ƒäº†è®­ç»ƒåçš„æ£€æŸ¥ç‚¹ï¼š

* [https://huggingface.co/collections/vwxyzjn/rloo-ppov2-tl-dr-summarize-checkpoints-66679a3bfd95ddf66c97420d](https://huggingface.co/collections/vwxyzjn/rloo-ppov2-tl-dr-summarize-checkpoints-66679a3bfd95ddf66c97420d)  

æˆ‘ä»¬ä»[Huang ç­‰äººï¼Œ2024](https://arxiv.org/abs/2403.17031)ç›´æ¥è·å– SFT / RM æ¨¡å‹ã€‚ä¸ºäº†è¯„ä¼°ï¼Œæˆ‘ä»¬ä½¿ç”¨ vLLM åŠ è½½æ£€æŸ¥ç‚¹ï¼Œå¹¶ä½¿ç”¨ GPT4 ä½œä¸ºè¯„åˆ¤æ¨¡å‹æ¥è¯„ä¼°ç”Ÿæˆçš„ TL;DR ä¸å‚è€ƒ TL;DR çš„å¯¹æ¯”ã€‚æˆ‘ä»¬è¿˜æŸ¥çœ‹äº† GPU å†…å­˜ä½¿ç”¨æƒ…å†µå’Œè¿è¡Œæ—¶é—´ï¼Œæ­£å¦‚åšå®¢å¼€å¤´æ‰€ç¤ºçš„å›¾è¡¨ã€‚è¦é‡ç°æˆ‘ä»¬çš„å·¥ä½œï¼Œè¯·éšæ—¶æŸ¥çœ‹æˆ‘ä»¬æ–‡æ¡£ä¸­çš„å‘½ä»¤ï¼š



* [https://huggingface.co/docs/trl/main/en/rloo_trainer#benchmark-experiments](https://huggingface.co/docs/trl/main/en/rloo_trainer#benchmark-experiments) 
* [https://huggingface.co/docs/trl/main/en/rloo_trainer#benchmark-experiments](https://huggingface.co/docs/trl/main/en/rloo_trainer#benchmark-experiments)  


å…³é”®ç»“æœå¦‚ä¸‹ï¼š



* **ğŸš€é«˜æ€§èƒ½ RLOO æ£€æŸ¥ç‚¹ï¼š**ä½¿ç”¨ GPT4 ä½œä¸ºè¯„åˆ¤æ¨¡å‹ï¼Œ6.9B æ£€æŸ¥ç‚¹è·å¾—äº† 78.7% (k=2)çš„åå¥½ç‡ï¼Œè¿™ç”šè‡³è¶…è¿‡äº†åŸå§‹[paper](https://arxiv.org/abs/2402.14740)ä¸­æŠ¥å‘Šçš„æœ€ä½³æ€§èƒ½ 77.9% (k=4)å’Œ 74.2 (k=2)ã€‚è¿™æ˜¯ä¸€ä¸ªå¾ˆå¥½çš„è¿¹è±¡ï¼Œè¡¨æ˜æˆ‘ä»¬çš„ RLOO è®­ç»ƒæŒ‰é¢„æœŸå·¥ä½œã€‚
    * RLOO 1B æ£€æŸ¥ç‚¹çš„èƒœç‡ä¸º 40.1%ï¼Œè€Œ SFT æ£€æŸ¥ç‚¹çš„èƒœç‡ä¸º 21.3%ã€‚è¿™æ˜¯ä¸€ä¸ªå¾ˆå¥½çš„è¿¹è±¡ï¼Œè¡¨æ˜ RLOO è®­ç»ƒæŒ‰é¢„æœŸå·¥ä½œã€‚
* ğŸ¤‘**å‡å°‘ GPU å†…å­˜å¹¶è¿è¡Œæ›´å¿«**ï¼šRLOO è®­ç»ƒä½¿ç”¨æ›´å°‘çš„å†…å­˜å¹¶è¿è¡Œæ›´å¿«ï¼Œä½¿å…¶æˆä¸ºåœ¨çº¿ RL è®­ç»ƒä¸­éå¸¸æœ‰ç”¨çš„ç®—æ³•ã€‚


# æ•°å€¼ç¨³å®šæ€§ï¼šé»‘æš—é¢

å°½ç®¡ RLOO åœ¨æ€§èƒ½å’Œè®¡ç®—æ•ˆç‡æ–¹é¢æœ‰ä¼˜åŠ¿ï¼Œä½†æˆ‘ä»¬æƒ³è¦å¼ºè°ƒä¸€äº›æ•°å€¼é—®é¢˜ã€‚å…·ä½“æ¥è¯´ï¼Œç”Ÿæˆè¿‡ç¨‹ä¸­è·å¾—çš„å“åº”å¯¹æ•°æ¦‚ç‡ä¸ `bf16` ä¸‹è®­ç»ƒå‰å‘ä¼ é€’æœŸé—´è·å¾—çš„å¯¹æ•°æ¦‚ç‡åœ¨æ•°å€¼ä¸Šç•¥æœ‰ä¸åŒã€‚è¿™ç»™ PPO å’Œ RLOO éƒ½å¸¦æ¥äº†é—®é¢˜ï¼Œä½†å¯¹äº RLOO æ¥è¯´ï¼Œé—®é¢˜æ›´ä¸¥é‡ï¼Œå¦‚ä¸‹æ‰€è¿°ã€‚

ä¾‹å¦‚ï¼Œå‡è®¾æˆ‘ä»¬æ­£åœ¨ä¸ºä¸¤ä¸ªåºåˆ—ç”Ÿæˆ 10 ä¸ª tokenã€‚åœ¨ `fp32` ç²¾åº¦ä¸‹ï¼Œè¾“å‡ºå¦‚ä¸‹æ‰€ç¤ºï¼Œå…¶ä¸­ `ratio = (forward_logprob - generation_logprob).exp()`ï¼Œè¿™æ˜¯ PPO ç”¨æ¥è£å‰ªçš„ã€‚åœ¨ç¬¬ä¸€ä¸ªå‘¨æœŸå’Œç¬¬ä¸€ä¸ªå°æ‰¹é‡ä¸­ï¼Œæ¯”ç‡åº”è¯¥æ˜¯å®Œå…¨ç›¸åŒçš„ï¼Œå› ä¸ºæ¨¡å‹è¿˜æ²¡æœ‰è¿›è¡Œä»»ä½•æ›´æ–°ï¼š

```
generation_logprob=tensor([[    -0.1527,     -0.2258,     -3.5535,     -3.4805,     -0.0519,
             -2.3097,     -2.0275,     -0.4597,     -0.1687,     -0.0000],
        [    -0.1527,     -0.2258,     -5.2855,     -0.1686,     -8.4760,
             -4.3118,     -1.0368,     -0.8274,     -1.6342,     -2.6128]],
       device='cuda:0')
forward_logprob=tensor([[-0.1527, -0.2258, -3.5535, -3.4805, -0.0519, -2.3097, -2.0275, -0.4597,
         -0.1687],
        [-0.1527, -0.2258, -5.2855, -0.1686, -8.4760, -4.3118, -1.0368, -0.8274,
         -1.6342]], device='cuda:0', grad_fn=<SqueezeBackward1>)
ratio=tensor([[1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],
        [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000]],
       device='cuda:0', grad_fn=<ExpBackward0>)
ratio.mean()=0.9999998211860657
ratio.std()=6.592738373001339e-06
ratio.max()=1.0000133514404297
ratio.min()=0.9999887943267822
```

ç„¶è€Œï¼Œåœ¨ bf16 ç²¾åº¦ä¸‹ï¼Œæˆ‘ä»¬å¾—åˆ°ç»“æœï¼š


```
generation_logprob=tensor([[    -0.1426,     -0.1904,     -3.5938,     -3.4688,     -0.0618,
             -2.3906,     -2.0781,     -0.4375,     -0.1562,     -0.0000],
        [    -0.1426,     -0.1904,     -5.2812,     -0.1641,     -8.5625,
             -4.2812,     -1.0078,     -0.8398,     -1.5781,     -2.5781]],
       device='cuda:0', dtype=torch.bfloat16)
forward_logprob=tensor([[-0.1445, -0.1670, -3.5938, -3.5156, -0.0554, -2.2969, -1.9688, -0.5273,
         -0.1953],
        [-0.1445, -0.1670, -5.2812, -0.1533, -8.5625, -4.3125, -1.0000, -0.7852,
         -1.6641]], device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<SqueezeBackward1>)
ratio=tensor([[1.0000, 0.9766, 1.0000, 1.0469, 0.9922, 0.9102, 0.8945, 1.0938, 1.0391],
        [1.0000, 0.9766, 1.0000, 0.9883, 1.0000, 1.0312, 0.9922, 0.9453, 1.0859]],
       device='cuda:0', dtype=torch.bfloat16, grad_fn=<ExpBackward0>)
ratio.mean()=1.0
ratio.std()=0.051025390625
ratio.max()=1.09375
ratio.min()=0.89453125
```

å’Œåœ¨ fp16 ç²¾åº¦ä¸‹ï¼Œæˆ‘ä»¬å¾—åˆ°çš„ç»“æœ

```
generation_logprob=tensor([[    -0.1486,     -0.2212,     -3.5586,     -3.4688,     -0.0526,
             -2.3105,     -2.0254,     -0.4629,     -0.1677,     -0.0000],
        [    -0.1486,     -0.2212,     -5.2852,     -0.1681,     -8.4844,
             -4.3008,     -1.0322,     -0.8286,     -1.6348,     -2.6074]],
       device='cuda:0', dtype=torch.float16)
forward_logprob=tensor([[-0.1486, -0.2212, -3.5586, -3.4805, -0.0529, -2.3066, -2.0332, -0.4629,
         -0.1676],
        [-0.1486, -0.2212, -5.2852, -0.1682, -8.4766, -4.3008, -1.0322, -0.8281,
         -1.6299]], device='cuda:0', dtype=torch.float16,
       grad_fn=<SqueezeBackward1>)
ratio=tensor([[1.0000, 1.0000, 1.0000, 1.0117, 1.0000, 0.9961, 1.0078, 1.0000, 1.0000],
        [1.0000, 1.0000, 1.0000, 1.0000, 0.9922, 1.0000, 1.0000, 0.9995, 0.9951]],
       device='cuda:0', dtype=torch.float16, grad_fn=<ExpBackward0>)
ratio.mean()=1.0
ratio.std()=0.00418853759765625
ratio.max()=1.01171875
ratio.min()=0.9921875
```

è¯·æ³¨æ„ï¼Œ`bf16`çš„æ¯”ç‡ç”±äºæŸç§åŸå› éå¸¸ä¸ç¨³å®šã€‚å½“æ¯”ç‡å˜å¾—å¾ˆå¤§æ—¶ï¼ŒPPO çš„è£å‰ªç³»æ•° = 0.2 å¼€å§‹å‘æŒ¥ä½œç”¨ï¼Œ**å–æ¶ˆ**é‚£äº›æ¯”ç‡å¤§äº 1.2 æˆ–å°äº 0.8 çš„ token çš„æ¢¯åº¦ã€‚å¯¹äº RLOOï¼Œè¿™ä¸ªé—®é¢˜æ›´ä¸ºæç«¯ï¼Œå› ä¸ºæˆ‘ä»¬çœ‹åˆ°çš„æ˜¯`(forward_logprob.sum(1) - generation_logprob.sum(1)).exp() = [ 1.0625, 12.1875]`ï¼Œè¿™æ„å‘³ç€æ•´ä¸ªç¬¬äºŒä¸ªåºåˆ—çš„æ¢¯åº¦è¢«å–æ¶ˆäº†ã€‚

åœ¨å®é™…æ“ä½œä¸­ï¼Œæˆ‘ä»¬æ³¨æ„åˆ° PPO å–æ¶ˆäº†å¤§çº¦ 3% çš„æ‰¹æ¬¡æ•°æ®çš„æ¢¯åº¦ï¼Œè€Œ RLOO å–æ¶ˆäº†å¤§çº¦ 20-40% çš„æ‰¹æ¬¡æ•°æ®ã€‚ä»ç†è®ºä¸Šè®²ï¼Œå½“ä¸ä½¿ç”¨å°æ‰¹é‡æ—¶ï¼ŒRLOO åº”è¯¥å–æ¶ˆ 0 %çš„æ‰¹æ¬¡æ•°æ®ã€‚é‡è¦çš„æ˜¯ï¼Œæˆ‘ä»¬è§‚å¯Ÿåˆ°ï¼Œä¸€æ—¦æˆ‘ä»¬å¢åŠ äº†åœ¨ç”Ÿæˆæ–°æ‰¹æ¬¡ä¹‹å‰çš„æ¢¯åº¦æ­¥éª¤æ•°ï¼ˆé€šè¿‡ num_ppo_epochs å’Œ num_mini_batchesï¼‰ï¼ŒRLOO çš„è£å‰ªæ¯”ç‡å¹¶æ²¡æœ‰æ˜¾è‘—å˜åŒ–ï¼›è¿™æä¾›äº†å®è¯è¯æ®ï¼Œè¡¨æ˜è£å‰ªæ¯”ç‡ç¡®å®æ˜¯ç”±äº bf16 çš„æ•°å€¼é—®é¢˜ï¼Œè€Œä¸æ˜¯å› ä¸ºè¡Œä¸ºå’Œæœ€æ–°ç­–ç•¥æœ‰å¾ˆå¤§ä¸åŒï¼Œæ­£å¦‚è®ºæ–‡ä¸­æ‰€å®šä½çš„ã€‚

è¦äº†è§£æœ‰å…³æœ€æ–°é—®é¢˜æ›´æ–°çš„æ›´å¤šä¿¡æ¯ï¼Œè¯·æŸ¥çœ‹[https://github.com/huggingface/transformers/issues/31267](https://github.com/huggingface/transformers/issues/31267)ã€‚

# ç»“è®º

TRL ä¸­å¼•å…¥çš„ RLOOï¼ˆREINFORCE Leave One-Outï¼‰è®­ç»ƒå™¨æ˜¯åœ¨çº¿ RLHF è®­ç»ƒä¸­ä¸€ä¸ªä»¤äººå…´å¥‹çš„ç®—æ³•ï¼Œå®ƒæä¾›äº†ä¸€ä¸ªæ¯” PPO æ›´æ˜“è®¿é—®å’Œé«˜æ•ˆçš„æ›¿ä»£æ–¹æ¡ˆã€‚é€šè¿‡å‡å°‘ GPU å†…å­˜ä½¿ç”¨å’Œç®€åŒ–è®­ç»ƒè¿‡ç¨‹ï¼ŒRLOO ä½¿å¾—å¯ä»¥ä½¿ç”¨æ›´å¤§çš„æ‰¹é‡å¤§å°å’Œæ›´å¿«çš„è®­ç»ƒæ—¶é—´ã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼ŒRLOO åœ¨å“åº”èƒœç‡æ–¹é¢ä¸ PPO ç«äº‰ï¼Œå¹¶ä¸”ä¼˜äº DPO æ£€æŸ¥ç‚¹ï¼Œä½¿å…¶æˆä¸ºæœ‰æ•ˆçš„åœ¨çº¿ RLHF çš„æœ‰åŠ›å·¥å…·ã€‚æŸ¥çœ‹æˆ‘ä»¬çš„æ–‡æ¡£æ¥å¼€å§‹ä½¿ç”¨å§ï¼


* [https://huggingface.co/docs/trl/main/en/rloo_trainer](https://huggingface.co/docs/trl/main/en/rloo_trainer) 
* [https://huggingface.co/docs/trl/main/en/ppov2_trainer](https://huggingface.co/docs/trl/main/en/ppov2_trainer) 


# è‡´è°¢å’Œæ„Ÿè°¢

æˆ‘ä»¬è¦æ„Ÿè°¢ Lewis Tunstall, Sara Hooker, Omar Sanseviero å’Œ Leandro Von Werra å¯¹è¿™ç¯‡åšå®¢æä¾›çš„å®è´µåé¦ˆã€‚

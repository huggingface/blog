---
title: "Kakao Brain çš„å¼€æº ViTã€ALIGN å’Œ COYO æ–‡å­—" 
thumbnail: /blog//assets/132_vit_align/thumbnail.png
authors:
- user: adirik
- user: Unso
- user: dylan-m
- user: jun-untitled
translators:
- user: conyzhang
---


# Kakao Brain çš„å¼€æº ViTã€ALIGN å’Œ COYO æ–‡å­—


æœ€è¿‘ Kakao Brain åœ¨ Hugging Face å‘å¸ƒäº†ä¸€ä¸ªå…¨æ–°çš„å¼€æºå›¾åƒæ–‡æœ¬æ•°æ®é›† [COYO](https://github.com/kakaobrain/coyo-dataset)ï¼ŒåŒ…å« 7 äº¿å¯¹å›¾åƒå’Œæ–‡æœ¬ï¼Œå¹¶è®­ç»ƒäº†ä¸¤ä¸ªæ–°çš„è§†è§‰è¯­è¨€æ¨¡å‹ [ViT](https://github.com/kakaobrain/coyo-vit) å’Œ [ALIGN](https://github.com/kakaobrain/coyo-align)ã€‚

è¿™æ˜¯ ALIGN æ¨¡å‹é¦–æ¬¡å…¬å¼€å‘å¸ƒä¾›å¼€æºä½¿ç”¨ï¼ŒåŒæ—¶ ViT å’Œ ALIGN æ¨¡å‹çš„å‘å¸ƒéƒ½é™„å¸¦æœ‰è®­ç»ƒæ•°æ®é›†ã€‚

Google çš„ [ViT](https://ai.googleblog.com/2020/12/transformers-for-image-recognition-at.html) å’Œ [ALIGN](https://ai.googleblog.com/2021/05/align-scaling-up-visual-and-vision.html) æ¨¡å‹éƒ½ä½¿ç”¨äº†å·¨å¤§çš„æ•°æ®é›† (ViT è®­ç»ƒäº 3 äº¿å¼ å›¾åƒï¼ŒALIGN è®­ç»ƒäº 18 äº¿ä¸ªå›¾åƒ - æ–‡æœ¬å¯¹) è¿›è¡Œè®­ç»ƒï¼Œå› ä¸ºæ•°æ®é›†ä¸å…¬å¼€å¯¼è‡´æ— æ³•å¤ç°ã€‚[Kakao Brain](https://hf.co/kakaobrain) çš„ ViT å’Œ ALIGN æ¨¡å‹é‡‡ç”¨ä¸ Google åŸå§‹æ¨¡å‹ç›¸åŒçš„æ¶æ„å’Œè¶…å‚æ•°ï¼Œä¸åŒçš„æ˜¯å…¶åœ¨å¼€æº  [COYO æ•°æ®é›†](https://github.com/kakaobrain/coyo-dataset) ä¸Šè¿›è¡Œè®­ç»ƒã€‚å¯¹äºæƒ³è¦æ‹¥æœ‰æ•°æ®å¹¶å¤ç°è§†è§‰è¯­è¨€æ¨¡å‹çš„ç ”ç©¶äººå‘˜æœ‰å¾ˆå¤§çš„ä»·å€¼ã€‚

è¿™ç¯‡åšå®¢å°†ä»‹ç»æ–°çš„ [COYO](https://github.com/kakaobrain/coyo-dataset) æ•°æ®é›†ã€Kakao Brain çš„ ViT å’Œ ALIGN æ¨¡å‹ï¼Œä»¥åŠå¦‚ä½•ä½¿ç”¨å®ƒä»¬ï¼ä»¥ä¸‹æ˜¯ä¸»è¦è¦ç‚¹:

* ç¬¬ä¸€ä¸ªå¼€æºçš„ ALIGN æ¨¡å‹ï¼
* ç¬¬ä¸€ä¸ªåœ¨å¼€æºæ•°æ®é›† [COYO](https://github.com/kakaobrain/coyo-dataset) ä¸Šè®­ç»ƒçš„å¼€æº ViT å’Œ ALIGN æ¨¡å‹ã€‚
* Kakao Brain çš„ ViT å’Œ ALIGN æ¨¡å‹è¡¨ç°ä¸ Google ç‰ˆæœ¬ç›¸å½“ã€‚
* ViT æ¨¡å‹åœ¨ HF ä¸Šå¯æ¼”ç¤ºï¼æ‚¨å¯ä»¥ä½¿ç”¨è‡ªå·±çš„å›¾åƒæ ·æœ¬åœ¨çº¿ä½“éªŒ ViTï¼

## æ€§èƒ½æ¯”è¾ƒ

Kakao Brain å‘å¸ƒçš„ ViT å’Œ ALIGN æ¨¡å‹ä¸ Google çš„æ¨¡å‹è¡¨ç°ç›¸å½“ï¼ŒæŸäº›æ–¹é¢ç”šè‡³æ›´å¥½ã€‚Kakao Brain çš„ `ALIGN-B7-Base` æ¨¡å‹è™½ç„¶è®­ç»ƒçš„æ•°æ®å¯¹å°‘å¾—å¤š ( 7 äº¿ VS 1.8 äº¿)ï¼Œä½†åœ¨å›¾åƒ KNN åˆ†ç±»ä»»åŠ¡ä¸Šè¡¨ç°ä¸ Google çš„ `ALIGN-B7-Base` ç›¸å½“ï¼Œåœ¨ MS-COCO å›¾åƒ - æ–‡æœ¬æ£€ç´¢ã€æ–‡æœ¬ - å›¾åƒæ£€ç´¢ä»»åŠ¡ä¸Šè¡¨ç°æ›´å¥½ã€‚Kakao Brain çš„ `ViT-L/16` åœ¨ 384Ã—512 çš„ ImageNet å’Œ ImageNet-ReaL æ•°æ®ä¸Šçš„è¡¨ç°ä¸ Google çš„ `ViT-L/16` ç›¸å½“ã€‚è¿™æ„å‘³ç€åŒè¡Œå¯ä»¥ä½¿ç”¨ Kakao Brain çš„ ViT å’Œ ALIGN æ¨¡å‹æ¥å¤ç° Google çš„ ViT å’Œ ALIGN ï¼Œå°¤å…¶æ˜¯å½“ç”¨æˆ·éœ€è¦è®­ç»ƒæ•°æ®æ—¶ã€‚æ‰€ä»¥æˆ‘ä»¬å¾ˆé«˜å…´å¼€æºè¿™äº›ä¸ç°æœ‰æŠ€æœ¯ç›¸å½“çš„æ¨¡å‹ï¼

<p>
<center>
<img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/132_vit_align/vit-align-performance.png" alt="ViT and ALIGN performance"/>
</center>
</p>

## COYO æ•°æ®é›†

<p>
<center>
<img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/132_vit_align/coyo-samples.png" alt="COYO samples"/>
</center>
</p>

æœ¬æ¬¡å‘å¸ƒçš„æ¨¡å‹ç‰¹åˆ«ä¹‹å¤„åœ¨äºéƒ½æ˜¯åŸºäºå¼€æºçš„ COYO æ•°æ®é›†è®­ç»ƒçš„ã€‚[COYO](https://github.com/kakaobrain/coyo-dataset#dataset-preview) æ•°æ®é›†åŒ…å« 7 äº¿å›¾åƒ - æ–‡æœ¬å¯¹ï¼Œç±»ä¼¼äº Google çš„ ALIGN 1.8B å›¾åƒ - æ–‡æœ¬æ•°æ®é›†ï¼Œæ˜¯ä»ç½‘é¡µä¸Šæ”¶é›†çš„â€œå˜ˆæ‚â€çš„ html æ–‡æœ¬ (alt-text) å’Œå›¾åƒå¯¹ã€‚COYO-700M å’Œ ALIGN 1.8Béƒ½æ˜¯â€œå˜ˆæ‚â€çš„ï¼Œåªä½¿ç”¨äº†é€‚å½“çš„æ¸…æ´—å¤„ç†ã€‚COYO ç±»ä¼¼äºå¦ä¸€ä¸ªå¼€æºçš„å›¾åƒâ€“æ–‡æœ¬æ•°æ®é›† `LAION`ï¼Œä½†æœ‰ä¸€äº›åŒºåˆ«ã€‚å°½ç®¡ `LAION` 2B æ˜¯ä¸€ä¸ªæ›´å¤§çš„æ•°æ®é›†ï¼ŒåŒ…å« 20 äº¿ä¸ªè‹±è¯­é…å¯¹ï¼Œä½† `COYO` çš„é™„å¸¦æœ‰æ›´å¤šå…ƒæ•°æ®ï¼Œä¸ºç”¨æˆ·æä¾›æ›´å¤šçµæ´»æ€§å’Œæ›´ç»†ç²’åº¦çš„ä½¿ç”¨ã€‚ä»¥ä¸‹è¡¨æ ¼æ˜¾ç¤ºäº†å®ƒä»¬ä¹‹é—´çš„åŒºåˆ«: `COYO` æ‰€æœ‰æ•°æ®å¯¹éƒ½æä¾›äº†ç¾æ„Ÿè¯„åˆ†ï¼Œæ›´å¥å£®çš„æ°´å°è¯„åˆ†å’Œé¢éƒ¨è®¡æ•°ä¿¡æ¯ (face count data)ã€‚


| COYO | LAION 2B| ALIGN 1.8B |
| :----: | :----: | :----: |
| Image-text similarity score calculated with CLIP ViT-B/32 and ViT-L/14 models, they are provided as metadata but nothing is filtered out so as to avoid possible elimination bias | Image-text similarity score provided with CLIP (ViT-B/32) - only examples above threshold 0.28 | Minimal, Frequency based filtering | 
| NSFW filtering on images and text | NSFW filtering on images | [Google Cloud API](https://cloud.google.com/vision) |
| Face recognition (face count) data provided as meta-data | No face recognition data | NA | 
| 700 million pairs all English | 2 billion English| 1.8 billion | 
| From CC 2020 Oct - 2021 Aug| From CC 2014-2020|  NA |
|Aesthetic Score | Aesthetic Score Partial | NA| 
|More robust Watermark score | Watermark Score |  NA| 
|Hugging Face Hub | Hugging Face Hub | Not made public |  
| English | English | English? | 
                                                                                                  

## ViT å’Œ ALIGN æ˜¯å¦‚ä½•å·¥ä½œçš„

è¿™äº›æ¨¡å‹æ˜¯å¹²ä»€ä¹ˆçš„ï¼Ÿè®©æˆ‘ä»¬ç®€è¦è®¨è®ºä¸€ä¸‹ ViT å’Œ ALIGN æ¨¡å‹çš„å·¥ä½œåŸç†ã€‚

ViTâ€”â€”Vision Transformer æ˜¯ [è°·æ­Œäº 2020 å¹´æå‡ºçš„ä¸€ç§è§†è§‰æ¨¡å‹](https://ai.googleblog.com/2020/12/transformers-for-image-recognition-at.html)ï¼Œç±»ä¼¼äºæ–‡æœ¬ Transformer æ¶æ„ã€‚è¿™æ˜¯ä¸€ç§ä¸å·ç§¯ç¥ç»ç½‘ç»œä¸åŒçš„è§†è§‰æ–¹æ³• (AlexNet è‡ª 2012 å¹´ä»¥æ¥ä¸€ç›´ä¸»å¯¼è§†è§‰ä»»åŠ¡)ã€‚åŒæ ·è¡¨ç°ä¸‹ï¼Œå®ƒçš„è®¡ç®—æ•ˆç‡æ¯” CNN é«˜è¾¾å››å€ï¼Œä¸”å…·æœ‰åŸŸä¸å¯çŸ¥æ€§ (domain agnostic)ã€‚ViT å°†è¾“å…¥çš„å›¾åƒåˆ†è§£æˆä¸€ç³»åˆ—å›¾åƒå— (patch)ï¼Œå°±åƒæ–‡æœ¬ Transformer è¾“å…¥æ–‡æœ¬åºåˆ—ä¸€æ ·ï¼Œç„¶åä¸ºæ¯ä¸ªå—æä¾›ä½ç½®åµŒå…¥ä»¥å­¦ä¹ å›¾åƒç»“æ„ã€‚ViT çš„æ€§èƒ½å°¤å…¶åœ¨äºå…·æœ‰å‡ºè‰²çš„æ€§èƒ½ - è®¡ç®—æƒè¡¡ã€‚è°·æ­Œçš„ä¸€äº› ViT æ¨¡å‹æ˜¯å¼€æºçš„ï¼Œä½†å…¶è®­ç»ƒä½¿ç”¨çš„ JFT-300 ç™¾ä¸‡å›¾åƒ - æ ‡ç­¾å¯¹æ•°æ®é›†å°šæœªå…¬å¼€å‘å¸ƒã€‚Kakao Brain çš„è®­ç»ƒæ¨¡å‹æ˜¯åŸºäºå…¬å¼€å‘å¸ƒçš„ [COYO-Labeled-300M](https://github.com/kakaobrain/coyo-dataset/tree/main/subset/COYO-Labeled-300M) è¿›è¡Œè®­ç»ƒï¼Œå¯¹åº”çš„ ViT æ¨¡å‹åœ¨å„ç§ä»»åŠ¡ä¸Šå…·æœ‰ç›¸ä¼¼è¡¨ç°ï¼Œå…¶ä»£ç ã€æ¨¡å‹å’Œè®­ç»ƒæ•°æ® (COYO-Labeled-300M) å®Œå…¨å…¬å¼€ï¼Œä»¥ä¾¿èƒ½å¤Ÿè¿›è¡Œå¤ç°å’Œç§‘å­¦ç ”ç©¶ã€‚

<p>
<center>
<img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/132_vit_align/vit-architecture.gif" alt="ViT architecture" width="700"/>
</center>
</p>
<p>
<center>
<em>A Visualization of How ViT Works from <a href="https://ai.googleblog.com/2020/12/transformers-for-image-recognition-at.html">Google Blog</a></em>
</center>
</p>

[è°·æ­Œåœ¨ 2021 å¹´æ¨å‡ºäº† ALIGN](https://ai.googleblog.com/2021/05/align-scaling-up-visual-and-vision.html)ï¼Œå®ƒæ˜¯ä¸€ç§åŸºäºâ€œå˜ˆæ‚â€æ–‡æœ¬â€“å›¾åƒæ•°æ®è®­ç»ƒçš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼Œå¯ç”¨äºå„ç§è§†è§‰å’Œè·¨æ¨¡æ€ä»»åŠ¡ï¼Œå¦‚æ–‡æœ¬ - å›¾åƒæ£€ç´¢ã€‚ALIGN é‡‡ç”¨ç®€å•çš„åŒç¼–ç å™¨æ¶æ„ï¼Œé€šè¿‡å¯¹æ¯”æŸå¤±å‡½æ•°å­¦ä¹ å›¾åƒå’Œæ–‡æœ¬å¯¹ï¼ŒALIGN çš„â€œå˜ˆæ‚â€è®­ç»ƒè¯­æ–™ç‰¹ç‚¹åŒ…æ‹¬ç”¨è¯­æ–™è§„æ¨¡å¼¥è¡¥å…¶å™ªéŸ³ä»¥åŠå¼ºå¤§çš„é²æ£’æ€§ã€‚ä¹‹å‰çš„è§†è§‰è¯­è¨€è¡¨ç¤ºå­¦ä¹ éƒ½æ˜¯åœ¨æ‰‹åŠ¨æ ‡æ³¨çš„å¤§è§„æ¨¡æ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒï¼Œè¿™å°±éœ€è¦å¤§é‡çš„é¢„å…ˆå¤„ç†å’Œæˆæœ¬ã€‚ALIGN çš„è¯­æ–™åº“ä½¿ç”¨ HTML æ–‡æœ¬ (alt-text) æ•°æ®ä½œä¸ºå›¾åƒçš„æè¿°ï¼Œå¯¼è‡´æ•°æ®é›†ä¸å¯é¿å…åœ°å˜ˆæ‚ï¼Œä½†æ›´å¤§çš„æ•°æ®é‡ (18 äº¿å¯¹) ä½¿ ALIGN èƒ½å¤Ÿåœ¨å„ç§ä»»åŠ¡ä¸Šè¡¨ç°å‡º SoTA æ°´å¹³ã€‚Kakao Brain çš„æ¨¡å‹æ˜¯ç¬¬ä¸€ä¸ª ALIGN å¼€æºç‰ˆæœ¬ï¼Œå®ƒåœ¨ `COYO` æ•°æ®é›†ä¸Šè®­ç»ƒï¼Œè¡¨ç°æ¯”è°·æ­Œçš„ç»“æœæ›´å¥½ã€‚

<p>
<center>
<img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/132_vit_align/align-architecture.png" width="700" />
</center>
</p>
<p>
<center>
<em>ALIGN Model from <a href="https://ai.googleblog.com/2021/05/align-scaling-up-visual-and-vision.html">Google Blog</a>
</em>
</center>
<p>


## å¦‚ä½•ä½¿ç”¨ COYO æ•°æ®é›†

æˆ‘ä»¬å¯ä»¥ä½¿ç”¨ Hugging Face ğŸ¤— æ•°æ®é›†åº“çš„ä¸€è¡Œä»£ç æ–¹ä¾¿åœ°ä¸‹è½½ COYO æ•°æ®é›†ã€‚è¦é¢„è§ˆ COYO æ•°æ®é›†å¹¶äº†è§£æ•°æ®å¤„ç†è¿‡ç¨‹å’ŒåŒ…å«çš„å…ƒå±æ€§ï¼Œè¯·å‰å¾€ [hub](https://huggingface.co/datasets/kakaobrain/coyo-700m) æ•°æ®é›†é¡µé¢ã€‚

å¼€å§‹å‰ï¼Œè¯·å®‰è£… Hugging Face ğŸ¤— æ•°æ®é›†åº“: pip install datasetsï¼Œç„¶åä¸‹è½½æ•°æ®é›†ã€‚

```shell
>>> from datasets import load_dataset

>>> dataset = load_dataset('kakaobrain/coyo-700m')
>>> dataset
```

ç”±äº `COYO` æ•°æ®é›†éå¸¸åºå¤§ï¼ŒåŒ…å« 747M ä¸ªå›¾åƒ - æ–‡æœ¬å¯¹ï¼Œæ‚¨å¯èƒ½æ— æ³•åœ¨æœ¬åœ°ä¸‹è½½æ•´ä¸ªæ•°æ®é›†ã€‚æˆ–è€…å¯èƒ½åªéœ€è¦ä¸‹è½½å’Œä½¿ç”¨æ•°æ®é›†çš„å­é›†ã€‚ä¸ºæ­¤ï¼Œå¯ä»¥ç®€å•åœ°å°† `streaming=True` å‚æ•°ä¼ é€’ç»™ `load_dataset()` æ–¹æ³•ï¼Œä»¥åˆ›å»ºå¯è¿­ä»£æ•°æ®é›†ï¼Œå¹¶åœ¨éœ€è¦æ—¶ä¸‹è½½æ•°æ®å®ä¾‹ã€‚


```shell
>>> from datasets import load_dataset

>>> dataset = load_dataset('kakaobrain/coyo-700m', streaming=True)
>>> print(next(iter(dataset['train'])))
{'id': 2680060225205, 'url': 'https://cdn.shopify.com/s/files/1/0286/3900/2698/products/TVN_Huile-olive-infuse-et-s-227x300_e9a90ffd-b6d2-4118-95a1-29a5c7a05a49_800x.jpg?v=1616684087', 'text': 'Olive oil infused with Tuscany herbs', 'width': 227, 'height': 300, 'image_phash': '9f91e133b1924e4e', 'text_length': 36, 'word_count': 6, 'num_tokens_bert': 6, 'num_tokens_gpt': 9, 'num_faces': 0, 'clip_similarity_vitb32': 0.19921875, 'clip_similarity_vitl14': 0.147216796875, 'nsfw_score_opennsfw2': 0.0058441162109375, 'nsfw_score_gantman': 0.018961310386657715, 'watermark_score': 0.11015450954437256, 'aesthetic_score_laion_v2': 4.871710777282715}
```

## å¦‚ä½•ä½¿ç”¨ Hub ä¸­çš„ ViT å’Œ ALIGN

è®©æˆ‘ä»¬å°è¯•ä¸€ä¸‹æ–°çš„ ViT å’Œ ALIGN æ¨¡å‹ã€‚ç”±äº ALIGN æ˜¯æ–°åŠ å…¥ Hugging Face ğŸ¤— Transformers çš„ï¼Œæˆ‘ä»¬å…ˆå®‰è£…æœ€æ–°ç‰ˆæœ¬çš„åº“: `pip install -q git+https://github.com/huggingface/transformers.git` ç„¶åå¯¼å…¥æˆ‘ä»¬å°†è¦ä½¿ç”¨çš„æ¨¡å—å’Œåº“ï¼Œå¼€å§‹ä½¿ç”¨ ViT è¿›è¡Œå›¾åƒåˆ†ç±»ã€‚è¯·æ³¨æ„ï¼Œæ–°æ·»åŠ çš„ ALIGN æ¨¡å‹å°†ä¼šåŒ…å«åˆ°ä¸‹ä¸€ç‰ˆ PyPI åŒ…ã€‚

```py
import requests
from PIL import Image
import torch
from transformers import ViTImageProcessor, ViTForImageClassification
```

æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å°†ä» COCO æ•°æ®é›†ä¸­éšæœºä¸‹è½½ä¸€å¼ æœ‰æ²™å‘å›¾åƒï¼Œä¸Šè¾¹æœ‰ä¸¤åªçŒ«å’Œä¸€ä¸ªé¥æ§å™¨ï¼Œå¹¶å¯¹å›¾åƒè¿›è¡Œé¢„å¤„ç†ä¸ºæ¨¡å‹æ‰€æœŸæœ›çš„è¾“å…¥æ ¼å¼ï¼Œæˆ‘ä»¬å¯ä»¥æ–¹ä¾¿åœ°ä½¿ç”¨ç›¸åº”çš„é¢„å¤„ç†å™¨ç±» (`ViTProcessor`) å®ç°è¿™ä¸€æ­¥ã€‚åˆå§‹åŒ–æ¨¡å‹å’Œé¢„å¤„ç†å™¨ï¼Œå¯ä»¥ä½¿ç”¨ Hub ä¸­ [Kakao Brain ViT repos](https://huggingface.co/models?search=kakaobrain/vit) ä¹‹ä¸€ã€‚è¯·æ³¨æ„ä½¿ç”¨ Hub ä¸­çš„åº“é¢„å¤„ç†å™¨ï¼Œç¡®ä¿é¢„å¤„ç†åçš„å›¾åƒç¬¦åˆç‰¹å®šé¢„è®­ç»ƒæ¨¡å‹æ‰€éœ€çš„æ ¼å¼ã€‚

```py
url = 'http://images.cocodataset.org/val2017/000000039769.jpg'
image = Image.open(requests.get(url, stream=True).raw)

processor = ViTImageProcessor.from_pretrained('kakaobrain/vit-large-patch16-384')
model = ViTForImageClassification.from_pretrained('kakaobrain/vit-large-patch16-384')
```

æ¥ä¸‹æ¥å°†å›¾åƒé¢„å¤„ç†å¹¶å°†å…¶è¾“å…¥åˆ°æ¨¡å‹ï¼Œå®ç°æ£€ç´¢ç±»åˆ«æ ‡ç­¾ã€‚Kakao Brain ViT å›¾åƒåˆ†ç±»æ¨¡å‹æ˜¯åœ¨ ImageNet æ ‡ç­¾ä¸Šè®­ç»ƒçš„ï¼Œè¾“å‡ºå½¢çŠ¶ä¸º batch_sizeÃ—1000 ç»´åº¦çš„ç±»åˆ« (logits)ã€‚
  
```py
# preprocess image or list of images
inputs = processor(images=image, return_tensors="pt")

# inference
with torch.no_grad():
    outputs = model(**inputs)

# apply SoftMax to logits to compute the probability of each class
preds = torch.nn.functional.softmax(outputs.logits, dim=-1)

# print the top 5 class predictions and their probabilities
top_class_preds = torch.argsort(preds, descending=True)[0, :5]

for c in top_class_preds:
    print(f"{model.config.id2label[c.item()]} with probability {round(preds[0, c.item()].item(), 4)}")
```

åˆ°è¿™é‡Œå°±å®Œæˆäº†ï¼ä¸ºäº†æ›´åŠ ç®€å•å’Œç®€æ´ï¼Œè¿˜å¯ä»¥ä½¿ç”¨å›¾åƒåˆ†ç±»ç®¡é“ ([pipeline](https://huggingface.co/docs/transformers/main_classes/pipelines#transformers.ImageClassificationPipeline)) å¹¶å°† Kakao Brain ViT ä»“åº“åç§°ä½œä¸ºç›®æ ‡æ¨¡å‹ä¼ é€’ç»™åˆå§‹åŒ–ç®¡é“ã€‚ç„¶åï¼Œæˆ‘ä»¬å¯ä»¥ä¼ å…¥å›¾åƒçš„ URL æˆ–æœ¬åœ°è·¯å¾„ï¼Œæˆ– Pillow å›¾åƒï¼Œå¯é€‰â€œtop_kâ€å‚æ•°è¡¨è¿°è¿”å›å‰ k ä¸ªé¢„æµ‹ã€‚è®©æˆ‘ä»¬ç»§ç»­å¯¹çŒ«å’Œé¥æ§å™¨å›¾ç‰‡è·å–å‰ 5 ä¸ªé¢„æµ‹ç»“æœã€‚

```shell
>>> from transformers import pipeline

>>> classifier = pipeline(task='image-classification', model='kakaobrain/vit-large-patch16-384')
>>> classifier('http://images.cocodataset.org/val2017/000000039769.jpg', top_k=5)
[{'score': 0.8223727941513062, 'label': 'remote control, remote'}, {'score': 0.06580372154712677, 'label': 'tabby, tabby cat'}, {'score': 0.0655883178114891, 'label': 'tiger cat'}, {'score': 0.0388941615819931, 'label': 'Egyptian cat'}, {'score': 0.0011215205304324627, 'label': 'lynx, catamount'}]
```

å¦‚æœæ‚¨æƒ³æ›´å¤šåœ°å°è¯• Kakao Brain ViT æ¨¡å‹ï¼Œè¯·å‰å¾€ ğŸ¤— Hub ä¸­å¿ƒçš„é¡¹ç›® [ç©ºé—´](https://huggingface.co/spaces/adirik/kakao-brain-vit)ã€‚

<center>
<img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/132_vit_align/vit_demo.png" alt="vit performance" width="900"/>
</center>

æˆ‘ä»¬å¼€å§‹å®éªŒ ALIGNï¼Œå®ƒå¯ç”¨äºæ£€ç´¢æ–‡æœ¬æˆ–å›¾åƒçš„å¤šæ¨¡æ€åµŒå…¥æˆ–æ‰§è¡Œé›¶æ ·æœ¬å›¾åƒåˆ†ç±»ã€‚ALIGN çš„ Transformer å®ç°å’Œç”¨æ³•ç±»ä¼¼äº [CLIP](https://huggingface.co/docs/transformers/main/en/model_doc/clip)ã€‚é¦–å…ˆï¼Œä¸‹è½½é¢„è®­ç»ƒæ¨¡å‹å’Œå…¶å¤„ç†å™¨ (processor)ï¼Œå¤„ç†å™¨é¢„å¤„ç†å›¾åƒå’Œæ–‡æœ¬ï¼Œä½¿å®ƒä»¬ç¬¦åˆ ALIGN çš„é¢„æœŸæ ¼å¼ï¼Œä»¥ä¾¿å°†å…¶è¾“å…¥åˆ°è§†è§‰å’Œæ–‡æœ¬ç¼–ç å™¨ä¸­ã€‚è¿™æ­¥å¯¼å…¥äº†æˆ‘ä»¬å°†è¦ä½¿ç”¨çš„æ¨¡å—å¹¶åˆå§‹åŒ–é¢„å¤„ç†å™¨å’Œæ¨¡å‹ã€‚

```py
import requests
from PIL import Image
import torch
from transformers import AlignProcessor, AlignModel


url = 'http://images.cocodataset.org/val2017/000000039769.jpg'
image = Image.open(requests.get(url, stream=True).raw)

processor = AlignProcessor.from_pretrained('kakaobrain/align-base')
model = AlignModel.from_pretrained('kakaobrain/align-base')
```

å…ˆä»é›¶æ ·æœ¬å›¾åƒåˆ†ç±»å¼€å§‹ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å°†æä¾›å€™é€‰æ ‡ç­¾ (è‡ªç”±æ ¼å¼æ–‡æœ¬)ï¼Œå¹¶ä½¿ç”¨ AlignModel æ‰¾å‡ºæ›´å¥½åœ°æè¿°å›¾åƒçš„è¡¨è¿°ã€‚æˆ‘ä»¬å°†é¦–å…ˆé¢„å¤„ç†å›¾åƒå’Œæ–‡æœ¬è¾“å…¥ï¼Œå¹¶å°†é¢„å¤„ç†åçš„è¾“å…¥é€åˆ° AlignModel ä¸­ã€‚

```py
candidate_labels = ['an image of a cat', 'an image of a dog']

inputs = processor(images=image, text=candidate_labels, return_tensors='pt')

with torch.no_grad():
    outputs = model(**inputs)

# this is the image-text similarity score
logits_per_image = outputs.logits_per_image  

# we can take the softmax to get the label probabilities
probs = logits_per_image.softmax(dim=1)  
print(probs)
```

å®Œæˆäº†ï¼Œå°±è¿™ä¹ˆç®€å•ã€‚è¦è¿›ä¸€æ­¥å°è¯• Kakao Brain ALIGN æ¨¡å‹è¿›è¡Œé›¶æ ·æœ¬å›¾åƒåˆ†ç±»ï¼Œåªéœ€å‰å¾€ Hugging Face ğŸ¤— Hub ä¸Šçš„ [demo](https://huggingface.co/spaces/adirik/ALIGN-zero-shot-image-classification) æ¼”ç¤ºã€‚è¯·æ³¨æ„ï¼Œ `AlignModel` çš„è¾“å‡ºåŒ…æ‹¬ `text_embeds` å’Œ  `image_embeds` (å‚é˜… ALIGN çš„ [æ–‡æ¡£](https://huggingface.co/docs/transformers/main/en/model_doc/align))ã€‚å¦‚æœä¸éœ€è¦è®¡ç®—ç”¨äºé›¶æ ·æœ¬åˆ†ç±»çš„æ¯ä¸ªå›¾åƒå’Œæ¯ä¸ªæ–‡æœ¬çš„é€»è¾‘ (logits)ï¼Œå¯ä»¥ä½¿ç”¨ `AlignModel` ç±»ä¸­çš„ `get_image_features()` å’Œ  `get_text_features()` æ–¹æ³•ä¾¿æ·åœ°æ£€ç´¢è§†è§‰å’Œæ–‡æœ¬åµŒå…¥ã€‚

```py
text_embeds = model.get_text_features(
    input_ids=inputs['input_ids'],
    attention_mask=inputs['attention_mask'],
    token_type_ids=inputs['token_type_ids'],
)
image_embeds = model.get_image_features(
    pixel_values=inputs['pixel_values'],
)
```

æˆ–è€…ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨ ALIGN çš„ç‹¬ç«‹è§†è§‰å’Œæ–‡æœ¬ç¼–ç å™¨è·å–å¤šæ¨¡æ€åµŒå…¥ã€‚ç„¶åå¯ä»¥ä½¿ç”¨è¿™äº›åµŒå…¥ç”¨äºå„ç§ä¸‹æ¸¸ä»»åŠ¡çš„æ¨¡å‹è®­ç»ƒï¼Œä¾‹å¦‚ç›®æ ‡æ£€æµ‹ã€å›¾åƒåˆ†å‰²å’Œå›¾åƒå­—å¹•ç”Ÿæˆã€‚è®©æˆ‘ä»¬çœ‹çœ‹å¦‚ä½•ä½¿ç”¨ `AlignTextModel` å’Œ  `AlignVisionModel` è·å–è¿™äº›åµŒå…¥ã€‚è¯·æ³¨æ„ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨ä¾¿æ·çš„ AlignProcessor ç±»åˆ†åˆ«å¯¹æ–‡æœ¬å’Œå›¾åƒè¿›è¡Œé¢„å¤„ç†ã€‚

```py
from transformers import AlignTextModel


processor = AlignProcessor.from_pretrained('kakaobrain/align-base')
model = AlignTextModel.from_pretrained('kakaobrain/align-base')

# get embeddings of two text queries
inputs = processor(['an image of a cat', 'an image of a dog'], return_tensors='pt')

with torch.no_grad():
    outputs = model(**inputs)

# get the last hidden state and the final pooled output 
last_hidden_state = outputs.last_hidden_state
pooled_output = outputs.pooler_output
```

æˆ‘ä»¬ä¹Ÿå¯ä»¥åœ¨æ¨ç†è¿‡ç¨‹ä¸­è®¾ç½® output_hidden_states å’Œ output_attentions å‚æ•°ä¸º Trueï¼Œä»¥è¿”å›æ‰€æœ‰éšè—çŠ¶æ€å’Œæ³¨æ„åŠ›å€¼ã€‚

```py
with torch.no_grad():
    outputs = model(**inputs, output_hidden_states=True, output_attentions=True)

# print what information is returned
for key, value in outputs.items():
    print(key)
```

åœ¨ `AlignVisionModel` ä¸­æ‰§è¡Œç›¸åŒçš„æ“ä½œï¼Œè·å–å›¾åƒçš„å¤šæ¨¡æ€åµŒå…¥ã€‚

```py
from transformers import AlignVisionModel


processor = AlignProcessor.from_pretrained('kakaobrain/align-base')
model = AlignVisionModel.from_pretrained('kakaobrain/align-base')

url = 'http://images.cocodataset.org/val2017/000000039769.jpg'
image = Image.open(requests.get(url, stream=True).raw)

inputs = processor(images=image, return_tensors='pt')

with torch.no_grad():
    outputs = model(**inputs)

# print the last hidden state and the final pooled output 
last_hidden_state = outputs.last_hidden_state
pooled_output = outputs.pooler_output
```

ä¸ ViT ç±»ä¼¼ï¼Œä½¿ç”¨é›¶æ ·æœ¬å›¾åƒåˆ†ç±»ç®¡é“ ([pipeline](https://huggingface.co/docs/transformers/main_classes/pipelines#transformers.ZeroShotImageClassificationPipeline)) å¯ä»¥è®©è¿‡ç¨‹æ›´åŠ è½»æ¾ã€‚ä»¥ä¸‹å®ç°äº†å¦‚ä½•ä½¿ç”¨æ­¤æµç¨‹ä½¿ç”¨è‡ªç”±æ–‡æœ¬å€™é€‰æ ‡ç­¾åœ¨é‡å¤–æ‰§è¡Œå›¾åƒåˆ†ç±»ã€‚

```shell
>>> from transformers import pipeline

>>> classifier = pipeline(task='zero-shot-image-classification', model='kakaobrain/align-base')
>>> classifier(
...     'https://huggingface.co/datasets/Narsil/image_dummy/raw/main/parrots.png',
...     candidate_labels=['animals', 'humans', 'landscape'],
... )
[{'score': 0.9263709783554077, 'label': 'animals'}, {'score': 0.07163811475038528, 'label': 'humans'}, {'score': 0.0019908479880541563, 'label': 'landscape'}]

>>> classifier(
...    'https://huggingface.co/datasets/Narsil/image_dummy/raw/main/parrots.png',
...    candidate_labels=['black and white', 'photorealist', 'painting'],
... )
[{'score': 0.9735308885574341, 'label': 'black and white'}, {'score': 0.025493400171399117, 'label': 'photorealist'}, {'score': 0.0009757201769389212, 'label': 'painting'}]
```

## ç»“è®º

è¿‘å¹´æ¥ï¼Œå¤šæ¨¡æ€å–å¾—äº†ä»¤äººéš¾ä»¥ç½®ä¿¡çš„è¿›å±•ï¼Œä¾‹å¦‚ CLIP å’Œ ALIGN ç­‰æ¨¡å‹èµ‹èƒ½äº†å„ç§ä¸‹æ¸¸ä»»åŠ¡ï¼Œä¾‹å¦‚å›¾åƒæè¿°ã€é›¶æ ·æœ¬å›¾åƒåˆ†ç±»å’Œå¼€æ”¾ä¸–ç•Œç›®æ ‡æ£€æµ‹ã€‚æœ¬åšå®¢ï¼Œæˆ‘ä»¬ä»‹ç»äº†ç”± Kakao Brain è´¡çŒ®çš„æœ€æ–°å¼€æºä»£ç  ViT å’Œ ALIGN æ¨¡å‹ï¼Œä»¥åŠæ–°çš„ COYO æ–‡æœ¬ - å›¾åƒæ•°æ®é›†ã€‚å±•ç¤ºäº†å¦‚ä½•ä½¿ç”¨è¿™äº›æ¨¡å‹æ‰§è¡Œå„ç§ä»»åŠ¡ï¼Œåªéœ€å‡ è¡Œä»£ç å³å¯å•ç‹¬ä½¿ç”¨æˆ–ä½œä¸º ğŸ¤— Transformers pipeline çš„ä¸€éƒ¨åˆ†ä½¿ç”¨ã€‚

æˆ‘ä»¬æ­£åœ¨ç»§ç»­æ•´åˆæœ€æœ‰å½±å“åŠ›çš„è®¡ç®—æœºè§†è§‰å’Œå¤šæ¨¡å‹æ¨¡å‹ï¼Œå¹¶ä¹äºå¬å–æ‚¨çš„åé¦ˆã€‚è¦äº†è§£è®¡ç®—æœºè§†è§‰å’Œå¤šæ¨¡æ€ç ”ç©¶çš„æœ€æ–°æ¶ˆæ¯ï¼Œä½œè€…åŠ Twitter:[@adirik](https://twitter.com/https://twitter.com/alaradirik), [@a_e_roberts](https://twitter.com/a_e_roberts), [@NielsRogge](https://twitter.com/NielsRogge), [@RisingSayak](https://twitter.com/RisingSayak), and [@huggingface](https://twitter.com/huggingface).

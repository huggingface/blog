---
title: "ä½¿ç”¨ ğŸ¤— Transformers ä¸ºå¤šè¯­ç§è¯­éŸ³è¯†åˆ«ä»»åŠ¡å¾®è°ƒ Whisper æ¨¡å‹" 
thumbnail: /blog/assets/111_fine_tune_whisper/thumbnail.jpg
authors:
- user: sanchit-gandhi
translators:
- user: MatrixYao
- user: zhongdongy
  proofreader: true
---

# ä½¿ç”¨ ğŸ¤— Transformers ä¸ºå¤šè¯­ç§è¯­éŸ³è¯†åˆ«ä»»åŠ¡å¾®è°ƒ Whisper æ¨¡å‹


<a target="_blank" href="https://colab.research.google.com/github/sanchit-gandhi/notebooks/blob/main/fine_tune_whisper.ipynb">
    <img src="https://colab.research.google.com/assets/colab-badge.svg" alt="åœ¨ Colab ä¸­æ‰“å¼€"/>
</a>

æœ¬æ–‡æä¾›äº†ä¸€ä¸ªä½¿ç”¨ Hugging Face ğŸ¤— Transformers åœ¨ä»»æ„å¤šè¯­ç§è¯­éŸ³è¯†åˆ« (ASR) æ•°æ®é›†ä¸Šå¾®è°ƒ Whisper çš„åˆ†æ­¥æŒ‡å—ã€‚åŒæ—¶ï¼Œæˆ‘ä»¬è¿˜æ·±å…¥è§£é‡Šäº† Whisper æ¨¡å‹ã€Common Voice æ•°æ®é›†ä»¥åŠå¾®è°ƒç­‰ç†è®ºçŸ¥è¯†ï¼Œå¹¶æä¾›äº†æ•°æ®å‡†å¤‡å’Œå¾®è°ƒçš„ç›¸å…³ä»£ç ã€‚å¦‚æœä½ æƒ³è¦ä¸€ä¸ªå…¨éƒ¨æ˜¯ä»£ç ï¼Œä»…æœ‰å°‘é‡è§£é‡Šçš„ Notebookï¼Œå¯ä»¥å‚é˜…è¿™ä¸ª [Google Colab](https://colab.research.google.com/github/sanchit-gandhi/notebooks/blob/main/fine_tune_whisper.ipynb)ã€‚

## ç›®å½•

1. [ç®€ä»‹](#ç®€ä»‹)
2. [åœ¨ Google Colab ä¸­å¾®è°ƒ Whisper](#åœ¨-google-colab-ä¸­å¾®è°ƒ-whisper)
    1. [å‡†å¤‡ç¯å¢ƒ](#å‡†å¤‡ç¯å¢ƒ)
    2. [åŠ è½½æ•°æ®é›†](#åŠ è½½æ•°æ®é›†)
    3. [å‡†å¤‡ç‰¹å¾æå–å™¨ã€åˆ†è¯å™¨å’Œæ•°æ®](#å‡†å¤‡ç‰¹å¾æå–å™¨åˆ†è¯å™¨å’Œæ•°æ®)
    4. [è®­ç»ƒä¸è¯„ä¼°](#è®­ç»ƒä¸è¯„ä¼°)
    5. [æ„å»ºæ¼”ç¤ºåº”ç”¨](#æ„å»ºæ¼”ç¤ºåº”ç”¨)
3. [ç»“æŸè¯­](#ç»“æŸè¯­)

## ç®€ä»‹

Whisper æ˜¯ä¸€ç³»åˆ—ç”¨äºè‡ªåŠ¨è¯­éŸ³è¯†åˆ« (automatic speech recognitionï¼ŒASR) çš„é¢„è®­ç»ƒæ¨¡å‹ï¼Œå®ƒç”±æ¥è‡ªäº OpenAI çš„ Alec Radford ç­‰äººäº [2022 å¹´ 9 æœˆ](https://openai.com/blog/whisper/) å‘å¸ƒã€‚ä¸ [Wav2Vec 2.0](https://arxiv.org/abs/2006.11477) ç­‰å‰ä½œä¸åŒï¼Œä»¥å¾€çš„æ¨¡å‹éƒ½æ˜¯åœ¨æœªæ ‡æ³¨çš„éŸ³é¢‘æ•°æ®ä¸Šé¢„è®­ç»ƒçš„ï¼Œè€Œ Whisper æ˜¯åœ¨å¤§é‡çš„ **å·²æ ‡æ³¨** éŸ³é¢‘è½¬å½•æ•°æ®ä¸Šé¢„è®­ç»ƒçš„ã€‚å…¶ç”¨äºè®­ç»ƒçš„æ ‡æ³¨éŸ³é¢‘æ—¶é•¿é«˜è¾¾ 68 ä¸‡å°æ—¶ï¼Œæ¯” Wav2Vec 2.0 ä½¿ç”¨çš„æœªæ ‡æ³¨è®­ç»ƒæ•°æ® (6 ä¸‡å°æ—¶) è¿˜å¤šä¸€ä¸ªæ•°é‡çº§ã€‚æ›´å¦™çš„æ˜¯ï¼Œè¯¥é¢„è®­ç»ƒæ•°æ®ä¸­è¿˜å«æœ‰ 11.7 ä¸‡å°æ—¶çš„å¤šè¯­ç§æ•°æ®ã€‚å› æ­¤ï¼ŒWhisper è®­å¾—çš„ checkpoint å¯åº”ç”¨äºè¶…è¿‡ 96 ç§è¯­è¨€ï¼Œè¿™å…¶ä¸­åŒ…å«ä¸å°‘ _æ•°æ®åŒ®ä¹_ çš„å°è¯­ç§ã€‚

è¿™ä¹ˆå¤šçš„æ ‡æ³¨æ•°æ®ä½¿å¾—æˆ‘ä»¬å¯ä»¥ç›´æ¥åœ¨ _æœ‰ç›‘ç£_ è¯­éŸ³è¯†åˆ«ä»»åŠ¡ä¸Šé¢„è®­ç»ƒ Whisperï¼Œä»æ ‡æ³¨éŸ³é¢‘è½¬å½•æ•°æ® ${}^1$ ä¸­ç›´æ¥ä¹ å¾—è¯­éŸ³åˆ°æ–‡æœ¬çš„æ˜ å°„ã€‚å› æ­¤ï¼ŒWhisper å‡ ä¹ä¸éœ€è¦é¢å¤–çš„å¾®è°ƒå°±å·²ç»æ˜¯é«˜æ€§èƒ½çš„ ASR æ¨¡å‹äº†ã€‚è¿™è®© Wav2Vec 2.0 ç›¸å½¢è§ç»Œï¼Œå› ä¸º Wav2Vec 2.0 æ˜¯åœ¨ _æ— ç›‘ç£_ æ©ç é¢„æµ‹ä»»åŠ¡ä¸Šé¢„è®­ç»ƒçš„ï¼Œæ‰€ä»¥å…¶è®­å¾—çš„æ¨¡å‹ä»…ä»æœªæ ‡æ³¨çš„çº¯éŸ³é¢‘æ•°æ®ä¸­ä¹ å¾—äº†ä»è¯­éŸ³åˆ°éšå«çŠ¶æ€çš„ä¸­é—´æ˜ å°„ã€‚è™½ç„¶æ— ç›‘ç£é¢„è®­ç»ƒèƒ½äº§ç”Ÿé«˜è´¨é‡çš„è¯­éŸ³è¡¨å¾ï¼Œä½†å®ƒ **å­¦ä¸åˆ°**è¯­éŸ³åˆ°æ–‡æœ¬çš„æ˜ å°„ï¼Œè¦å­¦åˆ°è¯­éŸ³åˆ°æ–‡æœ¬çš„æ˜ å°„åªèƒ½é å¾®è°ƒã€‚å› æ­¤ï¼ŒWav2Vec 2.0 éœ€è¦æ›´å¤šçš„å¾®è°ƒæ‰èƒ½è·å¾—è¾ƒæœ‰ç«äº‰åŠ›çš„æ€§èƒ½ã€‚

åœ¨ 68 ä¸‡å°æ—¶æ ‡æ³¨æ•°æ®çš„åŠ æŒä¸‹ï¼Œé¢„è®­ç»ƒ Whisper æ¨¡å‹è¡¨ç°å‡ºäº†å¼ºå¤§çš„æ³›åŒ–åˆ°å¤šç§æ•°æ®é›†å’Œé¢†åŸŸçš„èƒ½åŠ›ã€‚å…¶é¢„è®­ç»ƒ checkpoint è¡¨ç°å‡ºäº†ä¸æœ€å…ˆè¿›çš„ ASR ç³»ç»Ÿæ——é¼“ç›¸å½“çš„æ€§èƒ½: åœ¨ LibriSpeech ASR çš„æ— å™ªæµ‹è¯•å­é›†ä¸Šçš„å•è¯é”™è¯¯ç‡ (word error rateï¼ŒWER) ä»…ä¸ºçº¦ 3%ï¼Œå¦å¤–å®ƒè¿˜åœ¨ TED-LIUM ä¸Šåˆ›ä¸‹äº†æ–°çš„è®°å½• - 4.7% çš„ WER ( _è¯¦è§_ [Whisper è®ºæ–‡](https://cdn.openai.com/papers/whisper.pdf) çš„è¡¨ 8)ã€‚Whisper åœ¨é¢„è®­ç»ƒæœŸé—´è·å¾—çš„å¹¿æ³›çš„å¤šè¯­ç§ ASR çŸ¥è¯†å¯¹ä¸€äº›æ•°æ®åŒ®ä¹çš„å°è¯­ç§ç‰¹åˆ«æœ‰ç”¨ã€‚ç¨ç¨å¾®è°ƒä¸€ä¸‹ï¼Œé¢„è®­ç»ƒ checkpoint å°±å¯ä»¥è¿›ä¸€æ­¥é€‚é…ç‰¹å®šçš„æ•°æ®é›†å’Œè¯­ç§ï¼Œä»è€Œè¿›ä¸€æ­¥æ”¹è¿›åœ¨è¿™äº›è¯­ç§ä¸Šçš„è¯†åˆ«æ•ˆæœã€‚

Whisper æ˜¯ä¸€ä¸ªåŸºäº transformer çš„ç¼–ç å™¨ - è§£ç å™¨æ¨¡å‹ (ä¹Ÿç§°ä¸º _åºåˆ—åˆ°åºåˆ—_ æ¨¡å‹)ï¼Œå®ƒå°†éŸ³é¢‘çš„é¢‘è°±å›¾ç‰¹å¾ _åºåˆ—_ æ˜ å°„åˆ°æ–‡æœ¬çš„è¯ _åºåˆ—_ã€‚é¦–å…ˆï¼Œé€šè¿‡ç‰¹å¾æå–å™¨å°†åŸå§‹éŸ³é¢‘è¾“å…¥å˜æ¢ä¸ºå¯¹æ•°æ¢…å°”å£°è°±å›¾ (log-Mel spectrogram)ã€‚ç„¶åï¼Œtransformer ç¼–ç å™¨å¯¹å£°è°±å›¾è¿›è¡Œç¼–ç ï¼Œç”Ÿæˆä¸€ç³»åˆ—ç¼–ç å™¨éšå«çŠ¶æ€ã€‚æœ€åï¼Œè§£ç å™¨åŸºäºå…ˆå‰è¾“å‡ºçš„è¯ä»¥åŠç¼–ç å™¨éšå«çŠ¶æ€ï¼Œè‡ªå›å½’åœ°é¢„æµ‹ä¸‹ä¸€ä¸ªè¾“å‡ºè¯ã€‚å›¾ 1 æ˜¯ Whisper æ¨¡å‹çš„ç¤ºæ„å›¾ã€‚

<figure>
<img src="assets/111_fine_tune_whisper/whisper_architecture.svg" alt="Trulli" style="width:100%">
<figcaption align="center"><b>å›¾ 1:</b> Whisper æ¨¡å‹ï¼Œè¯¥æ¨¡å‹æ˜¯æ ‡å‡†çš„åŸºäº transformer çš„ç¼–ç å™¨-è§£ç å™¨æ¶æ„ã€‚é¦–å…ˆå°†å¯¹æ•°æ¢…å°”å£°è°±å›¾è¾“å…¥åˆ°ç¼–ç å™¨ï¼Œç„¶åå°†ç¼–ç å™¨ç”Ÿæˆçš„æœ€ç»ˆéšå«çŠ¶æ€é€šè¿‡äº¤å‰æ³¨æ„æœºåˆ¶è¾“å…¥ç»™è§£ç å™¨ã€‚æœ€åï¼Œè§£ç å™¨åŸºäºç¼–ç å™¨éšå«çŠ¶æ€å’Œå…ˆå‰çš„è¾“å‡ºè¯ï¼Œè‡ªå›å½’åœ°é¢„æµ‹ä¸‹ä¸€ä¸ªè¾“å‡ºè¯ã€‚å›¾æº: <a href="https://openai.com/blog/whisper/">OpenAI Whisper åšå®¢</a>ã€‚</figcaption>
</figure>

åœ¨åºåˆ—åˆ°åºåˆ—æ¨¡å‹ä¸­ï¼Œç¼–ç å™¨è´Ÿè´£ä»è¯­éŸ³ä¸­æå–å‡ºé‡è¦ç‰¹å¾ï¼Œå°†è¾“å…¥è½¬æ¢ä¸ºä¸€ç»„éšå«çŠ¶æ€è¡¨å¾ã€‚è§£ç å™¨æ‰®æ¼”è¯­è¨€æ¨¡å‹çš„è§’è‰²ï¼Œå¤„ç†éšå«çŠ¶æ€è¡¨å¾å¹¶ç”Ÿæˆå¯¹åº”çš„æ–‡æœ¬ã€‚æˆ‘ä»¬æŠŠåœ¨æ¨¡å‹æ¶æ„ **å†…éƒ¨** é›†æˆè¯­è¨€æ¨¡å‹çš„åšæ³•ç§°ä¸º _æ·±åº¦èåˆ_ã€‚ä¸ä¹‹ç›¸å¯¹çš„æ˜¯ _æµ…èåˆ_ï¼Œæ­¤æ—¶ï¼Œè¯­è¨€æ¨¡å‹åœ¨ **å¤–éƒ¨**ä¸ç¼–ç å™¨ç»„åˆï¼Œå¦‚ CTC + $n$-gram ( _è¯¦è§_ [Internal Language Model Estimation](https://arxiv.org/pdf/2011.01991.pdf) ä¸€æ–‡)ã€‚é€šè¿‡æ·±åº¦èåˆï¼Œå¯ä»¥ç”¨åŒä¸€ä»½è®­ç»ƒæ•°æ®å’ŒæŸå¤±å‡½æ•°å¯¹æ•´ä¸ªç³»ç»Ÿè¿›è¡Œç«¯åˆ°ç«¯è®­ç»ƒï¼Œä»è€Œè·å¾—æ›´å¤§çš„çµæ´»æ€§å’Œæ›´ä¼˜è¶Šçš„æ€§èƒ½ ( _è¯¦è§_ [ESB Benchmark](https://arxiv.org/abs/2210.13352))ã€‚

Whisper ä½¿ç”¨äº¤å‰ç†µç›®æ ‡å‡½æ•°è¿›è¡Œé¢„è®­ç»ƒå’Œå¾®è°ƒï¼Œäº¤å‰ç†µç›®æ ‡å‡½æ•°æ˜¯è®­ç»ƒåºåˆ—æ ‡æ³¨æ¨¡å‹çš„æ ‡å‡†ç›®æ ‡å‡½æ•°ã€‚ç»è¿‡è®­ç»ƒï¼Œæ¨¡å‹å¯ä»¥æ­£ç¡®åœ°å¯¹ç›®æ ‡è¯è¿›è¡Œåˆ†ç±»ï¼Œä»è€Œä»é¢„å®šä¹‰çš„è¯æ±‡è¡¨ä¸­é€‰å‡ºè¾“å‡ºè¯ã€‚

Whisper æœ‰äº”ç§ä¸åŒå°ºå¯¸çš„ checkpointã€‚å…¶ä¸­ï¼Œå››ä¸ªå°å°ºå¯¸ checkpoint åˆå„æœ‰ä¸¤ä¸ªç‰ˆæœ¬: è‹±è¯­ç‰ˆå’Œå¤šè¯­ç§ç‰ˆï¼Œè€Œæœ€å¤§çš„ checkpoint åªæœ‰å¤šè¯­ç§ç‰ˆã€‚æ‰€æœ‰ä¹ä¸ªé¢„è®­ç»ƒ checkpoints éƒ½å¯ä»¥åœ¨ [Hugging Face Hub](https://huggingface.co/models?search=openai/whisper) ä¸Šæ‰¾åˆ°ã€‚ä¸‹è¡¨æ€»ç»“äº†è¿™äº› checkpoint çš„ä¿¡æ¯åŠå…¶ Hub é“¾æ¥:

| å°ºå¯¸   | å±‚æ•° | å®½ | å¤šå¤´æ³¨æ„åŠ›çš„å¤´æ•° | å‚æ•°é‡ | è‹±è¯­ checkpoint                                         | å¤šè¯­ç§ checkpoint                                      |
|--------|--------|-------|-------|------------|------------------------------------------------------|---------------------------------------------------|
| tiny   | 4      | 384   | 6     | 39 M       | [âœ“](https://huggingface.co/openai/whisper-tiny.en)   | [âœ“](https://huggingface.co/openai/whisper-tiny.)  |
| base   | 6      | 512   | 8     | 74 M       | [âœ“](https://huggingface.co/openai/whisper-base.en)   | [âœ“](https://huggingface.co/openai/whisper-base)   |
| small  | 12     | 768   | 12    | 244 M      | [âœ“](https://huggingface.co/openai/whisper-small.en)  | [âœ“](https://huggingface.co/openai/whisper-small)  |
| medium | 24     | 1024  | 16    | 769 M      | [âœ“](https://huggingface.co/openai/whisper-medium.en) | [âœ“](https://huggingface.co/openai/whisper-medium) |
| large  | 32     | 1280  | 20    | 1550 M     | x                                                    | [âœ“](https://huggingface.co/openai/whisper-large)  |

ä¸‹é¢ï¼Œæˆ‘ä»¬å°†ä»¥å¤šè¯­ç§ç‰ˆçš„ [`small`](https://huggingface.co/openai/whisper-small)checkpoint (å‚æ•°é‡ 244M (~= 1GB)) ä¸ºä¾‹ï¼Œå¸¦å¤§å®¶èµ°ä¸€éå¾®è°ƒæ¨¡å‹çš„å…¨è¿‡ç¨‹ã€‚æˆ‘ä»¬å°†ä½¿ç”¨ [Common Voice](https://huggingface.co/datasets/mozilla-foundation/common_voice_11_0) æ•°æ®é›†é‡Œçš„å°è¯­ç§æ•°æ®æ¥è®­ç»ƒå’Œè¯„ä¼°æˆ‘ä»¬çš„ç³»ç»Ÿã€‚é€šè¿‡è¿™ä¸ªä¾‹å­ï¼Œæˆ‘ä»¬å°†è¯æ˜ï¼Œä»…éœ€ 8 å°æ—¶çš„è®­ç»ƒæ•°æ®å°±å¯ä»¥å¾®è°ƒå‡ºä¸€ä¸ªåœ¨è¯¥è¯­ç§ä¸Šè¡¨ç°å¼ºå¤§çš„è¯­éŸ³è¯†åˆ«æ¨¡å‹ã€‚

---

${}^1$ Whisper çš„åç§°æ¥è‡ªäº â€œWeb-scale Supervised Pre-training for Speech Recognition (ç½‘ç»œè§„æ¨¡çš„æœ‰ç›‘ç£è¯­éŸ³è¯†åˆ«é¢„è®­ç»ƒæ¨¡å‹)â€ çš„é¦–å­—æ¯ç¼©å†™ â€œWSPSRâ€ã€‚

## åœ¨ Google Colab ä¸­å¾®è°ƒ Whisper

### å‡†å¤‡ç¯å¢ƒ

åœ¨å¾®è°ƒ Whisper æ¨¡å‹æ—¶ï¼Œæˆ‘ä»¬ä¼šç”¨åˆ°å‡ ä¸ªæµè¡Œçš„ Python åŒ…ã€‚æˆ‘ä»¬ä½¿ç”¨ `datasets` æ¥ä¸‹è½½å’Œå‡†å¤‡è®­ç»ƒæ•°æ®ï¼Œä½¿ç”¨ `transformers` æ¥åŠ è½½å’Œè®­ç»ƒ Whisper æ¨¡å‹ã€‚å¦å¤–ï¼Œæˆ‘ä»¬è¿˜éœ€è¦ `soundfile` åŒ…æ¥é¢„å¤„ç†éŸ³é¢‘æ–‡ä»¶ï¼Œ`evaluate` å’Œ `jiwer` æ¥è¯„ä¼°æ¨¡å‹çš„æ€§èƒ½ã€‚æœ€åï¼Œæˆ‘ä»¬ç”¨ `gradio` æ¥ä¸ºå¾®è°ƒåçš„æ¨¡å‹æ„å»ºä¸€ä¸ªäº®é—ªé—ªçš„æ¼”ç¤ºåº”ç”¨ã€‚

```bash
!pip install datasets>=2.6.1
!pip install git+https://github.com/huggingface/transformers
!pip install librosa
!pip install evaluate>=0.30
!pip install jiwer
!pip install gradio
```

æˆ‘ä»¬å¼ºçƒˆå»ºè®®ä½ ç›´æ¥å°†è®­å¾—çš„æ¨¡å‹ checkpoint ä¸Šä¼ åˆ° [Hugging Face Hub](https://huggingface.co/)ã€‚Hub æä¾›äº†ä»¥ä¸‹åŠŸèƒ½:

- é›†æˆç‰ˆæœ¬æ§åˆ¶: ç¡®ä¿åœ¨è®­ç»ƒæœŸé—´ä¸ä¼šä¸¢å¤±ä»»ä½•æ¨¡å‹ checkpointã€‚
- Tensorboard æ—¥å¿—: è·Ÿè¸ªè®­ç»ƒè¿‡ç¨‹ä¸­çš„é‡è¦æŒ‡æ ‡ã€‚
- æ¨¡å‹å¡: è®°å½•æ¨¡å‹çš„ç”¨æ³•åŠå…¶åº”ç”¨åœºæ™¯ã€‚
- ç¤¾åŒº: è½»æ¾ä¸ç¤¾åŒºè¿›è¡Œåˆ†äº«å’Œåä½œï¼

å°† Python notebook è¿ä¸Š Hub éå¸¸ç®€å• - åªéœ€æ ¹æ®æç¤ºè¾“å…¥ä½ çš„ Hub èº«ä»½éªŒè¯ä»¤ç‰Œå³å¯ã€‚ä½ å¯ä»¥åœ¨ [æ­¤å¤„](https://huggingface.co/settings/tokens) æ‰¾åˆ°ä½ è‡ªå·±çš„ Hub èº«ä»½éªŒè¯ä»¤ç‰Œ:

```python
from huggingface_hub import notebook_login

notebook_login()
```

**æ‰“å°è¾“å‡º:**

```bash
Login successful
Your token has been saved to /root/.huggingface/token
```

### åŠ è½½æ•°æ®é›†

Common Voice ç”±ä¸€ç³»åˆ—ä¼—åŒ…æ•°æ®é›†ç»„æˆï¼Œå…¶ä¸­åŒ…å«äº†ç”¨å„ç§è¯­è¨€å½•åˆ¶çš„ç»´åŸºç™¾ç§‘æ–‡æœ¬ã€‚æœ¬æ–‡ä½¿ç”¨çš„æ˜¯æœ€æ–°ç‰ˆæœ¬çš„ Common Voice æ•°æ®é›† ([ç‰ˆæœ¬å·ä¸º 11](https://huggingface.co/datasets/mozilla-foundation/common_voice_11_0))ã€‚è¯­ç§ä¸Šï¼Œæˆ‘ä»¬é€‰æ‹©ç”¨ [_å°åœ°è¯­_](https://en.wikipedia.org/wiki/Hindi) æ¥å¾®è°ƒæˆ‘ä»¬çš„æ¨¡å‹ã€‚å°åœ°è¯­æ˜¯ä¸€ç§åœ¨å°åº¦åŒ—éƒ¨ã€ä¸­éƒ¨ã€ä¸œéƒ¨å’Œè¥¿éƒ¨ä½¿ç”¨çš„å°åº¦ - é›…åˆ©å®‰è¯­ã€‚Common Voice 11.0 ä¸­æœ‰å¤§çº¦ 12 å°æ—¶çš„æ ‡æ³¨å°åœ°è¯­æ•°æ®ï¼Œå…¶ä¸­ 4 å°æ—¶æ˜¯æµ‹è¯•æ•°æ®ã€‚

æˆ‘ä»¬å…ˆçœ‹ä¸‹ Hub ä¸Šçš„ Common Voice æ•°æ®é›†é¡µé¢: [mozilla-foundation/common_voice_11_0](https://huggingface.co/datasets/mozilla-foundation/common_voice_11_0)ã€‚å¦‚æœä½ æ˜¯é¦–æ¬¡æŸ¥çœ‹æ­¤é¡µé¢ï¼Œç³»ç»Ÿä¼šè¦æ±‚ä½ æ¥å—å…¶ä½¿ç”¨æ¡æ¬¾ï¼ŒåŒæ„åå°±å¯ä»¥è®¿é—®æ•°æ®é›†äº†ã€‚

ä¸€æ—¦èº«ä»½éªŒè¯æˆåŠŸï¼Œä½ å°±ä¼šçœ‹åˆ°æ•°æ®é›†é¢„è§ˆã€‚æ•°æ®é›†é¢„è§ˆå±•ç¤ºäº†æ•°æ®é›†çš„å‰ 100 ä¸ªæ ·æœ¬ã€‚æ›´é‡è¦çš„æ˜¯ï¼Œå®ƒè¿˜åŠ è½½äº†å¯ä¾›å®æ—¶æ”¶å¬çš„éŸ³é¢‘ã€‚æˆ‘ä»¬å¯ä»¥åœ¨ä¸‹æ‹‰èœå•é€‰æ‹© `hi` æ¥é€‰æ‹© Common Voice çš„å°åœ°è¯­å­é›† ( `hi` æ˜¯å°åœ°è¯­çš„è¯­è¨€æ ‡è¯†ç¬¦ä»£ç ):

<figure>
<img src="assets/111_fine_tune_whisper/select_hi.jpg" alt="Trulli" style="width:100%">
</figure>

ç‚¹å‡»ç¬¬ä¸€ä¸ªéŸ³é¢‘çš„æ’­æ”¾æŒ‰é’®ï¼Œä½ å°±å¯ä»¥æ”¶å¬éŸ³é¢‘å¹¶çœ‹åˆ°ç›¸åº”çš„æ–‡æœ¬äº†ã€‚ä½ è¿˜å¯ä»¥æ»šåŠ¨æµè§ˆè®­ç»ƒé›†å’Œæµ‹è¯•é›†ä¸­çš„æ ·æœ¬ï¼Œä»¥æ›´å¥½åœ°äº†è§£å¾…å¤„ç†éŸ³é¢‘å’Œæ–‡æœ¬æ•°æ®ã€‚ä»è¯­è°ƒå’Œé£æ ¼å¯ä»¥çœ‹å‡ºï¼Œè¿™äº›éŸ³é¢‘æ˜¯æ—ç™½å½•éŸ³ã€‚ä½ å¯èƒ½è¿˜ä¼šæ³¨æ„åˆ°å½•éŸ³è€…å’Œå½•éŸ³è´¨é‡çš„å·¨å¤§å·®å¼‚ï¼Œè¿™æ˜¯ä¼—åŒ…æ•°æ®çš„ä¸€ä¸ªå…±åŒç‰¹å¾ã€‚

ä½¿ç”¨ ğŸ¤— Datasets æ¥ä¸‹è½½å’Œå‡†å¤‡æ•°æ®éå¸¸ç®€å•ã€‚ä»…éœ€ä¸€è¡Œä»£ç å³å¯å®Œæˆ Common Voice æ•°æ®é›†çš„ä¸‹è½½å’Œå‡†å¤‡å·¥ä½œã€‚ç”±äºå°åœ°è¯­æ•°æ®éå¸¸åŒ®ä¹ï¼Œæˆ‘ä»¬æŠŠ `è®­ç»ƒé›†` å’Œ `éªŒè¯é›†`åˆå¹¶æˆçº¦ 8 å°æ—¶çš„è®­ç»ƒæ•°æ®ï¼Œè€Œæµ‹è¯•åˆ™åŸºäº 4 å°æ—¶çš„ `æµ‹è¯•é›†`:

```python
from datasets import load_dataset, DatasetDict

common_voice = DatasetDict()

common_voice["train"] = load_dataset("mozilla-foundation/common_voice_11_0", "hi", split="train+validation", use_auth_token=True)
common_voice["test"] = load_dataset("mozilla-foundation/common_voice_11_0", "hi", split="test", use_auth_token=True)

print(common_voice)
```

**æ‰“å°è¾“å‡º: **

```
DatasetDict({
    train: Dataset({
        features: ['client_id', 'path', 'audio', 'sentence', 'up_votes', 'down_votes', 'age', 'gender', 'accent', 'locale', 'segment'],
        num_rows: 6540
    })
    test: Dataset({
        features: ['client_id', 'path', 'audio', 'sentence', 'up_votes', 'down_votes', 'age', 'gender', 'accent', 'locale', 'segment'],
        num_rows: 2894
    })
})
```

å¤§å¤šæ•° ASR æ•°æ®é›†ä»…åŒ…å«è¾“å…¥éŸ³é¢‘æ ·æœ¬ ( `audio`) å’Œç›¸åº”çš„è½¬å½•æ–‡æœ¬ ( `sentence`)ã€‚ Common Voice è¿˜åŒ…å«é¢å¤–çš„å…ƒä¿¡æ¯ï¼Œä¾‹å¦‚ `accent` å’Œ  `locale`ï¼Œåœ¨ ASR åœºæ™¯ä¸­ï¼Œæˆ‘ä»¬å¯ä»¥å¿½ç•¥è¿™äº›ä¿¡æ¯ã€‚ä¸ºäº†ä½¿ä»£ç å°½å¯èƒ½é€šç”¨ï¼Œæˆ‘ä»¬åªè€ƒè™‘åŸºäºè¾“å…¥éŸ³é¢‘å’Œè½¬å½•æ–‡æœ¬è¿›è¡Œå¾®è°ƒï¼Œè€Œä¸ä½¿ç”¨é¢å¤–çš„å…ƒä¿¡æ¯:

```python
common_voice = common_voice.remove_columns(["accent", "age", "client_id", "down_votes", "gender", "locale", "path", "segment", "up_votes"])
```

é™¤äº† Common Voiceï¼ŒHub ä¸Šè¿˜æœ‰ä¸å°‘å…¶ä»–å¤šè¯­ç§ ASR æ•°æ®é›†å¯ä¾›ä½¿ç”¨ï¼Œä½ å¯ä»¥ç‚¹å‡»é“¾æ¥: [Hub ä¸Šçš„ ASR æ•°æ®é›†](https://huggingface.co/datasets?task_categories=task_categories:automatic-speech-recognition&sort=downloads) äº†è§£æ›´å¤šã€‚

### å‡†å¤‡ç‰¹å¾æå–å™¨ã€åˆ†è¯å™¨å’Œæ•°æ®

ASR çš„æµæ°´çº¿ä¸»è¦åŒ…å«ä¸‰ä¸ªæ¨¡å—:

1. å¯¹åŸå§‹éŸ³é¢‘è¾“å…¥è¿›è¡Œé¢„å¤„ç†çš„ç‰¹å¾æå–å™¨
2. æ‰§è¡Œåºåˆ—åˆ°åºåˆ—æ˜ å°„çš„æ¨¡å‹
3. å°†æ¨¡å‹è¾“å‡ºè½¬æ¢ä¸ºæ–‡æœ¬çš„åˆ†è¯å™¨

åœ¨ ğŸ¤— Transformers ä¸­ï¼ŒWhisper æ¨¡å‹æœ‰è‡ªå·±çš„ç‰¹å¾æå–å™¨å’Œåˆ†è¯å™¨ï¼Œå³ [WhisperFeatureExtractor](https://huggingface.co/docs/transformers/main/model_doc/whisper#transformers.WhisperFeatureExtractor) å’Œ [WhisperTokenizer](https://huggingface.co/docs/transformers/main/model_doc/whisper#transformers.WhisperTokenizer)ã€‚

ä¸‹é¢ï¼Œæˆ‘ä»¬é€ä¸€è¯¦ç»†ä»‹ç»ç‰¹å¾æå–å™¨å’Œåˆ†è¯å™¨ï¼

### åŠ è½½ WhisperFeatureExtractor

è¯­éŸ³å¯è¡¨ç¤ºä¸ºéšæ—¶é—´å˜åŒ–çš„ä¸€ç»´æ•°ç»„ï¼Œç»™å®šæ—¶åˆ»çš„æ•°ç»„å€¼å³è¡¨ç¤ºä¿¡å·åœ¨è¯¥æ—¶åˆ»çš„ _å¹…åº¦_ï¼Œè€Œæˆ‘ä»¬å¯ä»¥ä»…ä»å¹…åº¦ä¿¡æ¯é‡å»ºéŸ³é¢‘çš„é¢‘è°±å¹¶æ¢å¤å…¶æ‰€æœ‰å£°å­¦ç‰¹å¾ã€‚

ç”±äºè¯­éŸ³æ˜¯è¿ç»­çš„ï¼Œå› æ­¤å®ƒåŒ…å«æ— æ•°ä¸ªå¹…åº¦å€¼ï¼Œè€Œè®¡ç®—æœºåªèƒ½è¡¨ç¤ºå¹¶å­˜å‚¨æœ‰é™ä¸ªå€¼ã€‚å› æ­¤ï¼Œæˆ‘ä»¬éœ€è¦é€šè¿‡å¯¹è¯­éŸ³ä¿¡å·è¿›è¡Œç¦»æ•£åŒ–ï¼Œå³ä»¥å›ºå®šçš„æ—¶é—´é—´éš”å¯¹è¿ç»­ä¿¡å·è¿›è¡Œ _é‡‡æ ·_ã€‚æˆ‘ä»¬å°†æ¯ç§’é‡‡æ ·çš„æ¬¡æ•°ç§°ä¸º _é‡‡æ ·ç‡_ï¼Œé€šå¸¸ä»¥æ ·æœ¬æ•°/ç§’æˆ– _èµ«å…¹ (Hz)_ ä¸ºå•ä½ã€‚é«˜é‡‡æ ·ç‡å¯ä»¥æ›´å¥½åœ°é€¼è¿‘è¿ç»­è¯­éŸ³ä¿¡å·ï¼Œä½†åŒæ—¶æ¯ç§’æ‰€éœ€çš„å­˜å‚¨é‡ä¹Ÿæ›´å¤§ã€‚

éœ€è¦ç‰¹åˆ«æ³¨æ„çš„æ˜¯ï¼Œè¾“å…¥éŸ³é¢‘çš„é‡‡æ ·ç‡éœ€è¦ä¸æ¨¡å‹æœŸæœ›çš„é‡‡æ ·ç‡ç›¸åŒ¹é…ï¼Œå› ä¸ºä¸åŒé‡‡æ ·ç‡çš„éŸ³é¢‘ä¿¡å·çš„åˆ†å¸ƒæ˜¯ä¸åŒçš„ã€‚å¤„ç†éŸ³é¢‘æ—¶ï¼Œéœ€è¦ä½¿ç”¨æ­£ç¡®çš„é‡‡æ ·ç‡ï¼Œå¦åˆ™å¯èƒ½ä¼šå¼•èµ·æ„æƒ³ä¸åˆ°çš„ç»“æœï¼ä¾‹å¦‚ï¼Œä»¥ 16kHz çš„é‡‡æ ·ç‡é‡‡é›†éŸ³é¢‘ä½†ä»¥ 8kHz çš„é‡‡æ ·ç‡æ”¶å¬å®ƒï¼Œä¼šä½¿éŸ³é¢‘å¬èµ·æ¥å¥½åƒæ˜¯åŠé€Ÿçš„ã€‚åŒæ ·åœ°ï¼Œå‘ä¸€ä¸ªéœ€è¦æŸä¸€é‡‡æ ·ç‡çš„ ASR æ¨¡å‹é¦ˆé€ä¸€ä¸ªé”™è¯¯é‡‡æ ·ç‡çš„éŸ³é¢‘ä¹Ÿä¼šå½±å“æ¨¡å‹çš„æ€§èƒ½ã€‚Whisper ç‰¹å¾æå–å™¨éœ€è¦é‡‡æ ·ç‡ä¸º 16kHz çš„éŸ³é¢‘è¾“å…¥ï¼Œå› æ­¤è¾“å…¥çš„é‡‡æ ·ç‡è¦ä¸ä¹‹ç›¸åŒ¹é…ã€‚æˆ‘ä»¬ä¸æƒ³æ— æ„ä¸­ç”¨æ…¢é€Ÿè¯­éŸ³æ¥è®­ç»ƒ ASRï¼

Whisper ç‰¹å¾æå–å™¨æ‰§è¡Œä¸¤ä¸ªæ“ä½œã€‚é¦–å…ˆï¼Œå¡«å……æˆ–æˆªæ–­ä¸€æ‰¹éŸ³é¢‘æ ·æœ¬ï¼Œå°†æ‰€æœ‰æ ·æœ¬çš„è¾“å…¥é•¿åº¦ç»Ÿä¸€è‡³ 30 ç§’ã€‚é€šè¿‡åœ¨åºåˆ—æœ«å°¾æ·»åŠ é›¶ (éŸ³é¢‘ä¿¡å·ä¸­çš„é›¶å¯¹åº”äºæ— ä¿¡å·æˆ–é™éŸ³)ï¼Œå°†çŸ­äº 30 ç§’çš„æ ·æœ¬å¡«å……åˆ° 30 ç§’ã€‚è€Œå¯¹è¶…è¿‡ 30 ç§’çš„æ ·æœ¬ï¼Œç›´æ¥æˆªæ–­ä¸º 30 ç§’å°±å¥½äº†ã€‚ç”±äºè¿™ä¸€æ‰¹æ•°æ®ä¸­çš„æ‰€æœ‰æ ·æœ¬éƒ½è¢«å¡«å……æˆ–æˆªæ–­åˆ°ç»Ÿä¸€é•¿åº¦ (å³ 30 s) äº†ï¼Œå› æ­¤å°†éŸ³é¢‘é¦ˆé€ç»™ Whisper æ¨¡å‹æ—¶å°±ä¸éœ€è¦æ³¨æ„åŠ›æ©ç äº†ã€‚è¿™æ˜¯ Whisper çš„ç‹¬é—¨ç‰¹æ€§ï¼Œå…¶ä»–å¤§å¤šæ•°éŸ³é¢‘æ¨¡å‹éƒ½éœ€è¦ç”¨æˆ·æä¾›ä¸€ä¸ªæ³¨æ„åŠ›æ©ç ï¼Œè¯¦ç»†è¯´æ˜å¡«å……ä½ç½®ï¼Œè¿™æ ·æ¨¡å‹æ‰èƒ½åœ¨è‡ªæ³¨æ„åŠ›æœºåˆ¶ä¸­å¿½ç•¥å¡«å……éƒ¨åˆ†ã€‚ç»è¿‡è®­ç»ƒçš„ Whisper æ¨¡å‹å¯ä»¥ç›´æ¥ä»è¯­éŸ³ä¿¡å·ä¸­æ¨æ–­å‡ºåº”è¯¥å¿½ç•¥å“ªäº›éƒ¨åˆ†ï¼Œå› æ­¤æ— éœ€æ³¨æ„åŠ›æ©ç ã€‚

Whisper ç‰¹å¾æå–å™¨æ‰§è¡Œçš„ç¬¬äºŒä¸ªæ“ä½œæ˜¯å°†ç¬¬ä¸€æ­¥æ‰€å¾—çš„éŸ³é¢‘å˜æ¢ä¸ºå¯¹æ•°æ¢…å°”å£°è°±å›¾ã€‚è¿™äº›é¢‘è°±å›¾æ˜¯ä¿¡å·é¢‘ç‡çš„ç›´è§‚è¡¨ç¤ºï¼Œç±»ä¼¼äºå‚…é‡Œå¶å˜æ¢ã€‚å›¾ 2 å±•ç¤ºäº†ä¸€ä¸ªå£°è°±å›¾çš„ä¾‹å­ï¼Œå…¶ä¸­ $y$ è½´è¡¨ç¤ºæ¢…å°”é¢‘æ®µ (Mel channel)ï¼Œå¯¹åº”äºç‰¹å®šçš„é¢‘æ®µï¼Œ$x$ è½´è¡¨ç¤ºæ—¶é—´ï¼Œé¢œè‰²å¯¹åº”äºç»™å®šæ—¶åˆ»è¯¥é¢‘æ®µçš„å¯¹æ•°å¼ºåº¦ã€‚Whisper æ¨¡å‹è¦æ±‚è¾“å…¥ä¸ºå¯¹æ•°æ¢…å°”å£°è°±å›¾ã€‚

æ¢…å°”é¢‘æ®µæ˜¯è¯­éŸ³å¤„ç†çš„æ ‡å‡†æ–¹æ³•ï¼Œç ”ç©¶äººå‘˜ç”¨å®ƒæ¥è¿‘ä¼¼è¡¨ç¤ºäººç±»çš„å¬è§‰èŒƒå›´ã€‚å¯¹äº Whisper å¾®è°ƒè¿™ä¸ªä»»åŠ¡è€Œè¨€ï¼Œæˆ‘ä»¬åªéœ€è¦çŸ¥é“å£°è°±å›¾æ˜¯è¯­éŸ³ä¿¡å·ä¸­é¢‘ç‡çš„ç›´è§‚è¡¨ç¤ºã€‚æ›´å¤šæœ‰å…³æ¢…å°”é¢‘æ®µçš„è¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜… [æ¢…å°”å€’è°±](https://en.wikipedia.org/wiki/Mel-frequency_cepstrum) ä¸€æ–‡ã€‚

<figure>
<img src="assets/111_fine_tune_whisper/spectrogram.jpg" alt="Trulli" style="width:100%">
<figcaption align="center"><b>å›¾ 2ï¼š</b> å°†éŸ³é¢‘ä¿¡å·å˜æ¢ä¸ºå¯¹æ•°æ¢…å°”å£°è°±å›¾ã€‚å·¦å›¾ï¼šä¸€ç»´éŸ³é¢‘ç¦»æ•£ä¿¡å·ã€‚å³å›¾ï¼šå¯¹åº”çš„å¯¹æ•°æ¢…å°”å£°è°±å›¾ã€‚å›¾æºï¼š<a href="https://ai.googleblog.com/2019/04/specaugment-new-data-augmentation.html">è°·æ­Œ SpecAugment åšæ–‡</a>. </figcaption>
</figure>

å¹¸è¿çš„æ˜¯ï¼ŒğŸ¤— Transformers Whisper ç‰¹å¾æå–å™¨ä»…ç”¨ä¸€è¡Œä»£ç å³å¯æ‰§è¡Œå¡«å……å’Œå£°è°±å›¾å˜æ¢ä¸¤ä¸ªæ“ä½œï¼æˆ‘ä»¬ä½¿ç”¨ä»¥ä¸‹ä»£ç ä»é¢„è®­ç»ƒçš„ checkpoint ä¸­åŠ è½½ç‰¹å¾æå–å™¨ï¼Œä¸ºéŸ³é¢‘æ•°æ®å¤„ç†åšå¥½å‡†å¤‡:

```python
from transformers import WhisperFeatureExtractor

feature_extractor = WhisperFeatureExtractor.from_pretrained("openai/whisper-small")
```

### åŠ è½½ WhisperTokenizer

ç°åœ¨æˆ‘ä»¬åŠ è½½ Whisper åˆ†è¯å™¨ã€‚Whisper æ¨¡å‹ä¼šè¾“å‡ºè¯å…ƒï¼Œè¿™äº›è¯å…ƒè¡¨ç¤ºé¢„æµ‹æ–‡æœ¬åœ¨è¯å…¸ä¸­çš„ç´¢å¼•ã€‚åˆ†è¯å™¨è´Ÿè´£å°†è¿™ä¸€ç³»åˆ—è¯å…ƒæ˜ å°„ä¸ºæœ€ç»ˆçš„æ–‡æœ¬å­—ç¬¦ä¸² (ä¾‹å¦‚ [1169, 3797, 3332] -> â€œthe cat satâ€)ã€‚

è¿‡å»ï¼Œå½“ä½¿ç”¨ç¼–ç å™¨æ¨¡å‹è¿›è¡Œ ASR æ—¶ï¼Œæˆ‘ä»¬éœ€ä½¿ç”¨ [_è¿æ¥æ—¶åºåˆ†ç±»æ³•_ (Connectionist Temporal Classificationï¼ŒCTC) ](https://distill.pub/2017/ctc/) è¿›è¡Œè§£ç ã€‚åœ¨ä½¿ç”¨ CTC è¿›è¡Œè§£ç æ—¶ï¼Œæˆ‘ä»¬éœ€è¦ä¸ºæ¯ä¸ªæ•°æ®é›†è®­ç»ƒä¸€ä¸ª CTC åˆ†è¯å™¨ã€‚ä½†ä½¿ç”¨ç¼–ç å™¨ - è§£ç å™¨æ¶æ„çš„ä¸€ä¸ªä¼˜åŠ¿æ˜¯æˆ‘ä»¬å¯ä»¥ç›´æ¥ä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹çš„åˆ†è¯å™¨ã€‚

Whisper åˆ†è¯å™¨åœ¨ 96 ç§è¯­ç§æ•°æ®ä¸Šé¢„è®­ç»ƒè€Œå¾—ï¼Œå› æ­¤ï¼Œå…¶ [å­—èŠ‚å¯¹ (byte-pair) ](https://huggingface.co/course/chapter6/5?fw=pt#bytepair-encoding-tokenization) è¦†ç›–é¢å¾ˆå¹¿ï¼Œå‡ ä¹åŒ…å«äº†æ‰€æœ‰è¯­ç§ã€‚å°±å°åœ°è¯­è€Œè¨€ï¼Œæˆ‘ä»¬å¯ä»¥åŠ è½½åˆ†è¯å™¨å¹¶å°†å…¶ç›´æ¥ç”¨äºå¾®è°ƒã€‚ä»…éœ€æŒ‡å®šä¸€ä¸‹ç›®æ ‡è¯­ç§å’Œä»»åŠ¡ï¼Œåˆ†è¯å™¨å°±ä¼šæ ¹æ®è¿™äº›å‚æ•°å°†è¯­ç§å’Œä»»åŠ¡æ ‡è®°æ·»åŠ ä¸ºè¾“å‡ºåºåˆ—çš„å‰ç¼€:

```python
from transformers import WhisperTokenizer

tokenizer = WhisperTokenizer.from_pretrained("openai/whisper-small", language="Hindi", task="transcribe")
```

æˆ‘ä»¬å¯ä»¥é€šè¿‡å¯¹ Common Voice æ•°æ®é›†çš„ç¬¬ä¸€ä¸ªæ ·æœ¬è¿›è¡Œç¼–è§£ç æ¥éªŒè¯åˆ†è¯å™¨æ˜¯å¦æ­£ç¡®ç¼–ç äº†å°åœ°è¯­å­—ç¬¦ã€‚åœ¨å¯¹è½¬å½•æ–‡æœ¬è¿›è¡Œç¼–ç æ—¶ï¼Œåˆ†è¯å™¨åœ¨åºåˆ—çš„å¼€å¤´å’Œç»“å°¾æ·»åŠ â€œç‰¹æ®Šæ ‡è®°â€ï¼Œå…¶ä¸­åŒ…æ‹¬æ–‡æœ¬çš„å¼€å§‹/ç»“å°¾ã€è¯­ç§æ ‡è®°å’Œä»»åŠ¡æ ‡è®° (ç”±ä¸Šä¸€æ­¥ä¸­çš„å‚æ•°æŒ‡å®š)ã€‚åœ¨è§£ç æ—¶ï¼Œæˆ‘ä»¬å¯ä»¥é€‰æ‹©â€œè·³è¿‡â€è¿™äº›ç‰¹æ®Šæ ‡è®°ï¼Œä»è€Œä¿è¯è¾“å‡ºæ˜¯çº¯æ–‡æœ¬å½¢å¼çš„:

```python
input_str = common_voice["train"][0]["sentence"]
labels = tokenizer(input_str).input_ids
decoded_with_special = tokenizer.decode(labels, skip_special_tokens=False)
decoded_str = tokenizer.decode(labels, skip_special_tokens=True)

print(f"Input: {input_str}")
print(f"Decoded w/ special: {decoded_with_special}")
print(f"Decoded w/out special: {decoded_str}")
print(f"Are equal: {input_str == decoded_str}")
```

**æ‰“å°è¾“å‡º:**

```bash
Input: à¤–à¥€à¤° à¤•à¥€ à¤®à¤¿à¤ à¤¾à¤¸ à¤ªà¤° à¤—à¤°à¤®à¤¾à¤ˆ à¤¬à¤¿à¤¹à¤¾à¤° à¤•à¥€ à¤¸à¤¿à¤¯à¤¾à¤¸à¤¤, à¤•à¥à¤¶à¤µà¤¾à¤¹à¤¾ à¤¨à¥‡ à¤¦à¥€ à¤¸à¤«à¤¾à¤ˆ
Decoded w/ special: <|startoftranscript|><|hi|><|transcribe|><|notimestamps|>à¤–à¥€à¤° à¤•à¥€ à¤®à¤¿à¤ à¤¾à¤¸ à¤ªà¤° à¤—à¤°à¤®à¤¾à¤ˆ à¤¬à¤¿à¤¹à¤¾à¤° à¤•à¥€ à¤¸à¤¿à¤¯à¤¾à¤¸à¤¤, à¤•à¥à¤¶à¤µà¤¾à¤¹à¤¾ à¤¨à¥‡ à¤¦à¥€ à¤¸à¤«à¤¾à¤ˆ<|endoftext|>
Decoded w/out special: à¤–à¥€à¤° à¤•à¥€ à¤®à¤¿à¤ à¤¾à¤¸ à¤ªà¤° à¤—à¤°à¤®à¤¾à¤ˆ à¤¬à¤¿à¤¹à¤¾à¤° à¤•à¥€ à¤¸à¤¿à¤¯à¤¾à¤¸à¤¤, à¤•à¥à¤¶à¤µà¤¾à¤¹à¤¾ à¤¨à¥‡ à¤¦à¥€ à¤¸à¤«à¤¾à¤ˆ
Are equal: True
```

### ç»„è£…ä¸€ä¸ª WhisperProcessor

ä¸ºäº†ç®€åŒ–ä½¿ç”¨ï¼Œæˆ‘ä»¬å¯ä»¥å°†ç‰¹å¾æå–å™¨å’Œåˆ†è¯å™¨ _åŒ…è¿›_ åˆ°ä¸€ä¸ª `WhisperProcessor` ç±»ï¼Œè¯¥ç±»ç»§æ‰¿è‡ª `WhisperFeatureExtractor` åŠ  `WhisperTokenizer`ï¼Œå¯æ ¹æ®éœ€è¦ç”¨äºéŸ³é¢‘å¤„ç†å’Œæ¨¡å‹é¢„æµ‹ã€‚æœ‰äº†å®ƒï¼Œæˆ‘ä»¬åœ¨è®­ç»ƒæœŸé—´åªéœ€è¦ä¿ç•™ä¸¤ä¸ªå¯¹è±¡: `processor` å’Œ `model` å°±å¥½äº†ã€‚

```python
from transformers import WhisperProcessor

processor = WhisperProcessor.from_pretrained("openai/whisper-small", language="Hindi", task="transcribe")
```

### å‡†å¤‡æ•°æ®

æˆ‘ä»¬æŠŠ Common Voice æ•°æ®é›†çš„ç¬¬ä¸€ä¸ªæ ·æœ¬æ‰“å°å‡ºæ¥ï¼Œçœ‹çœ‹æ•°æ®é•¿ä»€ä¹ˆæ ·:

```python
print(common_voice["train"][0])
```

**æ‰“å°è¾“å‡º:**

```python
{'audio': {'path': '/home/sanchit_huggingface_co/.cache/huggingface/datasets/downloads/extracted/607848c7e74a89a3b5225c0fa5ffb9470e39b7f11112db614962076a847f3abf/cv-corpus-11.0-2022-09-21/hi/clips/common_voice_hi_25998259.mp3',
           'array': array([0.0000000e+00, 0.0000000e+00, 0.0000000e+00, ..., 9.6724887e-07,
       1.5334779e-06, 1.0415988e-06], dtype=float32),
           'sampling_rate': 48000},
 'sentence': 'à¤–à¥€à¤° à¤•à¥€ à¤®à¤¿à¤ à¤¾à¤¸ à¤ªà¤° à¤—à¤°à¤®à¤¾à¤ˆ à¤¬à¤¿à¤¹à¤¾à¤° à¤•à¥€ à¤¸à¤¿à¤¯à¤¾à¤¸à¤¤, à¤•à¥à¤¶à¤µà¤¾à¤¹à¤¾ à¤¨à¥‡ à¤¦à¥€ à¤¸à¤«à¤¾à¤ˆ'}
```

å¯ä»¥çœ‹åˆ°ï¼Œæ ·æœ¬å«æœ‰ä¸€ä¸ªä¸€ç»´éŸ³é¢‘æ•°ç»„åŠå…¶å¯¹åº”çš„è½¬å½•æ–‡æœ¬ã€‚ä¸Šæ–‡å·²ç»å¤šæ¬¡è°ˆåŠé‡‡æ ·ç‡ï¼Œä»¥åŠå°†éŸ³é¢‘çš„é‡‡æ ·ç‡ä¸ Whisper æ¨¡å‹æ‰€éœ€çš„é‡‡æ ·ç‡ (16kHz) ç›¸åŒ¹é…çš„é‡è¦æ€§ã€‚ç”±äºç°åœ¨è¾“å…¥éŸ³é¢‘çš„é‡‡æ ·ç‡ä¸º 48kHzï¼Œæ‰€ä»¥åœ¨å°†å…¶é¦ˆé€ç»™ Whisper ç‰¹å¾æå–å™¨ä¹‹å‰ï¼Œæˆ‘ä»¬éœ€è¦å°†å…¶ _ä¸‹é‡‡æ ·_è‡³ 16kHzã€‚

æˆ‘ä»¬å°†ä½¿ç”¨ `dataset` çš„ [`cast_column`](https://huggingface.co/docs/datasets/package_reference/main_classes.html?highlight=cast_column#datasets.DatasetDict.cast_column) æ–¹æ³•å°†è¾“å…¥éŸ³é¢‘è½¬æ¢è‡³æ‰€éœ€çš„é‡‡æ ·ç‡ã€‚è¯¥æ–¹æ³•ä»…æŒ‡ç¤º `datasets` è®©å…¶åœ¨é¦–æ¬¡åŠ è½½éŸ³é¢‘æ—¶ _å³æ—¶åœ°_å¯¹æ•°æ®è¿›è¡Œé‡é‡‡æ ·ï¼Œå› æ­¤å¹¶ä¸ä¼šæ”¹å˜åŸéŸ³é¢‘æ•°æ®:

```python
from datasets import Audio

common_voice = common_voice.cast_column("audio", Audio(sampling_rate=16000))
```

é‡æ–°æ‰“å°ä¸‹ Common Voice æ•°æ®é›†ä¸­çš„ç¬¬ä¸€ä¸ªéŸ³é¢‘æ ·æœ¬ï¼Œå¯ä»¥çœ‹åˆ°å…¶å·²è¢«é‡é‡‡æ ·:

```python
print(common_voice["train"][0])
```

**æ‰“å°è¾“å‡º:**

```python
{'audio': {'path': '/home/sanchit_huggingface_co/.cache/huggingface/datasets/downloads/extracted/607848c7e74a89a3b5225c0fa5ffb9470e39b7f11112db614962076a847f3abf/cv-corpus-11.0-2022-09-21/hi/clips/common_voice_hi_25998259.mp3',
           'array': array([ 0.0000000e+00, 0.0000000e+00, 0.0000000e+00, ...,
       -3.4206650e-07, 3.2979898e-07, 1.0042874e-06], dtype=float32),
           'sampling_rate': 16000},
 'sentence': 'à¤–à¥€à¤° à¤•à¥€ à¤®à¤¿à¤ à¤¾à¤¸ à¤ªà¤° à¤—à¤°à¤®à¤¾à¤ˆ à¤¬à¤¿à¤¹à¤¾à¤° à¤•à¥€ à¤¸à¤¿à¤¯à¤¾à¤¸à¤¤, à¤•à¥à¤¶à¤µà¤¾à¤¹à¤¾ à¤¨à¥‡ à¤¦à¥€ à¤¸à¤«à¤¾à¤ˆ'}
```

é…·ï¼æˆ‘ä»¬å¯ä»¥çœ‹åˆ°éŸ³é¢‘å·²è¢«ä¸‹é‡‡æ ·åˆ° 16kHz äº†ã€‚æ•°ç»„é‡Œé¢çš„å€¼ä¹Ÿå˜äº†ï¼Œç°åœ¨çš„ 1 ä¸ªå¹…åº¦å€¼å¤§è‡´å¯¹åº”äºä¹‹å‰çš„ 3 ä¸ªå¹…åº¦å€¼ã€‚

ç°åœ¨æˆ‘ä»¬ç¼–å†™ä¸€ä¸ªå‡½æ•°æ¥ä¸ºæ¨¡å‹å‡†å¤‡æ•°æ®:

1. è°ƒç”¨ `batch["audio"]` åŠ è½½å’Œé‡é‡‡æ ·éŸ³é¢‘æ•°æ®ã€‚å¦‚ä¸Šæ‰€è¿°ï¼ŒğŸ¤— Datasets ä¼šå³æ—¶æ‰§è¡Œä»»ä½•å¿…è¦çš„é‡é‡‡æ ·æ“ä½œã€‚
2. ä½¿ç”¨ç‰¹å¾æå–å™¨å°†ä¸€ç»´éŸ³é¢‘æ•°ç»„å˜æ¢ä¸ºå¯¹æ•°æ¢…å°”å£°è°±å›¾ç‰¹å¾ã€‚
3. ä½¿ç”¨åˆ†è¯å™¨å°†å½•éŸ³æ–‡æœ¬ç¼–ç ä¸º IDã€‚

```python
def prepare_dataset(batch):
    # load and resample audio data from 48 to 16kHz
    audio = batch["audio"]

    # compute log-Mel input features from input audio array
    batch["input_features"] = feature_extractor(audio["array"], sampling_rate=audio["sampling_rate"]).input_features[0]

    # encode target text to label ids
    batch["labels"] = tokenizer(batch["sentence"]).input_ids
    return batch
```

æˆ‘ä»¬å¯ä»¥ç”¨ `dataset` çš„ `.map` æ–¹æ³•åœ¨æ‰€æœ‰è®­ç»ƒæ ·æœ¬ä¸Šåº”ç”¨ä¸Šè¿°å‡½æ•°:

```python
common_voice = common_voice.map(prepare_dataset, remove_columns=common_voice.column_names["train"], num_proc=4)
```

å¥½äº†ï¼è®­ç»ƒæ•°æ®å‡†å¤‡å®Œæ¯•ï¼æˆ‘ä»¬ç»§ç»­çœ‹çœ‹å¦‚ä½•ä½¿ç”¨è¿™äº›æ•°æ®æ¥å¾®è°ƒ Whisperã€‚

 **æ³¨æ„**: ç›®å‰ `datasets` ä¸»è¦ä½¿ç”¨ [`torchaudio`](https://pytorch.org/audio/stable/index.html) å’Œ [`librosa`](https://librosa.org /doc/latest/index.html) æ¥è¿›è¡ŒéŸ³é¢‘åŠ è½½å’Œé‡é‡‡æ ·ã€‚å¦‚æœä½ è‡ªå·±å®šåˆ¶ä¸€ä¸ªæ•°æ®åŠ è½½/é‡‡æ ·å‡½æ•°çš„è¯ï¼Œä½ å®Œå…¨å¯ä»¥ç›´æ¥é€šè¿‡ `"path"` åˆ—è·å–éŸ³é¢‘æ–‡ä»¶è·¯å¾„è€Œä¸ç”¨ç®¡ `"audio"` åˆ—ã€‚

## è®­ç»ƒä¸è¯„ä¼°

è‡³æ­¤ï¼Œæ•°æ®å·²å‡†å¤‡å®Œæ¯•ï¼Œå¯ä»¥å¼€å§‹è®­ç»ƒäº†ã€‚è®­ç»ƒçš„å¤§éƒ¨åˆ†ç¹é‡çš„å·¥ä½œéƒ½ä¼šç”± [ğŸ¤— Trainer](https://huggingface.co/transformers/master/main_classes/trainer.html?highlight=trainer) æ¥å®Œæˆã€‚æˆ‘ä»¬è¦åšçš„ä¸»è¦æœ‰:

- å®šä¹‰æ•°æ®æ•´ç†å™¨ (data collator): æ•°æ®æ•´ç†å™¨è·å–é¢„å¤„ç†åçš„æ•°æ®å¹¶å°†å…¶è½¬æ¢ä¸º PyTorch å¼ é‡ã€‚
- è¯„ä¼°æŒ‡æ ‡: æˆ‘ä»¬ä½¿ç”¨ [å•è¯é”™è¯¯ç‡ (word error rateï¼ŒWER)](https://huggingface.co/metrics/wer) æŒ‡æ ‡æ¥è¯„ä¼°æ¨¡å‹ï¼Œå› æ­¤éœ€è¦å®šä¹‰ä¸€ä¸ª `compute_metrics` å‡½æ•°æ¥è®¡ç®—å®ƒã€‚
- åŠ è½½é¢„è®­ç»ƒ checkpoint: æˆ‘ä»¬éœ€è¦åŠ è½½é¢„è®­ç»ƒ checkpoint å¹¶æ­£ç¡®é…ç½®å®ƒä»¥è¿›è¡Œè®­ç»ƒã€‚
- å®šä¹‰è®­ç»ƒå‚æ•°: ğŸ¤— Trainer åœ¨åˆ¶è®¢è®­ç»ƒè®¡åˆ’æ—¶éœ€è¦ç”¨åˆ°è¿™äº›å‚æ•°ã€‚

å¾®è°ƒå®Œåï¼Œæˆ‘ä»¬éœ€è¦ä½¿ç”¨æµ‹è¯•æ•°æ®å¯¹å…¶è¿›è¡Œè¯„ä¼°ï¼Œä»¥éªŒè¯æœ€ç»ˆæ¨¡å‹åœ¨å°åœ°è¯­ä¸Šçš„è¯­éŸ³è¯†åˆ«æ•ˆæœã€‚

### å®šä¹‰æ•°æ®æ•´ç†å™¨

åºåˆ—åˆ°åºåˆ—è¯­éŸ³æ¨¡å‹çš„æ•°æ®æ•´ç†å™¨ä¸å…¶ä»–ä»»åŠ¡æœ‰æ‰€ä¸åŒï¼Œå› ä¸º `input_features` å’Œ `labels` çš„å¤„ç†æ–¹æ³•æ˜¯ä¸åŒçš„: `input_features` å¿…é¡»ç”±ç‰¹å¾æå–å™¨å¤„ç†ï¼Œè€Œ `labels` ç”±åˆ†è¯å™¨å¤„ç†ã€‚ 

`input_features` å·²ç»å¡«å……è‡³ 30s å¹¶è½¬æ¢ä¸ºå›ºå®šç»´åº¦çš„å¯¹æ•°æ¢…å°”å£°è°±å›¾ï¼Œæˆ‘ä»¬æ‰€è¦åšçš„åªå‰©å°†å…¶è½¬æ¢ä¸º PyTorch å¼ é‡ã€‚æˆ‘ä»¬ç”¨ç‰¹å¾æå–å™¨çš„ `.pad` æ–¹æ³•æ¥å®Œæˆè¿™ä¸€åŠŸèƒ½ï¼Œä¸”å°†å…¶å…¥å‚è®¾ä¸º `return_tensors=pt`ã€‚è¯·æ³¨æ„ï¼Œè¿™é‡Œä¸éœ€è¦é¢å¤–çš„å¡«å……ï¼Œå› ä¸ºè¾“å…¥ç»´åº¦å·²ç»å›ºå®šäº†ï¼Œæ‰€ä»¥æˆ‘ä»¬åªéœ€è¦ç®€å•åœ°å°† `input_features` è½¬æ¢ä¸º PyTorch å¼ é‡å°±å¥½äº†ã€‚

å¦ä¸€æ–¹é¢ï¼Œ`labels` æ•°æ®ä¹‹å‰å¹¶æœªå¡«å……ã€‚æ‰€ä»¥ï¼Œæˆ‘ä»¬é¦–å…ˆè¦ä½¿ç”¨åˆ†è¯å™¨çš„ `.pad` æ–¹æ³•å°†åºåˆ—å¡«å……è‡³æœ¬ batch çš„æœ€å¤§é•¿åº¦ã€‚ç„¶åå°†å¡«å……æ ‡è®°æ›¿æ¢ä¸º `-100`ï¼Œè¿™æ ·å®ƒä»¬å°±å¯ä»¥ **ä¸** ç”¨å‚ä¸æŸå¤±çš„è®¡ç®—äº†ã€‚ç„¶åæˆ‘ä»¬æŠŠ `SOT` ä»åºåˆ—çš„å¼€å¤´å»æ‰ï¼Œç¨åè®­ç»ƒçš„æ—¶å€™æˆ‘ä»¬å†æŠŠå®ƒåŠ å›æ¥ã€‚

æˆ‘ä»¬å¯ä»¥åˆ©ç”¨ä¹‹å‰å®šä¹‰çš„ `WhisperProcessor` æ¥æ‰§è¡Œç‰¹å¾æå–å’Œåˆ†è¯æ“ä½œ:

```python
import torch

from dataclasses import dataclass
from typing import Any, Dict, List, Union

@dataclass
class DataCollatorSpeechSeq2SeqWithPadding:
    processor: Any

    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:
        # split inputs and labels since they have to be of different lengths and need different padding methods
        # first treat the audio inputs by simply returning torch tensors
        input_features = [{"input_features": feature["input_features"]} for feature in features]
        batch = self.processor.feature_extractor.pad(input_features, return_tensors="pt")

        # get the tokenized label sequences
        label_features = [{"input_ids": feature["labels"]} for feature in features]
        # pad the labels to max length
        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors="pt")

        # replace padding with -100 to ignore loss correctly
        labels = labels_batch["input_ids"].masked_fill(labels_batch.attention_mask.ne(1), -100)

        # if bos token is appended in previous tokenization step,
        # cut bos token here as it's append later anyways
        if (labels[:, 0] == self.processor.tokenizer.bos_token_id).all().cpu().item():
            labels = labels[:, 1:]

        batch["labels"] = labels

        return batch
```

æˆ‘ä»¬åˆå§‹åŒ–ä¸€ä¸‹åˆšåˆšå®šä¹‰çš„æ•°æ®æ•´ç†å™¨:

```python
data_collator = DataCollatorSpeechSeq2SeqWithPadding(processor=processor)
```

### è¯„ä¼°æŒ‡æ ‡

æ¥ä¸‹æ¥è¦å®šä¹‰è¯„ä¼°æŒ‡æ ‡ã€‚æˆ‘ä»¬å°†ä½¿ç”¨è¯é”™è¯¯ç‡ (WER) æŒ‡æ ‡ï¼Œå®ƒæ˜¯è¯„ä¼° ASR ç³»ç»Ÿçš„â€œæ ‡å‡†â€æŒ‡æ ‡ã€‚æœ‰å…³å…¶è¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜… WER [æ–‡æ¡£](https://huggingface.co/metrics/wer)ã€‚ä¸‹é¢ï¼Œæˆ‘ä»¬ä» ğŸ¤— Evaluate ä¸­åŠ è½½ WER æŒ‡æ ‡:

```python
import evaluate

metric = evaluate.load("wer")
```

ç„¶åæˆ‘ä»¬åªéœ€è¦å®šä¹‰ä¸€ä¸ªå‡½æ•°æ¥æ¥å—æ¨¡å‹è¾“å‡ºå¹¶è¿”å› WER æŒ‡æ ‡ã€‚è¿™ä¸ªåä¸º `compute_metrics` çš„å‡½æ•°é¦–å…ˆå°† `-100` æ›¿æ¢ä¸º `label_ids` ä¸­çš„ `pad_token_id` (ä»¥ä¾¿åœ¨è®¡ç®—æŸå¤±æ—¶å°†å…¶å¿½ç•¥)ã€‚ç„¶åï¼Œå°†é¢„æµ‹åˆ°çš„ ID å’Œ `label_ids` è§£ç ä¸ºå­—ç¬¦ä¸²æ–‡æœ¬ã€‚æœ€åï¼Œè®¡ç®—è¾“å‡ºæ–‡æœ¬å’ŒçœŸå®æ–‡æœ¬ä¹‹é—´çš„ WER:

```python
def compute_metrics(pred):
    pred_ids = pred.predictions
    label_ids = pred.label_ids

    # replace -100 with the pad_token_id
    label_ids[label_ids == -100] = tokenizer.pad_token_id

    # we do not want to group tokens when computing the metrics
    pred_str = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)
    label_str = tokenizer.batch_decode(label_ids, skip_special_tokens=True)

    wer = 100 * metric.compute(predictions=pred_str, references=label_str)

    return {"wer": wer}
```

### åŠ è½½é¢„è®­ç»ƒ checkpoint

ç°åœ¨æˆ‘ä»¬åŠ è½½é¢„è®­ç»ƒ Whisper `small` æ¨¡å‹çš„ checkpointã€‚åŒæ ·ï¼Œå¯ä»¥é€šè¿‡ä½¿ç”¨ ğŸ¤— transformers å¾ˆè½»æ¾åœ°å®Œæˆè¿™ä¸€æ­¥ï¼

```python
from transformers import WhisperForConditionalGeneration

model = WhisperForConditionalGeneration.from_pretrained("openai/whisper-small")
```

åŸå§‹ Whisper æ¨¡å‹åœ¨è‡ªå›å½’ç”Ÿæˆå¼€å§‹ä¹‹å‰å¼ºåˆ¶æ·»åŠ äº†è‹¥å¹²å‰ç¼€è¯å…ƒ ID ([`forced_decoder_ids`](https://huggingface.co/docs/transformers/main_classes/text_generation#transformers.generation_utils.GenerationMixin.generate.forced_decoder_ids))ã€‚è¿™äº›è¯å…ƒ ID ä¸»è¦ç”¨äºåœ¨é›¶æ ·æœ¬ ASR ä»»åŠ¡ä¸­æ ‡è¯†è¯­ç§å’Œä»»åŠ¡ã€‚å› ä¸ºæˆ‘ä»¬ç°åœ¨æ˜¯å¯¹å·²çŸ¥è¯­ç§ (å°åœ°è¯­) å’Œä»»åŠ¡ (è½¬å½•) è¿›è¡Œå¾®è°ƒï¼Œæ‰€ä»¥æˆ‘ä»¬è¦å°† `forced_decoder_ids` è®¾ç½®ä¸º `None`ã€‚å¦å¤–ï¼Œæ¨¡å‹è¿˜æŠ‘åˆ¶äº†ä¸€äº›è¯å…ƒ ([`suppress_tokens`](https://huggingface.co/docs/transformers/main_classes/text_generation#transformers.generation_utils.GenerationMixin.generate.suppress_tokens))ï¼Œè¿™äº›è¯å…ƒçš„å¯¹æ•°æ¦‚ç‡è¢«å¼ºç½®ä¸º `-inf`ï¼Œä»¥ä¿è¯å®ƒä»¬æ°¸è¿œä¸ä¼šè¢«é‡‡æ ·åˆ°ã€‚æˆ‘ä»¬ä¼šç”¨ä¸€ä¸ªç©ºåˆ—è¡¨è¦†ç›– `suppress_tokens`ï¼Œå³æˆ‘ä»¬ä¸æŠ‘åˆ¶ä»»ä½•è¯å…ƒ:

```python
model.config.forced_decoder_ids = None
model.config.suppress_tokens = []
```

### å®šä¹‰è®­ç»ƒå‚æ•°

æœ€åä¸€æ­¥æ˜¯å®šä¹‰ä¸è®­ç»ƒç›¸å…³çš„æ‰€æœ‰å‚æ•°ï¼Œä¸‹é¢å¯¹å…¶ä¸­ä¸€éƒ¨åˆ†å‚æ•°è¿›è¡Œäº†è§£é‡Š:

- `output_dir`: ä¿å­˜æ¨¡å‹æƒé‡çš„æœ¬åœ°ç›®å½•ï¼Œå®ƒä¹Ÿä¼šæ˜¯ [Hugging Face Hub](https://huggingface.co/) ä¸Šçš„æ¨¡å‹å­˜å‚¨åº“åç§°ã€‚
- `generation_max_length`: è¯„ä¼°é˜¶æ®µï¼Œè‡ªå›å½’ç”Ÿæˆçš„æœ€å¤§è¯å…ƒæ•°ã€‚
- `save_steps`: è®­ç»ƒæœŸé—´ï¼Œæ¯ `save_steps` æ­¥ä¿å­˜ä¸€æ¬¡ä¸­é—´ checkpoint å¹¶å¼‚æ­¥ä¸Šä¼ åˆ° Hubã€‚
- `eval_steps`: è®­ç»ƒæœŸé—´ï¼Œæ¯ `eval_steps` æ­¥å¯¹ä¸­é—´ checkpoint è¿›è¡Œä¸€æ¬¡è¯„ä¼°ã€‚
- `report_to`: è®­ç»ƒæ—¥å¿—çš„ä¿å­˜ä½ç½®ï¼Œæ”¯æŒ `azure_ml` ã€`comet_ml` ã€`mlflow` ã€`neptune` ã€`tensorboard` ä»¥åŠ `wandb` è¿™äº›å¹³å°ã€‚ä½ å¯ä»¥æŒ‰ç…§è‡ªå·±çš„åå¥½è¿›è¡Œé€‰æ‹©ï¼Œä¹Ÿå¯ä»¥ç›´æ¥ä½¿ç”¨ç¼ºçœçš„ `tensorboard` ä¿å­˜è‡³ Hubã€‚

å¦‚éœ€æ›´å¤šå…¶ä»–è®­ç»ƒå‚æ•°çš„è¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜… Seq2SeqTrainingArguments [æ–‡æ¡£](https://huggingface.co/docs/transformers/main_classes/trainer#transformers.Seq2SeqTrainingArguments)ã€‚

```python
from transformers import Seq2SeqTrainingArguments

training_args = Seq2SeqTrainingArguments(
    output_dir="./whisper-small-hi", # change to a repo name of your choice
    per_device_train_batch_size=16,
    gradient_accumulation_steps=1, # increase by 2x for every 2x decrease in batch size
    learning_rate=1e-5,
    warmup_steps=500,
    max_steps=4000,
    gradient_checkpointing=True,
    fp16=True,
    evaluation_strategy="steps",
    per_device_eval_batch_size=8,
    predict_with_generate=True,
    generation_max_length=225,
    save_steps=1000,
    eval_steps=1000,
    logging_steps=25,
    report_to=["tensorboard"],
    load_best_model_at_end=True,
    metric_for_best_model="wer",
    greater_is_better=False,
    push_to_hub=True,
)
```

**æ³¨æ„**: å¦‚æœä¸æƒ³å°†æ¨¡å‹ checkpoint ä¸Šä¼ åˆ° Hubï¼Œä½ éœ€è¦è®¾ç½® `push_to_hub=False`ã€‚

æˆ‘ä»¬å¯ä»¥å°†è®­ç»ƒå‚æ•°ä»¥åŠæ¨¡å‹ã€æ•°æ®é›†ã€æ•°æ®æ•´ç†å™¨å’Œ `compute_metrics` å‡½æ•°ä¸€èµ·ä¼ ç»™ ğŸ¤— Trainer:

```python
from transformers import Seq2SeqTrainer

trainer = Seq2SeqTrainer(
    args=training_args,
    model=model,
    train_dataset=common_voice["train"],
    eval_dataset=common_voice["test"],
    data_collator=data_collator,
    compute_metrics=compute_metrics,
    tokenizer=processor.feature_extractor,
)
```

æœ‰äº†è¿™äº›ï¼Œå°±å¯ä»¥å¼€å§‹è®­ç»ƒäº†ï¼

### è®­ç»ƒ

è¦å¯åŠ¨è®­ç»ƒï¼Œåªéœ€æ‰§è¡Œ:

```python
trainer.train()
```

è®­ç»ƒå¤§çº¦éœ€è¦ 5-10 ä¸ªå°æ—¶ï¼Œå…·ä½“å–å†³äºä½ çš„ GPU æˆ– Google Colab åç«¯çš„ GPUã€‚æ ¹æ® GPU çš„æƒ…å†µï¼Œä½ å¯èƒ½ä¼šåœ¨å¼€å§‹è®­ç»ƒæ—¶é‡åˆ° CUDA `å†…å­˜è€—å°½`é”™è¯¯ã€‚æ­¤æ—¶ï¼Œä½ å¯ä»¥å°† `per_device_train_batch_size` é€æ¬¡å‡å°‘ 2 å€ï¼ŒåŒæ—¶å¢åŠ  [`gradient_accumulation_steps`](https://huggingface.co/docs/transformers/main_classes/trainer#transformers.Seq2SeqTrainingArguments.gradient_accumulation_steps) è¿›è¡Œè¡¥å¿ã€‚

**æ‰“å°è¾“å‡º:**

| æ­¥æ•° | è®­ç»ƒæŸå¤± | è½®æ•° | éªŒè¯æŸå¤± | WER |
| :-: | :-: | :-: | :-: | :-: |
| 1000 | 0.1011 | 2.44 | 0.3075 | 34.63 |
| 2000 | 0.0264 | 4.89 | 0.3558 | 33.13 |
| 3000 | 0.0025 | 7.33 | 0.4214 | 32.59 |
| 4000 | 0.0006 | 9.78 | 0.4519 | 32.01 |
| 5000 | 0.0002 | 12.22 | 0.4679 | 32.10 |

æœ€ä½³ WER æ˜¯ 32.0% â€”â€” å¯¹ 8 å°æ—¶çš„è®­ç»ƒæ•°æ®æ¥è¯´è¿˜ä¸é”™ï¼é‚£ä¸å…¶ä»– ASR ç³»ç»Ÿç›¸æ¯”ï¼Œè¿™ä¸ªè¡¨ç°åˆ°åº•å¤„äºä»€ä¹ˆæ°´å¹³ï¼Ÿä¸ºæ­¤ï¼Œæˆ‘ä»¬å¯ä»¥æŸ¥çœ‹ [`hf-speech-bench`](https://huggingface.co/spaces/huggingface/hf-speech-bench)ï¼Œè¿™æ˜¯ä¸€ä¸ªæŒ‰è¯­ç§å’Œæ•°æ®é›†å¯¹æ¨¡å‹åˆ†åˆ«è¿›è¡Œ WER æ’åçš„æ’è¡Œæ¦œã€‚

<figure>
<img src="assets/111_fine_tune_whisper/hf_speech_bench.jpg" alt="Trulli" style="width:100%">
</figure>

å¾®è°ƒåçš„æ¨¡å‹æ˜¾è‘—æé«˜äº† Whisper `small` checkpoint çš„é›¶æ ·æœ¬æ€§èƒ½ï¼Œä¹Ÿçªå‡ºå±•ç¤ºäº† Whisper å¼ºå¤§çš„è¿ç§»å­¦ä¹ èƒ½åŠ›ã€‚

å½“å°†è®­ç»ƒç»“æœæ¨é€åˆ° Hub æ—¶ï¼Œåªéœ€é…ç½®é€‚å½“çš„å…³é”®å­—å‚æ•° (key-word argumentsï¼Œkwargs) å°±å¯ä»¥è‡ªåŠ¨å°† checkpoint æäº¤åˆ°æ’è¡Œæ¦œã€‚å¦‚éœ€é€‚é…è‡ªå·±çš„æ•°æ®é›†ã€è¯­ç§å’Œæ¨¡å‹åç§°ï¼Œä»…éœ€å¯¹ä¸‹è¿°ä»£ç ä½œå‡ºç›¸åº”çš„ä¿®æ”¹å³å¯:

```python
kwargs = {
    "dataset_tags": "mozilla-foundation/common_voice_11_0",
    "dataset": "Common Voice 11.0", # a 'pretty' name for the training dataset
    "dataset_args": "config: hi, split: test",
    "language": "hi",
    "model_name": "Whisper Small Hi - Sanchit Gandhi", # a 'pretty' name for your model
    "finetuned_from": "openai/whisper-small",
    "tasks": "automatic-speech-recognition",
    "tags": "hf-asr-leaderboard",
}
```

ç°åœ¨ï¼Œåªéœ€æ‰§è¡Œ `push_to_hub` å‘½ä»¤å°±å¯ä»¥å°†è®­ç»ƒç»“æœä¸Šä¼ åˆ° Hub äº†:

```python
trainer.push_to_hub(**kwargs)
```

ä»»ä½•äººå¯ä»¥ç”¨ä½ çš„æ¨¡å‹çš„ Hub é“¾æ¥è®¿é—®å®ƒã€‚ä»–ä»¬è¿˜å¯ä»¥ä½¿ç”¨æ ‡è¯†ç¬¦ `"your-username/the-name-you-picked"`åŠ è½½å®ƒï¼Œä¾‹å¦‚:

```python
from transformers import WhisperForConditionalGeneration, WhisperProcessor

model = WhisperForConditionalGeneration.from_pretrained("sanchit-gandhi/whisper-small-hi")
processor = WhisperProcessor.from_pretrained("sanchit-gandhi/whisper-small-hi")
```

è™½ç„¶å¾®è°ƒåçš„æ¨¡å‹åœ¨ Common Voice Hindi æµ‹è¯•æ•°æ®ä¸Šçš„æ•ˆæœè¿˜ä¸é”™ï¼Œä½†å…¶æ•ˆæœè¿œç®—ä¸ä¸Šæœ€ä¼˜ã€‚æœ¬æ–‡çš„ç›®çš„ä»…ä¸ºæ¼”ç¤ºå¦‚ä½•åœ¨ä»»æ„å¤šè¯­ç§ ASR æ•°æ®é›†ä¸Šå¾®è°ƒé¢„è®­ç»ƒçš„ Whisper checkpointï¼Œå¯¹æ•ˆæœå¹¶æœªåšå¤ªå¤šæ·±ç©¶ã€‚å¦‚éœ€æå‡æ•ˆæœï¼Œä½ è¿˜å¯ä»¥å°è¯•æ›´å¤šæŠ€å·§ï¼Œå¦‚ä¼˜åŒ–è®­ç»ƒè¶…å‚ (ä¾‹å¦‚ _learning rate_ å’Œ _dropout_) ã€ä½¿ç”¨æ›´å¤§çš„é¢„è®­ç»ƒ checkpoint (`medium` æˆ– `large`) ç­‰ã€‚

### æ„å»ºæ¼”ç¤ºåº”ç”¨

ç°åœ¨æ¨¡å‹å·²ç»å¾®è°ƒç»“æŸï¼Œæˆ‘ä»¬å¼€å§‹æ„å»ºä¸€ä¸ªæ¼”ç¤ºåº”ç”¨æ¥å±•ç¤ºå…¶ ASR åŠŸèƒ½ï¼æˆ‘ä»¬å°†ä½¿ç”¨ ğŸ¤— Transformers `pipeline` æ¥å®Œæˆæ•´ä¸ª ASR æµæ°´çº¿: ä»å¯¹éŸ³é¢‘è¾“å…¥è¿›è¡Œé¢„å¤„ç†ä¸€ç›´åˆ°å¯¹æ¨¡å‹è¾“å‡ºè¿›è¡Œè§£ç ã€‚æˆ‘ä»¬ä½¿ç”¨ [Gradio](https://www.gradio.app) æ¥æ„å»ºæˆ‘ä»¬çš„äº¤äº’å¼æ¼”ç¤ºã€‚ Gradio æä¾›äº†æœ€ç›´æˆªäº†å½“çš„æ„å»ºæœºå™¨å­¦ä¹ æ¼”ç¤ºåº”ç”¨çš„æ–¹æ³•ï¼Œæˆ‘ä»¬å¯ä»¥ç”¨å®ƒåœ¨å‡ åˆ†é’Ÿå†…æ„å»ºä¸€ä¸ªæ¼”ç¤ºåº”ç”¨ï¼

è¿è¡Œä»¥ä¸‹ä»£ç ä¼šç”Ÿæˆä¸€ä¸ª Gradio æ¼”ç¤ºåº”ç”¨ï¼Œå®ƒç”¨è®¡ç®—æœºçš„éº¦å…‹é£å½•åˆ¶è¯­éŸ³å¹¶å°†å…¶é¦ˆé€ç»™å¾®è°ƒåçš„ Whisper æ¨¡å‹ä»¥è½¬å½•å‡ºç›¸åº”çš„æ–‡æœ¬:

```python
from transformers import pipeline
import gradio as gr

pipe = pipeline(model="sanchit-gandhi/whisper-small-hi") # change to "your-username/the-name-you-picked"

def transcribe(audio):
    text = pipe(audio)["text"]
    return text

iface = gr.Interface(
    fn=transcribe,
    inputs=gr.Audio(source="microphone", type="filepath"),
    outputs="text",
    title="Whisper Small Hindi",
    description="Realtime demo for Hindi speech recognition using a fine-tuned Whisper small model.",
)

iface.launch()
```

## ç»“æŸè¯­

é€šè¿‡æœ¬æ–‡ï¼Œæˆ‘ä»¬ä»‹ç»äº†å¦‚ä½•ä½¿ç”¨ ğŸ¤— Datasetsã€Transformers å’Œ Hugging Face Hub ä¸€æ­¥æ­¥ä¸ºå¤šè¯­ç§ ASR å¾®è°ƒä¸€ä¸ª Whisper æ¨¡å‹ã€‚å¦‚æœä½ æƒ³è‡ªå·±å°è¯•å¾®è°ƒä¸€ä¸ªï¼Œè¯·å‚é˜… [Google Colab](https://colab.research.google.com/github/sanchit-gandhi/notebooks/blob/main/fine_tune_whisper.ipynb)ã€‚å¦‚æœä½ æœ‰å…´è¶£é’ˆå¯¹è‹±è¯­å’Œå¤šè¯­ç§ ASR å¾®è°ƒä¸€ä¸ªå…¶å®ƒçš„ Transformers æ¨¡å‹ï¼Œè¯·åŠ¡å¿…å‚è€ƒä¸‹ [examples/pytorch/speech-recognition](https://github.com/huggingface/transformers/tree/main/examples/pytorch/speech-recognition)ã€‚
---
title: åœ¨ ğŸ¤— Transformers ä¸­ä½¿ç”¨çº¦æŸæ³¢æŸæœç´¢å¼•å¯¼æ–‡æœ¬ç”Ÿæˆ
thumbnail: /blog/assets/53_constrained_beam_search/thumbnail.png
authors:
- user: cwkeam
  guest: true
translators:
- user: MatrixYao
- user: zhongdongy
  proofreader: true
---

# åœ¨ ğŸ¤— Transformers ä¸­ä½¿ç”¨çº¦æŸæ³¢æŸæœç´¢å¼•å¯¼æ–‡æœ¬ç”Ÿæˆ


<a target="_blank" href="https://colab.research.google.com/github/huggingface/blog/blob/main/notebooks/53_constrained_beam_search.ipynb">
    <img src="https://colab.research.google.com/assets/colab-badge.svg" alt=" åœ¨ Colab ä¸­æ‰“å¼€ "/>
</a>

## **å¼•è¨€**

æœ¬æ–‡å‡è®¾è¯»è€…å·²ç»ç†Ÿæ‚‰æ–‡æœ¬ç”Ÿæˆé¢†åŸŸæ³¢æŸæœç´¢ç›¸å…³çš„èƒŒæ™¯çŸ¥è¯†ï¼Œå…·ä½“å¯å‚è§åšæ–‡ [å¦‚ä½•ç”Ÿæˆæ–‡æœ¬: é€šè¿‡ Transformers ç”¨ä¸åŒçš„è§£ç æ–¹æ³•ç”Ÿæˆæ–‡æœ¬](https://huggingface.co/blog/zh/how-to-generate)ã€‚

ä¸æ™®é€šçš„æ³¢æŸæœç´¢ä¸åŒï¼Œ**çº¦æŸ** æ³¢æŸæœç´¢å…è®¸æˆ‘ä»¬æ§åˆ¶æ‰€ç”Ÿæˆçš„æ–‡æœ¬ã€‚è¿™å¾ˆæœ‰ç”¨ï¼Œå› ä¸ºæœ‰æ—¶æˆ‘ä»¬ç¡®åˆ‡åœ°çŸ¥é“è¾“å‡ºä¸­éœ€è¦åŒ…å«ä»€ä¹ˆã€‚ä¾‹å¦‚ï¼Œåœ¨æœºå™¨ç¿»è¯‘ä»»åŠ¡ä¸­ï¼Œæˆ‘ä»¬å¯èƒ½é€šè¿‡æŸ¥å­—å…¸å·²ç»çŸ¥é“å“ªäº›è¯å¿…é¡»åŒ…å«åœ¨æœ€ç»ˆçš„è¯‘æ–‡ä¸­; è€Œåœ¨æŸäº›ç‰¹å®šçš„åœºåˆä¸­ï¼Œè™½ç„¶æŸå‡ ä¸ªè¯å¯¹äºè¯­è¨€æ¨¡å‹è€Œè¨€å·®ä¸å¤šï¼Œä½†å¯¹æœ€ç»ˆç”¨æˆ·è€Œè¨€å¯èƒ½å´ç›¸å·®å¾ˆå¤§ã€‚è¿™ä¸¤ç§æƒ…å†µéƒ½å¯ä»¥é€šè¿‡å…è®¸ç”¨æˆ·å‘Šè¯‰æ¨¡å‹æœ€ç»ˆè¾“å‡ºä¸­å¿…é¡»åŒ…å«å“ªäº›è¯æ¥è§£å†³ã€‚

### **è¿™äº‹å„¿ä¸ºä»€ä¹ˆè¿™ä¹ˆéš¾**

ç„¶è€Œï¼Œè¿™ä¸ªäº‹æƒ…æ“ä½œèµ·æ¥å¹¶ä¸å®¹æ˜“ï¼Œå®ƒè¦æ±‚æˆ‘ä»¬åœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­çš„ _æŸä¸ªæ—¶åˆ»_ åœ¨è¾“å‡ºæ–‡æœ¬çš„ _æŸä¸ªä½ç½®_ å¼ºåˆ¶ç”ŸæˆæŸäº›ç‰¹å®šå­åºåˆ—ã€‚

å‡è®¾æˆ‘ä»¬è¦ç”Ÿæˆä¸€ä¸ªå¥å­ `S`ï¼Œå®ƒå¿…é¡»æŒ‰ç…§å…ˆ $t_1$ å†  $t_2$ çš„é¡ºåºåŒ…å«çŸ­è¯­ $p_1={ t_1, t_2 }$ã€‚ä»¥ä¸‹å®šä¹‰äº†æˆ‘ä»¬å¸Œæœ›ç”Ÿæˆçš„å¥å­ $S$:

$$ S_{æœŸæœ›} = { s_1, s_2, â€¦, s_k, t_1, t_2, s_{k+1}, â€¦, s_n } $$

é—®é¢˜æ˜¯æ³¢æŸæœç´¢æ˜¯é€è¯è¾“å‡ºæ–‡æœ¬çš„ã€‚æˆ‘ä»¬å¯ä»¥å¤§è‡´å°†æ³¢æŸæœç´¢è§†ä¸ºå‡½æ•° $B(\mathbf{s}_{0:i}) = s_{i+1}$ï¼Œå®ƒæ ¹æ®å½“å‰ç”Ÿæˆçš„åºåˆ— $\mathbf{s}_{0:i}$ é¢„æµ‹ä¸‹ä¸€æ—¶åˆ» $i+1$ çš„è¾“å‡ºã€‚ä½†æ˜¯è¿™ä¸ªå‡½æ•°åœ¨ä»»æ„æ—¶åˆ» $i < k$ æ€ä¹ˆçŸ¥é“ï¼Œæœªæ¥çš„æŸä¸ªæ—¶åˆ» $k$ å¿…é¡»ç”ŸæˆæŸä¸ªæŒ‡å®šè¯ï¼Ÿæˆ–è€…å½“å®ƒåœ¨æ—¶åˆ» $i=k$ æ—¶ï¼Œå®ƒå¦‚ä½•ç¡®å®šå½“å‰é‚£ä¸ªæŒ‡å®šè¯çš„æœ€ä½³ä½ç½®ï¼Œè€Œä¸æ˜¯æœªæ¥çš„æŸä¸€æ—¶åˆ» $i>k$ï¼Ÿ

![ä¸ºä½•çº¦æŸæœç´¢å¾ˆéš¾](https://raw.githubusercontent.com/huggingface/blog/main/assets/53_constrained_beam_search/why_constraints_are_hard.png)

å¦‚æœä½ åŒæ—¶æœ‰å¤šä¸ªä¸åŒçš„çº¦æŸæ€ä¹ˆåŠï¼Ÿå¦‚æœä½ æƒ³åŒæ—¶æŒ‡å®šä½¿ç”¨çŸ­è¯­ $p_1={t_1, t_2}$ _å’Œ_ çŸ­è¯­ $p_2={ t_3, t_4, t_5, t_6}$ æ€ä¹ˆåŠï¼Ÿå¦‚æœä½ å¸Œæœ›æ¨¡å‹åœ¨ä¸¤ä¸ªçŸ­è¯­ä¹‹é—´ **ä»»é€‰ä¸€ä¸ª** æ€ä¹ˆåŠï¼Ÿå¦‚æœä½ æƒ³åŒæ—¶æŒ‡å®šä½¿ç”¨çŸ­è¯­ $p_1$ ä»¥åŠçŸ­è¯­åˆ—è¡¨ ${p_{21}, p_{22}, p_{23}}$ ä¸­çš„ä»»ä¸€çŸ­è¯­æ€ä¹ˆåŠï¼Ÿ

ä¸Šè¿°éœ€æ±‚åœ¨å®é™…åœºæ™¯ä¸­æ˜¯å¾ˆåˆç†çš„éœ€æ±‚ï¼Œä¸‹æ–‡ä»‹ç»çš„æ–°çš„çº¦æŸæ³¢æŸæœç´¢åŠŸèƒ½å¯ä»¥æ»¡è¶³æ‰€æœ‰è¿™äº›éœ€æ±‚ï¼

æˆ‘ä»¬ä¼šå…ˆç®€è¦ä»‹ç»ä¸€ä¸‹æ–°çš„ _**çº¦æŸæ³¢æŸæœç´¢**_ å¯ä»¥åšäº›ä»€ä¹ˆï¼Œç„¶åå†æ·±å…¥ä»‹ç»å…¶åŸç†ã€‚

## **ä¾‹ 1: æŒ‡å®šåŒ…å«æŸè¯**

å‡è®¾æˆ‘ä»¬è¦å°† `"How old are you?"` ç¿»è¯‘æˆå¾·è¯­ã€‚å®ƒå¯¹åº”ä¸¤ç§å¾·è¯­è¡¨è¾¾ï¼Œå…¶ä¸­ `"Wie alt bist du?"` æ˜¯éæ­£å¼åœºåˆçš„è¡¨è¾¾ï¼Œè€Œ `"Wie alt sind Sie?"` æ˜¯æ­£å¼åœºåˆçš„è¡¨è¾¾ã€‚

ä¸åŒçš„åœºåˆï¼Œæˆ‘ä»¬å¯èƒ½å€¾å‘äºä¸åŒçš„è¡¨è¾¾ï¼Œä½†æˆ‘ä»¬å¦‚ä½•å‘Šè¯‰æ¨¡å‹å‘¢ï¼Ÿ

### **ä½¿ç”¨ä¼ ç»Ÿæ³¢æŸæœç´¢**

æˆ‘ä»¬å…ˆçœ‹ä¸‹å¦‚ä½•ä½¿ç”¨ _**ä¼ ç»Ÿæ³¢æŸæœç´¢**_ æ¥å®Œæˆç¿»è¯‘ã€‚

```
!pip install -q git+https://github.com/huggingface/transformers.git
```

```python
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM

tokenizer = AutoTokenizer.from_pretrained("t5-base")
model = AutoModelForSeq2SeqLM.from_pretrained("t5-base")

encoder_input_str = "translate English to German: How old are you?"

input_ids = tokenizer(encoder_input_str, return_tensors="pt").input_ids

outputs = model.generate(
    input_ids,
    num_beams=10,
    num_return_sequences=1,
    no_repeat_ngram_size=1,
    remove_invalid_values=True,
)

print("Output:\n" + 100 *'-')
print(tokenizer.decode(outputs[0], skip_special_tokens=True))
```


    Output:
    ----------------------------------------------------------------------------------------------------
    Wie alt bist du?


### **ä½¿ç”¨çº¦æŸæ³¢æŸæœç´¢**

ä½†æ˜¯å¦‚æœæˆ‘ä»¬æƒ³è¦ä¸€ä¸ªæ­£å¼çš„è¡¨è¾¾è€Œä¸æ˜¯éæ­£å¼çš„è¡¨è¾¾å‘¢ï¼Ÿå¦‚æœæˆ‘ä»¬å·²ç»å…ˆéªŒåœ°çŸ¥é“è¾“å‡ºä¸­å¿…é¡»åŒ…å«ä»€ä¹ˆï¼Œæˆ‘ä»¬è¯¥å¦‚ä½• _å°†å…¶_ æ³¨å…¥åˆ°è¾“å‡ºä¸­å‘¢ï¼Ÿ

æˆ‘ä»¬å¯ä»¥é€šè¿‡ `model.generate()` çš„ `force_words_ids` å‚æ•°æ¥å®ç°è¿™ä¸€åŠŸèƒ½ï¼Œä»£ç å¦‚ä¸‹:

```python
tokenizer = AutoTokenizer.from_pretrained("t5-base")
model = AutoModelForSeq2SeqLM.from_pretrained("t5-base")

encoder_input_str = "translate English to German: How old are you?"

force_words = ["Sie"]

input_ids = tokenizer(encoder_input_str, return_tensors="pt").input_ids
force_words_ids = tokenizer(force_words, add_special_tokens=False).input_ids

outputs = model.generate(
    input_ids,
    force_words_ids=force_words_ids,
    num_beams=5,
    num_return_sequences=1,
    no_repeat_ngram_size=1,
    remove_invalid_values=True,
)

print("Output:\n" + 100 *'-')
print(tokenizer.decode(outputs[0], skip_special_tokens=True))
```

    Output:
    ----------------------------------------------------------------------------------------------------
    Wie alt sind Sie?


å¦‚ä½ æ‰€è§ï¼Œç°åœ¨æˆ‘ä»¬èƒ½ç”¨æˆ‘ä»¬å¯¹è¾“å‡ºçš„å…ˆéªŒçŸ¥è¯†æ¥æŒ‡å¯¼æ–‡æœ¬çš„ç”Ÿæˆã€‚ä»¥å‰æˆ‘ä»¬å¿…é¡»å…ˆç”Ÿæˆä¸€å †å€™é€‰è¾“å‡ºï¼Œç„¶åæ‰‹åŠ¨ä»ä¸­æŒ‘é€‰å‡ºç¬¦åˆæˆ‘ä»¬è¦æ±‚çš„è¾“å‡ºã€‚ç°åœ¨æˆ‘ä»¬å¯ä»¥ç›´æ¥åœ¨ç”Ÿæˆé˜¶æ®µåšåˆ°è¿™ä¸€ç‚¹ã€‚

## **ä¾‹ 2: æå–å¼çº¦æŸ**

åœ¨ä¸Šé¢çš„ä¾‹å­ä¸­ï¼Œæˆ‘ä»¬çŸ¥é“éœ€è¦åœ¨æœ€ç»ˆè¾“å‡ºä¸­åŒ…å«å“ªäº›å•è¯ã€‚è¿™æ–¹é¢çš„ä¸€ä¸ªä¾‹å­å¯èƒ½æ˜¯åœ¨ç¥ç»æœºå™¨ç¿»è¯‘è¿‡ç¨‹ä¸­ç»“åˆä½¿ç”¨å­—å…¸ã€‚

ä½†æ˜¯ï¼Œå¦‚æœæˆ‘ä»¬ä¸çŸ¥é“è¦ä½¿ç”¨å“ªç§ _è¯å½¢_å‘¢ï¼Œæˆ‘ä»¬å¯èƒ½å¸Œæœ›ä½¿ç”¨å•è¯ `rain` ä½†å¯¹å…¶ä¸åŒçš„è¯æ€§æ²¡æœ‰åå¥½ï¼Œå³ `["raining", "rained", "rains", ...]` æ˜¯ç­‰æ¦‚çš„ã€‚æ›´ä¸€èˆ¬åœ°ï¼Œå¾ˆå¤šæƒ…å†µä¸‹ï¼Œæˆ‘ä»¬å¯èƒ½å¹¶ä¸åˆ»æ¿åœ°å¸Œæœ› _é€å­—æ¯ä¸€è‡´_ ï¼Œæ­¤æ—¶æˆ‘ä»¬å¸Œæœ›åˆ’å®šä¸€ä¸ªèŒƒå›´ç”±æ¨¡å‹å»ä»ä¸­é€‰æ‹©æœ€åˆé€‚çš„ã€‚

æ”¯æŒè¿™ç§è¡Œä¸ºçš„çº¦æŸå« _**æå–å¼çº¦æŸ (Disjunctive Constraints)**_ ï¼Œå…¶å…è®¸ç”¨æˆ·è¾“å…¥ä¸€ä¸ªå•è¯åˆ—è¡¨æ¥å¼•å¯¼æ–‡æœ¬ç”Ÿæˆï¼Œæœ€ç»ˆè¾“å‡ºä¸­ä»…é¡»åŒ…å«è¯¥åˆ—è¡¨ä¸­çš„ _è‡³å°‘ä¸€ä¸ª_ è¯å³å¯ã€‚

ä¸‹é¢æ˜¯ä¸€ä¸ªæ··åˆä½¿ç”¨ä¸Šè¿°ä¸¤ç±»çº¦æŸçš„ä¾‹å­:

```python
from transformers import GPT2LMHeadModel, GPT2Tokenizer

model = GPT2LMHeadModel.from_pretrained("gpt2")
tokenizer = GPT2Tokenizer.from_pretrained("gpt2")

force_word = "scared"
force_flexible = ["scream", "screams", "screaming", "screamed"]

force_words_ids = [
    tokenizer([force_word], add_prefix_space=True, add_special_tokens=False).input_ids,
    tokenizer(force_flexible, add_prefix_space=True, add_special_tokens=False).input_ids,
]

starting_text = ["The soldiers", "The child"]

input_ids = tokenizer(starting_text, return_tensors="pt").input_ids

outputs = model.generate(
    input_ids,
    force_words_ids=force_words_ids,
    num_beams=10,
    num_return_sequences=1,
    no_repeat_ngram_size=1,
    remove_invalid_values=True,
)

print("Output:\n" + 100 *'-')
print(tokenizer.decode(outputs[0], skip_special_tokens=True))
print(tokenizer.decode(outputs[1], skip_special_tokens=True))

```

    Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.

    Output:
    ----------------------------------------------------------------------------------------------------
    The soldiers, who were all scared and screaming at each other as they tried to get out of the
    The child was taken to a local hospital where she screamed and scared for her life, police said.


å¦‚ä½ æ‰€è§ï¼Œç¬¬ä¸€ä¸ªè¾“å‡ºé‡Œæœ‰ `"screaming"` ï¼Œç¬¬äºŒä¸ªè¾“å‡ºé‡Œæœ‰ `"screamed"` ï¼ŒåŒæ—¶å®ƒä»¬éƒ½åŸåŸæœ¬æœ¬åœ°åŒ…å«äº† `"scared"` ã€‚æ³¨æ„ï¼Œå…¶å® `["screaming", "screamed", ...]` åˆ—è¡¨ä¸­ä¸å¿…ä¸€å®šæ˜¯åŒä¸€å•è¯çš„ä¸åŒè¯å½¢ï¼Œå®ƒå¯ä»¥æ˜¯ä»»ä½•å•è¯ã€‚ä½¿ç”¨è¿™ç§æ–¹å¼ï¼Œå¯ä»¥æ»¡è¶³æˆ‘ä»¬åªéœ€è¦ä»å€™é€‰å•è¯åˆ—è¡¨ä¸­é€‰æ‹©ä¸€ä¸ªå•è¯çš„åº”ç”¨åœºæ™¯ã€‚

## **ä¼ ç»Ÿæ³¢æŸæœç´¢**

ä»¥ä¸‹æ˜¯ä¼ ç»Ÿ **æ³¢æŸæœç´¢** çš„ä¸€ä¸ªä¾‹å­ï¼Œæ‘˜è‡ªä¹‹å‰çš„ [åšæ–‡](https://huggingface.co/blog/zh/how-to-generate):

![æ³¢æŸæœç´¢](https://raw.githubusercontent.com/patrickvonplaten/scientific_images/master/beam_search.png)

ä¸è´ªå¿ƒæœç´¢ä¸åŒï¼Œæ³¢æŸæœç´¢ä¼šä¿ç•™æ›´å¤šçš„å€™é€‰è¯ã€‚ä¸Šå›¾ä¸­ï¼Œæˆ‘ä»¬æ¯ä¸€æ­¥éƒ½å±•ç¤ºäº† 3 ä¸ªæœ€å¯èƒ½çš„é¢„æµ‹è¯ã€‚

åœ¨ `num_beams=3` æ—¶ï¼Œæˆ‘ä»¬å¯ä»¥å°†ç¬¬ 1 æ­¥æ³¢æŸæœç´¢è¡¨ç¤ºæˆä¸‹å›¾:

![æ³¢æŸæœç´¢ç¬¬ 1 æ­¥](https://raw.githubusercontent.com/huggingface/blog/main/assets/53_constrained_beam_search/beam_1.jpg)

æ³¢æŸæœç´¢ä¸åƒè´ªå¿ƒæœç´¢é‚£æ ·åªé€‰æ‹© `"The dog"` ï¼Œè€Œæ˜¯å…è®¸å°† `"The nice"` å’Œ `"The car"` _ç•™å¾…è¿›ä¸€æ­¥è€ƒè™‘_ ã€‚

ä¸‹ä¸€æ­¥ï¼Œæˆ‘ä»¬ä¼šä¸ºä¸Šä¸€æ­¥åˆ›å»ºçš„ä¸‰ä¸ªåˆ†æ”¯åˆ†åˆ«é¢„æµ‹å¯èƒ½çš„ä¸‹ä¸€ä¸ªè¯ã€‚

![æ³¢æŸæœç´¢ç¬¬ 2 æ­¥](https://raw.githubusercontent.com/huggingface/blog/main/assets/53_constrained_beam_search/beam_2.jpg)

è™½ç„¶æˆ‘ä»¬ _è€ƒæŸ¥_ äº†æ˜æ˜¾å¤šäº `num_beams` ä¸ªå€™é€‰è¯ï¼Œä½†åœ¨æ¯æ­¥ç»“æŸæ—¶ï¼Œæˆ‘ä»¬åªä¼šè¾“å‡º `num_beams` ä¸ªæœ€ç»ˆå€™é€‰è¯ã€‚æˆ‘ä»¬ä¸èƒ½ä¸€ç›´åˆ†å‰ï¼Œé‚£æ ·çš„è¯ï¼Œ `beams` çš„æ•°ç›®å°†åœ¨ $n$ æ­¥åå˜æˆ $\text{beams}^{n}$ ä¸ªï¼Œæœ€ç»ˆå˜æˆæŒ‡æ•°çº§çš„å¢é•¿ (å½“æ³¢æŸæ•°ä¸º $10$ æ—¶ï¼Œåœ¨ $10$ æ­¥ä¹‹åå°±ä¼šå˜æˆ $10,000,000,000$ ä¸ªåˆ†æ”¯ï¼)ã€‚

æ¥ç€ï¼Œæˆ‘ä»¬é‡å¤ä¸Šè¿°æ­¥éª¤ï¼Œç›´åˆ°æ»¡è¶³ä¸­æ­¢æ¡ä»¶ï¼Œå¦‚ç”Ÿæˆ `<eos>` æ ‡è®°æˆ–è¾¾åˆ° `max_length` ã€‚æ•´ä¸ªè¿‡ç¨‹å¯ä»¥æ€»ç»“ä¸º: åˆ†å‰ã€æ’åºã€å‰ªæï¼Œå¦‚æ­¤å¾€å¤ã€‚

## **çº¦æŸæ³¢æŸæœç´¢**

çº¦æŸæ³¢æŸæœç´¢è¯•å›¾é€šè¿‡åœ¨æ¯ä¸€æ­¥ç”Ÿæˆè¿‡ç¨‹ä¸­ _æ³¨å…¥_æ‰€éœ€è¯æ¥æ»¡è¶³çº¦æŸã€‚

å‡è®¾æˆ‘ä»¬è¯•å›¾æŒ‡å®šè¾“å‡ºä¸­é¡»åŒ…å«çŸ­è¯­ `"is fast"` ã€‚

åœ¨ä¼ ç»Ÿæ³¢æŸæœç´¢ä¸­ï¼Œæˆ‘ä»¬åœ¨æ¯ä¸ªåˆ†æ”¯ä¸­æ‰¾åˆ° `k` ä¸ªæ¦‚ç‡æœ€é«˜çš„å€™é€‰è¯ï¼Œä»¥ä¾›ä¸‹ä¸€æ­¥ä½¿ç”¨ã€‚åœ¨çº¦æŸæ³¢æŸæœç´¢ä¸­ï¼Œé™¤äº†æ‰§è¡Œä¸ä¼ ç»Ÿæ³¢æŸæœç´¢ç›¸åŒçš„æ“ä½œå¤–ï¼Œæˆ‘ä»¬è¿˜ä¼šè¯•ç€æŠŠçº¦æŸè¯åŠ è¿›å»ï¼Œä»¥ _çœ‹çœ‹æˆ‘ä»¬æ˜¯å¦èƒ½å°½é‡æ»¡è¶³çº¦æŸ_ã€‚å›¾ç¤ºå¦‚ä¸‹:

![çº¦æŸæœç´¢ç¬¬ 1 æ­¥](https://raw.githubusercontent.com/huggingface/blog/main/assets/53_constrained_beam_search/cbeam_1.jpg)

ä¸Šå›¾ä¸­ï¼Œæˆ‘ä»¬æœ€ç»ˆå€™é€‰è¯é™¤äº†åŒ…æ‹¬åƒ `"dog"` å’Œ `"nice"` è¿™æ ·çš„é«˜æ¦‚ç‡è¯ä¹‹å¤–ï¼Œæˆ‘ä»¬è¿˜æŠŠ `"is"` å¡äº†è¿›å»ï¼Œä»¥å°½é‡æ»¡è¶³ç”Ÿæˆçš„å¥å­ä¸­é¡»å« `"is fast"` çš„çº¦æŸã€‚

ç¬¬äºŒæ­¥ï¼Œæ¯ä¸ªåˆ†æ”¯çš„å€™é€‰è¯é€‰æ‹©ä¸ä¼ ç»Ÿçš„æ³¢æŸæœç´¢å¤§éƒ¨åˆ†ç±»ä¼¼ã€‚å”¯ä¸€çš„ä¸åŒæ˜¯ï¼Œä¸ä¸Šé¢ç¬¬ä¸€æ­¥ä¸€æ ·ï¼Œçº¦æŸæ³¢æŸæœç´¢ä¼šåœ¨æ¯ä¸ªæ–°åˆ†å‰ä¸Šç»§ç»­å¼ºåŠ çº¦æŸï¼ŒæŠŠæ»¡è¶³çº¦æŸçš„å€™é€‰è¯å¼ºåŠ è¿›æ¥ï¼Œå¦‚ä¸‹å›¾æ‰€ç¤º:

![çº¦æŸæœç´¢ç¬¬ 2 æ­¥](https://raw.githubusercontent.com/huggingface/blog/main/assets/53_constrained_beam_search/cbeam_2.jpg)

### **ç»„ (Banks)**

åœ¨è®¨è®ºä¸‹ä¸€æ­¥ä¹‹å‰ï¼Œæˆ‘ä»¬åœä¸‹æ¥æ€è€ƒä¸€ä¸‹ä¸Šè¿°æ–¹æ³•çš„ç¼ºé™·ã€‚

åœ¨è¾“å‡ºä¸­é‡è›®åœ°å¼ºåˆ¶æ’å…¥çº¦æŸçŸ­è¯­ `is fast` çš„é—®é¢˜åœ¨äºï¼Œå¤§å¤šæ•°æƒ…å†µä¸‹ï¼Œä½ æœ€ç»ˆä¼šå¾—åˆ°åƒä¸Šé¢çš„ `The is fast` è¿™æ ·çš„æ— æ„ä¹‰è¾“å‡ºã€‚æˆ‘ä»¬éœ€è¦è§£å†³è¿™ä¸ªé—®é¢˜ã€‚ä½ å¯ä»¥ä» `huggingface/transformers` ä»£ç åº“ä¸­çš„è¿™ä¸ª [é—®é¢˜](https://github.com/huggingface/transformers/issues/14081#issuecomment-1004479944) ä¸­äº†è§£æ›´å¤šæœ‰å…³è¿™ä¸ªé—®é¢˜åŠå…¶å¤æ‚æ€§çš„æ·±å…¥è®¨è®ºã€‚

ç»„æ–¹æ³•é€šè¿‡åœ¨æ»¡è¶³çº¦æŸå’Œäº§ç”Ÿåˆç†è¾“å‡ºä¸¤è€…ä¹‹é—´å–å¾—å¹³è¡¡æ¥è§£å†³è¿™ä¸ªé—®é¢˜ã€‚

æˆ‘ä»¬æŠŠæ‰€æœ‰å€™é€‰æ³¢æŸæŒ‰ç…§å…¶ `æ»¡è¶³äº†å¤šå°‘æ­¥çº¦æŸ`åˆ†åˆ°ä¸åŒçš„ç»„ä¸­ï¼Œå…¶ä¸­ç»„ $n$ é‡ŒåŒ…å«çš„æ˜¯ _**æ»¡è¶³äº† $n$ æ­¥çº¦æŸçš„æ³¢æŸåˆ—è¡¨**_ ã€‚ç„¶åæˆ‘ä»¬æŒ‰ç…§é¡ºåºè½®æµé€‰æ‹©å„ç»„çš„å€™é€‰æ³¢æŸã€‚åœ¨ä¸Šå›¾ä¸­ï¼Œæˆ‘ä»¬å…ˆä»ç»„ 2 (Bank 2) ä¸­é€‰æ‹©æ¦‚ç‡æœ€å¤§çš„è¾“å‡ºï¼Œç„¶åä»ç»„ 1 (Bank 1) ä¸­é€‰æ‹©æ¦‚ç‡æœ€å¤§çš„è¾“å‡ºï¼Œæœ€åä»ç»„ 0 (Bank 0) ä¸­é€‰æ‹©æœ€å¤§çš„è¾“å‡º; æ¥ç€æˆ‘ä»¬ä»ç»„ 2 (Bank 2) ä¸­é€‰æ‹©æ¦‚ç‡æ¬¡å¤§çš„è¾“å‡ºï¼Œä»ç»„ 1 (Bank 1) ä¸­é€‰æ‹©æ¦‚ç‡æ¬¡å¤§çš„è¾“å‡ºï¼Œä¾æ­¤ç±»æ¨ã€‚å› ä¸ºæˆ‘ä»¬ä½¿ç”¨çš„æ˜¯ `num_beams=3`ï¼Œæ‰€ä»¥æˆ‘ä»¬åªéœ€æ‰§è¡Œä¸Šè¿°è¿‡ç¨‹ä¸‰æ¬¡ï¼Œå°±å¯ä»¥å¾—åˆ° `["The is fast", "The dog is", "The dog and"]`ã€‚

è¿™æ ·ï¼Œå³ä½¿æˆ‘ä»¬ _å¼ºåˆ¶_ æ¨¡å‹è€ƒè™‘æˆ‘ä»¬æ‰‹åŠ¨æ·»åŠ çš„çº¦æŸè¯åˆ†æ”¯ï¼Œæˆ‘ä»¬ä¾ç„¶ä¼šè·Ÿè¸ªå…¶ä»–å¯èƒ½æ›´æœ‰æ„ä¹‰çš„é«˜æ¦‚ç‡åºåˆ—ã€‚å°½ç®¡ `The is fast` å®Œå…¨æ»¡è¶³çº¦æŸï¼Œä½†è¿™å¹¶ä¸æ˜¯ä¸€ä¸ªæœ‰æ„ä¹‰çš„çŸ­è¯­ã€‚å¹¸è¿çš„æ˜¯ï¼Œæˆ‘ä»¬æœ‰ `"The dog is"` å’Œ `"The dog and"` å¯ä»¥åœ¨æœªæ¥çš„æ­¥éª¤ä¸­ä½¿ç”¨ï¼Œå¸Œæœ›åœ¨å°†æ¥è¿™ä¼šäº§ç”Ÿæ›´æœ‰æ„ä¹‰çš„è¾“å‡ºã€‚

å›¾ç¤ºå¦‚ä¸‹ (ä»¥ä¸Šä¾‹çš„ç¬¬ 3 æ­¥ä¸ºä¾‹):

![çº¦æŸæœç´¢ç¬¬ 3 æ­¥](https://raw.githubusercontent.com/huggingface/blog/main/assets/53_constrained_beam_search/cbeam_3.jpg)

è¯·æ³¨æ„ï¼Œä¸Šå›¾ä¸­ä¸éœ€è¦å¼ºåˆ¶æ·»åŠ  `"The is fast"`ï¼Œå› ä¸ºå®ƒå·²ç»è¢«åŒ…å«åœ¨æ¦‚ç‡æ’åºä¸­äº†ã€‚å¦å¤–ï¼Œè¯·æ³¨æ„åƒ `"The dog is slow"` æˆ– `"The dog is mad"` è¿™æ ·çš„æ³¢æŸå®é™…ä¸Šæ˜¯å±äºç»„ 0 (Bank 0) çš„ï¼Œä¸ºä»€ä¹ˆå‘¢ï¼Ÿå› ä¸ºå°½ç®¡å®ƒåŒ…å«è¯ `"is"` ï¼Œä½†å®ƒä¸å¯ç”¨äºç”Ÿæˆ `"is fast"` ï¼Œå› ä¸º `fast` çš„ä½å­å·²ç»è¢« `slow` æˆ– `mad` å æ‰äº†ï¼Œä¹Ÿå°±æœç»äº†åç»­èƒ½ç”Ÿæˆ `"is fast"` çš„å¯èƒ½æ€§ã€‚ä»å¦ä¸€ä¸ªè§’åº¦è®²ï¼Œå› ä¸º `slow` è¿™æ ·çš„è¯çš„åŠ å…¥ï¼Œè¯¥åˆ†æ”¯ _æ»¡è¶³çº¦æŸçš„è¿›åº¦_ è¢«é‡ç½®æˆäº† 0ã€‚

æœ€åè¯·æ³¨æ„ï¼Œæˆ‘ä»¬æœ€ç»ˆç”Ÿæˆäº†åŒ…å«çº¦æŸçŸ­è¯­çš„åˆç†è¾“å‡º: `"The dog is fast"` ï¼

èµ·åˆæˆ‘ä»¬å¾ˆæ‹…å¿ƒï¼Œå› ä¸ºç›²ç›®åœ°æ·»åŠ çº¦æŸè¯ä¼šå¯¼è‡´å‡ºç°è¯¸å¦‚ `"The is fast"` ä¹‹ç±»çš„æ— æ„ä¹‰çŸ­è¯­ã€‚ç„¶è€Œï¼Œä½¿ç”¨åŸºäºç»„çš„è½®æµé€‰æ‹©æ–¹æ³•ï¼Œæˆ‘ä»¬æœ€ç»ˆéšå¼åœ°æ‘†è„±äº†æ— æ„ä¹‰çš„è¾“å‡ºï¼Œä¼˜å…ˆé€‰æ‹©äº†æ›´åˆç†çš„è¾“å‡ºã€‚

## **å…³äº `Constraint` ç±»çš„æ›´å¤šä¿¡æ¯åŠè‡ªå®šä¹‰çº¦æŸ**

æˆ‘ä»¬æ€»ç»“ä¸‹è¦ç‚¹ã€‚æ¯ä¸€æ­¥ï¼Œæˆ‘ä»¬éƒ½ä¸æ–­åœ°çº ç¼ æ¨¡å‹ï¼Œå¼ºåˆ¶æ·»åŠ çº¦æŸè¯ï¼ŒåŒæ—¶ä¹Ÿè·Ÿè¸ªä¸æ»¡è¶³çº¦æŸçš„åˆ†æ”¯ï¼Œç›´åˆ°æœ€ç»ˆç”ŸæˆåŒ…å«æ‰€éœ€çŸ­è¯­çš„åˆç†çš„é«˜æ¦‚ç‡åºåˆ—ã€‚

åœ¨å®ç°æ—¶ï¼Œæˆ‘ä»¬çš„ä¸»è¦æ–¹æ³•æ˜¯å°†æ¯ä¸ªçº¦æŸè¡¨ç¤ºä¸ºä¸€ä¸ª `Constraint` å¯¹è±¡ï¼Œå…¶ç›®çš„æ˜¯è·Ÿè¸ªæ»¡è¶³çº¦æŸçš„è¿›åº¦å¹¶å‘Šè¯‰æ³¢æŸæœç´¢æ¥ä¸‹æ¥è¦ç”Ÿæˆå“ªäº›è¯ã€‚å°½ç®¡æˆ‘ä»¬å¯ä»¥ä½¿ç”¨ `model.generate()` çš„å…³é”®å­—å‚æ•° `force_words_ids` ï¼Œä½†ä½¿ç”¨è¯¥å‚æ•°æ—¶åç«¯å®é™…å‘ç”Ÿçš„æƒ…å†µå¦‚ä¸‹:

```python
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, PhrasalConstraint

tokenizer = AutoTokenizer.from_pretrained("t5-base")
model = AutoModelForSeq2SeqLM.from_pretrained("t5-base")

encoder_input_str = "translate English to German: How old are you?"

constraints = [
    PhrasalConstraint(
        tokenizer("Sie", add_special_tokens=False).input_ids
    )
]

input_ids = tokenizer(encoder_input_str, return_tensors="pt").input_ids

outputs = model.generate(
    input_ids,
    constraints=constraints,
    num_beams=10,
    num_return_sequences=1,
    no_repeat_ngram_size=1,
    remove_invalid_values=True,
)

print("Output:\n" + 100 *'-')
print(tokenizer.decode(outputs[0], skip_special_tokens=True))
```

    Output:
    ----------------------------------------------------------------------------------------------------
    Wie alt sind Sie?

ä½ ç”šè‡³å¯ä»¥å®šä¹‰ä¸€ä¸ªè‡ªå·±çš„çº¦æŸå¹¶å°†å…¶é€šè¿‡ `constraints` å‚æ•°è¾“å…¥ç»™ `model.generate()` ã€‚æ­¤æ—¶ï¼Œä½ åªéœ€è¦åˆ›å»º `Constraint` æŠ½è±¡æ¥å£ç±»çš„å­ç±»å¹¶éµå¾ªå…¶è¦æ±‚å³å¯ã€‚ä½ å¯ä»¥åœ¨ [æ­¤å¤„](https://github.com/huggingface/transformers/blob/main/src/transformers/generation/beam_constraints.py) çš„ `Constraint` å®šä¹‰ä¸­æ‰¾åˆ°æ›´å¤šä¿¡æ¯ã€‚

æˆ‘ä»¬è¿˜å¯ä»¥å°è¯•å…¶ä»–ä¸€äº›æœ‰æ„æ€çš„çº¦æŸ (å°šæœªå®ç°ï¼Œä¹Ÿè®¸ä½ å¯ä»¥è¯•ä¸€è¯•ï¼) å¦‚  `OrderedConstraints` ã€ `TemplateConstraints` ç­‰ã€‚ç›®å‰ï¼Œåœ¨æœ€ç»ˆè¾“å‡ºä¸­çº¦æŸçŸ­è¯­é—´æ˜¯æ— åºçš„ã€‚ä¾‹å¦‚ï¼Œå‰é¢çš„ä¾‹å­ä¸€ä¸ªè¾“å‡ºä¸­çš„çº¦æŸçŸ­è¯­é¡ºåºä¸º `scared -> screaming` ï¼Œè€Œå¦ä¸€ä¸ªè¾“å‡ºä¸­çš„çº¦æŸçŸ­è¯­é¡ºåºä¸º `screamed -> scared` ã€‚ å¦‚æœæœ‰äº† `OrderedConstraints`ï¼Œ æˆ‘ä»¬å°±å¯ä»¥å…è®¸ç”¨æˆ·æŒ‡å®šçº¦æŸçŸ­è¯­çš„é¡ºåºã€‚ `TemplateConstraints` çš„åŠŸèƒ½æ›´å°ä¼—ï¼Œå…¶çº¦æŸå¯ä»¥åƒè¿™æ ·:

```python
starting_text = "The woman"
template = ["the", "", "School of", "", "in"]

possible_outputs == [
   "The woman attended the Ross School of Business in Michigan.",
   "The woman was the administrator for the Harvard School of Business in MA."
]
```

æˆ–æ˜¯è¿™æ ·:

```python
starting_text = "The woman"
template = ["the", "", "", "University", "", "in"]

possible_outputs == [
   "The woman attended the Carnegie Mellon University in Pittsburgh.",
]
impossible_outputs == [
  "The woman attended the Harvard University in MA."
]
```

æˆ–è€…ï¼Œå¦‚æœç”¨æˆ·ä¸å…³å¿ƒä¸¤ä¸ªè¯ä¹‹é—´åº”è¯¥éš”å¤šå°‘ä¸ªè¯ï¼Œé‚£ä»…ç”¨ `OrderedConstraint` å°±å¯ä»¥äº†ã€‚

## **æ€»ç»“**

çº¦æŸæ³¢æŸæœç´¢ä¸ºæˆ‘ä»¬æä¾›äº†ä¸€ç§å°†å¤–éƒ¨çŸ¥è¯†å’Œéœ€æ±‚æ³¨å…¥æ–‡æœ¬ç”Ÿæˆè¿‡ç¨‹çš„çµæ´»æ–¹æ³•ã€‚ä»¥å‰ï¼Œæ²¡æœ‰ä¸€ä¸ªç®€å•çš„æ–¹æ³•å¯ç”¨äºå‘Šè¯‰æ¨¡å‹ 1. è¾“å‡ºä¸­éœ€è¦åŒ…å«æŸåˆ—è¡¨ä¸­çš„è¯æˆ–çŸ­è¯­ï¼Œå…¶ä¸­ 2. å…¶ä¸­æœ‰ä¸€äº›æ˜¯å¯é€‰çš„ï¼Œæœ‰äº›å¿…é¡»åŒ…å«çš„ï¼Œè¿™æ · 3. å®ƒä»¬å¯ä»¥æœ€ç»ˆç”Ÿæˆè‡³åœ¨åˆç†çš„ä½ç½®ã€‚ç°åœ¨ï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡ç»¼åˆä½¿ç”¨ `Constraint` çš„ä¸åŒå­ç±»æ¥å®Œå…¨æ§åˆ¶æˆ‘ä»¬çš„ç”Ÿæˆï¼

è¯¥æ–°ç‰¹æ€§ä¸»è¦åŸºäºä»¥ä¸‹è®ºæ–‡:

- [Guided Open Vocabulary Image Captioning with Constrained Beam Search](https://arxiv.org/pdf/1612.00576.pdf)
- [Fast Lexically Constrained Decoding with Dynamic Beam Allocation for Neural Machine Translation](https://arxiv.org/abs/1804.06609)
- [Improved Lexically Constrained Decoding for Translation and Monolingual Rewriting](https://aclanthology.org/N19-1090/)
- [Guided Generation of Cause and Effect](https://arxiv.org/pdf/2107.09846.pdf)

ä¸ä¸Šè¿°è¿™äº›å·¥ä½œä¸€æ ·ï¼Œè¿˜æœ‰è®¸å¤šæ–°çš„ç ”ç©¶æ­£åœ¨æ¢ç´¢å¦‚ä½•ä½¿ç”¨å¤–éƒ¨çŸ¥è¯† (ä¾‹å¦‚ KG (Knowledge Graph) ã€KB (Knowledge Base) ) æ¥æŒ‡å¯¼å¤§å‹æ·±åº¦å­¦ä¹ æ¨¡å‹è¾“å‡ºã€‚æˆ‘ä»¬å¸Œæœ›çº¦æŸæ³¢æŸæœç´¢åŠŸèƒ½æˆä¸ºå®ç°æ­¤ç›®çš„çš„æœ‰æ•ˆæ–¹æ³•ä¹‹ä¸€ã€‚

æ„Ÿè°¢æ‰€æœ‰ä¸ºæ­¤åŠŸèƒ½æä¾›æŒ‡å¯¼çš„äºº: Patrick von Platen å‚ä¸äº†ä» [åˆå§‹é—®é¢˜](https://github.com/huggingface/transformers/issues/14081) è®¨è®ºåˆ° [æœ€ç»ˆ PR](https://github.com/huggingface/transformers/pull/15761) çš„å…¨è¿‡ç¨‹ï¼Œè¿˜æœ‰ Narsil Patryï¼Œä»–ä»¬äºŒä½å¯¹ä»£ç è¿›è¡Œäº†è¯¦ç»†çš„åé¦ˆã€‚

_æœ¬æ–‡ä½¿ç”¨çš„å›¾æ ‡æ¥è‡ªäº <a href="https://www.flaticon.com/free-icons/shorthand" title="shorthand icons">Freepik - Flaticon</a>ã€‚_
---
title: "LayerSkipï¼šä½¿ç”¨è‡ªæ¨æµ‹è§£ç åŠ é€Ÿå¤§æ¨¡å‹æ¨ç†"
thumbnail: /blog/assets/layerskip/thumbnail.png
authors:
- user: ariG23498
- user: melhoushi
  guest: true
  org: facebook
- user: pcuenq
- user: reach-vb
translators:
- user: smartisan
---

# LayerSkipï¼šä½¿ç”¨è‡ªæ¨æµ‹è§£ç åŠ é€Ÿå¤§æ¨¡å‹æ¨ç†

è‡ªæ¨æµ‹è§£ç æ˜¯ä¸€ç§æ–°é¢–çš„æ–‡æœ¬ç”Ÿæˆæ–¹æ³•ï¼Œå®ƒç»“åˆäº†æ¨æµ‹è§£ç  (Speculative Decoding) çš„ä¼˜åŠ¿å’Œå¤§è¯­è¨€æ¨¡å‹ (LLM) çš„æå‰é€€å‡º (Early Exit) æœºåˆ¶ã€‚è¯¥æ–¹æ³•å‡ºè‡ªè®ºæ–‡ [LayerSkip: Enabling Early-Exit Inference and Self-Speculative Decoding](https://arxiv.org/abs/2404.16710)ã€‚å®ƒé€šè¿‡ä½¿ç”¨*åŒä¸€ä¸ªæ¨¡å‹*çš„æ—©æœŸå±‚æ¥ç”Ÿæˆå€™é€‰è¯å…ƒ (token)ï¼Œå¹¶ä½¿ç”¨åæœŸå±‚è¿›è¡ŒéªŒè¯ï¼Œä»è€Œå®ç°é«˜æ•ˆç”Ÿæˆã€‚

è¿™é¡¹æŠ€æœ¯ä¸ä»…åŠ å¿«äº†æ–‡æœ¬ç”Ÿæˆé€Ÿåº¦ï¼Œè¿˜æ˜¾è‘—èŠ‚çœäº†å†…å­˜å¹¶é™ä½äº†è®¡ç®—å»¶è¿Ÿã€‚ä¸ºäº†å®ç°ç«¯åˆ°ç«¯çš„åŠ é€Ÿï¼Œæ—©æœŸå±‚çš„è¾“å‡ºéœ€è¦ä¸æœ€ç»ˆå±‚çš„è¾“å‡ºè¶³å¤Ÿæ¥è¿‘ã€‚æ­£å¦‚è®ºæ–‡ä¸­æ‰€è¿°ï¼Œè¿™å¯ä»¥é€šè¿‡ä¸€ç§è®­ç»ƒæ–¹æ³•æ¥å®ç°ï¼Œè¯¥æ–¹æ³•å¯ä»¥åœ¨é¢„è®­ç»ƒæœŸé—´åº”ç”¨ï¼Œä¹Ÿå¯ä»¥åœ¨ç‰¹å®šé¢†åŸŸè¿›è¡Œå¾®è°ƒæ—¶åº”ç”¨ã€‚è‡ªæ¨æµ‹è§£ç å¯¹äºå®é™…åº”ç”¨ç‰¹åˆ«é«˜æ•ˆï¼Œå®ƒå¯ä»¥åœ¨è¾ƒå°çš„ GPU ä¸Šéƒ¨ç½²ï¼Œå¹¶é™ä½**å¤§è§„æ¨¡æ¨ç†**æ‰€éœ€çš„æ•´ä½“ç¡¬ä»¶èµ„æºã€‚

åœ¨æœ¬åšå®¢ä¸­ï¼Œæˆ‘ä»¬å°†æ¢è®¨è‡ªæ¨æµ‹è§£ç çš„æ¦‚å¿µã€å…¶å®ç°æ–¹å¼ä»¥åŠåœ¨ ğŸ¤— transformers åº“ä¸­çš„å®é™…åº”ç”¨ã€‚æ‚¨å°†äº†è§£åˆ°å…¶æŠ€æœ¯åŸç†ï¼ŒåŒ…æ‹¬**æå‰é€€å‡ºå±‚ (Early-Exit Layers)**ã€**ååµŒå…¥ (Unembedding)** å’Œ**è®­ç»ƒä¿®æ”¹ (Training Modifications)**ã€‚ä¸ºäº†å°†è¿™äº›æ¦‚å¿µä»˜è¯¸å®è·µï¼Œæˆ‘ä»¬æä¾›äº†ä»£ç ç¤ºä¾‹ã€ä¸ä¼ ç»Ÿæ¨æµ‹è§£ç çš„åŸºå‡†æ¯”è¾ƒï¼Œä»¥åŠå¯¹æ€§èƒ½æƒè¡¡çš„è§è§£ã€‚

æ‚¨è¿˜å¯ä»¥ç›´æ¥æŸ¥çœ‹ä»¥ä¸‹ Hugging Face èµ„æºï¼Œäº†è§£æ›´å¤šå…³äºè¯¥æ–¹æ³•çš„ä¿¡æ¯å¹¶äº²è‡ªå°è¯•ï¼š

1. [Hugging Face è®ºæ–‡è®¨è®ºè®ºå›](https://huggingface.co/papers/2404.16710)
2. [LayerSkip æ¨¡å‹é›†åˆ](https://huggingface.co/collections/facebook/layerskip-666b25c50c8ae90e1965727a)
3. [å±•ç¤ºè‡ªæ¨æµ‹è§£ç æ·±å…¥å·¥ä½œåŸç†çš„ Colab ç¬”è®°æœ¬](https://huggingface.co/datasets/ariG23498/layer-skip-assets/blob/main/early_exit_self_speculative_decoding.ipynb)

## æ¨æµ‹è§£ç ä¸è‡ªæ¨æµ‹è§£ç 

![LayerSkip æ¼”ç¤º GIF](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/layerskip-assets/LayerSkip-Demo.gif)
*åœ¨ [`facebook/layerskip-llama2-7B`](https://huggingface.co/facebook/layerskip-llama2-7B) ä¸Šçš„ LayerSkip æ¨ç†æ¼”ç¤º
ï¼ˆä½¿ç”¨ LayerSkip æ–¹æ³•æŒç»­é¢„è®­ç»ƒçš„ Llama2 7Bï¼‰ã€‚*

[ä¼ ç»Ÿçš„æ¨æµ‹è§£ç ](https://huggingface.co/blog/assisted-generation)ä½¿ç”¨**ä¸¤ä¸ª**æ¨¡å‹ï¼šä¸€ä¸ªè¾ƒå°çš„æ¨¡å‹ï¼ˆè‰ç¨¿æ¨¡å‹ï¼‰ç”¨äºç”Ÿæˆä¸€ç³»åˆ—å€™é€‰è¯å…ƒï¼Œä¸€ä¸ªè¾ƒå¤§çš„æ¨¡å‹ï¼ˆéªŒè¯æ¨¡å‹ï¼‰ç”¨äºéªŒè¯è‰ç¨¿çš„å‡†ç¡®æ€§ã€‚è¾ƒå°çš„æ¨¡å‹æ‰§è¡Œå¤§éƒ¨åˆ†ç”Ÿæˆå·¥ä½œï¼Œè€Œè¾ƒå¤§çš„æ¨¡å‹åˆ™è´Ÿè´£æ”¹è¿›ç»“æœã€‚è¿™æé«˜äº†æ–‡æœ¬ç”Ÿæˆé€Ÿåº¦ï¼Œå› ä¸ºè¾ƒå¤§çš„æ¨¡å‹ä¸€æ¬¡æ€§éªŒè¯å®Œæ•´åºåˆ—ï¼Œè€Œä¸æ˜¯é€ä¸ªç”Ÿæˆè¯å…ƒã€‚

åœ¨è‡ªæ¨æµ‹è§£ç ä¸­ï¼Œä½œè€…åœ¨æ­¤æ¦‚å¿µçš„åŸºç¡€ä¸Šï¼Œä½¿ç”¨å¤§æ¨¡å‹çš„æ—©æœŸå±‚æ¥ç”Ÿæˆè‰ç¨¿è¯å…ƒï¼Œç„¶åç”±æ¨¡å‹çš„æ›´æ·±å±‚è¿›è¡ŒéªŒè¯ã€‚è¿™ç§æ¨æµ‹è§£ç çš„â€œè‡ªæ´½â€ç‰¹æ€§éœ€è¦ç‰¹å®šçš„è®­ç»ƒï¼Œä½¿æ¨¡å‹èƒ½å¤ŸåŒæ—¶æ‰§è¡Œè‰ç¨¿ç”Ÿæˆå’ŒéªŒè¯ã€‚è¿™åè¿‡æ¥åˆæ¯”ä¼ ç»Ÿçš„æ¨æµ‹è§£ç æé«˜äº†é€Ÿåº¦å¹¶é™ä½äº†è®¡ç®—æˆæœ¬ã€‚

## åœ¨ `transformers` ä¸­çš„ä½¿ç”¨

ä¸ºäº†åœ¨ ğŸ¤— transformers åº“ä¸­å¯ç”¨æå‰é€€å‡ºè‡ªæ¨æµ‹è§£ç ï¼Œæˆ‘ä»¬åªéœ€åœ¨ `generate()` å‡½æ•°ä¸­æ·»åŠ  `assistant_early_exit` å‚æ•°ã€‚

ä»¥ä¸‹æ˜¯ä¸€ä¸ªç®€å•çš„ä»£ç ç‰‡æ®µï¼Œå±•ç¤ºäº†è¯¥åŠŸèƒ½ï¼š

```python
pip install transformers

from transformers import AutoTokenizer, AutoModelForCausalLM

early_exit_layer = 4
prompt = "Alice and Bob"
checkpoint = "facebook/layerskip-llama2-7B"

tokenizer = AutoTokenizer.from_pretrained(checkpoint)
inputs = tokenizer(prompt, return_tensors="pt").to("cuda")

model = AutoModelForCausalLM.from_pretrained(checkpoint).to("cuda")
outputs = model.generate(**inputs, assistant_early_exit=early_exit_layer)
```

æ³¨æ„ï¼šè™½ç„¶ `assistant_early_exit` å‚æ•°å¯ä»¥ä¸ºä»»ä½•ä»…è§£ç å™¨çš„ transformer å¯ç”¨æå‰é€€å‡ºè‡ªæ¨æµ‹è§£ç ï¼Œä½†é™¤éæ¨¡å‹ç»è¿‡ä¸“é—¨è®­ç»ƒï¼Œå¦åˆ™æ— æ³•ååµŒå…¥ï¼ˆé€šè¿‡ LM å¤´è¿›è¡Œè§£ç çš„è¿‡ç¨‹ï¼Œåœ¨åšå®¢æ–‡ç« åé¢æœ‰æè¿°ï¼‰ä¸­é—´å±‚çš„ logitsã€‚åªæœ‰å¯¹æ£€æŸ¥ç‚¹è¿›è¡Œè¿™æ ·çš„è®­ç»ƒï¼Œä»¥æé«˜æ—©æœŸå±‚çš„å‡†ç¡®æ€§ï¼Œæ‚¨æ‰èƒ½è·å¾—åŠ é€Ÿã€‚LayerSkip è®ºæ–‡æå‡ºäº†ä¸€ç§è®­ç»ƒæ–¹æ³•æ¥å®ç°è¿™ä¸€ç‚¹ï¼ˆå³åº”ç”¨æå‰é€€å‡ºæŸå¤±ï¼Œå¹¶é€æ­¥å¢åŠ å±‚ä¸¢å¼ƒç‡ï¼‰ã€‚[è¿™é‡Œ](https://huggingface.co/collections/facebook/layerskip-666b25c50c8ae90e1965727a) æä¾›äº†ä½¿ç”¨ LayerSkip è®­ç»ƒæ–¹æ³•æŒç»­é¢„è®­ç»ƒçš„ Llama2ã€Llama3 å’Œ Code Llama æ£€æŸ¥ç‚¹çš„é›†åˆã€‚

## åŸºå‡†æµ‹è¯•

æˆ‘ä»¬è¿›è¡Œäº†ä¸€ç³»åˆ—å¹¿æ³›çš„åŸºå‡†æµ‹è¯•ï¼Œä»¥è¡¡é‡ LayerSkip çš„è‡ªæ¨æµ‹è§£ç ç›¸å¯¹äºè‡ªå›å½’è§£ç åœ¨å„ç§æ¨¡å‹ä¸Šçš„åŠ é€Ÿæƒ…å†µã€‚æˆ‘ä»¬è¿˜å°†è‡ªæ¨æµ‹è§£ç ï¼ˆåŸºäºæå‰é€€å‡ºï¼‰ä¸æ ‡å‡†æ¨æµ‹è§£ç æŠ€æœ¯è¿›è¡Œäº†æ¯”è¾ƒã€‚è¦å¤ç°è¿™äº›ç»“æœï¼Œæ‚¨å¯ä»¥åœ¨[è¿™é‡Œ](https://github.com/aritra24rg/LayerSkip-Benchmarking)æ‰¾åˆ°ä»£ç ï¼Œå¹¶åœ¨[æ­¤ç”µå­è¡¨æ ¼](https://docs.google.com/spreadsheets/d/15poLaR_7tG_5xZo-LzLMFd4dzz-dHl_h/edit#gid=1155443081)ä¸­æ‰¾åˆ°è¿è¡Œæ¯ä¸ªå®éªŒçš„å‘½ä»¤ã€‚æ‰€æœ‰å®éªŒå‡åœ¨å•ä¸ª 80GB A100 GPU ä¸Šè¿è¡Œï¼Œé™¤äº† Llama2 70B å®éªŒåœ¨ 8 ä¸ª A100 GPU çš„èŠ‚ç‚¹ä¸Šè¿è¡Œã€‚

#### Llama3.2 1B

| Model Variant (æ¨¡å‹å˜ä½“)       | Layers (å±‚æ•°) | Assistant Model (è¾…åŠ©æ¨¡å‹) | Assistant Layers (è¾…åŠ©å±‚æ•°) | Task (ä»»åŠ¡)   | Total Layers (æ€»å±‚æ•°) | FLOPs/Input (G) (è¾“å…¥ FLOPs) | Time/Input (s) (è¾“å…¥æ—¶é—´) | FLOPs/Output (G) (è¾“å‡º FLOPs) | Time/Output (s) (è¾“å‡ºæ—¶é—´) | Efficiency (æ•ˆç‡) |
| :----------------------------- | :------------ | :------------------------- | :-------------------------- | :------------ | :-------------------- | :--------------------------- | :------------------------ | :---------------------------- | :------------------------- | :---------------- |
| facebook/layerskip-llama3.2-1B | 1             | Early Exit @ Layer 4       |                             | summarization | 1                     | 1195.28                      | 9.96                      | 2147.7                        | 17.9                       | 1.80              |

#### Llama3 8B

| Model Variant (æ¨¡å‹å˜ä½“)     | Layers (å±‚æ•°) | Assistant Model (è¾…åŠ©æ¨¡å‹) | Assistant Layers (è¾…åŠ©å±‚æ•°) | Task (ä»»åŠ¡)   | Total Layers (æ€»å±‚æ•°) | FLOPs/Input (G) (è¾“å…¥ FLOPs) | Time/Input (s) (è¾“å…¥æ—¶é—´) | FLOPs/Output (G) (è¾“å‡º FLOPs) | Time/Output (s) (è¾“å‡ºæ—¶é—´) | Efficiency (æ•ˆç‡) |
| :--------------------------- | :------------ | :------------------------- | :-------------------------- | :------------ | :-------------------- | :--------------------------- | :------------------------ | :---------------------------- | :------------------------- | :---------------- |
| meta-llama/Meta-Llama-3-8B   | 8             | meta-llama/Llama-3.2-1B    | 1                           | summarization | 9                     | 1872.46                      | 19.04                     | 2859.35                       | 29.08                      | 1.53              |
| meta-llama/Meta-Llama-3-8B   | 8             | meta-llama/Llama-3.2-3B    | 3                           | summarization | 11                    | 2814.82                      | 28.63                     | 2825.36                       | 28.73                      | 1.00              |
| facebook/layerskip-llama3-8B | 8             | Early Exit @ Layer 4       |                             | summarization | 8                     | 1949.02                      | 15.75                     | 3571.81                       | 28.87                      | 1.83              |

#### Llama2 70B

| Model Variant (æ¨¡å‹å˜ä½“)          | Layers (å±‚æ•°) | Assistant Model (è¾…åŠ©æ¨¡å‹) | Assistant Layers (è¾…åŠ©å±‚æ•°) | Task (ä»»åŠ¡)   | Total Layers (æ€»å±‚æ•°) | FLOPs/Input (G) (è¾“å…¥ FLOPs) | Time/Input (s) (è¾“å…¥æ—¶é—´) | FLOPs/Output (G) (è¾“å‡º FLOPs) | Time/Output (s) (è¾“å‡ºæ—¶é—´) | Efficiency (æ•ˆç‡) |
| :-------------------------------- | :------------ | :------------------------- | :-------------------------- | :------------ | :-------------------- | :--------------------------- | :------------------------ | :---------------------------- | :------------------------- | :---------------- |
| meta-llama/Llama-2-70b-hf         | 70            | meta-llama/Llama-2-13b-hf  | 13                          | summarization | 83                    | 5036.54                      | 46.3                      | 12289.01                      | 112.97                     | 2.44              |
| meta-llama/Llama-2-70b-hf         | 70            | meta-llama/Llama-2-7b-hf   | 7                           | summarization | 77                    | 4357.55                      | 40.06                     | 12324.19                      | 113.3                      | 2.83              |
| meta-llama/Llama-2-70b-hf         | 70            | TinyLlama/TinyLlama_v1.1   | 1                           | summarization | 71                    | 4356.21                      | 40.05                     | 12363.22                      | 113.66                     | 2.84              |
| **facebook/layerskip-llama2-70B** | 70            | Early Exit @ Layer 10      |                             | summarization | 70                    | 6012.04                      | 54.96                     | 1283.34                       | 113.2                      | 2.06              |

#### Llama2 13B

| Model Variant (æ¨¡å‹å˜ä½“)          | Layers (å±‚æ•°) | Assistant Model (è¾…åŠ©æ¨¡å‹) | Assistant Layers (è¾…åŠ©å±‚æ•°) | Task (ä»»åŠ¡)       | Total Layers (æ€»å±‚æ•°) | FLOPs/Input (G) (è¾“å…¥ FLOPs) | Time/Input (s) (è¾“å…¥æ—¶é—´) | FLOPs/Output (G) (è¾“å‡º FLOPs) | Time/Output (s) (è¾“å‡ºæ—¶é—´) | Efficiency (æ•ˆç‡) |
| :-------------------------------- | :------------ | :------------------------- | :-------------------------- | :---------------- | :-------------------- | :--------------------------- | :------------------------ | :---------------------------- | :------------------------- | :---------------- |
| meta-llama/Llama-2-13b-hf         | 13            | meta-llama/Llama-2-7b-hf   | 7                           | summarization     | 20                    | 3557.07                      | 27.79                     | 4088.48                       | 31.94                      | 1.15              |
| meta-llama/Llama-2-13b-hf         | 13            | TinyLlama/TinyLlama_v1.1   | 1                           | summarization     | 14                    | 2901.92                      | 22.67                     | 4190.42                       | 32.74                      | 1.44              |
| meta-llama/Llama-2-13b-hf         | 13            | apple/OpenELM-270M         | 0.27                        | summarization     | 13.27                 | 2883.33                      | 22.53                     | 4521.12                       | 35.32                      | 1.57              |
| meta-llama/Llama-2-13b-hf         | 13            | apple/OpenELM-450M         | 0.45                        | summarization     | 13.45                 | 3267.69                      | 25.53                     | 4321.75                       | 33.76                      | 1.32              |
| **facebook/layerskip-llama2-13B** | **13**        | **Early Exit @ Layer 4**   |                             | **summarization** | **13**                | **4238.45**                  | **33.11**                 | **4217.78**                   | **32.95**                  | **0.995**         |
| **facebook/layerskip-llama2-13B** | 13            | Early Exit @ Layer 8       |                             | summarization     | 13                    | 2459.61                      | 19.22                     | 4294.98                       | 33.55                      | 1.746             |

#### Llama2 7B

| Model Variant (æ¨¡å‹å˜ä½“)         | Layers (å±‚æ•°) | Assistant Model (è¾…åŠ©æ¨¡å‹) | Assistant Layers (è¾…åŠ©å±‚æ•°) | Task (ä»»åŠ¡)   | Total Layers (æ€»å±‚æ•°) | FLOPs/Input (G) (è¾“å…¥ FLOPs) | Time/Input (s) (è¾“å…¥æ—¶é—´) | FLOPs/Output (G) (è¾“å‡º FLOPs) | Time/Output (s) (è¾“å‡ºæ—¶é—´) | Efficiency (æ•ˆç‡) |
| :------------------------------- | :------------ | :------------------------- | :-------------------------- | :------------ | :-------------------- | :--------------------------- | :------------------------ | :---------------------------- | :------------------------- | :---------------- |
| meta-llama/Llama-2-7b-hf         | 7             | TinyLlama/TinyLlama_v1.1   | 1                           | summarization | 8                     | 2771.54                      | 21.65                     | 3368.48                       | 26.32                      | 1.22              |
| meta-llama/Llama-2-7b-hf         | 7             | apple/OpenELM-270M         | 0.27                        | summarization | 7.27                  | 2607.82                      | 20.37                     | 4221.14                       | 32.98                      | 1.62              |
| meta-llama/Llama-2-7b-hf         | 7             | apple/OpenELM-450M         | 0.45                        | summarization | 7.45                  | 3324.68                      | 25.97                     | 4178.66                       | 32.65                      | 1.26              |
| **facebook/layerskip-llama2-7B** | 7             | Early Exit @ Layer 4       |                             | summarization | 7                     | 2548.4                       | 19.91                     | 3306.73                       | 25.83                      | 1.297             |

æˆ‘ä»¬å¯ä»¥è§‚å¯Ÿåˆ°ä»¥ä¸‹å‡ ç‚¹ï¼š

*   ä»â€œ**æ€»å‚æ•°æ•°é‡**â€åˆ—å¯ä»¥çœ‹å‡ºï¼Œè‡ªæ¨æµ‹è§£ç æ¶ˆè€—çš„å†…å­˜æ›´å°‘ï¼Œå› ä¸ºå®ƒä¸éœ€è¦å•ç‹¬çš„è‰ç¨¿æ¨¡å‹ï¼Œå¹¶ä¸”è‰ç¨¿é˜¶æ®µå±‚çš„æƒé‡è¢«é‡ç”¨ã€‚
*   å¯¹äºé™¤ Llama2 70B ä¹‹å¤–çš„æ‰€æœ‰æ¨¡å‹å¤§å°å’Œç”Ÿæˆï¼Œæå‰é€€å‡ºè‡ªæ¨æµ‹è§£ç æ¯”å¸¸è§„çš„ä¸¤æ¨¡å‹æ¨æµ‹è§£ç æ›´å¿«ã€‚
*   ä¸å…¶å®ƒæ¨¡å‹ç›¸æ¯”ï¼ŒLlama2 70B çš„è‡ªæ¨æµ‹è§£ç é€Ÿåº¦æå‡ç›¸å¯¹æœ‰é™ï¼Œå¯èƒ½æœ‰ä¸åŒçš„åŸå› ï¼Œä¾‹å¦‚ï¼ŒLlama2 70B çš„ LayerSkip æ£€æŸ¥ç‚¹æŒç»­é¢„è®­ç»ƒçš„ token è¾ƒå°‘ï¼ˆLlama2 70B ä¸º 328M tokenï¼Œè€Œ Llama2 7B ä¸º 52B tokenï¼‰ã€‚ä½†è¿™æ˜¯æœªæ¥ç ”ç©¶éœ€è¦æ”¹è¿›çš„ä¸€ä¸ªæ–¹é¢ã€‚å°½ç®¡å¦‚æ­¤ï¼Œ70B çš„è‡ªæ¨æµ‹è§£ç æ˜æ˜¾å¿«äºè‡ªå›å½’è§£ç ã€‚

## **è‡ªç”Ÿæˆå’Œè‡ªéªŒè¯**

è‡ªæ¨æµ‹è§£ç è¿‡ç¨‹ä»è‡ªç”Ÿæˆå¼€å§‹ï¼Œå…¶ä¸­è¯å…ƒæ˜¯é€šè¿‡ä»æŸä¸ªä¸­é—´å±‚æå‰é€€å‡ºæ¥ç”Ÿæˆçš„ã€‚æ¨æµ‹è¯å…ƒçš„æ•°é‡å®šä¹‰äº†åœ¨æ­¤é˜¶æ®µç”Ÿæˆå¤šå°‘è‰ç¨¿è¯å…ƒï¼Œè€Œæˆ‘ä»¬é€€å‡ºçš„å±‚å®šä¹‰äº†è‰ç¨¿é˜¶æ®µçš„è§„æ¨¡å’Œå‡†ç¡®æ€§ã€‚è¿™ä¸¤ä¸ªå‚æ•°éƒ½å¯ä»¥åœ¨æ¨ç†æ—¶æ ¹æ®è‰ç¨¿é˜¶æ®µçš„é€Ÿåº¦å’Œå‡†ç¡®æ€§ä¹‹é—´çš„æƒè¡¡æ¥æŒ‡å®šã€‚

ä¸‹ä¸€æ­¥æ˜¯è‡ªéªŒè¯ï¼Œå…¶ä¸­ä½¿ç”¨å®Œæ•´æ¨¡å‹æ¥éªŒè¯è‰ç¨¿è¯å…ƒã€‚éªŒè¯æ¨¡å‹é‡ç”¨è‰ç¨¿æ¨¡å‹ä¸­çš„ç¼“å­˜éƒ¨åˆ†ã€‚å¦‚æœè‰ç¨¿è¯å…ƒä¸éªŒè¯çš„è¯å…ƒä¸€è‡´ï¼Œåˆ™å°†å®ƒä»¬æ·»åŠ åˆ°æœ€ç»ˆè¾“å‡ºä¸­ï¼Œä»è€Œæ›´å¥½åœ°åˆ©ç”¨æˆ‘ä»¬ç³»ç»Ÿä¸­çš„å†…å­˜å¸¦å®½ï¼Œå› ä¸ºä½¿ç”¨å®Œæ•´æ¨¡å‹ç”Ÿæˆä¸€ç³»åˆ—è¯å…ƒæ¯”éªŒè¯è‰ç¨¿è¦æ˜‚è´µå¾—å¤šï¼Œåªè¦æœ‰å‡ ä¸ªè¯å…ƒåŒ¹é…å³å¯ã€‚

åœ¨è‡ªéªŒè¯é˜¶æ®µï¼Œåªæœ‰å‰©ä½™çš„å±‚æ‰ä¼šè¢«è®¡ç®—ä»¥è¿›è¡ŒéªŒè¯ï¼Œå› ä¸ºæ—©æœŸå±‚çš„ç»“æœåœ¨è‰ç¨¿é˜¶æ®µå·²è¢«ç¼“å­˜ã€‚

## **æå‰é€€å‡ºå’ŒååµŒå…¥**

è‡ªæ¨æµ‹è§£ç ä¸­çš„ä¸€é¡¹å…³é”®æŠ€æœ¯æ˜¯æå‰é€€å‡ºï¼Œå³ç”Ÿæˆè¿‡ç¨‹å¯ä»¥åœ¨é¢„å…ˆæŒ‡å®šçš„å±‚åœæ­¢ã€‚ä¸ºäº†å®ç°è¿™ä¸€ç‚¹ï¼Œæˆ‘ä»¬é€šè¿‡å°†è¿™äº›å±‚çš„ logits æŠ•å½±åˆ°è¯­è¨€æ¨¡å‹ (LM) å¤´ä¸Šæ¥ååµŒå…¥å®ƒä»¬ï¼Œä»¥é¢„æµ‹ä¸‹ä¸€ä¸ªè¯å…ƒã€‚è¿™å…è®¸æ¨¡å‹è·³è¿‡åç»­å±‚å¹¶æé«˜æ¨ç†æ—¶é—´ã€‚

å¯ä»¥åœ¨ä»»ä½• transformer å±‚æ‰§è¡ŒååµŒå…¥ï¼Œå°†æå‰é€€å‡ºè½¬å˜ä¸ºä¸€ç§é«˜æ•ˆçš„è¯å…ƒé¢„æµ‹æœºåˆ¶ã€‚ä¸€ä¸ªè‡ªç„¶è€Œç„¶çš„é—®é¢˜å‡ºç°äº†ï¼šå½“ LM å¤´æœ€åˆè¢«è®­ç»ƒä¸ºä»…ä¸æœ€ç»ˆå±‚ä¸€èµ·å·¥ä½œæ—¶ï¼Œå¦‚ä½•ä½¿å…¶é€‚åº”ååµŒå…¥è¾ƒæ—©å±‚çš„ logitsï¼Ÿè¿™å°±æ˜¯è®­ç»ƒä¿®æ”¹å‘æŒ¥ä½œç”¨çš„åœ°æ–¹ã€‚

## **è®­ç»ƒä¿®æ”¹**

åœ¨è®­ç»ƒé˜¶æ®µï¼Œæˆ‘ä»¬å¼•å…¥äº†å±‚ä¸¢å¼ƒï¼Œå®ƒå…è®¸æ¨¡å‹åœ¨è®­ç»ƒæœŸé—´è·³è¿‡æŸäº›å±‚ã€‚ä¸¢å¼ƒç‡åœ¨è¾ƒæ·±çš„å±‚ä¸­é€æ¸å¢åŠ ï¼Œä½¿æ¨¡å‹ä¸å¤ªä¾èµ–å…¶åé¢çš„å±‚ï¼Œå¹¶å¢å¼ºæ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›å¹¶åŠ å¿«è®­ç»ƒé€Ÿåº¦ã€‚

é™¤äº†å±‚ä¸¢å¼ƒä¹‹å¤–ï¼Œè¿˜åº”ç”¨äº†æå‰é€€å‡ºæŸå¤±ï¼Œä»¥ç¡®ä¿ LM å¤´å­¦ä¹ ååµŒå…¥ä¸åŒçš„å±‚ã€‚ä½¿ç”¨æ¯ä¸ªå‡ºå£ï¼ˆä¸­é—´å±‚ï¼‰çš„å½’ä¸€åŒ–æŸå¤±çš„æ€»å’Œæ¥ç»™å‡ºä½¿ç”¨æå‰å‡ºå£è®­ç»ƒæ¨¡å‹çš„æ€»æŸå¤±å‡½æ•°ã€‚è¿™ç§æŠ€æœ¯é€šè¿‡åœ¨æ‰€æœ‰å±‚ä¹‹é—´åˆ†é…å­¦ä¹ ä»»åŠ¡æ¥å®ç°é«˜æ•ˆè®­ç»ƒã€‚

## ä¼˜åŒ–ï¼šå…±äº«æƒé‡ã€å…±äº« KV ç¼“å­˜å’Œå…±äº«è®¡ç®—

è‡ªæ¨æµ‹è§£ç æ˜¾è‘—å—ç›Šäºç¼“å­˜é‡ç”¨ï¼Œç‰¹åˆ«æ˜¯ KV ç¼“å­˜ï¼Œå®ƒå­˜å‚¨åœ¨è‰ç¨¿é˜¶æ®µè®¡ç®—çš„é”®å€¼å¯¹ã€‚æ­¤ç¼“å­˜å…è®¸æ¨¡å‹è·³è¿‡å†—ä½™è®¡ç®—ï¼Œå› ä¸ºè‰ç¨¿å’ŒéªŒè¯é˜¶æ®µéƒ½ä½¿ç”¨ç›¸åŒçš„æ—©æœŸå±‚ã€‚æ­¤å¤–ï¼Œé€€å‡ºæŸ¥è¯¢ç¼“å­˜å­˜å‚¨æ¥è‡ªé€€å‡ºå±‚çš„æŸ¥è¯¢å‘é‡ï¼Œå…è®¸éªŒè¯ä»è‰ç¨¿é˜¶æ®µæ— ç¼ç»§ç»­ã€‚

ä¸ä¼ ç»Ÿçš„åŒæ¨¡å‹æ¨æµ‹è§£ç ç›¸æ¯”ï¼Œæå‰é€€å‡ºè‡ªæ¨æµ‹è§£ç å¯ä»¥ä»ä»¥ä¸‹èŠ‚çœä¸­å—ç›Šï¼š

*   **å…±äº«æƒé‡**ï¼šä¸ºè‰ç¨¿å’ŒéªŒè¯é‡ç”¨å‰ E å±‚ çš„æƒé‡ã€‚
*   **å…±äº« KV ç¼“å­˜**ï¼š ä¸ºè‰ç¨¿å’ŒéªŒè¯é‡ç”¨å‰ E å±‚ çš„é”®å€¼å¯¹
*   **å…±äº«è®¡ç®—**ï¼š é€šè¿‡ä½¿ç”¨ä»…ä¿å­˜é€€å‡ºå±‚ E-1 çš„æŸ¥è¯¢å‘é‡çš„é€€å‡ºæŸ¥è¯¢ç¼“å­˜æ¥é‡ç”¨å‰ E å±‚çš„è®¡ç®—ï¼Œä»¥ä¾¿éªŒè¯è¿‡ç¨‹æ— éœ€è®¡ç®—å±‚ 0 åˆ° E-1ã€‚

KV å’Œé€€å‡ºæŸ¥è¯¢ç¼“å­˜çš„ç»„åˆç§°ä¸º KVQ ç¼“å­˜ï¼Œå¯å‡å°‘å†…å­˜å¼€é”€å¹¶æé«˜æ¨ç†å»¶è¿Ÿã€‚

åˆ°ç›®å‰ä¸ºæ­¢ï¼ŒğŸ¤— transformers åº“å·²åœ¨æ­¤ [pull request](https://github.com/huggingface/transformers/pull/30890) ä¸­å®ç°äº†ç¬¬ä¸€ä¸ªä¼˜åŒ–ï¼ˆå…±äº«æƒé‡ï¼‰ã€‚éšç€ä½¿ç”¨æ­¤æ–¹æ³•çš„æ¨¡å‹æ•°é‡å¢åŠ ï¼Œæˆ‘ä»¬å°†è€ƒè™‘å…¶ä»–ä¼˜åŒ–ã€‚å¦‚æœæ‚¨æœ‰å…´è¶£ï¼Œè¯·éšæ—¶æå‡º PRï¼

## æå‰é€€å‡ºå±‚çš„é€‰æ‹©ç­–ç•¥

è‰ç¨¿é˜¶æ®µçš„æå‰é€€å‡ºå±‚æ˜¯ä¸€ä¸ªè¶…å‚æ•°ï¼Œæˆ‘ä»¬å¯ä»¥åœ¨æ¨ç†æœŸé—´è°ƒæ•´æˆ–ä¿®æ”¹ï¼š

*   æˆ‘ä»¬è¶Šæ—©é€€å‡ºï¼Œç”Ÿæˆè‰ç¨¿è¯å…ƒçš„é€Ÿåº¦å°±è¶Šå¿«ï¼Œä½†å®ƒä»¬çš„å‡†ç¡®æ€§å°±è¶Šä½ã€‚
*   æˆ‘ä»¬è¶Šæ™šé€€å‡ºï¼Œç”Ÿæˆçš„è‰ç¨¿è¯å…ƒå°±è¶Šå‡†ç¡®ï¼Œä½†å®ƒä»¬çš„é€Ÿåº¦å°±è¶Šæ…¢ã€‚

æˆ‘ä»¬ç¼–å†™äº†ä¸€ä¸ªè„šæœ¬æ¥éå†ä¸åŒçš„æå‰é€€å‡ºå±‚å¹¶æµ‹é‡ A100 GPU ä¸Šçš„æ¯ç§’è¯å…ƒæ•°ã€‚åœ¨ä¸‹é¢çš„è¡¨æ ¼ä¸­ï¼Œæˆ‘ä»¬ç»˜åˆ¶äº†é’ˆå¯¹ä¸åŒ Llama æ¨¡å‹çš„ LayerSkip å’ŒåŸºçº¿æ£€æŸ¥ç‚¹çš„æ¯ç§’è¯å…ƒæ•°ä¸æå‰é€€å‡ºå±‚çš„å…³ç³»å›¾ï¼ˆæ‚¨å¯ä»¥åœ¨[æ­¤å¤„](https://docs.google.com/spreadsheets/d/15poLaR_7tG_5xZo-LzLMFd4dzz-dHl_h/edit#gid=1155443081)æŸ¥çœ‹å®Œæ•´æ—¥å¿—ï¼‰ã€‚

#### Llama3.2 1B

|                      Normal (å¸¸è§„æ¨¡å‹)                       |                  LayerSkip (LayerSkip æ¨¡å‹)                  |
| :----------------------------------------------------------: | :----------------------------------------------------------: |
| ![llama 3.2 1b](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/layerskip-assets/Llama-3.2-1B.png) | ![layer skip llama 3.2 1b](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/layerskip-assets/LS-Llama3.2-1B.png) |

#### Llama3 8B

|                      Normal (å¸¸è§„æ¨¡å‹)                       |                  LayerSkip (LayerSkip æ¨¡å‹)                  |
| :----------------------------------------------------------: | :----------------------------------------------------------: |
| ![llama 3 8b](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/layerskip-assets/Llama-3-8B.png) | ![layer skip llama 3 8b](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/layerskip-assets/LS-Llama3-8B.png) |

#### Code Llama3 34B

|                      Normal (å¸¸è§„æ¨¡å‹)                       |                  LayerSkip (LayerSkip æ¨¡å‹)                  |
| :----------------------------------------------------------: | :----------------------------------------------------------: |
| ![code llama 3 34b](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/layerskip-assets/CodeLlama-34B.png) | ![code layer skip llama 3 34b](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/layerskip-assets/LS-CodeLlama-34B.png) |

#### Code Llama3 7B

|                      Normal (å¸¸è§„æ¨¡å‹)                       |                  LayerSkip (LayerSkip æ¨¡å‹)                  |
| :----------------------------------------------------------: | :----------------------------------------------------------: |
| ![code llama 3 7b](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/layerskip-assets/CodeLlama-7B.png) | ![code layer skip llama 3 7b](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/layerskip-assets/LS-CodeLlama-7B.png) |

#### Llama2 70B

|                      Normal (å¸¸è§„æ¨¡å‹)                       |                  LayerSkip (LayerSkip æ¨¡å‹)                  |
| :----------------------------------------------------------: | :----------------------------------------------------------: |
| ![llama 2 70b](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/layerskip-assets/Llama-2-70B.png) | ![layer skip llama 2 70b](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/layerskip-assets/LS-Llama2-70B.png) |

#### Llama2 13B

|                      Normal (å¸¸è§„æ¨¡å‹)                       |                  LayerSkip (LayerSkip æ¨¡å‹)                  |
| :----------------------------------------------------------: | :----------------------------------------------------------: |
| ![llama 2 13b](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/layerskip-assets/Llama-2-13B.png) | ![layer skip llama 2 13b](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/layerskip-assets/LS-Llama2-13B.png) |

#### Llama2 7B

|                      Normal (å¸¸è§„æ¨¡å‹)                       |                  LayerSkip (LayerSkip æ¨¡å‹)                  |
| :----------------------------------------------------------: | :----------------------------------------------------------: |
| ![llama 2 7b](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/layerskip-assets/Llama-2-7B.png) | ![layer skip llama 2 7b](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/layerskip-assets/LS-Llama2-7B.png) |

æˆ‘ä»¬å¯ä»¥è§‚å¯Ÿåˆ°ä»¥ä¸‹å‡ ç‚¹ï¼š

*   å¯¹äºæ²¡æœ‰ä½¿ç”¨ LayerSkip è®­ç»ƒæ–¹æ³•è¿›è¡Œé¢„è®­ç»ƒæˆ–æŒç»­é¢„è®­ç»ƒçš„åŸºçº¿æ£€æŸ¥ç‚¹ï¼Œæå‰é€€å‡ºè‡ªæ¨æµ‹è§£ç æ¯”è‡ªå›å½’è§£ç æ›´æ…¢ã€‚è¿™æ˜¯å› ä¸ºåœ¨å¤§å¤šæ•° LLM çš„è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œæ—©æœŸå±‚å¹¶æ²¡æœ‰è¢«æ¿€åŠ±å»å­¦ä¹ é¢„æµ‹è¾“å‡ºï¼Œå› æ­¤ä½¿ç”¨æ—©æœŸå±‚ç”Ÿæˆè¯å…ƒçš„æ¥å—ç‡ä¼šéå¸¸ä½ã€‚
*   å¦ä¸€æ–¹é¢ï¼Œå¯¹äºä½¿ç”¨ LayerSkip è®­ç»ƒæ–¹æ³•æŒç»­é¢„è®­ç»ƒçš„ Llama æ£€æŸ¥ç‚¹ï¼Œæå‰é€€å‡ºè‡ªæ¨æµ‹è§£ç åœ¨è‡³å°‘ä¸€éƒ¨åˆ†å±‚ä¸­æ¯”è‡ªå›å½’è§£ç å…·æœ‰æ›´é«˜çš„åŠ é€Ÿæ¯”ã€‚
    *   å¯¹äºå¤§å¤šæ•°æ¨¡å‹ï¼ˆé™¤äº† Llama3.2 1Bï¼‰ï¼Œå½“æˆ‘ä»¬éå†å„å±‚æ—¶ï¼Œæˆ‘ä»¬æ³¨æ„åˆ°ä¸€ä¸ªè§„å¾‹æ¨¡å¼ï¼šåŠ é€Ÿæ¯”åœ¨å‰å‡ å±‚è¾ƒä½ï¼Œé€æ¸å¢åŠ åˆ°ä¸€ä¸ªæœ€ä½³ç‚¹ï¼Œç„¶åå†æ¬¡ä¸‹é™ã€‚
    *   æå‰é€€å‡ºå±‚çš„æœ€ä½³ç‚¹æ˜¯åœ¨é¢„æµ‹çš„é«˜å‡†ç¡®æ€§å’Œç”Ÿæˆè¯å…ƒçš„ä½å¼€é”€ä¹‹é—´è¾¾åˆ°æœ€ä½³æƒè¡¡æ—¶ã€‚è¿™ä¸ªæœ€ä½³ç‚¹å–å†³äºæ¯ä¸ªæ¨¡å‹ï¼Œä¹Ÿå¯èƒ½å–å†³äºæç¤ºæˆ–æç¤ºçš„é¢†åŸŸã€‚

è¿™äº›è§‚å¯Ÿä¸ºè¿›ä¸€æ­¥çš„å®éªŒå’Œæ¢ç´¢æä¾›äº†æœ‰è¶£çš„æœºä¼šã€‚æˆ‘ä»¬é¼“åŠ±è¯»è€…åœ¨è¿™äº›æƒ³æ³•çš„åŸºç¡€ä¸Šè¿›è¡Œæ„å»ºï¼Œæµ‹è¯•å˜ä½“ï¼Œå¹¶è¿›è¡Œè‡ªå·±çš„ç ”ç©¶ã€‚è¿™äº›åŠªåŠ›å¯ä»¥å¸¦æ¥æœ‰ä»·å€¼çš„è§è§£ï¼Œå¹¶ä¸ºè¯¥é¢†åŸŸåšå‡ºæœ‰æ„ä¹‰çš„è´¡çŒ®ã€‚

## ç»“è®º

LayerSkip åˆ©ç”¨æå‰é€€å‡ºã€å±‚ä¸¢å¼ƒå’Œç¼“å­˜é‡ç”¨ä¹‹é—´çš„ååŒä½œç”¨ï¼Œåˆ›å»ºäº†ä¸€ä¸ªå¿«é€Ÿé«˜æ•ˆçš„æ–‡æœ¬ç”Ÿæˆæµç¨‹ã€‚é€šè¿‡è®­ç»ƒæ¨¡å‹ä»ä¸åŒå±‚ååµŒå…¥è¾“å‡ºï¼Œå¹¶ä½¿ç”¨ç¼“å­˜ä¼˜åŒ–éªŒè¯è¿‡ç¨‹ï¼Œè¿™ç§æ–¹æ³•åœ¨é€Ÿåº¦å’Œå‡†ç¡®æ€§ä¹‹é—´å–å¾—äº†å¹³è¡¡ã€‚å› æ­¤ï¼Œå®ƒæ˜¾è‘—æ”¹å–„äº†å¤§è¯­è¨€æ¨¡å‹çš„æ¨ç†æ—¶é—´ï¼ŒåŒæ—¶ä¿æŒäº†é«˜è´¨é‡çš„è¾“å‡ºã€‚ç”±äºä½¿ç”¨å•ä¸ªæ¨¡å‹ä½œä¸ºè‰ç¨¿å’ŒéªŒè¯æ¨¡å‹ï¼Œå®ƒè¿˜æ¯”ä¼ ç»Ÿçš„æ¨æµ‹è§£ç æŠ€æœ¯å‡å°‘äº†å†…å­˜ä½¿ç”¨ã€‚

è‡ªæ¨æµ‹æ˜¯ä¸€ä¸ªä»¤äººå…´å¥‹çš„é¢†åŸŸï¼ŒåŒä¸€ä¸ª LLM å¯ä»¥åˆ›å»ºè‰ç¨¿è¯å…ƒå¹¶è‡ªæˆ‘ä¿®æ­£ã€‚å…¶ä»–è‡ªæ¨æµ‹æ–¹æ³•åŒ…æ‹¬ï¼š

*   [Draft & Verify](https://aclanthology.org/2024.acl-long.607/)ï¼šå…¶ä¸­è‰ç¨¿é˜¶æ®µæ¶‰åŠè·³è¿‡é¢„å®šçš„æ³¨æ„åŠ›å’Œå‰é¦ˆå±‚ã€‚
*   [MagicDec](https://arxiv.org/abs/2408.11049)ï¼šå…¶ä¸­è‰ç¨¿é˜¶æ®µä½¿ç”¨ KV ç¼“å­˜çš„å­é›†ï¼Œè¿™å¯¹é•¿ä¸Šä¸‹æ–‡è¾“å…¥å¾ˆæœ‰ç”¨ã€‚
*   [Jacobi Decoding](https://arxiv.org/abs/2305.10427) å’Œ [Lookahead Decoding](https://arxiv.org/abs/2402.02057)ï¼šå…¶ä¸­è‰ç¨¿é˜¶æ®µæ˜¯ä¸€ç³»åˆ—â€œçŒœæµ‹è¯å…ƒâ€ï¼Œå¯ä»¥æ˜¯éšæœºçš„æˆ–ä» n-gram æŸ¥æ‰¾è¡¨ä¸­è·å¾—çš„ã€‚

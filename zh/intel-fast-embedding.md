---
title: "åˆ©ç”¨ ğŸ¤— Optimum Intel å’Œ fastRAG åœ¨ CPU ä¸Šä¼˜åŒ–æ–‡æœ¬åµŒå…¥"
thumbnail: /blog/assets/optimum_intel/thumbnail.png
authors:
- user: peterizsak
  guest: true
- user: mber
  guest: true
- user: danf
  guest: true
- user: echarlaix
- user: mfuntowicz
- user: moshew
  guest: true
translators:
- user: MatrixYao
- user: zhongdongy
  proofreader: true
---

# åˆ©ç”¨ ğŸ¤— Optimum Intel å’Œ fastRAG åœ¨ CPU ä¸Šä¼˜åŒ–æ–‡æœ¬åµŒå…¥

åµŒå…¥æ¨¡å‹åœ¨å¾ˆå¤šåœºåˆéƒ½æœ‰å¹¿æ³›åº”ç”¨ï¼Œå¦‚æ£€ç´¢ã€é‡æ’ã€èšç±»ä»¥åŠåˆ†ç±»ã€‚è¿‘å¹´æ¥ï¼Œç ”ç©¶ç•Œåœ¨åµŒå…¥æ¨¡å‹é¢†åŸŸå–å¾—äº†å¾ˆå¤§çš„è¿›å±•ï¼Œè¿™äº›è¿›å±•å¤§å¤§æé«˜äº†åŸºäºè¯­ä¹‰çš„åº”ç”¨çš„ç«äº‰åŠ›ã€‚[BGE](https://huggingface.co/BAAI/bge-large-en-v1.5)ã€[GTE](https://huggingface.co/thenlper/gte-small) ä»¥åŠ [E5](https://huggingface.co/intfloat/multilingual-e5-large) ç­‰æ¨¡å‹åœ¨ [MTEB](https://github.com/embeddings-benchmark/mteb) åŸºå‡†ä¸Šé•¿æœŸéœ¸æ¦œï¼Œåœ¨æŸäº›æƒ…å†µä¸‹ç”šè‡³ä¼˜äºç§æœ‰çš„åµŒå…¥æœåŠ¡ã€‚ Hugging Face æ¨¡å‹ hub æä¾›äº†å¤šç§å°ºå¯¸çš„åµŒå…¥æ¨¡å‹ï¼Œä»è½»é‡çº§ (100-350M å‚æ•°) åˆ° 7B (å¦‚ [Salesforce/SFR-Embedding-Mistral](http://Salesforce/SFR-Embedding-Mistral) ) ä¸€åº”ä¿±å…¨ã€‚ä¸å°‘åŸºäºè¯­ä¹‰æœç´¢çš„åº”ç”¨ä¼šé€‰ç”¨åŸºäºç¼–ç å™¨æ¶æ„çš„è½»é‡çº§æ¨¡å‹ä½œä¸ºå…¶åµŒå…¥æ¨¡å‹ï¼Œæ­¤æ—¶ï¼ŒCPU å°±æˆä¸ºè¿è¡Œè¿™äº›è½»é‡çº§æ¨¡å‹çš„æœ‰åŠ›å€™é€‰ï¼Œä¸€ä¸ªå…¸å‹çš„åœºæ™¯å°±æ˜¯ [æ£€ç´¢å¢å¼ºç”Ÿæˆ (Retrieval Augmented Generationï¼ŒRAG)](https://en.wikipedia.org/wiki/Prompt_engineering#Retrieval-augmented_generation)ã€‚

## ä½¿ç”¨åµŒå…¥æ¨¡å‹è¿›è¡Œä¿¡æ¯æ£€ç´¢

åµŒå…¥æ¨¡å‹æŠŠæ–‡æœ¬æ•°æ®ç¼–ç ä¸ºç¨ å¯†å‘é‡ï¼Œè¿™äº›ç¨ å¯†å‘é‡ä¸­æµ“ç¼©äº†æ–‡æœ¬çš„è¯­ä¹‰åŠä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚è¿™ç§ä¸Šä¸‹æ–‡ç›¸å…³çš„å•è¯å’Œæ–‡æ¡£è¡¨å¾æ–¹å¼ä½¿å¾—æˆ‘ä»¬æœ‰å¯èƒ½å®ç°æ›´å‡†ç¡®çš„ä¿¡æ¯æ£€ç´¢ã€‚é€šå¸¸ï¼Œæˆ‘ä»¬å¯ä»¥ç”¨åµŒå…¥å‘é‡ä¹‹é—´çš„ä½™å¼¦ç›¸ä¼¼åº¦æ¥åº¦é‡æ–‡æœ¬é—´çš„è¯­ä¹‰ç›¸ä¼¼åº¦ã€‚

åœ¨ä¿¡æ¯æ£€ç´¢ä¸­æ˜¯å¦ä»…ä¾èµ–ç¨ å¯†å‘é‡å°±å¯ä»¥äº†ï¼Ÿè¿™éœ€è¦ä¸€å®šçš„æƒè¡¡:

- ç¨€ç–æ£€ç´¢é€šè¿‡æŠŠæ–‡æœ¬é›†å»ºæ¨¡æˆ n- å…ƒç»„ã€çŸ­è¯­æˆ–å…ƒæ•°æ®çš„é›†åˆï¼Œå¹¶é€šè¿‡åœ¨é›†åˆä¸Šè¿›è¡Œé«˜æ•ˆã€å¤§è§„æ¨¡çš„æœç´¢æ¥å®ç°ä¿¡æ¯æ£€ç´¢ã€‚ç„¶è€Œï¼Œç”±äºæŸ¥è¯¢å’Œæ–‡æ¡£åœ¨ç”¨è¯ä¸Šå¯èƒ½å­˜åœ¨å·®å¼‚ï¼Œè¿™ç§æ–¹æ³•æœ‰å¯èƒ½ä¼šæ¼æ‰ä¸€äº›ç›¸å…³çš„æ–‡æ¡£ã€‚
- è¯­ä¹‰æ£€ç´¢å°†æ–‡æœ¬ç¼–ç ä¸ºç¨ å¯†å‘é‡ï¼Œç›¸æ¯”äºè¯è¢‹ï¼Œå…¶èƒ½æ›´å¥½åœ°æ•è·ä¸Šä¸‹æ–‡åŠè¯ä¹‰ã€‚æ­¤æ—¶ï¼Œå³ä½¿ç”¨è¯ä¸Šä¸èƒ½ç²¾ç¡®åŒ¹é…ï¼Œè¿™ç§æ–¹æ³•ä»ç„¶å¯ä»¥æ£€ç´¢å‡ºè¯­ä¹‰ç›¸å…³çš„æ–‡æ¡£ã€‚ç„¶è€Œï¼Œä¸ BM25 ç­‰è¯åŒ¹é…æ–¹æ³•ç›¸æ¯”ï¼Œè¯­ä¹‰æ£€ç´¢çš„è®¡ç®—é‡æ›´å¤§ï¼Œå»¶è¿Ÿæ›´é«˜ï¼Œå¹¶ä¸”éœ€è¦ç”¨åˆ°å¤æ‚çš„ç¼–ç æ¨¡å‹ã€‚

### åµŒå…¥æ¨¡å‹ä¸ RAG

åµŒå…¥æ¨¡å‹åœ¨ RAG åº”ç”¨çš„å¤šä¸ªç¯èŠ‚ä¸­å‡èµ·åˆ°äº†å…³é”®çš„ä½œç”¨:

- ç¦»çº¿å¤„ç†: åœ¨ç”Ÿæˆæˆ–æ›´æ–°æ–‡æ¡£æ•°æ®åº“çš„ç´¢å¼•æ—¶ï¼Œè¦ç”¨åµŒå…¥æ¨¡å‹å°†æ–‡æ¡£ç¼–ç ä¸ºç¨ å¯†å‘é‡ã€‚
- æŸ¥è¯¢ç¼–ç : åœ¨æŸ¥è¯¢æ—¶ï¼Œè¦ç”¨åµŒå…¥æ¨¡å‹å°†è¾“å…¥æŸ¥è¯¢ç¼–ç ä¸ºç¨ å¯†å‘é‡ä»¥ä¾›åç»­æ£€ç´¢ã€‚
- é‡æ’: é¦–è½®æ£€ç´¢å‡ºåˆå§‹å€™é€‰æ–‡æ¡£åˆ—è¡¨åï¼Œè¦ç”¨åµŒå…¥æ¨¡å‹å°†æ£€ç´¢åˆ°çš„æ–‡æ¡£ç¼–ç ä¸ºç¨ å¯†å‘é‡å¹¶ä¸æŸ¥è¯¢å‘é‡è¿›è¡Œæ¯”è¾ƒï¼Œä»¥å®Œæˆé‡æ’ã€‚

å¯è§ï¼Œä¸ºäº†è®©æ•´ä¸ªåº”ç”¨æ›´é«˜æ•ˆï¼Œä¼˜åŒ– RAG æµæ°´çº¿ä¸­çš„åµŒå…¥æ¨¡å‹è¿™ä¸€ç¯èŠ‚éå¸¸å¿…è¦ï¼Œå…·ä½“æ¥è¯´:

- æ–‡æ¡£ç´¢å¼•/æ›´æ–°: è¿½æ±‚é«˜ååï¼Œè¿™æ ·å°±èƒ½æ›´å¿«åœ°å¯¹å¤§å‹æ–‡æ¡£é›†è¿›è¡Œç¼–ç å’Œç´¢å¼•ï¼Œä»è€Œå¤§å¤§ç¼©çŸ­å»ºåº“å’Œæ›´æ–°è€—æ—¶ã€‚
- æŸ¥è¯¢ç¼–ç : è¾ƒä½çš„æŸ¥è¯¢ç¼–ç å»¶è¿Ÿå¯¹äºæ£€ç´¢çš„å®æ—¶æ€§è‡³å…³é‡è¦ã€‚æ›´é«˜çš„ååå¯ä»¥æ”¯æŒæ›´é«˜æŸ¥è¯¢å¹¶å‘åº¦ï¼Œä»è€Œå®ç°é«˜æ‰©å±•åº¦ã€‚
- å¯¹æ£€ç´¢åˆ°çš„æ–‡æ¡£è¿›è¡Œé‡æ’: é¦–è½®æ£€ç´¢åï¼ŒåµŒå…¥æ¨¡å‹éœ€è¦å¿«é€Ÿå¯¹æ£€ç´¢åˆ°çš„å€™é€‰æ–‡æ¡£è¿›è¡Œç¼–ç ä»¥æ”¯æŒé‡æ’ã€‚è¾ƒä½çš„ç¼–ç å»¶è¿Ÿæ„å‘³ç€é‡æ’çš„é€Ÿåº¦ä¼šæ›´å¿«ï¼Œä»è€Œæ›´èƒ½æ»¡è¶³æ—¶é—´æ•æ„Ÿå‹åº”ç”¨çš„è¦æ±‚ã€‚åŒæ—¶ï¼Œæ›´é«˜çš„ååæ„å‘³ç€å¯ä»¥å¹¶è¡Œå¯¹æ›´å¤§çš„å€™é€‰é›†è¿›è¡Œé‡æ’ï¼Œä»è€Œä½¿å¾—æ›´å…¨é¢çš„é‡æ’æˆä¸ºå¯èƒ½ã€‚

## ä½¿ç”¨ Optimum Intel å’Œ IPEX ä¼˜åŒ–åµŒå…¥æ¨¡å‹

[Optimum Intel](https://github.com/huggingface/optimum-intel) æ˜¯ä¸€ä¸ªå¼€æºåº“ï¼Œå…¶é’ˆå¯¹è‹±ç‰¹å°”ç¡¬ä»¶å¯¹ä½¿ç”¨ Hugging Face åº“æ„å»ºçš„ç«¯åˆ°ç«¯æµæ°´çº¿è¿›è¡ŒåŠ é€Ÿå’Œä¼˜åŒ–ã€‚ `Optimum Intel` å®ç°äº†å¤šç§æ¨¡å‹åŠ é€ŸæŠ€æœ¯ï¼Œå¦‚ä½æ¯”ç‰¹é‡åŒ–ã€æ¨¡å‹æƒé‡ä¿®å‰ªã€è’¸é¦ä»¥åŠè¿è¡Œæ—¶ä¼˜åŒ–ã€‚

[Optimum Intel](https://github.com/huggingface/optimum-intel) åœ¨ä¼˜åŒ–æ—¶å……åˆ†åˆ©ç”¨äº†è‹±ç‰¹å°”Â® å…ˆè¿›çŸ¢é‡æ‰©å±• 512 (è‹±ç‰¹å°”Â® AVX-512) ã€çŸ¢é‡ç¥ç»ç½‘ç»œæŒ‡ä»¤ (Vector Neural Network Instructionsï¼ŒVNNI) ä»¥åŠè‹±ç‰¹å°”Â® é«˜çº§çŸ©é˜µæ‰©å±• (è‹±ç‰¹å°”Â® AMX) ç­‰ç‰¹æ€§ä»¥åŠ é€Ÿæ¨¡å‹çš„è¿è¡Œã€‚å…·ä½“æ¥è¯´ï¼Œæ¯ä¸ª CPU æ ¸ä¸­éƒ½å†…ç½®äº† [BFloat16](https://en.wikipedia.org/wiki/Bfloat16_floating-point_format) ( `bf16` ) å’Œ `int8` GEMM åŠ é€Ÿå™¨ï¼Œä»¥åŠ é€Ÿæ·±åº¦å­¦ä¹ è®­ç»ƒå’Œæ¨ç†å·¥ä½œè´Ÿè½½ã€‚é™¤äº†é’ˆå¯¹å„ç§å¸¸è§è¿ç®—çš„ä¼˜åŒ–ä¹‹å¤–ï¼ŒPyTorch 2.0 å’Œ [Intel Extension for PyTorch](https://github.com/intel/intel-extension-for-pytorch) (IPEX) ä¸­è¿˜å……åˆ†åˆ©ç”¨äº† AMX ä»¥åŠ é€Ÿæ¨ç†ã€‚

ä½¿ç”¨ Optimum Intel å¯ä»¥è½»æ¾ä¼˜åŒ–é¢„è®­ç»ƒæ¨¡å‹çš„æ¨ç†ä»»åŠ¡ã€‚ä½ å¯åœ¨ [æ­¤å¤„](https://huggingface.co/docs/optimum/main/en/intel/optimization_inc) æ‰¾åˆ°å¾ˆå¤šç®€å•çš„ä¾‹å­ã€‚

## ç¤ºä¾‹: ä¼˜åŒ– BGE åµŒå…¥æ¨¡å‹

æœ¬æ–‡ï¼Œæˆ‘ä»¬ä¸»è¦å…³æ³¨ [åŒ—äº¬äººå·¥æ™ºèƒ½ç ”ç©¶é™¢](https://arxiv.org/pdf/2309.07597.pdf) çš„ç ”ç©¶äººå‘˜æœ€è¿‘å‘å¸ƒçš„åµŒå…¥æ¨¡å‹ï¼Œå®ƒä»¬åœ¨å¹¿ä¸ºäººçŸ¥çš„ [MTEB](https://github.com/embeddings-benchmark/mteb) æ’è¡Œæ¦œä¸Šå–å¾—äº†äº®çœ¼çš„æ’åã€‚

### BGE æŠ€æœ¯ç»†èŠ‚

åŒç¼–ç å™¨æ¨¡å‹åŸºäº Transformer ç¼–ç å™¨æ¶æ„ï¼Œå…¶è®­ç»ƒç›®æ ‡æ˜¯æœ€å¤§åŒ–ä¸¤ä¸ªè¯­ä¹‰ç›¸ä¼¼çš„æ–‡æœ¬çš„åµŒå…¥å‘é‡ä¹‹é—´çš„ç›¸ä¼¼åº¦ï¼Œå¸¸è§çš„æŒ‡æ ‡æ˜¯ä½™å¼¦ç›¸ä¼¼åº¦ã€‚ä¸¾ä¸ªå¸¸è§çš„ä¾‹å­ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨ BERT æ¨¡å‹ä½œä¸ºåŸºç¡€é¢„è®­ç»ƒæ¨¡å‹ï¼Œå¹¶å¯¹å…¶è¿›è¡Œå¾®è°ƒä»¥ç”ŸæˆåµŒå…¥æ¨¡å‹ä»è€Œä¸ºæ–‡æ¡£ç”ŸæˆåµŒå…¥å‘é‡ã€‚æœ‰å¤šç§æ–¹æ³•å¯ç”¨äºæ ¹æ®æ¨¡å‹è¾“å‡ºæ„é€ å‡ºæ–‡æœ¬çš„åµŒå…¥å‘é‡ï¼Œä¾‹å¦‚ï¼Œå¯ä»¥ç›´æ¥å– [CLS] è¯å…ƒçš„åµŒå…¥å‘é‡ï¼Œä¹Ÿå¯ä»¥å¯¹æ‰€æœ‰è¾“å…¥è¯å…ƒçš„åµŒå…¥å‘é‡å–å¹³å‡å€¼ã€‚

åŒç¼–ç å™¨æ¨¡å‹æ˜¯ä¸ªç›¸å¯¹æ¯”è¾ƒç®€å•çš„åµŒå…¥ç¼–ç æ¶æ„ï¼Œå…¶ä»…é’ˆå¯¹å•ä¸ªæ–‡æ¡£ä¸Šä¸‹æ–‡è¿›è¡Œç¼–ç ï¼Œå› æ­¤å®ƒä»¬æ— æ³•å¯¹è¯¸å¦‚ `æŸ¥è¯¢ - æ–‡æ¡£` åŠ `æ–‡æ¡£ - æ–‡æ¡£` è¿™æ ·çš„äº¤å‰ä¸Šä¸‹æ–‡è¿›è¡Œç¼–ç ã€‚ç„¶è€Œï¼Œæœ€å…ˆè¿›çš„åŒç¼–ç å™¨åµŒå…¥æ¨¡å‹å·²èƒ½è¡¨ç°å‡ºç›¸å½“æœ‰ç«äº‰åŠ›çš„æ€§èƒ½ï¼Œå†åŠ ä¸Šå› å…¶æ¶æ„ç®€å•å¸¦æ¥çš„æå¿«çš„é€Ÿåº¦ï¼Œå› æ­¤è¯¥æ¶æ„çš„æ¨¡å‹æˆä¸ºäº†å½“çº¢ç‚¸å­é¸¡ã€‚

è¿™é‡Œï¼Œæˆ‘ä»¬ä¸»è¦å…³æ³¨ 3 ä¸ª BGE æ¨¡å‹: [small](https://huggingface.co/BAAI/bge-small-en-v1.5)ã€[base](https://huggingface.co/BAAI/bge-base-en-v1.5) ä»¥åŠ [large](https://huggingface.co/BAAI/bge-large-en-v1.5)ï¼Œå®ƒä»¬çš„å‚æ•°é‡åˆ†åˆ«ä¸º 45Mã€110M ä»¥åŠ 355Mï¼ŒåµŒå…¥å‘é‡ç»´åº¦åˆ†åˆ«ä¸º 384ã€768 ä»¥åŠ 1024ã€‚

è¯·æ³¨æ„ï¼Œä¸‹æ–‡å±•ç¤ºçš„ä¼˜åŒ–è¿‡ç¨‹æ˜¯é€šç”¨çš„ï¼Œä½ å¯ä»¥å°†å®ƒä»¬åº”ç”¨äºä»»ä½•å…¶ä»–åµŒå…¥æ¨¡å‹ (åŒ…æ‹¬åŒç¼–ç å™¨æ¨¡å‹ã€äº¤å‰ç¼–ç å™¨æ¨¡å‹ç­‰)ã€‚

### æ¨¡å‹é‡åŒ–åˆ†æ­¥æŒ‡å—

ä¸‹é¢ï¼Œæˆ‘ä»¬å±•ç¤ºå¦‚ä½•æé«˜åµŒå…¥æ¨¡å‹åœ¨ CPU ä¸Šçš„æ€§èƒ½ï¼Œæˆ‘ä»¬çš„ä¼˜åŒ–é‡ç‚¹æ˜¯é™ä½å»¶è¿Ÿ (batch size ä¸º 1) ä»¥åŠæé«˜ååé‡ (ä»¥æ¯ç§’ç¼–ç çš„æ–‡æ¡£æ•°æ¥è¡¡é‡)ã€‚æˆ‘ä»¬ç”¨ `optimum-intel` å’Œ [INC (Intel Neural Compressor) ](https://github.com/intel/neural-compressor) å¯¹æ¨¡å‹è¿›è¡Œé‡åŒ–ï¼Œå¹¶ç”¨ [IPEX](https://github.com/intel/intel-extension-for-pytorch) æ¥ä¼˜åŒ–æ¨¡å‹åœ¨ Intel çš„ç¡¬ä»¶ä¸Šçš„è¿è¡Œæ—¶é—´ã€‚

##### ç¬¬ 1 æ­¥: å®‰è£…è½¯ä»¶åŒ…

è¯·è¿è¡Œä»¥ä¸‹å‘½ä»¤å®‰è£… `optimum-intel` å’Œ `intel-extension-for-transformers` :

```bash
pip install -U optimum[neural-compressor] intel-extension-for-transformers
```

##### ç¬¬ 2 æ­¥: è®­åé™æ€é‡åŒ–

è®­åé™æ€é‡åŒ–éœ€è¦ä¸€ä¸ªæ ¡å‡†é›†ä»¥ç¡®å®šæƒé‡å’Œæ¿€æ´»çš„åŠ¨æ€èŒƒå›´ã€‚æ ¡å‡†æ—¶ï¼Œæ¨¡å‹ä¼šè¿è¡Œä¸€ç»„æœ‰ä»£è¡¨æ€§çš„æ•°æ®æ ·æœ¬ï¼Œæ”¶é›†ç»Ÿè®¡æ•°æ®ï¼Œç„¶åæ ¹æ®æ”¶é›†åˆ°çš„ä¿¡æ¯é‡åŒ–æ¨¡å‹ä»¥æœ€å¤§ç¨‹åº¦åœ°é™ä½å‡†ç¡®ç‡æŸå¤±ã€‚

ä»¥ä¸‹å±•ç¤ºäº†å¯¹æ¨¡å‹è¿›è¡Œé‡åŒ–çš„ä»£ç ç‰‡æ®µ:

```python
def quantize(model_name: str, output_path: str, calibration_set: "datasets.Dataset"):
    model = AutoModel.from_pretrained(model_name)
    tokenizer = AutoTokenizer.from_pretrained(model_name)

    def preprocess_function(examples):
        return tokenizer(examples["text"], padding="max_length", max_length=512, truncation=True)

    vectorized_ds = calibration_set.map(preprocess_function, num_proc=10)
    vectorized_ds = vectorized_ds.remove_columns(["text"])

    quantizer = INCQuantizer.from_pretrained(model)
    quantization_config = PostTrainingQuantConfig(approach="static", backend="ipex", domain="nlp")
    quantizer.quantize(
        quantization_config=quantization_config,
        calibration_dataset=vectorized_ds,
        save_directory=output_path,
        batch_size=1,
    )
    tokenizer.save_pretrained(output_path)
```

æœ¬ä¾‹ä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨ [qasper](https://huggingface.co/datasets/allenai/qasper) æ•°æ®é›†çš„ä¸€ä¸ªå­é›†ä½œä¸ºæ ¡å‡†é›†ã€‚

##### ç¬¬ 2 æ­¥: åŠ è½½æ¨¡å‹ï¼Œè¿è¡Œæ¨ç†

ä»…éœ€è¿è¡Œä»¥ä¸‹å‘½ä»¤ï¼Œå³å¯åŠ è½½é‡åŒ–æ¨¡å‹:

```python
from optimum.intel import IPEXModel

model = IPEXModel.from_pretrained("Intel/bge-small-en-v1.5-rag-int8-static")
```

éšåï¼Œæˆ‘ä»¬ä½¿ç”¨ [transformers](https://github.com/huggingface/transformers) çš„ API å°†å¥å­ç¼–ç ä¸ºå‘é‡:

```python
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("Intel/bge-small-en-v1.5-rag-int8-static")
inputs = tokenizer(sentences, return_tensors="pt")

with torch.no_grad():
    outputs = model(**inputs)
    # get the [CLS] token
    embeddings = outputs[0][:, 0]
```

æˆ‘ä»¬å°†åœ¨éšåçš„æ¨¡å‹è¯„ä¼°éƒ¨åˆ†è¯¦ç»†è¯´æ˜å¦‚ä½•æ­£ç¡®é…ç½® CPU ä»¥è·å¾—æœ€ä½³æ€§èƒ½ã€‚

### ä½¿ç”¨ MTEB è¿›è¡Œæ¨¡å‹è¯„ä¼°

å°†æ¨¡å‹çš„æƒé‡é‡åŒ–åˆ°è¾ƒä½çš„ç²¾åº¦ä¼šå¯¼è‡´å‡†ç¡®åº¦çš„æŸå¤±ï¼Œå› ä¸ºåœ¨æƒé‡ä» `fp32` è½¬æ¢åˆ° `int8` çš„è¿‡ç¨‹ä¸­ä¼šæŸå¤±ç²¾åº¦ã€‚æ‰€ä»¥ï¼Œæˆ‘ä»¬åœ¨å¦‚ä¸‹ä¸¤ä¸ª [MTEB](https://github.com/embeddings-benchmark/mteb) ä»»åŠ¡ä¸Šå¯¹é‡åŒ–æ¨¡å‹ä¸åŸå§‹æ¨¡å‹è¿›è¡Œæ¯”è¾ƒä»¥éªŒè¯é‡åŒ–æ¨¡å‹çš„å‡†ç¡®åº¦åˆ°åº•å¦‚ä½•:

- **æ£€ç´¢** - å¯¹è¯­æ–™åº“è¿›è¡Œç¼–ç ï¼Œå¹¶ç”Ÿæˆç´¢å¼•åº“ï¼Œç„¶ååœ¨ç´¢å¼•åº“ä¸­æœç´¢ç»™å®šæŸ¥è¯¢ï¼Œä»¥æ‰¾å‡ºä¸ç»™å®šæŸ¥è¯¢ç›¸ä¼¼çš„æ–‡æœ¬å¹¶æ’åºã€‚
- **é‡æ’** - å¯¹æ£€ç´¢ç»“æœè¿›è¡Œé‡æ’ï¼Œä»¥ç»†åŒ–ä¸ç»™å®šæŸ¥è¯¢çš„ç›¸å…³æ€§æ’åã€‚

ä¸‹è¡¨å±•ç¤ºäº†æ¯ä¸ªä»»åŠ¡åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„å¹³å‡å‡†ç¡®åº¦ (å…¶ä¸­ï¼ŒMAP ç”¨äºé‡æ’ï¼ŒNDCG@10 ç”¨äºæ£€ç´¢)ï¼Œè¡¨ä¸­ `int8` è¡¨ç¤ºé‡åŒ–æ¨¡å‹ï¼Œ `fp32` è¡¨ç¤ºåŸå§‹æ¨¡å‹ (åŸå§‹æ¨¡å‹ç»“æœå–è‡ªå®˜æ–¹ MTEB æ’è¡Œæ¦œ)ã€‚ä¸åŸå§‹æ¨¡å‹ç›¸æ¯”ï¼Œé‡åŒ–æ¨¡å‹åœ¨é‡æ’ä»»åŠ¡ä¸Šçš„å‡†ç¡®åº¦æŸå¤±ä½äº 1%ï¼Œåœ¨æ£€ç´¢ä»»åŠ¡ä¸­çš„å‡†ç¡®åº¦æŸå¤±ä½äº 1.55%ã€‚

<table>
<tr><th>  </th><th>   é‡æ’ </th><th> æ£€ç´¢ </th></tr>
<tr><td>

|           |
| --------- |
| BGE-small |
| BGE-base  |
| BGE-large |

</td><td>

|  int8  |  fp32  |  å‡†ç¡®åº¦æŸå¤±  |
| ------ | ------ | ------ |
| 0.5826 | 0.5836 | -0.17% |
| 0.5886 | 0.5886 |  0%    |
| 0.5985 | 0.6003 | -0.3%  |

</td><td>

|  int8  |  fp32  |  å‡†ç¡®åº¦æŸå¤±  |
| ------ | ------ | ------ |
| 0.5138 | 0.5168 | -0.58% |
| 0.5242 | 0.5325 | -1.55% |
| 0.5346 | 0.5429 | -1.53% |

</td></tr> </table>

### é€Ÿåº¦ä¸å»¶è¿Ÿ

æˆ‘ä»¬ç”¨é‡åŒ–æ¨¡å‹è¿›è¡Œæ¨ç†ï¼Œå¹¶å°†å…¶ä¸å¦‚ä¸‹ä¸¤ç§å¸¸è§çš„æ¨¡å‹æ¨ç†æ–¹æ³•è¿›è¡Œæ€§èƒ½æ¯”è¾ƒ:

1. ä½¿ç”¨ PyTorch å’Œ Hugging Face çš„ `transformers` åº“ä»¥ `bf16` ç²¾åº¦è¿è¡Œæ¨¡å‹ã€‚
2. ä½¿ç”¨ [IPEX](https://intel.github.io/intel-extension-for-pytorch/#introduction) ä»¥ `bf16` ç²¾åº¦è¿è¡Œæ¨¡å‹ï¼Œå¹¶ä½¿ç”¨ torchscript å¯¹æ¨¡å‹è¿›è¡Œå›¾åŒ–ã€‚

å®éªŒç¯å¢ƒé…ç½®:

- ç¡¬ä»¶ (CPU): ç¬¬å››ä»£ Intel è‡³å¼º 8480+ï¼Œæ•´æœºæœ‰ 2 è·¯ CPUï¼Œæ¯è·¯ 56 ä¸ªæ ¸ã€‚
- å¯¹ PyTorch æ¨¡å‹è¿›è¡Œè¯„ä¼°æ—¶ä»…ä½¿ç”¨å•è·¯ CPU ä¸Šçš„ 56 ä¸ªæ ¸ã€‚
- IPEX/Optimum æµ‹ä¾‹ä½¿ç”¨ ipexrunã€å•è·¯ CPUã€ä½¿ç”¨çš„æ ¸æ•°åœ¨ 22-56 ä¹‹é—´ã€‚
- æ‰€æœ‰æµ‹ä¾‹ TCMallocï¼Œæˆ‘ä»¬å®‰è£…å¹¶å¦¥å–„è®¾ç½®äº†ç›¸åº”çš„ç¯å¢ƒå˜é‡ä»¥ä¿è¯ç”¨åˆ°å®ƒã€‚

### å¦‚ä½•è¿è¡Œè¯„ä¼°ï¼Ÿ

æˆ‘ä»¬å†™äº†ä¸€ä¸ªåŸºäºæ¨¡å‹çš„è¯æ±‡è¡¨ç”Ÿæˆéšæœºæ ·æœ¬çš„è„šæœ¬ã€‚ç„¶ååˆ†åˆ«åŠ è½½åŸå§‹æ¨¡å‹å’Œé‡åŒ–æ¨¡å‹ï¼Œå¹¶æ¯”è¾ƒäº†å®ƒä»¬åœ¨ä¸Šè¿°ä¸¤ç§åœºæ™¯ä¸­çš„ç¼–ç æ—¶é—´: ä½¿ç”¨å• batch size åº¦é‡ç¼–ç å»¶è¿Ÿï¼Œä½¿ç”¨å¤§ batch size åº¦é‡ç¼–ç ååã€‚

1. åŸºçº¿ - ç”¨ PyTorch åŠ Hugging Face è¿è¡Œ `bf16` æ¨¡å‹:

```python
import torch
from transformers import AutoModel

model = AutoModel.from_pretrained("BAAI/bge-small-en-v1.5")

@torch.inference_mode()
def encode_text():
    outputs = model(inputs)

with torch.cpu.amp.autocast(dtype=torch.bfloat16):
    encode_text()
```

1. ç”¨ IPEX torchscript è¿è¡Œ `bf16` æ¨¡å‹:

```python
import torch
from transformers import AutoModel
import intel_extension_for_pytorch as ipex

model = AutoModel.from_pretrained("BAAI/bge-small-en-v1.5")
model = ipex.optimize(model, dtype=torch.bfloat16)

vocab_size = model.config.vocab_size
batch_size = 1
seq_length = 512
d = torch.randint(vocab_size, size=[batch_size, seq_length])
model = torch.jit.trace(model, (d,), check_trace=False, strict=False)
model = torch.jit.freeze(model)

@torch.inference_mode()
def encode_text():
    outputs = model(inputs)

with torch.cpu.amp.autocast(dtype=torch.bfloat16):
    encode_text()
```

1. ç”¨åŸºäº IPEX åç«¯çš„ Optimum Intel è¿è¡Œ `int8` æ¨¡å‹:

```python
import torch
from optimum.intel import IPEXModel

model = IPEXModel.from_pretrained("Intel/bge-small-en-v1.5-rag-int8-static")

@torch.inference_mode()
def encode_text():
    outputs = model(inputs)

encode_text()
```

### å»¶è¿Ÿæ€§èƒ½

è¿™é‡Œï¼Œæˆ‘ä»¬ä¸»è¦æµ‹é‡æ¨¡å‹çš„å“åº”é€Ÿåº¦ï¼Œè¿™å…³ç³»åˆ° RAG æµæ°´çº¿ä¸­å¯¹æŸ¥è¯¢è¿›è¡Œç¼–ç çš„é€Ÿåº¦ã€‚æ­¤æ—¶ï¼Œæˆ‘ä»¬å°† batch size è®¾ä¸º 1ï¼Œå¹¶æµ‹é‡åœ¨å„ç§æ–‡æ¡£é•¿åº¦ä¸‹çš„å»¶è¿Ÿã€‚

æˆ‘ä»¬å¯ä»¥çœ‹åˆ°ï¼Œæ€»çš„æ¥è®²ï¼Œé‡åŒ–æ¨¡å‹å»¶è¿Ÿæœ€å°ï¼Œå…¶ä¸­ `small` æ¨¡å‹å’Œ `base` æ¨¡å‹çš„å»¶è¿Ÿä½äº 10 æ¯«ç§’ï¼Œ `large` æ¨¡å‹çš„å»¶è¿Ÿä½äº 20 æ¯«ç§’ã€‚ä¸åŸå§‹æ¨¡å‹ç›¸æ¯”ï¼Œé‡åŒ–æ¨¡å‹çš„å»¶è¿Ÿæé«˜äº† 4.5 å€ã€‚

<p align="center">
 <img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/178_intel_ipex_quantization/latency.png" alt="latency" style="width: 90%; height: auto;"><br>
<em> å›¾ 1: å„å°ºå¯¸ BGE æ¨¡å‹çš„å»¶è¿Ÿ </em>
</p>

### ååæ€§èƒ½

åœ¨è¯„ä¼°ååæ—¶ï¼Œæˆ‘ä»¬çš„ç›®æ ‡æ˜¯å¯»æ‰¾å³°å€¼ç¼–ç æ€§èƒ½ï¼Œå…¶å•ä½ä¸ºæ¯ç§’å¤„ç†æ–‡æ¡£æ•°ã€‚æˆ‘ä»¬å°†æ–‡æœ¬é•¿åº¦è®¾ç½®ä¸º 256 ä¸ªè¯å…ƒï¼Œè¿™ä¸ªé•¿åº¦èƒ½è¾ƒå¥½åœ°ä»£è¡¨ RAG æµæ°´çº¿ä¸­çš„å¹³å‡æ–‡æ¡£é•¿åº¦ï¼ŒåŒæ—¶æˆ‘ä»¬åœ¨ä¸åŒçš„ batch size (4ã€8ã€16ã€32ã€64ã€128ã€256) ä¸Šè¿›è¡Œè¯„ä¼°ã€‚

ç»“æœè¡¨æ˜ï¼Œä¸å…¶ä»–æ¨¡å‹ç›¸æ¯”ï¼Œé‡åŒ–æ¨¡å‹ååæ›´é«˜ï¼Œä¸”åœ¨ batch size ä¸º 128 æ—¶è¾¾åˆ°å³°å€¼ã€‚æ€»ä½“è€Œè¨€ï¼Œå¯¹äºæ‰€æœ‰å°ºå¯¸çš„æ¨¡å‹ï¼Œé‡åŒ–æ¨¡å‹çš„åååœ¨å„ batch size ä¸Šå‡æ¯”åŸºçº¿ `bf16` æ¨¡å‹é«˜ 4 å€å·¦å³ã€‚

<p align="center">
 <img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/178_intel_ipex_quantization/throughput_small.png" alt="throughput small" style="width: 60%; height: auto;"><br>
<em> å›¾ 2: BGE small æ¨¡å‹çš„åå </em>
</p>

<p align="center">
 <img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/178_intel_ipex_quantization/throughput_base.png" alt="throughput base" style="width: 60%; height: auto;"><br>
<em> å›¾ 3: BGE base æ¨¡å‹çš„åå </em>
</p>

<p align="center">
 <img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/178_intel_ipex_quantization/throughput_large.png" alt="throughput large" style="width: 60%; height: auto;"><br>
<em> å›¾ 3: BGE large æ¨¡å‹çš„åå </em>
</p>

## åœ¨ fastRAG ä¸­ä½¿ç”¨é‡åŒ–åµŒå…¥æ¨¡å‹

æˆ‘ä»¬é€šè¿‡ä¸€ä¸ªä¾‹å­æ¥æ¼”ç¤ºå¦‚ä½•å°†ä¼˜åŒ–åçš„æ£€ç´¢/é‡æ’æ¨¡å‹é›†æˆè¿› [fastRAG](https://github.com/IntelLabs/fastRAG) ä¸­ (ä½ ä¹Ÿå¯ä»¥å¾ˆè½»æ¾åœ°å°†å…¶é›†æˆåˆ°å…¶ä»– RAG æ¡†æ¶ä¸­ï¼Œå¦‚ Langchain åŠ LlamaIndex) ã€‚

[fastRAG](https://github.com/IntelLabs/fastRAG) æ˜¯ä¸€ä¸ªé«˜æ•ˆä¸”ä¼˜åŒ–çš„æ£€ç´¢å¢å¼ºç”Ÿæˆæµæ°´çº¿ç ”ç©¶æ¡†æ¶ï¼Œå…¶å¯ä¸æœ€å…ˆè¿›çš„ LLM å’Œä¿¡æ¯æ£€ç´¢ç®—æ³•ç»“åˆä½¿ç”¨ã€‚fastRAG ä¸ [Haystack](https://haystack.deepset.ai/) å®Œå…¨å…¼å®¹ï¼Œå¹¶å®ç°äº†å¤šç§æ–°çš„ã€é«˜æ•ˆçš„ RAG æ¨¡å—ï¼Œå¯é«˜æ•ˆéƒ¨ç½²åœ¨è‹±ç‰¹å°”ç¡¬ä»¶ä¸Šã€‚

å¤§å®¶å¯ä»¥å‚è€ƒ [æ­¤è¯´æ˜](https://github.com/IntelLabs/fastRAG#round_pushpin-installation) å®‰è£… fastRAGï¼Œå¹¶é˜…è¯»æˆ‘ä»¬çš„ [æŒ‡å—](https://github.com/IntelLabs/fastRAG/blob/main/getting_started.md) ä»¥å¼€å§‹ fastRAG ä¹‹æ—…ã€‚

æˆ‘ä»¬éœ€è¦å°†ä¼˜åŒ–çš„åŒç¼–ç å™¨åµŒå…¥æ¨¡å‹ç”¨äºä¸‹è¿°ä¸¤ä¸ªæ¨¡å—ä¸­:

1. [`QuantizedBiEncoderRetriever`](https://github.com/IntelLabs/fastRAG/blob/main/fastrag/retrievers/optimized.py#L17) â€“ ç”¨äºåˆ›å»ºç¨ å¯†å‘é‡ç´¢å¼•åº“ï¼Œä»¥åŠä»å»ºå¥½çš„å‘é‡åº“ä¸­æ£€ç´¢æ–‡æ¡£
2. [`QuantizedBiEncoderRanker`](https://github.com/IntelLabs/fastRAG/blob/main/fastrag/rankers/quantized_bi_encoder.py#L17) â€“ åœ¨å¯¹æ–‡æ¡£åˆ—è¡¨è¿›è¡Œé‡æ’çš„æµæ°´çº¿ä¸­éœ€è¦ç”¨åˆ°åµŒå…¥æ¨¡å‹ã€‚

### ä½¿ç”¨ä¼˜åŒ–çš„æ£€ç´¢å™¨å®ç°å¿«é€Ÿç´¢å¼•

æˆ‘ä»¬ç”¨åŸºäºé‡åŒ–åµŒå…¥æ¨¡å‹çš„ç¨ å¯†æ£€ç´¢å™¨æ¥åˆ›å»ºç¨ å¯†ç´¢å¼•ã€‚

é¦–å…ˆï¼Œåˆ›å»ºä¸€ä¸ªæ–‡æ¡£åº“:

```python
from haystack.document_store import InMemoryDocumentStore

document_store = InMemoryDocumentStore(use_gpu=False, use_bm25=False, embedding_dim=384, return_embedding=True)
```

æ¥ç€ï¼Œå‘å…¶ä¸­æ·»åŠ ä¸€äº›æ–‡æ¡£:

```python
from haystack.schema import Document

# example documents to index
examples = [
   "There is a blue house on Oxford Street.",
   "Paris is the capital of France.",
   "The first commit in fastRAG was in 2022"
]

documents = []
for i, d in enumerate(examples):
    documents.append(Document(content=d, id=i))
document_store.write_documents(documents)
```

ä½¿ç”¨ä¼˜åŒ–çš„åŒç¼–ç å™¨åµŒå…¥æ¨¡å‹åˆå§‹åŒ–æ£€ç´¢å™¨ï¼Œå¹¶å¯¹æ–‡æ¡£åº“ä¸­çš„æ‰€æœ‰æ–‡æ¡£è¿›è¡Œç¼–ç :

```python
from fastrag.retrievers import QuantizedBiEncoderRetriever

model_id = "Intel/bge-small-en-v1.5-rag-int8-static"
retriever = QuantizedBiEncoderRetriever(document_store=document_store, embedding_model=model_id)
document_store.update_embeddings(retriever=retriever)
```

### ä½¿ç”¨ä¼˜åŒ–çš„æ’åå™¨è¿›è¡Œé‡æ’

ä¸‹é¢çš„ä»£ç ç‰‡æ®µå±•ç¤ºäº†å¦‚ä½•å°†é‡åŒ–æ¨¡å‹åŠ è½½åˆ°æ’åºå™¨ä¸­ï¼Œè¯¥ç»“ç‚¹ä¼šå¯¹æ£€ç´¢å™¨æ£€ç´¢åˆ°çš„æ‰€æœ‰æ–‡æ¡£è¿›è¡Œç¼–ç å’Œé‡æ’:

```python
from haystack import Pipeline
from fastrag.rankers import QuantizedBiEncoderRanker

ranker = QuantizedBiEncoderRanker("Intel/bge-large-en-v1.5-rag-int8-static")

p = Pipeline()
p.add_node(component=retriever, name="retriever", inputs=["Query"])
p.add_node(component=ranker, name="ranker", inputs=["retriever"])
results = p.run(query="What is the capital of France?")

# print the documents retrieved
print(results)
```

æå®šï¼æˆ‘ä»¬åˆ›å»ºçš„è¿™ä¸ªæµæ°´çº¿é¦–å…ˆä»æ–‡æ¡£åº“ä¸­æ£€ç´¢æ–‡æ¡£ï¼Œå¹¶ä½¿ç”¨ (å¦ä¸€ä¸ª) åµŒå…¥æ¨¡å‹å¯¹æ£€ç´¢åˆ°çš„æ–‡æ¡£è¿›è¡Œé‡æ’ã€‚ä½ ä¹Ÿå¯ä»è¿™ä¸ª [Notebook](https://github.com/IntelLabs/fastRAG/blob/main/examples/optimized-embeddings.ipynb) ä¸­è·å–æ›´å®Œæ•´çš„ä¾‹å­ã€‚

å¦‚æ¬²äº†è§£æ›´å¤š RAG ç›¸å…³çš„æ–¹æ³•ã€æ¨¡å‹å’Œç¤ºä¾‹ï¼Œæˆ‘ä»¬é‚€è¯·å¤§å®¶é€šè¿‡ [fastRAG/examples](https://github.com/IntelLabs/fastRAG/tree/main/examples) å°½æƒ…æ¢ç´¢ã€‚
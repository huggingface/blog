---
title: Optimize and deploy with Optimum-Intel and OpenVINO GenAI
authors:
- user: AlexKoff88
  guest: true
  org: Intel
- user: MrOpenVINO
  guest: true
  org: Intel
- user: katuni4ka
  guest: true
  org: Intel
- user: sandye51
  guest: true
  org: Intel
- user: raymondlo84
  guest: true
  org: Intel
- user: helenai
  guest: true
  org: Intel
- user: echarlaix
translators:
- user: Zipxuan
---

# ä½¿ç”¨Optimum-Intelå’ŒOpenVINO GenAIä¼˜åŒ–å’Œéƒ¨ç½²æ¨¡å‹

åœ¨ç«¯ä¾§éƒ¨ç½²Transformeræ¨¡å‹éœ€è¦ä»”ç»†è€ƒè™‘æ€§èƒ½å’Œå…¼å®¹æ€§ã€‚Pythonè™½ç„¶åŠŸèƒ½å¼ºå¤§ï¼Œä½†å¯¹äºéƒ¨ç½²æ¥è¯´æœ‰æ—¶å¹¶ä¸ç®—ç†æƒ³ï¼Œç‰¹åˆ«æ˜¯åœ¨ç”±C++ä¸»å¯¼çš„ç¯å¢ƒä¸­ã€‚è¿™ç¯‡åšå®¢å°†æŒ‡å¯¼æ‚¨å¦‚ä½•ä½¿ç”¨Optimum-Intelå’ŒOpenVINOâ„¢ GenAIæ¥ä¼˜åŒ–å’Œéƒ¨ç½²Hugging Face Transformersæ¨¡å‹ï¼Œç¡®ä¿åœ¨æœ€å°ä¾èµ–æ€§çš„æƒ…å†µä¸‹è¿›è¡Œé«˜æ•ˆçš„AIæ¨ç†ã€‚

## ç›®å½•
- [ä¸ºä»€ä¹ˆä½¿ç”¨OpenVINOæ¥è¿›è¡Œç«¯ä¾§éƒ¨ç½²](#ä¸ºä»€ä¹ˆä½¿ç”¨OpenVINOæ¥è¿›è¡Œç«¯ä¾§éƒ¨ç½²)
- [ç¬¬ä¸€æ­¥ï¼šåˆ›å»ºç¯å¢ƒ](#ç¬¬ä¸€æ­¥åˆ›å»ºç¯å¢ƒ)
- [ç¬¬äºŒæ­¥ï¼šå°†æ¨¡å‹å¯¼å‡ºä¸ºOpenVINO IR](#ç¬¬äºŒæ­¥å°†æ¨¡å‹å¯¼å‡ºä¸ºopenvino-ir)
- [ç¬¬ä¸‰æ­¥ï¼šæ¨¡å‹ä¼˜åŒ–](#ç¬¬ä¸‰æ­¥æ¨¡å‹ä¼˜åŒ–)
- [ç¬¬å››æ­¥ï¼šä½¿ç”¨OpenVINO GenAI APIè¿›è¡Œéƒ¨ç½²](#ç¬¬å››æ­¥ä½¿ç”¨openvino-genai-apiè¿›è¡Œéƒ¨ç½²)
- [ç»“è®º](#ç»“è®º)

## ä¸ºä»€ä¹ˆä½¿ç”¨OpenVINOæ¥è¿›è¡Œç«¯ä¾§éƒ¨ç½²
OpenVINOâ„¢ æœ€åˆæ˜¯ä½œä¸º C++ AI æ¨ç†è§£å†³æ–¹æ¡ˆå¼€å‘çš„ï¼Œä½¿å…¶éå¸¸é€‚åˆåœ¨ç«¯ä¾§è®¾å¤‡éƒ¨ç½²ä¸­ï¼Œå…¶ä¸­æœ€å°åŒ–ä¾èµ–æ€§è‡³å…³é‡è¦ã€‚éšç€å¼•å…¥ GenAI APIï¼Œå°†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰é›†æˆåˆ° C++ æˆ– Python åº”ç”¨ç¨‹åºä¸­å˜å¾—æ›´åŠ ç®€å•ï¼Œå…¶ç‰¹æ€§æ—¨åœ¨ç®€åŒ–éƒ¨ç½²å¹¶æå‡æ€§èƒ½ã€‚

## ç¬¬ä¸€æ­¥ï¼šåˆ›å»ºç¯å¢ƒ

### é¢„å…ˆå‡†å¤‡

å¼€å§‹ä¹‹å‰ï¼Œè¯·ç¡®ä¿æ‚¨çš„ç¯å¢ƒå·²æ­£ç¡®é…ç½®äº†Pythonå’ŒC++ã€‚å®‰è£…å¿…è¦çš„PythonåŒ…ï¼š
```sh
pip install --upgrade --upgrade-strategy eager optimum[openvino]
```

ä»¥ä¸‹æ˜¯æœ¬æ–‡ä¸­ä½¿ç”¨çš„å…·ä½“åŒ…ï¼š
```
transformers==4.44
openvino==24.3
openvino-tokenizers==24.3
optimum-intel==1.20
lm-eval==0.4.3
```

æœ‰å…³GenAI C++åº“çš„å®‰è£…ï¼Œè¯·æŒ‰ç…§[æ­¤å¤„](https://docs.openvino.ai/2024/get-started/install-openvino/install-openvino-genai.html)çš„è¯´æ˜è¿›è¡Œæ“ä½œã€‚

## ç¬¬äºŒæ­¥ï¼šå°†æ¨¡å‹å¯¼å‡ºä¸ºOpenVINO IR

Hugging Face å’Œ Intel çš„åˆä½œä¿ƒæˆäº† [Optimum-Intel](https://huggingface.co/docs/optimum/en/intel/index) é¡¹ç›®ã€‚è¯¥é¡¹ç›®æ—¨åœ¨ä¼˜åŒ– Transformers æ¨¡å‹åœ¨ Intel ç¡¬ä»¶ä¸Šçš„æ¨ç†æ€§èƒ½ã€‚Optimum-Intel æ”¯æŒ OpenVINO ä½œä¸ºæ¨ç†åç«¯ï¼Œå…¶ API ä¸ºå„ç§åŸºäº OpenVINO æ¨ç† API æ„å»ºçš„æ¨¡å‹æ¶æ„æä¾›äº†å°è£…ã€‚è¿™äº›å°è£…éƒ½ä»¥ `OV` å‰ç¼€å¼€å¤´ï¼Œä¾‹å¦‚ `OVModelForCausalLM`ã€‚é™¤æ­¤ä¹‹å¤–ï¼Œå®ƒä¸ ğŸ¤— Transformers åº“çš„ API ç±»ä¼¼ã€‚

è¦å°† Transformers æ¨¡å‹å¯¼å‡ºä¸º OpenVINO ä¸­é—´è¡¨ç¤ºï¼ˆIRï¼‰ï¼Œå¯ä»¥ä½¿ç”¨ä¸¤ç§æ–¹æ³•ï¼šå¯ä»¥ä½¿ç”¨ Python çš„ `.from_pretrained()` æ–¹æ³•æˆ– Optimum å‘½ä»¤è¡Œç•Œé¢ï¼ˆCLIï¼‰ã€‚ä»¥ä¸‹æ˜¯ä½¿ç”¨è¿™ä¸¤ç§æ–¹æ³•çš„ç¤ºä¾‹ï¼š
### ä½¿ç”¨ Python API
```python
from optimum.intel import OVModelForCausalLM

model_id = "meta-llama/Meta-Llama-3.1-8B"
model = OVModelForCausalLM.from_pretrained(model_id, export=True)
model.save_pretrained("./llama-3.1-8b-ov")
```

### ä½¿ç”¨å‘½ä»¤è¡Œ (CLI)
```sh
optimum-cli export openvino -m meta-llama/Meta-Llama-3.1-8B ./llama-3.1-8b-ov
```

./llama-3.1-8b-ov æ–‡ä»¶å¤¹å°†åŒ…å« .xml å’Œ bin IR æ¨¡å‹æ–‡ä»¶ä»¥åŠæ¥è‡ªæºæ¨¡å‹çš„æ‰€éœ€é…ç½®æ–‡ä»¶ã€‚ğŸ¤— tokenizer ä¹Ÿå°†è½¬æ¢ä¸º openvino-tokenizers åº“çš„æ ¼å¼ï¼Œå¹¶åœ¨åŒä¸€æ–‡ä»¶å¤¹ä¸­åˆ›å»ºç›¸åº”çš„é…ç½®æ–‡ä»¶ã€‚

## ç¬¬ä¸‰æ­¥ï¼šæ¨¡å‹ä¼˜åŒ–

åœ¨èµ„æºå—é™çš„ç«¯ä¾§è®¾å¤‡ä¸Šè¿è¡Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰æ—¶ï¼Œæ¨¡å‹ä¼˜åŒ–æ˜¯ä¸€ä¸ªæä¸ºé‡è¦çš„æ­¥éª¤ã€‚ä»…é‡åŒ–æƒé‡æ˜¯ä¸€ç§ä¸»æµæ–¹æ³•ï¼Œå¯ä»¥æ˜¾è‘—é™ä½å»¶è¿Ÿå’Œæ¨¡å‹å ç”¨ç©ºé—´ã€‚Optimum-Intel é€šè¿‡ç¥ç»ç½‘ç»œå‹ç¼©æ¡†æ¶ï¼ˆNNCFï¼‰æä¾›äº†ä»…é‡åŒ–æƒé‡ï¼ˆweight-only quantizationï¼‰çš„åŠŸèƒ½ï¼Œè¯¥æ¡†æ¶å…·æœ‰å¤šç§ä¸“ä¸ºLLMsè®¾è®¡çš„ä¼˜åŒ–æŠ€æœ¯ï¼šä»æ— æ•°æ®ï¼ˆdata-freeï¼‰çš„ INT8 å’Œ INT4 æƒé‡é‡åŒ–åˆ°æ•°æ®æ„ŸçŸ¥æ–¹æ³•ï¼Œå¦‚ [AWQ](https://huggingface.co/docs/transformers/main/en/quantization/awq)ã€[GPTQ](https://huggingface.co/docs/transformers/main/en/quantization/gptq)ã€é‡åŒ–scaleä¼°è®¡ã€æ··åˆç²¾åº¦é‡åŒ–ç­‰ã€‚é»˜è®¤æƒ…å†µä¸‹ï¼Œè¶…è¿‡åäº¿å‚æ•°çš„æ¨¡å‹çš„æƒé‡ä¼šè¢«é‡åŒ–ä¸º INT8 ç²¾åº¦ï¼Œè¿™åœ¨å‡†ç¡®æ€§æ–¹é¢æ˜¯å®‰å…¨çš„ã€‚è¿™æ„å‘³ç€ä¸Šè¿°å¯¼å‡ºæ­¥éª¤ä¼šç”Ÿæˆå…·æœ‰8ä½æƒé‡çš„æ¨¡å‹ã€‚ç„¶è€Œï¼Œ4ä½æ•´æ•°çš„ä»…é‡åŒ–æƒé‡å…è®¸å®ç°æ›´å¥½çš„å‡†ç¡®æ€§å’Œæ€§èƒ½çš„æƒè¡¡ã€‚

å¯¹äº `meta-llama/Meta-Llama-3.1-8B` æ¨¡å‹ï¼Œæˆ‘ä»¬å»ºè®®ç»“åˆ AWQã€é‡åŒ–scaleä¼°è®¡ä»¥åŠä½¿ç”¨åæ˜ éƒ¨ç½²ç”¨ä¾‹çš„æ ¡å‡†æ•°æ®é›†è¿›è¡Œæ··åˆç²¾åº¦ INT4/INT8 æƒé‡çš„é‡åŒ–ã€‚ä¸å¯¼å‡ºæƒ…å†µç±»ä¼¼ï¼Œåœ¨å°†4æ¯”ç‰¹ä»…é‡åŒ–æƒé‡åº”ç”¨äºLLMæ¨¡å‹æ—¶æœ‰ä¸¤ç§é€‰é¡¹ï¼š

### ä½¿ç”¨Python API
- åœ¨ `.from_pretrained()` æ–¹æ³•ä¸­æŒ‡å®š `quantization_config` å‚æ•°ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œåº”åˆ›å»º `OVWeightQuantizationConfig` å¯¹è±¡ï¼Œå¹¶å°†å…¶è®¾ç½®ä¸ºè¯¥å‚æ•°ï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼š
```python
from optimum.intel import OVModelForCausalLM, OVWeightQuantizationConfig

MODEL_ID = "meta-llama/Meta-Llama-3.1-8B"
quantization_config = OVWeightQuantizationConfig(bits=4, awq=True, scale_estimation=True, group_size=64, dataset="c4")
model = OVModelForCausalLM.from_pretrained(MODEL_ID, export=True, quantization_config=quantization_config)
model.save_pretrained("./llama-3.1-8b-ov")
```

### ä½¿ç”¨å‘½ä»¤è¡Œ (CLI)
```sh
optimum-cli export openvino -m meta-llama/Meta-Llama-3.1-8B --weight-format int4 --awq --scale-estimation --group-size 64 --dataset wikitext2 ./llama-3.1-8b-ov
```


## ç¬¬å››æ­¥ï¼šä½¿ç”¨OpenVINO GenAI APIè¿›è¡Œéƒ¨ç½²
åœ¨è½¬æ¢å’Œä¼˜åŒ–ä¹‹åï¼Œä½¿ç”¨OpenVINO GenAIéƒ¨ç½²æ¨¡å‹éå¸¸ç®€å•ã€‚OpenVINO GenAIä¸­çš„LLMPipelineç±»æä¾›äº†Pythonå’ŒC++ APIï¼Œæ”¯æŒå„ç§æ–‡æœ¬ç”Ÿæˆæ–¹æ³•ï¼Œå¹¶å…·æœ‰æœ€å°çš„ä¾èµ–å…³ç³»ã€‚


### Python APIçš„ä¾‹å­
```python
import argparse
import openvino_genai

device = "CPU"  # GPU can be used as well
pipe = openvino_genai.LLMPipeline(args.model_dir, device)
config = openvino_genai.GenerationConfig()
config.max_new_tokens = 100
print(pipe.generate(args.prompt, config))
```

ä¸ºäº†è¿è¡Œè¿™ä¸ªç¤ºä¾‹ï¼Œæ‚¨éœ€è¦åœ¨Pythonç¯å¢ƒä¸­å®‰è£…æœ€å°çš„ä¾èµ–é¡¹ï¼Œå› ä¸ºOpenVINO GenAIæ—¨åœ¨æä¾›è½»é‡çº§éƒ¨ç½²ã€‚æ‚¨å¯ä»¥å°†OpenVINO GenAIåŒ…å®‰è£…åˆ°ç›¸åŒçš„Pythonç¯å¢ƒä¸­ï¼Œæˆ–è€…åˆ›å»ºä¸€ä¸ªå•ç‹¬çš„ç¯å¢ƒæ¥æ¯”è¾ƒåº”ç”¨ç¨‹åºçš„å ç”¨ç©ºé—´ï¼š
```sh
pip install openvino-genai==24.3
```

### C++ APIçš„ä¾‹å­

è®©æˆ‘ä»¬çœ‹çœ‹å¦‚ä½•ä½¿ç”¨OpenVINO GenAI C++ APIè¿è¡Œç›¸åŒçš„æµç¨‹ã€‚GenAI APIçš„è®¾è®¡éå¸¸ç›´è§‚ï¼Œå¹¶æä¾›äº†ä¸ ğŸ¤— Transformers API æ— ç¼è¿ç§»çš„åŠŸèƒ½ã€‚

> **æ³¨æ„**ï¼šåœ¨ä¸‹é¢çš„ç¤ºä¾‹ä¸­ï¼Œæ‚¨å¯ä»¥ä¸º "device" å˜é‡æŒ‡å®šç¯å¢ƒä¸­çš„ä»»ä½•å…¶ä»–å¯ç”¨è®¾å¤‡ã€‚ä¾‹å¦‚ï¼Œå¦‚æœæ‚¨æ­£åœ¨ä½¿ç”¨å¸¦æœ‰é›†æˆæ˜¾å¡çš„Intel CPUï¼Œåˆ™å°è¯•ä½¿ç”¨ "GPU" æ˜¯ä¸€ä¸ªä¸é”™çš„é€‰æ‹©ã€‚è¦æ£€æŸ¥å¯ç”¨è®¾å¤‡ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨ ov::Core::get_available_devices æ–¹æ³•ï¼ˆå‚è€ƒ [query-device-properties](https://docs.openvino.ai/2024/openvino-workflow/running-inference/inference-devices-and-modes/query-device-properties.html)ï¼‰ã€‚

```cpp
#include "openvino/genai/llm_pipeline.hpp"
#include <iostream>

int main(int argc, char* argv[]) {
   std::string model_path = "./llama-3.1-8b-ov";
   std::string device = "CPU"  // GPU can be used as well
   ov::genai::LLMPipeline pipe(model_path, device);
   std::cout << pipe.generate("What is LLM model?", ov::genai::max_new_tokens(256));
}
```

### è‡ªå®šä¹‰ç”Ÿæˆé…ç½®
`LLMPipeline` è¿˜å…è®¸é€šè¿‡ `ov::genai::GenerationConfig` æ¥æŒ‡å®šè‡ªå®šä¹‰ç”Ÿæˆé€‰é¡¹ï¼š
```cpp
ov::genai::GenerationConfig config;
config.max_new_tokens = 256;
std::string result = pipe.generate(prompt, config);
```

ä½¿ç”¨LLMPipelineï¼Œç”¨æˆ·ä¸ä»…å¯ä»¥è½»æ¾åˆ©ç”¨å„ç§è§£ç ç®—æ³•ï¼Œå¦‚ Beam Searchï¼Œè¿˜å¯ä»¥åƒä¸‹é¢çš„ç¤ºä¾‹ä¸­é‚£æ ·æ„å»ºå…·æœ‰ Streamer çš„äº¤äº’å¼èŠå¤©åœºæ™¯ã€‚æ­¤å¤–ï¼Œç”¨æˆ·å¯ä»¥åˆ©ç”¨LLMPipelineçš„å¢å¼ºå†…éƒ¨ä¼˜åŒ–ï¼Œä¾‹å¦‚åˆ©ç”¨å…ˆå‰èŠå¤©å†å²çš„KVç¼“å­˜å‡å°‘æç¤ºå¤„ç†æ—¶é—´ï¼Œä½¿ç”¨ chat æ–¹æ³•ï¼šstart_chat() å’Œ finish_chat()ï¼ˆå‚è€ƒ [using-genai-in-chat-scenario](https://docs.openvino.ai/2024/learn-openvino/llm_inference_guide/genai-guide.html#using-genai-in-chat-scenario)ï¼‰ã€‚


```cpp
ov::genai::GenerationConfig config;
config.max_new_tokens = 100;
config.do_sample = true;
config.top_p = 0.9;
config.top_k = 30;

auto streamer = [](std::string subword) {
    std::cout << subword << std::flush;
    return false;
};

// Since the streamer is set, the results will
// be printed each time a new token is generated.
pipe.generate(prompt, config, streamer);
```

æœ€åä½ å¯ä»¥çœ‹åˆ°å¦‚ä½•åœ¨èŠå¤©åœºæ™¯ä¸‹ä½¿ç”¨LLMPipelineï¼š
```cpp
pipe.start_chat()
for (size_t i = 0; i < questions.size(); i++) {
   std::cout << "question:\n";
   std::getline(std::cin, prompt);

   std::cout << pipe.generate(prompt) << std::endl;
}
pipe.finish_chat();
```

## ç»“è®º
Optimum-Intelå’ŒOpenVINOâ„¢ GenAIçš„ç»“åˆä¸ºåœ¨ç«¯ä¾§éƒ¨ç½²Hugging Faceæ¨¡å‹æä¾›äº†å¼ºå¤§è€Œçµæ´»çš„è§£å†³æ–¹æ¡ˆã€‚é€šè¿‡éµå¾ªè¿™äº›æ­¥éª¤ï¼Œæ‚¨å¯ä»¥åœ¨Pythonå¯èƒ½ä¸æ˜¯ç†æƒ³é€‰æ‹©çš„ç¯å¢ƒä¸­å®ç°ä¼˜åŒ–çš„é«˜æ€§èƒ½AIæ¨ç†ï¼Œä»¥ç¡®ä¿æ‚¨çš„åº”ç”¨åœ¨Intelç¡¬ä»¶ä¸Šå¹³ç¨³è¿è¡Œã€‚

## å…¶ä»–èµ„æº
1. æ‚¨å¯ä»¥åœ¨è¿™ä¸ª [æ•™ç¨‹](https://docs.openvino.ai/2024/learn-openvino/llm_inference_guide/genai-guide.html) ä¸­æ‰¾åˆ°æ›´å¤šè¯¦ç»†ä¿¡æ¯ã€‚
2. è¦æ„å»ºä¸Šè¿°çš„C++ç¤ºä¾‹ï¼Œè¯·å‚è€ƒè¿™ä¸ª [æ–‡æ¡£](https://github.com/openvinotoolkit/openvino.genai/blob/releases/2024/3/src/docs/BUILD.md)ã€‚
3. [OpenVINOæ–‡æ¡£](docs.openvino.ai)
4. [Jupyterç¬”è®°æœ¬](https://docs.openvino.ai/2024/learn-openvino/interactive-tutorials-python.html)
5. [Optimumæ–‡æ¡£](https://huggingface.co/docs/optimum/main/en/intel/index)

![OpenVINO GenAI C++èŠå¤©æ¼”ç¤º](https://huggingface.co/datasets/OpenVINO/documentation/resolve/main/blog/openvino_genai_workflow/demo.gif)

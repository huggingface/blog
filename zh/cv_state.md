---
title: Hugging Face ä¸­è®¡ç®—æœºè§†è§‰çš„ç°çŠ¶
thumbnail: /blog/assets/cv_state/thumbnail.png
authors:
- user: sayakpaul
---

# Hugging Face ä¸­è®¡ç®—æœºè§†è§‰çš„ç°çŠ¶


åœ¨Hugging Faceä¸Šï¼Œæˆ‘ä»¬ä¸ºä¸ç¤¾åŒºä¸€èµ·æ¨åŠ¨äººå·¥æ™ºèƒ½é¢†åŸŸçš„æ°‘ä¸»åŒ–è€Œæ„Ÿåˆ°è‡ªè±ªã€‚ä½œä¸ºè¿™ä¸ªä½¿å‘½çš„ä¸€éƒ¨åˆ†ï¼Œæˆ‘ä»¬ä»å»å¹´å¼€å§‹ä¸“æ³¨äºè®¡ç®—æœºè§†è§‰ã€‚å¼€å§‹åªæ˜¯ [ğŸ¤— Transformersä¸­Vision Transformers (ViT) çš„ä¸€ä¸ª PR](https://github.com/huggingface/transformers/pull/10950)ï¼Œç°åœ¨å·²ç»å‘å±•å£®å¤§ï¼š8ä¸ªæ ¸å¿ƒè§†è§‰ä»»åŠ¡ï¼Œè¶…è¿‡3000ä¸ªæ¨¡å‹ï¼Œåœ¨Hugging Face Hubä¸Šæœ‰è¶…è¿‡1000ä¸ªæ•°æ®é›†ã€‚

è‡ªä» ViTs åŠ å…¥ Hub åï¼Œå·²ç»å‘ç”Ÿäº†å¤§é‡æ¿€åŠ¨äººå¿ƒçš„äº‹æƒ…ã€‚åœ¨è¿™ç¯‡åšå®¢æ–‡ç« ä¸­ï¼Œæˆ‘ä»¬å°†ä» ğŸ¤—Hugging Face ç”Ÿæ€ç³»ç»Ÿä¸­æ€»ç»“å·²ç»å‘ç”Ÿçš„å’Œå°†è¦å‘ç”Ÿçš„è¿›å±•ï¼Œä»¥æ”¯æŒè®¡ç®—æœºè§†è§‰çš„æŒç»­å‘å±•ã€‚

ä¸‹é¢æ˜¯æˆ‘ä»¬è¦è¦†ç›–çš„å†…å®¹ï¼š

- [æ”¯æŒçš„è§†è§‰ä»»åŠ¡å’Œæµæ°´çº¿](https://huggingface.co/blog/cv_state#support-for-pipelines)
- [è®­ç»ƒä½ è‡ªå·±çš„è§†è§‰æ¨¡å‹](https://huggingface.co/blog/cv_state#training-your-own-models)
- [å’Œ`timm`æ•´åˆ](https://huggingface.co/blog/cv_state#ğŸ¤—-ğŸ¤-timm)

- [Diffusers](https://huggingface.co/blog/cv_state#ğŸ§¨-diffusers)
- [å¯¹ç¬¬ä¸‰æ–¹åº“çš„æ”¯æŒ](https://huggingface.co/blog/cv_state#support-for-third-party-libraries)
- [å¼€å‘](https://huggingface.co/blog/cv_state#deployment)
- ä»¥åŠæ›´å¤šå†…å®¹ï¼

## å¯åŠ¨ç¤¾åŒº: ä¸€æ¬¡ä¸€ä¸ªä»»åŠ¡ 

Hugging Face Hub æ‹¥æœ‰è¶…è¿‡10ä¸‡ä¸ªç”¨äºä¸åŒä»»åŠ¡çš„å…¬å…±æ¨¡å‹ï¼Œä¾‹å¦‚ï¼šä¸‹ä¸€è¯é¢„æµ‹ã€æ©ç å¡«å……ã€è¯ç¬¦åˆ†ç±»ã€åºåˆ—åˆ†ç±»ç­‰ã€‚æˆªæ­¢ä»Šå¤©ï¼Œæˆ‘ä»¬æ”¯æŒ[8ä¸ªæ ¸å¿ƒè§†è§‰ä»»åŠ¡](https://huggingface.co/tasks)ï¼Œæä¾›è®¸å¤šæ¨¡å‹çš„ checkpointsï¼š

- å›¾åƒåˆ†ç±»
- å›¾åƒåˆ†å‰²
- ï¼ˆé›¶æ ·æœ¬ï¼‰ç›®æ ‡æ£€æµ‹
- è§†é¢‘åˆ†ç±»
- æ·±åº¦ä¼°è®¡
- å›¾åƒåˆ°å›¾åƒåˆæˆ
- æ— æ¡ä»¶å›¾åƒç”Ÿæˆ
- é›¶æ ·æœ¬å›¾åƒåˆ†ç±»

æ¯ä¸ªä»»åŠ¡åœ¨ Hub ä¸Šè‡³å°‘æœ‰10ä¸ªæ¨¡å‹ç­‰å¾…ä½ å»æ¢ç´¢ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æ”¯æŒè§†è§‰å’Œè¯­è¨€çš„äº¤å‰ä»»åŠ¡ï¼Œæ¯”å¦‚ï¼š

- å›¾åƒåˆ°æ–‡å­—ï¼ˆå›¾åƒè¯´æ˜ï¼Œå…‰å­¦å­—ç¬¦è¯†åˆ«ï¼‰
- æ–‡å­—åˆ°å›¾åƒ
- æ–‡æ¡£é—®ç­”
- è§†è§‰é—®ç­”

è¿™äº›ä»»åŠ¡ä¸ä»…éœ€è¦æœ€å…ˆè¿›çš„åŸºäº Transformer çš„æ¶æ„ï¼Œå¦‚ [ViT](https://huggingface.co/docs/transformers/model_doc/vit)ã€[Swin](https://huggingface.co/docs/transformers/model_doc/swin)ã€[DETR](https://huggingface.co/docs/transformers/model_doc/detr)ï¼Œè¿˜éœ€è¦*çº¯å·ç§¯*çš„æ¶æ„ï¼Œå¦‚ [ConvNeXt](https://huggingface.co/docs/transformers/model_doc/convnext)ã€[ResNet](https://huggingface.co/docs/transformers/model_doc/resnet)ã€[RegNet](https://huggingface.co/docs/transformers/model_doc/regnet)ï¼Œç”šè‡³æ›´å¤šï¼åƒ ResNets è¿™æ ·çš„æ¶æ„ä»ç„¶ä¸æ— æ•°çš„å·¥ä¸šç”¨ä¾‹éå¸¸ç›¸å…³ï¼Œå› æ­¤åœ¨ ğŸ¤— Transformers ä¸­ä¹Ÿæ”¯æŒè¿™äº›é Transformers çš„æ¶æ„ã€‚

è¿˜éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œåœ¨ Hub ä¸Šçš„è¿™äº›æ¨¡å‹ä¸ä»…æ¥è‡ª Transformers åº“ï¼Œä¹Ÿæ¥è‡ªäºå…¶ä»–ç¬¬ä¸‰æ–¹åº“ã€‚ä¾‹å¦‚ï¼Œå°½ç®¡æˆ‘ä»¬åœ¨ Hub ä¸Šæ”¯æŒæ— æ¡ä»¶å›¾åƒç”Ÿæˆç­‰ä»»åŠ¡ï¼Œä½†æˆ‘ä»¬åœ¨ Transformers ä¸­è¿˜æ²¡æœ‰ä»»ä½•æ¨¡å‹æ”¯æŒè¯¥ä»»åŠ¡ï¼ˆæ¯”å¦‚[è¿™ä¸ª](https://huggingface.co/ceyda/butterfly_cropped_uniq1K_512)ï¼‰ã€‚æ”¯æŒæ‰€æœ‰çš„æœºå™¨å­¦ä¹ ä»»åŠ¡ï¼Œæ— è®ºæ˜¯ä½¿ç”¨ Transformers è¿˜æ˜¯ç¬¬ä¸‰æ–¹åº“æ¥è§£å†³ï¼Œéƒ½æ˜¯æˆ‘ä»¬ä¿ƒè¿›ä¸€ä¸ªåä½œçš„å¼€æºæœºå™¨å­¦ä¹ ç”Ÿæ€ç³»ç»Ÿä½¿å‘½çš„ä¸€éƒ¨åˆ†ã€‚

## å¯¹ Pipelines çš„æ”¯æŒ

æˆ‘ä»¬å¼€å‘äº† [Pipelines](https://huggingface.co/docs/transformers/main/en/main_classes/pipelines) æ¥ä¸ºä»ä¸šè€…æä¾›ä»–ä»¬éœ€è¦çš„å·¥å…·ï¼Œä»¥ä¾¿è½»æ¾åœ°å°†æœºå™¨å­¦ä¹ æ•´åˆåˆ°ä»–ä»¬çš„å·¥å…·ç®±ä¸­ã€‚å¯¹äºç»™å®šä¸ä»»åŠ¡ç›¸å…³çš„è¾“å…¥ï¼Œä»–ä»¬æä¾›äº†ä¸€ç§ç®€å•çš„æ–¹æ³•æ¥æ‰§è¡Œæ¨ç†ã€‚æˆ‘ä»¬åœ¨Pipelinesé‡Œæ”¯æŒ[7ç§è§†è§‰ä»»åŠ¡](https://huggingface.co/docs/transformers/main/en/main_classes/pipelines#computer-vision)ã€‚ä¸‹é¢æ˜¯ä¸€ä¸ªä½¿ç”¨ Pipelines è¿›è¡Œæ·±åº¦ä¼°è®¡çš„ä¾‹å­ï¼š

```python
from transformers import pipeline

depth_estimator = pipeline(task="depth-estimation", model="Intel/dpt-large")
output = depth_estimator("http://images.cocodataset.org/val2017/000000039769.jpg")

# This is a tensor with the values being the depth expressed
# in meters for each pixel
output["depth"]
```

![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/cv_state/depth_estimation_output.png)

å³ä½¿å¯¹äºè§†è§‰é—®ç­”ä»»åŠ¡ï¼Œæ¥å£ä¹Ÿä¿æŒä¸å˜ï¼š

```python
from transformers import pipeline

oracle = pipeline(model="dandelin/vilt-b32-finetuned-vqa")
image_url = "https://huggingface.co/datasets/Narsil/image_dummy/raw/main/lena.png"

oracle(question="What is she wearing?", image=image_url, top_k=1)
# [{'score': 0.948, 'answer': 'hat'}]
```

## è®­ç»ƒä½ è‡ªå·±çš„æ¨¡å‹

è™½ç„¶èƒ½å¤Ÿä½¿ç”¨ç°æˆæ¨ç†æ¨¡å‹æ˜¯ä¸€ä¸ªå¾ˆå¥½çš„å…¥é—¨æ–¹å¼ï¼Œä½†å¾®è°ƒæ˜¯ç¤¾åŒºè·å¾—æœ€å¤§æ”¶ç›Šçš„åœ°æ–¹ã€‚å½“ä½ çš„æ•°æ®é›†æ˜¯è‡ªå®šä¹‰çš„ã€å¹¶ä¸”é¢„è®­ç»ƒæ¨¡å‹çš„æ€§èƒ½ä¸ä½³æ—¶ï¼Œè¿™ä¸€ç‚¹å°¤å…¶æ­£ç¡®ã€‚

Transformers ä¸ºä¸€åˆ‡ä¸è®­ç»ƒç›¸å…³çš„ä¸œè¥¿æä¾›äº†[è®­ç»ƒå™¨ API](https://huggingface.co/docs/transformers/main_classes/trainer)ã€‚å½“å‰ï¼Œ`Trainer`æ— ç¼åœ°æ”¯æŒä»¥ä¸‹ä»»åŠ¡ï¼šå›¾åƒåˆ†ç±»ã€å›¾åƒåˆ†å‰²ã€è§†é¢‘åˆ†ç±»ã€ç›®æ ‡æ£€æµ‹å’Œæ·±åº¦ä¼°è®¡ã€‚å¾®è°ƒå…¶ä»–è§†è§‰ä»»åŠ¡çš„æ¨¡å‹ä¹Ÿæ˜¯æ”¯æŒçš„ï¼Œåªæ˜¯å¹¶ä¸é€šè¿‡`Trainer`ã€‚

åªè¦æŸå¤±è®¡ç®—åŒ…å«åœ¨ Transformers è®¡ç®—ç»™å®šä»»åŠ¡æŸå¤±çš„æ¨¡å‹ä¸­ï¼Œå®ƒå°±åº”è¯¥æœ‰èµ„æ ¼å¯¹è¯¥ä»»åŠ¡è¿›è¡Œå¾®è°ƒã€‚å¦‚æœä½ å‘ç°é—®é¢˜ï¼Œè¯·åœ¨ GitHub ä¸Š[æŠ¥å‘Š](https://github.com/huggingface/transformers/issues)ã€‚

æˆ‘ä»å“ªé‡Œå¯ä»¥æ‰¾åˆ°ä»£ç ï¼Ÿ

- [æ¨¡å‹æ–‡æ¡£](https://huggingface.co/docs/transformers/index#supported-models)
- [Hugging Face ç¬”è®°æœ¬](https://github.com/huggingface/notebooks)
- [Hugging Face ç¤ºä¾‹è„šæœ¬](https://github.com/huggingface/transformers/tree/main/examples)
- [ä»»åŠ¡é¡µé¢](https://huggingface.co/tasks)

[Hugging Face ç¤ºä¾‹è„šæœ¬](https://github.com/huggingface/transformers/tree/main/examples)åŒ…æ‹¬ä¸åŒçš„[è‡ªç›‘ç£é¢„è®­ç»ƒç­–ç•¥](https://github.com/huggingface/transformers/tree/main/examples/pytorch/image-pretraining)å¦‚ [MAE](https://arxiv.org/abs/2111.06377)ï¼Œå’Œ[å¯¹æ¯”å›¾åƒåˆ°æ–‡æœ¬é¢„è®­ç»ƒç­–ç•¥](https://github.com/huggingface/transformers/tree/main/examples/pytorch/contrastive-image-text)å¦‚ [CLIP](https://arxiv.org/abs/2103.00020)ã€‚è¿™äº›è„šæœ¬å¯¹äºç ”ç©¶ç¤¾åŒºå’Œæ„¿æ„åœ¨é¢„è®­ç»ƒæ¨¡å‹ä¸Šä»å¤´è®­ç»ƒè‡ªå®šä¹‰æ•°æ®è¯­æ–™çš„ä»ä¸šè€…æ¥è¯´æ˜¯éå¸¸å®è´µçš„èµ„æºã€‚

ä¸è¿‡æœ‰äº›ä»»åŠ¡æœ¬æ¥å°±ä¸é€‚åˆå¾®è°ƒã€‚ä¾‹å­åŒ…æ‹¬é›¶æ ·æœ¬å›¾åƒåˆ†ç±»ï¼ˆæ¯”å¦‚ [CLIP](https://huggingface.co/docs/transformers/main/en/model_doc/clip)ï¼‰ï¼Œé›¶æ ·æœ¬ç›®æ ‡æ£€æµ‹ï¼ˆæ¯”å¦‚ [OWL-ViT](https://huggingface.co/docs/transformers/main/en/model_doc/owlvit)ï¼‰ï¼Œå’Œé›¶æ ·æœ¬åˆ†å‰²ï¼ˆæ¯”å¦‚ [CLIPSeg](https://huggingface.co/docs/transformers/model_doc/clipseg)ï¼‰ã€‚æˆ‘ä»¬å°†åœ¨è¿™ç¯‡æ–‡ç« ä¸­é‡æ–°è®¨è®ºè¿™äº›æ¨¡å‹ã€‚

## ä¸ Datasets é›†æˆ

Datasets æä¾›äº†å¯¹æ•°åƒä¸ªä¸åŒæ¨¡æ€æ•°æ®é›†çš„è½»æ¾è®¿é—®ã€‚å¦‚å‰æ‰€è¿°ï¼ŒHub æœ‰è¶…è¿‡1000ä¸ªè®¡ç®—æœºè§†è§‰çš„æ•°æ®é›†ã€‚ä¸€äº›ä¾‹å­å€¼å¾—å…³æ³¨ï¼š[ImageNet-1k](https://huggingface.co/datasets/imagenet-1k)ã€[Scene Parsing](https://huggingface.co/datasets/scene_parse_150)ã€[NYU Depth V2](https://huggingface.co/datasets/sayakpaul/nyu_depth_v2)ã€[COYO-700M](https://huggingface.co/datasets/kakaobrain/coyo-700m) å’Œ [LAION-400M](https://huggingface.co/datasets/laion/laion400m)ã€‚è¿™äº›åœ¨ Hub ä¸Šçš„æ•°æ®é›†ï¼Œåªéœ€ä¸¤è¡Œä»£ç å°±å¯ä»¥åŠ è½½å®ƒä»¬ï¼š

```python
from datasets import load_dataset

dataset = load_dataset("scene_parse_150")
```

é™¤äº†è¿™äº›æ•°æ®é›†ï¼Œæˆ‘ä»¬æä¾›äº†å¯¹å¢å¼ºåº“å¦‚ [albumentations](https://github.com/huggingface/notebooks/blob/main/examples/image_classification_albumentations.ipynb) å’Œ [Kornia](https://github.com/huggingface/notebooks/blob/main/examples/image_classification_kornia.ipynb) çš„é›†æˆæ”¯æŒã€‚ç¤¾åŒºå¯ä»¥åˆ©ç”¨ Datasets çš„çµæ´»æ€§å’Œæ€§èƒ½ï¼Œè¿˜æœ‰è¿™äº›åº“æä¾›çš„å¼ºå¤§çš„å¢å¼ºå˜æ¢èƒ½åŠ›ã€‚é™¤æ­¤ä¹‹å¤–ï¼Œæˆ‘ä»¬ä¹Ÿä¸ºæ ¸å¿ƒè§†è§‰ä»»åŠ¡æä¾›[ä¸“ç”¨çš„æ•°æ®åŠ è½½æŒ‡å—](https://huggingface.co/docs/datasets/image_load)ï¼šå›¾åƒåˆ†ç±»ï¼Œå›¾åƒåˆ†å‰²ï¼Œç›®æ ‡æ£€æµ‹å’Œæ·±åº¦ä¼°è®¡ã€‚

## ğŸ¤— ğŸ¤ timm

`timm`ï¼Œå³ [pytorch-image-models](https://github.com/rwightman/pytorch-image-models)ï¼Œæ˜¯ä¸€ä¸ªæœ€å…ˆè¿›çš„ PyTorch å›¾åƒæ¨¡å‹ã€é¢„è®­ç»ƒæƒé‡å’Œç”¨äºè®­ç»ƒã€æ¨ç†ã€éªŒè¯çš„å®ç”¨è„šæœ¬çš„å¼€æºé›†åˆã€‚

æˆ‘ä»¬åœ¨ Hub ä¸Šæœ‰è¶…è¿‡200ä¸ªæ¥è‡ª `timm` çš„æ¨¡å‹ï¼Œå¹¶ä¸”æœ‰æ›´å¤šæ¨¡å‹å³å°†ä¸Šçº¿ã€‚æŸ¥çœ‹[æ–‡æ¡£](https://huggingface.co/docs/timm/index)ä»¥äº†è§£æ›´å¤šå…³äºæ­¤é›†æˆçš„ä¿¡æ¯ã€‚

## ğŸ§¨ Diffusers

Diffusers æä¾›é¢„è®­ç»ƒçš„è§†è§‰å’ŒéŸ³é¢‘æ‰©æ•£æ¨¡å‹ï¼Œå¹¶ä¸”ç”¨ä½œæ¨ç†å’Œè®­ç»ƒçš„æ¨¡å—åŒ–å·¥å…·ç®±ã€‚æœ‰äº†è¿™ä¸ªåº“ï¼Œä½ å¯ä»¥ä»è‡ªç„¶è¯­è¨€è¾“å…¥å’Œå…¶ä»–åˆ›é€ æ€§ç”¨ä¾‹ä¸­ç”Ÿæˆå¯ä¿¡çš„å›¾åƒã€‚ä¸‹é¢æ˜¯ä¸€ä¸ªä¾‹å­ï¼š

```python
from diffusers import DiffusionPipeline

generator = DiffusionPipeline.from_pretrained("CompVis/stable-diffusion-v1-4")
generator.to(â€œcudaâ€)

image = generator("An image of a squirrel in Picasso style").images[0]
```

![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/cv_state/sd_output.png)

è¿™ç§ç±»å‹çš„æŠ€æœ¯å¯ä»¥èµ‹äºˆæ–°ä¸€ä»£çš„åˆ›é€ æ€§åº”ç”¨ï¼Œä¹Ÿå¯ä»¥å¸®åŠ©æ¥è‡ªä¸åŒèƒŒæ™¯çš„è‰ºæœ¯å®¶ã€‚æŸ¥çœ‹[å®˜æ–¹æ–‡æ¡£](https://huggingface.co/docs/diffusers)ä»¥äº†è§£æ›´å¤šå…³äº Diffusers å’Œä¸åŒç”¨ä¾‹çš„ä¿¡æ¯ã€‚

åŸºäºæ‰©æ•£æ¨¡å‹çš„æ–‡çŒ®æ­£åœ¨å¿«é€Ÿå‘å±•ï¼Œè¿™å°±æ˜¯ä¸ºä»€ä¹ˆæˆ‘ä»¬ä¸[ä¹”çº³æ£®Â·æƒ ç‰¹å…‹](https://github.com/johnowhitaker)åˆä½œå¼€å‘ä¸€é—¨è¯¾ç¨‹ã€‚è¿™é—¨è¯¾ç¨‹æ˜¯å…è´¹çš„ï¼Œä½ å¯ä»¥ç‚¹å‡»[è¿™é‡Œ](https://github.com/huggingface/diffusion-models-class)æŸ¥çœ‹ã€‚

## å¯¹ç¬¬ä¸‰æ–¹åº“çš„æ”¯æŒ

Hugging Face ç”Ÿæ€ç³»ç»Ÿçš„æ ¸å¿ƒæ˜¯ [Hugging Face Hub](https://huggingface.co/docs/hub)ï¼Œå®ƒè®©äººä»¬åœ¨æœºå™¨å­¦ä¹ ä¸Šæœ‰æ•ˆåˆä½œã€‚æ­£å¦‚å‰é¢æ‰€æåˆ°çš„ï¼Œæˆ‘ä»¬åœ¨ Hub ä¸Šä¸ä»…æ”¯æŒæ¥è‡ª ğŸ¤— Transformers çš„æ¨¡å‹ï¼Œè¿˜æ”¯æŒæ¥è‡ªå…¶ä»–ç¬¬ä¸‰æ–¹åŒ…çš„æ¨¡å‹ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æä¾›äº†å‡ ä¸ª[å®ç”¨ç¨‹åº](https://huggingface.co/docs/hub/models-adding-libraries)ï¼Œä»¥ä¾¿ä½ å¯ä»¥å°†è‡ªå·±çš„åº“ä¸ Hub é›†æˆã€‚è¿™æ ·åšçš„ä¸»è¦ä¼˜ç‚¹ä¹‹ä¸€æ˜¯ï¼Œä¸ç¤¾åŒºå…±äº«å·¥ä»¶ï¼ˆå¦‚æ¨¡å‹å’Œæ•°æ®é›†ï¼‰å˜å¾—éå¸¸å®¹æ˜“ï¼Œä»è€Œä½¿ä½ çš„ç”¨æˆ·å¯ä»¥æ›´å®¹æ˜“åœ°å°è¯•ä½ çš„æ¨¡å‹ã€‚

å½“ä½ çš„æ¨¡å‹æ‰˜ç®¡åœ¨ Hub ä¸Šæ—¶ï¼Œä½ è¿˜å¯ä»¥ä¸ºå®ƒä»¬[æ·»åŠ è‡ªå®šä¹‰æ¨ç†éƒ¨ä»¶](https://github.com/huggingface/api-inference-community)ã€‚æ¨ç†éƒ¨ä»¶å…è®¸ç”¨æˆ·å¿«é€Ÿåœ°æ£€æŸ¥æ¨¡å‹ã€‚è¿™æœ‰åŠ©äºæé«˜ç”¨æˆ·çš„å‚ä¸åº¦ã€‚

![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/cv_state/task_widget_generation.png)

## è®¡ç®—æœºè§†è§‰æ¼”ç¤ºç©ºé—´

ä½¿ç”¨ Hugging Hub Spacesåº”ç”¨ï¼Œäººä»¬å¯ä»¥è½»æ¾åœ°æ¼”ç¤ºä»–ä»¬çš„æœºå™¨å­¦ä¹ æ¨¡å‹ã€‚ç©ºé—´æ”¯æŒä¸ [Gradio](https://gradio.app/)ã€[Streamlit](https://streamlit.io/) å’Œ [Docker](https://www.docker.com/) çš„ç›´æ¥é›†æˆï¼Œä½¿ä»ä¸šè€…åœ¨å±•ç¤ºä»–ä»¬çš„æ¨¡å‹æ—¶æœ‰å¾ˆå¤§çš„çµæ´»æ€§ã€‚ä½ å¯ä»¥ç”¨ Spaces å¼•å…¥è‡ªå·±çš„æœºå™¨å­¦ä¹ æ¡†æ¶æ¥æ„å»ºæ¼”ç¤ºã€‚

åœ¨ Spaces é‡Œï¼ŒGradio åº“æä¾›å‡ ä¸ªéƒ¨ä»¶æ¥æ„å»ºè®¡ç®—æœºè§†è§‰åº”ç”¨ï¼Œæ¯”å¦‚ [Video](https://gradio.app/docs/#video)ã€[Gallery](https://gradio.app/docs/#gallery) å’Œ [Model3D](https://gradio.app/docs/#model3d)ã€‚ç¤¾åŒºä¸€ç›´åœ¨åŠªåŠ›æ„å»ºä¸€äº›ç”± Spaces æä¾›æ”¯æŒçš„ä»¤äººæƒŠå¹çš„è®¡ç®—æœºè§†è§‰åº”ç”¨ï¼š

- [ä»è¾“å…¥å›¾åƒçš„é¢„æµ‹æ·±åº¦å›¾ç”Ÿæˆ3Dä½“ç´ ](https://huggingface.co/spaces/radames/dpt-depth-estimation-3d-voxels)
- [å¼€æ”¾è¯æ±‡è¯­ä¹‰åˆ†å‰²](https://huggingface.co/spaces/facebook/ov-seg)
- [é€šè¿‡ç”Ÿæˆå­—å¹•æ¥è®²è¿°è§†é¢‘](https://huggingface.co/spaces/nateraw/lavila)
- [å¯¹æ¥è‡ªYouTubeçš„è§†é¢‘è¿›è¡Œåˆ†ç±»](https://huggingface.co/spaces/fcakyon/video-classification)
- [é›¶æ ·æœ¬è§†é¢‘åˆ†ç±»](https://huggingface.co/spaces/fcakyon/zero-shot-video-classification)
- [è§†è§‰é—®ç­”](https://huggingface.co/spaces/nielsr/vilt-vqa)
- [ä½¿ç”¨é›¶æ ·æœ¬å›¾åƒåˆ†ç±»ä¸ºå›¾åƒæ‰¾åˆ°æœ€ä½³è¯´æ˜ä»¥ç”Ÿæˆç›¸ä¼¼çš„å›¾åƒ](https://huggingface.co/spaces/pharma/CLIP-Interrogator)

## ğŸ¤— AutoTrain

[AutoTrain](https://huggingface.co/autotrain) æä¾›ä¸€ä¸ªâ€é›¶ä»£ç â€œçš„è§£å†³æ–¹æ¡ˆï¼Œä¸ºæ–‡æœ¬åˆ†ç±»ã€æ–‡æœ¬æ‘˜è¦ã€å‘½åå®ä½“è¯†åˆ«ç­‰è¿™æ ·çš„ä»»åŠ¡è®­ç»ƒæœ€å…ˆè¿›çš„æœºå™¨å­¦ä¹ æ¨¡å‹ã€‚å¯¹äºè®¡ç®—æœºè§†è§‰ï¼Œæˆ‘ä»¬å½“å‰æ”¯æŒ[å›¾åƒåˆ†ç±»](https://huggingface.co/blog/autotrain-image-classification)ï¼Œä½†å¯ä»¥æœŸå¾…æ›´å¤šçš„ä»»åŠ¡è¦†ç›–ã€‚

AutoTrain è¿˜æ”¯æŒ[è‡ªåŠ¨æ¨¡å‹è¯„ä¼°](https://huggingface.co/spaces/autoevaluate/model-evaluator)ã€‚æ­¤åº”ç”¨ç¨‹åºå…è®¸ä½ ç”¨åœ¨ Hub ä¸Šçš„å„ç§[æ•°æ®é›†](https://huggingface.co/datasets)è¯„ä¼° ğŸ¤— Transformers [æ¨¡å‹](https://huggingface.co/models?library=transformers&sort=downloads)ã€‚ä½ çš„è¯„ä¼°ç»“æœå°†ä¼šæ˜¾ç¤ºåœ¨[å…¬å…±æ’è¡Œæ¦œ](https://huggingface.co/spaces/autoevaluate/leaderboards)ä¸Šã€‚ä½ å¯ä»¥æŸ¥çœ‹[è¿™ç¯‡åšå®¢](https://huggingface.co/blog/eval-on-the-hub)ä»¥è·å¾—æ›´å¤šç»†èŠ‚ã€‚

## æŠ€æœ¯ç†å¿µ

åœ¨æ­¤éƒ¨åˆ†ï¼Œæˆ‘ä»¬åƒå‘ä½ åˆ†äº«åœ¨ ğŸ¤— Transformers é‡Œæ·»åŠ è®¡ç®—æœºè§†è§‰èƒŒåçš„ç†å¿µï¼Œä»¥ä¾¿ç¤¾åŒºçŸ¥é“é’ˆå¯¹è¯¥é¢†åŸŸçš„è®¾è®¡é€‰æ‹©ã€‚

å°½ç®¡ Transformers æ˜¯ä» NLP å¼€å§‹çš„ï¼Œä½†æˆ‘ä»¬ä»Šå¤©æ”¯æŒå¤šç§æ¨¡å¼ï¼Œæ¯”å¦‚ï¼šè§†è§‰ã€éŸ³é¢‘ã€è§†è§‰è¯­è¨€å’Œå¼ºåŒ–å­¦ä¹ ã€‚å¯¹äºæ‰€æœ‰çš„è¿™äº›æ¨¡å¼ï¼ŒTransformers ä¸­æ‰€æœ‰ç›¸åº”çš„æ¨¡å‹éƒ½äº«æœ‰ä¸€äº›å…±åŒçš„ä¼˜åŠ¿ï¼š

- ä½¿ç”¨ä¸€è¡Œä»£ç `from_pretrained()`å³å¯è½»æ¾ä¸‹è½½æ¨¡å‹
- ç”¨`push_to_hub()`è½»æ¾ä¸Šä¼ æ¨¡å‹
- æ”¯æŒä½¿ç”¨ checkpoint åˆ†ç‰‡æŠ€æœ¯åŠ è½½å¤§å‹çš„ checkpoints
- ä¼˜åŒ–æ”¯æŒï¼ˆä½¿ç”¨ [Optimum](https://huggingface.co/docs/optimum) ä¹‹ç±»çš„å·¥å…·ï¼‰

- ä»æ¨¡å‹é…ç½®ä¸­åˆå§‹åŒ–
- æ”¯æŒ PyTorch å’Œ TensorFlowï¼ˆéå…¨é¢æ”¯æŒï¼‰
- ä»¥åŠæ›´å¤š

ä¸åˆ†è¯å™¨ä¸åŒï¼Œæˆ‘ä»¬æœ‰é¢„å¤„ç†å™¨ï¼ˆä¾‹å¦‚[è¿™ä¸ª](https://huggingface.co/docs/transformers/model_doc/vit#transformers.ViTImageProcessor)ï¼‰è´Ÿè´£ä¸ºè§†è§‰æ¨¡å‹å‡†å¤‡æ•°æ®ã€‚æˆ‘ä»¬ä¸€ç›´åŠªåŠ›ç¡®ä¿åœ¨ä½¿ç”¨è§†è§‰æ¨¡å‹æ—¶ä¾ç„¶æœ‰è½»æ¾å’Œç›¸ä¼¼çš„ç”¨æˆ·ä½“éªŒï¼š

```python
from transformers import ViTImageProcessor, ViTForImageClassification
import torch
from datasets import load_dataset

dataset = load_dataset("huggingface/cats-image")
image = dataset["test"]["image"][0]

image_processorÂ  = ViTImageProcessor.from_pretrained("google/vit-base-patch16-224")
model = ViTForImageClassification.from_pretrained("google/vit-base-patch16-224")
inputs = image_processor(image, return_tensors="pt")

with torch.no_grad():
    logits = model(**inputs).logits

# model predicts one of the 1000 ImageNet classes
predicted_label = logits.argmax(-1).item()
print(model.config.id2label[predicted_label])
# Egyptian cat
```

å³ä½¿å¯¹äºä¸€ä¸ªå›°éš¾çš„ä»»åŠ¡å¦‚ç›®æ ‡æ£€æµ‹ï¼Œç”¨æˆ·ä½“éªŒä¹Ÿä¸ä¼šæ”¹å˜å¾ˆå¤šï¼š

```python
from transformers import AutoImageProcessor, AutoModelForObjectDetection
from PIL import Image
import requests

url = "http://images.cocodataset.org/val2017/000000039769.jpg"
image = Image.open(requests.get(url, stream=True).raw)

image_processor = AutoImageProcessor.from_pretrained("microsoft/conditional-detr-resnet-50")
model = AutoModelForObjectDetection.from_pretrained("microsoft/conditional-detr-resnet-50")
inputs = image_processor(images=image, return_tensors="pt")

outputs = model(**inputs)

# convert outputs (bounding boxes and class logits) to COCO API
target_sizes = torch.tensor([image.size[::-1]])
results = image_processor.post_process_object_detection(
    outputs, threshold=0.5, target_sizes=target_sizes
)[0]

for score, label, box in zip(results["scores"], results["labels"], results["boxes"]):
    box = [round(i, 2) for i in box.tolist()]
    print(
        f"Detected {model.config.id2label[label.item()]} with confidence "
        f"{round(score.item(), 3)} at location {box}"
    )
```

è¾“å‡ºä¸ºï¼š

```
Detected remote with confidence 0.833 at location [38.31, 72.1, 177.63, 118.45]
Detected cat with confidence 0.831 at location [9.2, 51.38, 321.13, 469.0]
Detected cat with confidence 0.804 at location [340.3, 16.85, 642.93, 370.95]
Detected remote with confidence 0.683 at location [334.48, 73.49, 366.37, 190.01]
Detected couch with confidence 0.535 at location [0.52, 1.19, 640.35, 475.1]
```

## è§†è§‰é›¶æ ·æœ¬æ¨¡å‹

å¤§é‡çš„æ¨¡å‹ä»¥æœ‰è¶£çš„æ–¹å¼é‡æ–°ä¿®è®¢äº†åˆ†å‰²å’Œæ£€æµ‹ç­‰æ ¸å¿ƒè§†è§‰ä»»åŠ¡ï¼Œå¹¶å¼•å…¥äº†æ›´å¤§çš„çµæ´»æ€§ã€‚æˆ‘ä»¬æ”¯æŒ Transformers ä¸­çš„ä¸€äº›ï¼š

- [CLIP](https://huggingface.co/docs/transformers/main/en/model_doc/clip) æ”¯æŒå¸¦æç¤ºçš„é›¶æ ·æœ¬å›¾åƒåˆ†ç±»ã€‚ç»™å®šä¸€å¼ å›¾ç‰‡ï¼Œä½ å¯ä»¥ç”¨ç±»ä¼¼â€ä¸€å¼ {}çš„å›¾ç‰‡â€œè¿™æ ·çš„è‡ªç„¶è¯­è¨€è¯¢é—®æ¥æç¤º CLIP æ¨¡å‹ã€‚æœŸæœ›æ˜¯å¾—åˆ°ç±»åˆ«æ ‡ç­¾ä½œä¸ºç­”æ¡ˆã€‚
- [OWL-ViT](https://huggingface.co/docs/transformers/main/en/model_doc/owlvit) å…è®¸ä»¥è¯­è¨€ä¸ºæ¡ä»¶çš„é›¶æ ·æœ¬ç›®æ ‡æ£€æµ‹å’Œä»¥å›¾åƒä¸ºæ¡ä»¶çš„å•æ ·æœ¬ç›®æ ‡æ£€æµ‹ã€‚è¿™æ„å‘³ç€ä½ å¯ä»¥åœ¨ä¸€å¼ å›¾ç‰‡ä¸­æ£€æµ‹ç‰©ä½“å³ä½¿åº•å±‚æ¨¡å‹åœ¨è®­ç»ƒæœŸé—´æ²¡æœ‰å­¦è¿‡æ£€æµ‹å®ƒä»¬ï¼ä½ å¯ä»¥å‚è€ƒ[è¿™ä¸ªç¬”è®°æœ¬](https://github.com/huggingface/notebooks/tree/main/examples#:~:text=zeroshot_object_detection_with_owlvit.ipynb)ä»¥äº†è§£æ›´å¤šã€‚

- [CLIPSeg](https://huggingface.co/docs/transformers/model_doc/clipseg) æ”¯æŒä»¥è¯­è¨€ä¸ºæ¡ä»¶çš„é›¶æ ·æœ¬å›¾åƒåˆ†å‰²å’Œä»¥å›¾åƒä¸ºæ¡ä»¶çš„å•æ ·æœ¬å›¾åƒåˆ†å‰²ã€‚è¿™æ„å‘³ç€ä½ å¯ä»¥åœ¨ä¸€å¼ å›¾ç‰‡ä¸­åˆ†å‰²ç‰©ä½“å³ä½¿åº•å±‚æ¨¡å‹åœ¨è®­ç»ƒæœŸé—´æ²¡æœ‰å­¦è¿‡åˆ†å‰²å®ƒä»¬ï¼ä½ å¯ä»¥å‚è€ƒè¯´æ˜æ­¤æƒ³æ³•çš„[è¿™ç¯‡åšå®¢æ–‡ç« ](https://huggingface.co/blog/clipseg-zero-shot)ã€‚[GroupViT](https://huggingface.co/docs/transformers/model_doc/groupvit) ä¹Ÿæ”¯æŒé›¶æ ·æœ¬åˆ†å‰²ã€‚
- [X-CLIP](https://huggingface.co/docs/transformers/main/en/model_doc/xclip) å±•ç¤ºå¯¹è§†é¢‘çš„é›¶æ ·æœ¬æ³›åŒ–ã€‚å‡†ç¡®åœ°è¯´æ˜¯æ”¯æŒé›¶æ ·æœ¬è§†é¢‘åˆ†ç±»ã€‚æŸ¥çœ‹[è¿™ä¸ªç¬”è®°æœ¬](https://github.com/NielsRogge/Transformers-Tutorials/blob/master/X-CLIP/Zero_shot_classify_a_YouTube_video_with_X_CLIP.ipynb)ä»¥è·å¾—æ›´å¤šç»†èŠ‚ã€‚

ç¤¾åŒºæœŸå¾…åœ¨ä»Šåçš„æ—¥å­é‡Œçœ‹åˆ° ğŸ¤—Transformers æ”¯æŒæ›´å¤šçš„è®¡ç®—æœºè§†è§‰é›¶æ ·æœ¬æ¨¡å‹ã€‚

## å¼€å‘

æˆ‘ä»¬çš„ CTO è¯´ï¼šâ€çœŸæ­£çš„è‰ºæœ¯å®¶èƒ½å°†äº§å“ä¸Šå¸‚â€œğŸš€

æˆ‘ä»¬é€šè¿‡ ğŸ¤—[Inference Endpoints](https://huggingface.co/inference-endpoints) æ”¯æŒè¿™äº›è§†è§‰æ¨¡å‹çš„å¼€å‘ã€‚Inference Endpoints ç›´æ¥é›†æˆäº†ä¸å›¾åƒåˆ†ç±»ã€ç›®æ ‡æ£€æµ‹ã€å›¾åƒåˆ†å‰²ç›¸å…³çš„å…¼å®¹æ¨¡å‹ã€‚å¯¹äºå…¶ä»–æ¨¡å‹ï¼Œä½ å¯ä»¥ä½¿ç”¨è‡ªå®šä¹‰å¤„ç†ç¨‹åºã€‚ç”±äºæˆ‘ä»¬è¿˜åœ¨ TensorFlow ä¸­æä¾›äº†è®¸å¤šæ¥è‡ª ğŸ¤—Transformers çš„è§†è§‰æ¨¡å‹ç”¨äºéƒ¨ç½²ï¼Œæˆ‘ä»¬å»ºè®®ä½¿ç”¨è‡ªå®šä¹‰å¤„ç†ç¨‹åºæˆ–éµå¾ªè¿™äº›èµ„æºï¼š

- [åœ¨ Hugging Face ä¸Šç”¨ TF æœåŠ¡å¼€å‘ TensorFlow è§†è§‰æ¨¡å‹](https://huggingface.co/blog/tf-serving-vision)
- [åœ¨ Kubernets ä¸Šç”¨ TF æœåŠ¡å¼€å‘ ViT](https://huggingface.co/blog/deploy-tfserving-kubernetes)
- [åœ¨ Vertex AI ä¸Šå¼€å‘ ViT](https://huggingface.co/blog/deploy-vertex-ai)
- [ç”¨ TFX å’Œ Vertex AI å¼€å‘ ViT](https://github.com/deep-diver/mlops-hf-tf-vision-models)

## ç»“è®º

åœ¨è¿™ç¯‡æ–‡ç« ä¸­ï¼Œæˆ‘ä»¬å‘ä½ ç®€è¦ä»‹ç»äº† Hugging Face ç”Ÿæ€ç³»ç»Ÿç›®å‰ä¸ºä¸‹ä¸€ä»£è®¡ç®—æœºè§†è§‰åº”ç”¨æä¾›çš„æ”¯æŒã€‚æˆ‘ä»¬å¸Œæœ›ä½ ä¼šå–œæ¬¢ä½¿ç”¨è¿™äº›äº§å“æ¥å¯é åœ°æ„å»ºåº”ç”¨ã€‚

ä¸è¿‡è¿˜æœ‰å¾ˆå¤šå·¥ä½œè¦åšã€‚ ä»¥ä¸‹æ˜¯æ‚¨å¯ä»¥æœŸå¾…çœ‹åˆ°çš„ä¸€äº›å†…å®¹ï¼š

- ğŸ¤— Datasets å¯¹è§†é¢‘çš„ç›´æ¥æ”¯æŒ
- æ”¯æŒæ›´å¤šå’Œå·¥ä¸šç•Œç›¸å…³çš„ä»»åŠ¡ï¼Œæ¯”å¦‚å›¾åƒç›¸ä¼¼æ€§
- å›¾åƒæ•°æ®é›†ä¸ TensorFlow çš„äº¤äº’
- æ¥è‡ª ğŸ¤—Hugging Face ç¤¾åŒºå…³äºè®¡ç®—æœºè§†è§‰çš„è¯¾ç¨‹

åƒå¾€å¸¸ä¸€æ ·ï¼Œæˆ‘ä»¬æ¬¢è¿ä½ çš„è¡¥ä¸ã€PRã€æ¨¡å‹ checkpointsã€æ•°æ®é›†å’Œå…¶ä»–è´¡çŒ®ï¼ğŸ¤—

*Acknowlegements: Thanks to Omar Sanseviero, Nate Raw, Niels Rogge, Alara Dirik, Amy Roberts, Maria Khalusova, and Lysandre Debut for their rigorous and timely reviews on the blog draft. Thanks to Chunte Lee for creating the blog thumbnail.*

---
title: åŸºäº Hugging Face Datasets å’Œ Transformers çš„å›¾åƒç›¸ä¼¼æ€§æœç´¢
thumbnail: /blog/assets/image_similarity/thumbnail.png
authors:
- user: sayakpaul
translators:
- user: MatrixYao
- user: zhongdongy
  proofreader: true
---

# åŸºäº Hugging Face Datasets å’Œ Transformers çš„å›¾åƒç›¸ä¼¼æ€§æœç´¢

<a target="_blank" href="https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/image_similarity.ipynb">
    <img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/>
</a>

é€šè¿‡æœ¬æ–‡ï¼Œä½ å°†å­¦ä¹ ä½¿ç”¨ ğŸ¤— Transformers æ„å»ºå›¾åƒç›¸ä¼¼æ€§æœç´¢ç³»ç»Ÿã€‚æ‰¾å‡ºæŸ¥è¯¢å›¾åƒå’Œæ½œåœ¨å€™é€‰å›¾åƒä¹‹é—´çš„ç›¸ä¼¼æ€§æ˜¯ä¿¡æ¯æ£€ç´¢ç³»ç»Ÿçš„ä¸€ä¸ªé‡è¦ç”¨ä¾‹ï¼Œä¾‹å¦‚åå‘å›¾åƒæœç´¢ (å³æ‰¾å‡ºæŸ¥è¯¢å›¾åƒçš„åŸå›¾)ã€‚æ­¤ç±»ç³»ç»Ÿè¯•å›¾è§£ç­”çš„é—®é¢˜æ˜¯ï¼Œç»™å®šä¸€ä¸ª _æŸ¥è¯¢_ å›¾åƒå’Œä¸€ç»„ _å€™é€‰_ å›¾åƒï¼Œæ‰¾å‡ºå€™é€‰å›¾åƒä¸­å“ªäº›å›¾åƒä¸æŸ¥è¯¢å›¾åƒæœ€ç›¸ä¼¼ã€‚

æˆ‘ä»¬å°†ä½¿ç”¨ [ğŸ¤— datasets åº“](https://huggingface.co/docs/datasets/)ï¼Œå› ä¸ºå®ƒæ— ç¼æ”¯æŒå¹¶è¡Œå¤„ç†ï¼Œè¿™åœ¨æ„å»ºç³»ç»Ÿæ—¶ä¼šæ´¾ä¸Šç”¨åœºã€‚

å°½ç®¡è¿™ç¯‡æ–‡ç« ä½¿ç”¨äº†åŸºäº ViT çš„æ¨¡å‹ ([`nateraw/vit-base-beans`](https://huggingface.co/nateraw/vit-base-beans)) å’Œç‰¹å®šçš„ ([Beans](https://huggingface.co/datasets/beans)) æ•°æ®é›†ï¼Œä½†å®ƒå¯ä»¥æ‰©å±•åˆ°å…¶ä»–æ”¯æŒè§†è§‰æ¨¡æ€çš„æ¨¡å‹ï¼Œä¹Ÿå¯ä»¥æ‰©å±•åˆ°å…¶ä»–å›¾åƒæ•°æ®é›†ã€‚ä½ å¯ä»¥å°è¯•çš„ä¸€äº›è‘—åæ¨¡å‹æœ‰: 

* [Swin Transformer](https://huggingface.co/docs/transformers/model_doc/swin)
* [ConvNeXT](https://huggingface.co/docs/transformers/model_doc/convnext)
* [RegNet](https://huggingface.co/docs/transformers/model_doc/regnet)

æ­¤å¤–ï¼Œæ–‡ç« ä¸­ä»‹ç»çš„æ–¹æ³•ä¹Ÿæœ‰å¯èƒ½æ‰©å±•åˆ°å…¶ä»–æ¨¡æ€ã€‚

è¦ç ”ç©¶å®Œæ•´çš„å›¾åƒç›¸ä¼¼åº¦ç³»ç»Ÿï¼Œä½ å¯ä»¥å‚è€ƒ [è¿™ä¸ª Colab Notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/image_similarity.ipynb) 

## æˆ‘ä»¬å¦‚ä½•å®šä¹‰ç›¸ä¼¼æ€§ï¼Ÿ

è¦æ„å»ºè¿™ä¸ªç³»ç»Ÿï¼Œæˆ‘ä»¬é¦–å…ˆéœ€è¦å®šä¹‰æˆ‘ä»¬æƒ³è¦å¦‚ä½•è®¡ç®—ä¸¤ä¸ªå›¾åƒä¹‹é—´çš„ç›¸ä¼¼åº¦ã€‚ä¸€ç§å¹¿æ³›æµè¡Œçš„åšæ³•æ˜¯å…ˆè®¡ç®—ç»™å®šå›¾åƒçš„ç¨ å¯†è¡¨å¾ (å³åµŒå…¥ (embedding))ï¼Œç„¶åä½¿ç”¨ ä½™å¼¦ç›¸ä¼¼æ€§åº¦é‡ ([cosine similarity metric](https://en.wikipedia.org/wiki/Cosine_similarity)) æ¥ç¡®å®šä¸¤å¹…å›¾åƒçš„ç›¸ä¼¼ç¨‹åº¦ã€‚

åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨ â€œåµŒå…¥â€ æ¥è¡¨ç¤ºå‘é‡ç©ºé—´ä¸­çš„å›¾åƒã€‚å®ƒä¸ºæˆ‘ä»¬æä¾›äº†ä¸€ç§å°†å›¾åƒä»é«˜ç»´åƒç´ ç©ºé—´ (ä¾‹å¦‚ 224 Ã— 224 Ã— 3) æœ‰æ„ä¹‰åœ°å‹ç¼©åˆ°ä¸€ä¸ªä½å¾—å¤šçš„ç»´åº¦ (ä¾‹å¦‚ 768) çš„å¥½æ–¹æ³•ã€‚è¿™æ ·åšçš„ä¸»è¦ä¼˜ç‚¹æ˜¯å‡å°‘äº†åç»­æ­¥éª¤ä¸­çš„è®¡ç®—æ—¶é—´ã€‚

<div align="center">
<img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/image_similarity/embeddings.png" width=700/>
</div>

## è®¡ç®—åµŒå…¥

ä¸ºäº†è®¡ç®—å›¾åƒçš„åµŒå…¥ï¼Œæˆ‘ä»¬éœ€è¦ä½¿ç”¨ä¸€ä¸ªè§†è§‰æ¨¡å‹ï¼Œè¯¥æ¨¡å‹çŸ¥é“å¦‚ä½•åœ¨å‘é‡ç©ºé—´ä¸­è¡¨ç¤ºè¾“å…¥å›¾åƒã€‚è¿™ç§ç±»å‹çš„æ¨¡å‹é€šå¸¸ä¹Ÿç§°ä¸ºå›¾åƒç¼–ç å™¨ (image encoder)ã€‚

æˆ‘ä»¬åˆ©ç”¨ [`AutoModel` ç±»](https://huggingface.co/docs/transformers/model_doc/auto#transformers.AutoModel) æ¥åŠ è½½æ¨¡å‹ã€‚å®ƒä¸ºæˆ‘ä»¬æä¾›äº†ä¸€ä¸ªæ¥å£ï¼Œå¯ä»¥ä» HuggingFace Hub åŠ è½½ä»»ä½•å…¼å®¹çš„æ¨¡å‹ checkpointã€‚é™¤äº†æ¨¡å‹ï¼Œæˆ‘ä»¬è¿˜ä¼šåŠ è½½ä¸æ¨¡å‹å…³è”çš„å¤„ç†å™¨ (processor) ä»¥è¿›è¡Œæ•°æ®é¢„å¤„ç†ã€‚


```py
from transformers import AutoImageProcessor, AutoModel


model_ckpt = "nateraw/vit-base-beans"
processor = AutoImageProcessor.from_pretrained(model_ckpt)
model = AutoModel.from_pretrained(model_ckpt)
```

æœ¬ä¾‹ä¸­ä½¿ç”¨çš„ checkpoint æ˜¯ä¸€ä¸ªåœ¨ [`beans` ](https://huggingface.co/datasets/beans) ä¸Šå¾®è°ƒè¿‡çš„ [ViT æ¨¡å‹](https://huggingface.co/google/vit-base-patch16-224-in21k)ã€‚

è¿™é‡Œå¯èƒ½ä½ ä¼šé—®ä¸€äº›é—®é¢˜: 

**Q1**: ä¸ºä»€ä¹ˆæˆ‘ä»¬ä¸ä½¿ç”¨ `AutoModelForImageClassification`ï¼Ÿ

è¿™æ˜¯å› ä¸ºæˆ‘ä»¬æƒ³è¦è·å¾—å›¾åƒçš„ç¨ å¯†è¡¨å¾ï¼Œè€Œ `AutoModelForImageClassification` åªèƒ½è¾“å‡ºç¦»æ•£ç±»åˆ«ã€‚

**Q2**: ä¸ºä»€ä¹ˆä½¿ç”¨è¿™ä¸ªç‰¹å®šçš„ checkpointï¼Ÿ

å¦‚å‰æ‰€è¿°ï¼Œæˆ‘ä»¬ä½¿ç”¨ç‰¹å®šçš„æ•°æ®é›†æ¥æ„å»ºç³»ç»Ÿã€‚å› æ­¤ï¼Œä¸å…¶ä½¿ç”¨é€šç”¨æ¨¡å‹ (ä¾‹å¦‚ åœ¨ [ImageNet-1k æ•°æ®é›†ä¸Šè®­ç»ƒçš„æ¨¡å‹](https://huggingface.co/models?dataset=dataset:imagenet-1k&sort=downloads))ï¼Œä¸å¦‚ä½¿ç”¨ä½¿ç”¨å·²é’ˆå¯¹æ‰€ç”¨æ•°æ®é›†å¾®è°ƒè¿‡çš„æ¨¡å‹ã€‚è¿™æ ·ï¼Œæ¨¡å‹èƒ½æ›´å¥½åœ°ç†è§£è¾“å…¥å›¾åƒã€‚

**æ³¨æ„** ä½ è¿˜å¯ä»¥ä½¿ç”¨é€šè¿‡è‡ªç›‘ç£é¢„è®­ç»ƒè·å¾—çš„ checkpoint, ä¸å¿…å¾—ç”±æœ‰ç›‘ç£å­¦ä¹ è®­ç»ƒè€Œå¾—ã€‚äº‹å®ä¸Šï¼Œå¦‚æœé¢„è®­ç»ƒå¾—å½“ï¼Œè‡ªç›‘ç£æ¨¡å‹å¯ä»¥ [è·å¾—](https://ai.facebook.com/blog/dino-paws-computer-vision-with-self-supervised-transformers-and-10x-more-efficient-training/) ä»¤äººå°è±¡æ·±åˆ»çš„æ£€ç´¢æ€§èƒ½ã€‚

ç°åœ¨æˆ‘ä»¬æœ‰äº†ä¸€ä¸ªç”¨äºè®¡ç®—åµŒå…¥çš„æ¨¡å‹ï¼Œæˆ‘ä»¬éœ€è¦ä¸€äº›å€™é€‰å›¾åƒæ¥è¢«æŸ¥è¯¢ã€‚

## åŠ è½½å€™é€‰å›¾åƒæ•°æ®é›†

åé¢ï¼Œæˆ‘ä»¬ä¼šæ„å»ºå°†å€™é€‰å›¾åƒæ˜ å°„åˆ°å“ˆå¸Œå€¼çš„å“ˆå¸Œè¡¨ã€‚åœ¨æŸ¥è¯¢æ—¶ï¼Œæˆ‘ä»¬ä¼šä½¿ç”¨åˆ°è¿™äº›å“ˆå¸Œè¡¨ï¼Œè¯¦ç»†è®¨è®ºçš„è®¨è®ºç¨åè¿›è¡Œã€‚ç°åœ¨ï¼Œæˆ‘ä»¬å…ˆä½¿ç”¨ [`beans` æ•°æ®é›†](https://huggingface.co/datasets/beans) ä¸­çš„è®­ç»ƒé›†æ¥è·å–ä¸€ç»„å€™é€‰å›¾åƒã€‚

```py
from datasets import load_dataset


dataset = load_dataset("beans")
```

This is how a single sample from the training split looks like:

<div align="center">
    <img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/image_similarity/beans.png" width=600/>
</div>

è¯¥æ•°æ®é›†çš„ä¸‰ä¸ª `features` å¦‚ä¸‹: 

```py
dataset["train"].features
>>> {'image_file_path': Value(dtype='string', id=None),
 'image': Image(decode=True, id=None),
 'labels': ClassLabel(names=['angular_leaf_spot', 'bean_rust', 'healthy'], id=None)}
```

ä¸ºäº†ä½¿å›¾åƒç›¸ä¼¼æ€§ç³»ç»Ÿå¯æ¼”ç¤ºï¼Œç³»ç»Ÿçš„æ€»ä½“è¿è¡Œæ—¶é—´éœ€è¦æ¯”è¾ƒçŸ­ï¼Œå› æ­¤æˆ‘ä»¬è¿™é‡Œåªä½¿ç”¨å€™é€‰å›¾åƒæ•°æ®é›†ä¸­çš„ 100 å¼ å›¾åƒã€‚

```py
num_samples = 100
seed = 42
candidate_subset = dataset["train"].shuffle(seed=seed).select(range(num_samples))
```

## å¯»æ‰¾ç›¸ä¼¼å›¾ç‰‡çš„è¿‡ç¨‹

ä¸‹å›¾å±•ç¤ºäº†è·å–ç›¸ä¼¼å›¾åƒçš„åŸºæœ¬è¿‡ç¨‹ã€‚

<div align="center">
    <img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/image_similarity/fetch-similar-process.png">
</div>

ç¨å¾®æ‹†è§£ä¸€ä¸‹ä¸Šå›¾ï¼Œæˆ‘ä»¬åˆ†ä¸º 4 æ­¥èµ°ï¼š

1. ä»å€™é€‰å›¾åƒ (`candidate_subset`) ä¸­æå–åµŒå…¥ï¼Œå°†å®ƒä»¬å­˜å‚¨åœ¨ä¸€ä¸ªçŸ©é˜µä¸­ã€‚
2. è·å–æŸ¥è¯¢å›¾åƒå¹¶æå–å…¶åµŒå…¥ã€‚
3. éå†åµŒå…¥çŸ©é˜µ (æ­¥éª¤ 1 ä¸­å¾—åˆ°çš„) å¹¶è®¡ç®—æŸ¥è¯¢åµŒå…¥å’Œå½“å‰å€™é€‰åµŒå…¥ä¹‹é—´çš„ç›¸ä¼¼åº¦å¾—åˆ†ã€‚æˆ‘ä»¬é€šå¸¸ç»´æŠ¤ä¸€ä¸ªç±»ä¼¼å­—å…¸çš„æ˜ å°„ï¼Œæ¥ç»´æŠ¤å€™é€‰å›¾åƒçš„ ID ä¸ç›¸ä¼¼æ€§åˆ†æ•°ä¹‹é—´çš„å¯¹åº”å…³ç³»ã€‚
4. æ ¹æ®ç›¸ä¼¼åº¦å¾—åˆ†è¿›è¡Œæ’åºå¹¶è¿”å›ç›¸åº”çš„å›¾åƒ IDã€‚æœ€åï¼Œä½¿ç”¨è¿™äº› ID æ¥è·å–å€™é€‰å›¾åƒã€‚

æˆ‘ä»¬å¯ä»¥ç¼–å†™ä¸€ä¸ªç®€å•çš„å·¥å…·å‡½æ•°ç”¨äºè®¡ç®—åµŒå…¥å¹¶ä½¿ç”¨ `map()` æ–¹æ³•å°†å…¶ä½œç”¨äºå€™é€‰å›¾åƒæ•°æ®é›†çš„æ¯å¼ å›¾åƒï¼Œä»¥æœ‰æ•ˆåœ°è®¡ç®—åµŒå…¥ã€‚

```py
import torch 

def extract_embeddings(model: torch.nn.Module):
    """Utility to compute embeddings."""
    device = model.device

    def pp(batch):
        images = batch["image"]
        # `transformation_chain` is a compostion of preprocessing
        # transformations we apply to the input images to prepare them
        # for the model. For more details, check out the accompanying Colab Notebook.
        image_batch_transformed = torch.stack(
            [transformation_chain(image) for image in images]
        )
        new_batch = {"pixel_values": image_batch_transformed.to(device)}
        with torch.no_grad():
            embeddings = model(**new_batch).last_hidden_state[:, 0].cpu()
        return {"embeddings": embeddings}

    return pp
```

æˆ‘ä»¬å¯ä»¥åƒè¿™æ ·æ˜ å°„ `extract_embeddings()`: 

```py
device = "cuda" if torch.cuda.is_available() else "cpu"
extract_fn = extract_embeddings(model.to(device))
candidate_subset_emb = candidate_subset.map(extract_fn, batched=True, batch_size=batch_size)
```

æ¥ä¸‹æ¥ï¼Œä¸ºæ–¹ä¾¿èµ·è§ï¼Œæˆ‘ä»¬åˆ›å»ºä¸€ä¸ªå€™é€‰å›¾åƒ ID çš„åˆ—è¡¨ã€‚

```py
candidate_ids = []

for id in tqdm(range(len(candidate_subset_emb))):
    label = candidate_subset_emb[id]["labels"]

    # Create a unique indentifier.
    entry = str(id) + "_" + str(label)

    candidate_ids.append(entry)
```

æˆ‘ä»¬ç”¨åŒ…å«æ‰€æœ‰å€™é€‰å›¾åƒçš„åµŒå…¥çŸ©é˜µæ¥è®¡ç®—ä¸æŸ¥è¯¢å›¾åƒçš„ç›¸ä¼¼åº¦åˆ†æ•°ã€‚æˆ‘ä»¬ä¹‹å‰å·²ç»è®¡ç®—äº†å€™é€‰å›¾åƒåµŒå…¥ï¼Œåœ¨è¿™é‡Œæˆ‘ä»¬åªæ˜¯å°†å®ƒä»¬é›†ä¸­åˆ°ä¸€ä¸ªçŸ©é˜µä¸­ã€‚

```py
all_candidate_embeddings = np.array(candidate_subset_emb["embeddings"])
all_candidate_embeddings = torch.from_numpy(all_candidate_embeddings)
```

æˆ‘ä»¬å°†ä½¿ç”¨ä½™å¼¦ç›¸ä¼¼åº¦æ¥è®¡ç®—ä¸¤ä¸ªåµŒå…¥å‘é‡ä¹‹é—´çš„ [ç›¸ä¼¼åº¦åˆ†æ•°](https://en.wikipedia.org/wiki/Cosine_similarity)ã€‚ç„¶åï¼Œæˆ‘ä»¬ç”¨å®ƒæ¥è·å–ç»™å®šæŸ¥è¯¢å›¾åƒçš„ç›¸ä¼¼å€™é€‰å›¾åƒã€‚

```py
def compute_scores(emb_one, emb_two):
    """Computes cosine similarity between two vectors."""
    scores = torch.nn.functional.cosine_similarity(emb_one, emb_two)
    return scores.numpy().tolist()


def fetch_similar(image, top_k=5):
    """Fetches the `top_k` similar images with `image` as the query."""
    # Prepare the input query image for embedding computation.
    image_transformed = transformation_chain(image).unsqueeze(0)
    new_batch = {"pixel_values": image_transformed.to(device)}

    # Comute the embedding.
    with torch.no_grad():
        query_embeddings = model(**new_batch).last_hidden_state[:, 0].cpu()

    # Compute similarity scores with all the candidate images at one go.
    # We also create a mapping between the candidate image identifiers
    # and their similarity scores with the query image.
    sim_scores = compute_scores(all_candidate_embeddings, query_embeddings)
    similarity_mapping = dict(zip(candidate_ids, sim_scores))
 
    # Sort the mapping dictionary and return `top_k` candidates.
    similarity_mapping_sorted = dict(
        sorted(similarity_mapping.items(), key=lambda x: x[1], reverse=True)
    )
    id_entries = list(similarity_mapping_sorted.keys())[:top_k]

    ids = list(map(lambda x: int(x.split("_")[0]), id_entries))
    labels = list(map(lambda x: int(x.split("_")[-1]), id_entries))
    return ids, labels
```

## æ‰§è¡ŒæŸ¥è¯¢

ç»è¿‡ä»¥ä¸Šå‡†å¤‡ï¼Œæˆ‘ä»¬å¯ä»¥è¿›è¡Œç›¸ä¼¼æ€§æœç´¢äº†ã€‚æˆ‘ä»¬ä» `beans` æ•°æ®é›†çš„æµ‹è¯•é›†ä¸­é€‰å–ä¸€å¼ æŸ¥è¯¢å›¾åƒæ¥æœç´¢: 

```py
test_idx = np.random.choice(len(dataset["test"]))
test_sample = dataset["test"][test_idx]["image"]
test_label = dataset["test"][test_idx]["labels"]

sim_ids, sim_labels = fetch_similar(test_sample)
print(f"Query label: {test_label}")
print(f"Top 5 candidate labels: {sim_labels}")
```

ç»“æœä¸º:

```
Query label: 0
Top 5 candidate labels: [0, 0, 0, 0, 0]
```

çœ‹èµ·æ¥æˆ‘ä»¬çš„ç³»ç»Ÿå¾—åˆ°äº†ä¸€ç»„æ­£ç¡®çš„ç›¸ä¼¼å›¾åƒã€‚å°†ç»“æœå¯è§†åŒ–ï¼Œå¦‚ä¸‹: 

<div align="center">
<img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/image_similarity/results_one.png">
</div>

## è¿›ä¸€æ­¥æ‰©å±•ä¸ç»“è®º

ç°åœ¨ï¼Œæˆ‘ä»¬æœ‰äº†ä¸€ä¸ªå¯ç”¨çš„å›¾åƒç›¸ä¼¼åº¦ç³»ç»Ÿã€‚ä½†å®é™…ç³»ç»Ÿéœ€è¦å¤„ç†æ¯”è¿™å¤šå¾—å¤šçš„å€™é€‰å›¾åƒã€‚è€ƒè™‘åˆ°è¿™ä¸€ç‚¹ï¼Œæˆ‘ä»¬ç›®å‰çš„ç¨‹åºæœ‰ä¸å°‘ç¼ºç‚¹: 

* å¦‚æœæˆ‘ä»¬æŒ‰åŸæ ·å­˜å‚¨åµŒå…¥ï¼Œå†…å­˜éœ€æ±‚ä¼šè¿…é€Ÿå¢åŠ ï¼Œå°¤å…¶æ˜¯åœ¨å¤„ç†æ•°ç™¾ä¸‡å¼ å€™é€‰å›¾åƒæ—¶ã€‚åœ¨æˆ‘ä»¬çš„ä¾‹å­ä¸­åµŒå…¥æ˜¯ 768 ç»´ï¼Œè¿™å³ä½¿å¯¹å¤§è§„æ¨¡ç³»ç»Ÿè€Œè¨€å¯èƒ½ä¹Ÿæ˜¯ç›¸å¯¹æ¯”è¾ƒé«˜çš„ç»´åº¦ã€‚
* é«˜ç»´çš„åµŒå…¥å¯¹æ£€ç´¢éƒ¨åˆ†æ¶‰åŠçš„åç»­è®¡ç®—æœ‰ç›´æ¥å½±å“ã€‚

å¦‚æœæˆ‘ä»¬èƒ½ä»¥æŸç§æ–¹å¼é™ä½åµŒå…¥çš„ç»´åº¦è€Œä¸å½±å“å®ƒä»¬çš„æ„ä¹‰ï¼Œæˆ‘ä»¬ä»ç„¶å¯ä»¥åœ¨é€Ÿåº¦å’Œæ£€ç´¢è´¨é‡ä¹‹é—´ä¿æŒè‰¯å¥½çš„æŠ˜è¡·ã€‚æœ¬æ–‡ [é™„å¸¦çš„ Colab Notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/image_similarity.ipynb) å®ç°å¹¶æ¼”ç¤ºäº†å¦‚ä½•é€šè¿‡éšæœºæŠ•å½± (random projection) å’Œä½ç½®æ•æ„Ÿå“ˆå¸Œ (locality-sensitive hashingï¼ŒLSH) è¿™ä¸¤ç§æ–¹æ³•æ¥å–å¾—æŠ˜è¡·ã€‚

ğŸ¤— Datasets æä¾›ä¸ [FAISS](https://github.com/facebookresearch/faiss) çš„ç›´æ¥é›†æˆï¼Œè¿›ä¸€æ­¥ç®€åŒ–äº†æ„å»ºç›¸ä¼¼æ€§ç³»ç»Ÿçš„è¿‡ç¨‹ã€‚å‡è®¾ä½ å·²ç»æå–äº†å€™é€‰å›¾åƒçš„åµŒå…¥ (beans æ•°æ®é›†) å¹¶æŠŠä»–ä»¬å­˜å‚¨åœ¨ç§°ä¸º embedding çš„ feature ä¸­ã€‚ä½ ç°åœ¨å¯ä»¥è½»æ¾åœ°ä½¿ç”¨ dataset çš„ [`add_faiss_index()`](https://huggingface.co/docs/datasets/v2.7.1/en/package_reference/main_classes#datasets.Dataset.add_faiss_index) æ–¹æ³•æ¥æ„å»ºç¨ å¯†ç´¢å¼•:

```py
dataset_with_embeddings.add_faiss_index(column="embeddings")
```

å»ºç«‹ç´¢å¼•åï¼Œå¯ä»¥ä½¿ç”¨ `dataset_with_embeddings` æ¨¡å—çš„ [`get_nearest_examples()`](https://huggingface.co/docs/datasets/v2.7.1/en/package_reference/main_classes#datasets.Dataset.get_nearest_examples) æ–¹æ³•ä¸ºç»™å®šæŸ¥è¯¢åµŒå…¥æ£€ç´¢æœ€è¿‘é‚»:

```py
scores, retrieved_examples = dataset_with_embeddings.get_nearest_examples(
    "embeddings", qi_embedding, k=top_k
)
```

è¯¥æ–¹æ³•è¿”å›æ£€ç´¢åˆ†æ•°åŠå…¶å¯¹åº”çš„å›¾åƒã€‚è¦äº†è§£æ›´å¤šä¿¡æ¯ï¼Œä½ å¯ä»¥æŸ¥çœ‹ [å®˜æ–¹æ–‡æ¡£](https://huggingface.co/docs/datasets/faiss_es) å’Œ [è¿™ä¸ª Notebook](https://colab.research.google.com/gist/sayakpaul/5b5b5a9deabd3c5d8cb5ef8c7b4bb536/image_similarity_faiss.ipynb)ã€‚

æœ€åï¼Œä½ å¯ä»¥è¯•è¯•ä¸‹é¢çš„ Hugging Face Spaceï¼Œè¿™æ˜¯ä¸€ä¸ªç®€å•çš„å›¾ç‰‡ç›¸ä¼¼åº¦åº”ç”¨ï¼š

<script type="module" src="https://gradio.s3-us-west-2.amazonaws.com/3.12.0/gradio.js"></script>

<gradio-app theme_mode="light" space="sayakpaul/fetch-similar-images"></gradio-app>

åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å¿«é€Ÿå…¥é—¨å¹¶æ„å»ºäº†ä¸€ä¸ªå›¾åƒç›¸ä¼¼åº¦ç³»ç»Ÿã€‚å¦‚æœä½ è§‰å¾—è¿™ç¯‡æ–‡ç« å¾ˆæœ‰è¶£ï¼Œæˆ‘ä»¬å¼ºçƒˆå»ºè®®ä½ åŸºäºæˆ‘ä»¬è®¨è®ºçš„æ¦‚å¿µç»§ç»­æ„å»ºä½ çš„ç³»ç»Ÿï¼Œè¿™æ ·ä½ å°±å¯ä»¥æ›´åŠ ç†Ÿæ‚‰å†…éƒ¨å·¥ä½œåŸç†ã€‚

è¿˜æƒ³äº†è§£æ›´å¤šå—ï¼Ÿä»¥ä¸‹æ˜¯ä¸€äº›å¯èƒ½å¯¹ä½ æœ‰ç”¨çš„å…¶ä»–èµ„æº: 

* [Faiss: é«˜æ•ˆç›¸ä¼¼æ€§æœç´¢åº“](https://engineering.fb.com/2017/03/29/data-infrastructure/faiss-a-library-for-efficient-similarity-search/)
* [ScaNN: é«˜æ•ˆå‘é‡ç›¸ä¼¼æ€§æœç´¢](http://ai.googleblog.com/2020/07/announcing-scann-efficient-vector.html)
* [åœ¨ç§»åŠ¨åº”ç”¨ç¨‹åºä¸­é›†æˆå›¾åƒæœç´¢å¼•æ“](https://www.tensorflow.org/lite/inference_with_metadata/task_library/image_searcher)

---
title: 'ä¸ºè§†è§‰è¯­è¨€å¤šæ¨¡æ€æ¨¡å‹è¿›è¡Œåå¥½ä¼˜åŒ–'
thumbnail: /blog/assets/dpo_vlm/thumbnail.png
authors:
- user: qgallouedec
- user: vwxyzjn
- user: merve
- user: kashif
translators:
- user: hugging-hoi2022
- user: zhongdongy 
  proofreader: false
---

# ä¸ºè§†è§‰è¯­è¨€å¤šæ¨¡æ€æ¨¡å‹è¿›è¡Œåå¥½ä¼˜åŒ–

è®­ç»ƒæ¨¡å‹ä½¿å¾—å®ƒèƒ½å¤Ÿç†è§£å¹¶é¢„æµ‹äººç±»åå¥½æ˜¯ä¸€é¡¹æ¯”è¾ƒå¤æ‚çš„ä»»åŠ¡ã€‚è¯¸å¦‚ SFT (Supervised finetuning) çš„ä¼ ç»Ÿçš„æ–¹æ³•ä¸€èˆ¬éƒ½éœ€è¦è€—è´¹è¾ƒå¤§æˆæœ¬ï¼Œå› ä¸ºè¿™äº›ç®—æ³•éœ€è¦å¯¹æ•°æ®æ‰“ä¸Šç‰¹å®šçš„æ ‡ç­¾ã€‚è€Œåå¥½ä¼˜åŒ– (Preference Optimization) ä½œä¸ºä¸€ç§æ›¿ä»£é€‰é¡¹ï¼Œé€šå¸¸å¯ä»¥ç®€åŒ–è¿™ä¸€è¿‡ç¨‹ï¼Œå¹¶äº§å‡ºæ›´å‡†ç¡®çš„ç»“æœã€‚é€šè¿‡å¯¹å€™é€‰å›ç­”çš„å¯¹æ¯”å’Œæ’åºï¼Œè€Œä¸æ˜¯èµ‹äºˆå›ºå®šçš„æ ‡ç­¾ï¼Œåå¥½ä¼˜åŒ–ä½¿å¾—æ¨¡å‹èƒ½æ›´é«˜æ•ˆåœ°æ•æ‰äººç±»åå¥½ä¸­çš„ç»†å¾®å·®åˆ«ã€‚

åå¥½ä¼˜åŒ–å·²ç»åœ¨å¤§è¯­è¨€æ¨¡å‹ä¸­å¹¿æ³›ä½¿ç”¨äº†ï¼Œä½†ç°åœ¨ï¼Œå®ƒä¹Ÿå¯ä»¥ç”¨åœ¨è§†è§‰è¯­è¨€æ¨¡å‹ (VLM) ä¸Šã€‚å¾—ç›Šäº **[TRL](https://huggingface.co/docs/trl/index)** çš„å¼€å‘ï¼Œç°åœ¨æˆ‘ä»¬å¯ä»¥ **ä½¿ç”¨ TRL å¯¹ VLM è¿›è¡Œç›´æ¥åå¥½ä¼˜åŒ–** (Direct Preference Optimization)ã€‚æœ¬æ–‡å°†ä¼šä»‹ç»ä½¿ç”¨ TRL å’Œ DPO å¯¹è§†è§‰è¯­è¨€æ¨¡å‹è¿›è¡Œè®­ç»ƒçš„å…¨è¿‡ç¨‹ã€‚

## åå¥½æ•°æ®é›†

è¿›è¡Œåå¥½ä¼˜åŒ–ï¼Œé¦–å…ˆæˆ‘ä»¬éœ€è¦æœ‰ä¸€ä¸ªèƒ½ä½“ç°ç”¨æˆ·åå¥½çš„æ•°æ®é›†ã€‚åœ¨åŒé¡¹é€‰æ‹©çš„è®¾å®šä¸‹ï¼Œç›¸åº”çš„æ•°æ®ä¸€èˆ¬åŒ…å«ä¸€ä¸ªæç¤ºè¯ (Prompt) å’Œä¸¤ä¸ªå€™é€‰å›ç­”ï¼Œä¸¤ä¸ªå›ç­”ä¸­ä¸€ä¸ªè¢«è®°ä¸ºé€‰ä¸­ (chosen)ï¼Œå¦ä¸€ä¸ªè¢«è®°ä¸ºæ·˜æ±° (rejected)ã€‚æ¨¡å‹å°†è¦å»å­¦ä¹ ç€ç»™å‡ºé€‰ä¸­çš„å›ç­”ï¼Œè€Œä¸æ˜¯è¢«æ·˜æ±°çš„é‚£ä¸ªã€‚ä¸‹å›¾å°±æ˜¯ä¸€ä¸ªä¾‹å­:

<figure class="image table text-center m-0 w-full">
  <img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/dpo_vlm/how-many-families.jpg"/>
  <figcaption>å›¾ç‰‡æ¥è‡ª <a href="https://huggingface.co/datasets/openbmb/RLAIF-V-Dataset">openbmb/RLAIF-V-Dataset</a> æ•°æ®é›†</figcaption>
</figure>

**â” Question**: _How many families?_

- **âŒ Rejected:** _The image does not provide any information about families._
- **âœ… Chosen:** _The image shows a Union Organization table setup with 18,000 families._

éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œå°½ç®¡é€‰ä¸­çš„å›ç­”ä¹Ÿä¸æ˜¯å®Œå…¨æ­£ç¡®çš„ (å›ç­” 18000 ä¸ªå®¶åº­è¿˜æ˜¯ä¸å¯¹ï¼Œåº”è¯¥æ˜¯ 18000000)ï¼Œä½†å®ƒä¹Ÿå¥½äºé‚£ä¸ªè¢«æ·˜æ±°çš„å›ç­”ã€‚

æœ¬æ–‡å°†ä½¿ç”¨ [openbmb/RLAIF-V-Dataset](https://huggingface.co/datasets/openbmb/RLAIF-V-Dataset) ä½œä¸ºç¤ºä¾‹æ•°æ®é›†ï¼Œå®ƒåŒ…å«äº†è¶…è¿‡ 83000 æ¡æ ‡æ³¨çš„æ•°æ®ã€‚å¯ä»¥é€šè¿‡ä¸‹é¢ä»£ç æŸ¥çœ‹ä¸€ä¸‹æ•°æ®é›†:

```python
>>> from datasets import load_dataset
>>> dataset = load_dataset("openbmb/RLAIF-V-Dataset", split="train[:1%]")
>>> sample = dataset[1]
>>> sample["image"].show()
>>> sample["question"]
'how many families?'
>>> sample["rejected"]
'The image does not provide any information about families.'
>>> sample["chosen"]
'The image shows a Union Organization table setup with 18,000 families.'
```

æˆ‘ä»¬å°†è¦è®­ç»ƒçš„ VLM æ¨¡å‹éœ€è¦æ–‡æœ¬å’Œå›¾åƒåŒæ—¶ä½œä¸ºè¾“å…¥ï¼Œæ‰€ä»¥è¿™é‡Œçš„ç¬¬ä¸€æ­¥è¿˜æ˜¯è¦å¯¹æ•°æ®é›†æ ¼å¼è¿›è¡Œæ”¹é€ ã€‚ä¸€æ¡æ•°æ®åº”è¯¥è¢«ç»“æ„åŒ–æˆèƒ½æ¨¡æ‹Ÿäººæœºå¯¹è¯çš„å½¢å¼ã€‚ç”¨æˆ·æä¾›ä¸€ä¸ªæç¤ºè¯­ï¼Œå…¶ä¸­åŒ…å«ä¸€å¼ å›¾ç‰‡å’Œä¸€ä¸ªé—®é¢˜ï¼Œç„¶åæ¨¡å‹éœ€è¦èƒ½å¤Ÿç»™å‡ºä¸€ä¸ªå›ç­”ã€‚æˆ‘ä»¬ç”¨ä»¥ä¸‹ä»£ç å®ç°æ ¼å¼è½¬æ¢:

```python
from datasets import features
from transformers import AutoProcessor

processor = AutoProcessor.from_pretrained("HuggingFaceM4/idefics2-8b", do_image_splitting=False)

def format(example):
    # Prepare the input for the chat template
    prompt = [
        {
            "role": "user",
            "content": [{"type": "image"}, {"type": "text", "text": example["question"]}],
        },
    ]
    chosen = [
        {
            "role": "assistant",
            "content": [{"type": "text", "text": example["chosen"]}],
        },
    ]
    rejected = [
        {
            "role": "assistant",
            "content": [{"type": "text", "text": example["rejected"]}],
        },
    ]
    # Apply the chat template
    prompt = processor.apply_chat_template(prompt, tokenize=False)
    chosen = processor.apply_chat_template(chosen, tokenize=False)
    rejected = processor.apply_chat_template(rejected, tokenize=False)
    # Resize the image to ensure it fits within the maximum allowable
    # size of the processor to prevent OOM errors.
    max_size = processor.image_processor.size["longest_edge"]
    example["image"].thumbnail((max_size, max_size))
    return {"images": [example["image"]], "prompt": prompt, "chosen": chosen, "rejected": rejected}

# Apply the formatting function to the dataset,
# remove columns to end up with only "images", "prompt", "chosen", "rejected" columns
dataset = dataset.map(format, remove_columns=dataset.column_names)

# Make sure that the images are decoded, it prevents from storing bytes.
# More info here https://github.com/huggingface/blog/pull/2148#discussion_r1667400478
f = dataset.features
f["images"] = features.Sequence(features.Image(decode=True)) # to avoid bytes
dataset = dataset.cast(f)
```

å®Œæˆäº†æ ¼å¼è½¬æ¢ï¼Œæˆ‘ä»¬æ¥çœ‹çœ‹ç¬¬ä¸€æ¡æ•°æ®:

```python
>>> dataset[1]
{'images': [<PIL.JpegImagePlugin.JpegImageFile image mode=L size=980x812 at 0x154505570>],
 'prompt': 'User:<image>how many families?<end_of_utterance>\n',
 'rejected': 'Assistant: The image does not provide any information about families.<end_of_utterance>\n',
 'chosen': 'Assistant: The image shows a Union Organization table setup with 18,000 families.<end_of_utterance>\n'}
```

OKï¼æ¥ä¸‹æ¥å‡†å¤‡å¥½ GPUï¼Œè®­ç»ƒé©¬ä¸Šå¼€å§‹ã€‚

## è®­ç»ƒ

æˆ‘ä»¬å°†ä½¿ç”¨ [Idefics2-8b](https://huggingface.co/HuggingFaceM4/idefics2-8b) ä½œä¸ºæˆ‘ä»¬çš„ç¤ºä¾‹æ¨¡å‹ï¼Œä½† TRL é‡Œçš„ DPO ä¹Ÿæ˜¯èƒ½ç”¨åœ¨åƒ [Llava 1.5](https://huggingface.co/llava-hf/llava-1.5-7b-hf) å’Œ [PaliGemma](https://huggingface.co/google/paligemma-3b-pt-224) è¿™æ ·çš„æ¨¡å‹ä¸Šçš„ (å¯å‚è€ƒè¿™ç¯‡æ–‡ç« : [Finetuning Llava 1.5, PaliGemma and others](#finetuning-llava-15-paligemma-and-others))ã€‚ä¸è¿‡è®­ç»ƒä¹‹å‰ï¼Œæˆ‘ä»¬å…ˆæ£€æŸ¥ä¸€ä¸‹æˆ‘ä»¬çš„ GPU æ˜¾å­˜æ˜¯å¦å¤Ÿç”¨:

### è®­ç»ƒéœ€è¦å¤šå¤§çš„ GPU æ˜¾å­˜ï¼Ÿ

ä¸€ä¸ª 80GB VRAM çš„ GPU è¶³å¤Ÿç”¨æ¥å¯¹ Idefics2-8b è¿›è¡Œ DPO è®­ç»ƒå—ï¼Ÿæˆ‘ä»¬å¯ä»¥å…ˆè®¡ç®—ä¸€ä¸‹:

æˆ‘ä»¬ç”¨ \\( N \\) è¡¨ç¤ºå‚æ•°çš„æ•°é‡ï¼Œç”¨ \\( P \\) è¡¨ç¤ºè®­ç»ƒä½¿ç”¨çš„ç²¾åº¦ã€‚è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œä¸‹åˆ—éƒ¨åˆ†éœ€è¦å…±åŒæ”¾å…¥æ˜¾å­˜ä¸­:

- **è¦è®­ç»ƒçš„æ¨¡å‹**: \\( N \times P \\)
- **ç”¨ä»¥é˜²æ­¢æ¨¡å‹äº§ç”Ÿåç¦»çš„å‚è€ƒæ¨¡å‹**: å’Œè¦è®­ç»ƒçš„æ¨¡å‹ä¸€æ ·å¤§ï¼Œæ‰€ä»¥ä¹Ÿæ˜¯ \\( N \times P \\)
- **æ¢¯åº¦**: æˆ‘ä»¬å¯¹æ‰€æœ‰å‚æ•°éƒ½è¿›è¡Œè®­ç»ƒï¼Œæ‰€ä»¥æ¯ä¸ªå‚æ•°éƒ½æœ‰æ¢¯åº¦: \\( N \times P \\)
- **ä¼˜åŒ–å™¨çš„çŠ¶æ€é‡**: æˆ‘ä»¬ä½¿ç”¨ [AdamW](https://pytorch.org/docs/stable/generated/torch.optim.AdamW.html)ï¼Œä¸€ä¸ªå‚æ•°ä¼šä¿å­˜ä¸¤ä¸ªçŠ¶æ€é‡ï¼Œæ‰€ä»¥éœ€è¦: \\( 2 \times N \times P \\)

Idefics2-8b æœ‰ 80 äº¿ (8B) å‚æ•°ï¼Œæˆ‘ä»¬ä½¿ç”¨ `float32` ç²¾åº¦ï¼Œæ¯ä¸ªå‚æ•°å  4 ä¸ªå­—èŠ‚ã€‚æ‰€ä»¥æ€»çš„æ˜¾å­˜éœ€æ±‚æ˜¯:

| å‚æ•°æ¥æº        | è®¡ç®—å…¬å¼                           | æ˜¾å­˜éœ€æ±‚     |
| ---------------- | ------------------------------------- | ---------- |
| è¦è®­ç»ƒçš„æ¨¡å‹   | \\( 8 \times 10^9 \times 4 \\)          | 32 GB      |
| å‚è€ƒæ¨¡å‹  | \\( 8 \times 10^9 \times 4 \\)          | 32 GB      |
| æ¢¯åº¦        | \\( 8 \times 10^9 \times 4 \\)          | 32 GB      |
| ä¼˜åŒ–å™¨çŠ¶æ€é‡ | \\( 2 \times 8 \times 10^9 \times 4 \\) | 64 GB      |
| **åˆè®¡**        |                                       | **160 GB** |

è¿™è¿œè¶…æˆ‘ä»¬å‰é¢è¯´çš„ 80GB æ˜¾å­˜äº†ï¼å¹¸è¿çš„æ˜¯ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨é‡åŒ–ã€LoRA ç­‰æŠ€æœ¯æ¥å¤§å¹…åº¦åœ°å‡å°‘æ˜¾å­˜éœ€æ±‚ï¼Œè®©è®­ç»ƒå¯ä»¥è¿›è¡Œã€‚æ¥ä¸‹æ¥æˆ‘ä»¬å°†ä»‹ç»è¿™äº›æŠ€æœ¯ã€‚

### é‡åŒ–

é‡åŒ–ä¼šé™ä½æ¨¡å‹æƒé‡å’Œæ¿€æ´»å€¼çš„ç²¾åº¦ï¼Œä½†ä¹ŸåŒæ—¶æ˜¾è‘—å‡å°‘å†…å­˜éœ€æ±‚ã€‚å°†ç²¾åº¦ä» `float32` æ”¹ä¸º `bfloat16` ï¼Œä¼šè®©æ¯ä¸ªå‚æ•°éœ€è¦çš„æ¯”ç‰¹æ•°ä» 4 æ¯”ç‰¹å‡å°‘åˆ° 2 æ¯”ç‰¹ã€‚è¿™ä¸€ç­–ç•¥ä¸ä»…èƒ½å‡å°‘å†…å­˜ä½¿ç”¨ï¼Œè¿˜ä¼šæ˜¾è‘—åŠ é€Ÿè®­ç»ƒï¼Œç¡®ä¿ä»¥æœ€å°ä»£ä»·ä¿è¯è¶³å¤Ÿé«˜çš„æ€§èƒ½ã€‚å…·ä½“åšæ³•å¦‚ä¸‹:

```python
import torch
from transformers import AutoModelForVision2Seq

model = AutoModelForVision2Seq.from_pretrained("HuggingFaceM4/idefics2-8b", torch_dtype=torch.bfloat16)
```

é€šè¿‡å¦‚ä¸‹ `bf16=True` çš„è®¾ç½®ï¼Œ `bfloat16` ä¹Ÿå¯ä»¥è¢«ç”¨åœ¨ä¼˜åŒ–å™¨ä¸Š:

```python
from transformers import TrainingArguments

training_args = TrainingArguments(..., bf16=True)
```

### LoRA

[LoRA](https://arxiv.org/abs/2106.09685) å¯¹å‚æ•°çŸ©é˜µè¿›è¡Œä½ç§©åˆ†è§£; åœ¨è®­ç»ƒæ—¶ï¼Œå›ºå®šä½åŸå‚æ•°çŸ©é˜µï¼Œä»…è®­ç»ƒåˆ†è§£å‡ºçš„ä¸¤ä¸ªçŸ©é˜µã€‚æ˜¯ä¸€ç§å¤§è§„æ¨¡å‡å°‘ LLM è®­ç»ƒå‚æ•°çš„æ–¹æ³•ã€‚LoRA å·²è¢«é›†æˆåœ¨äº† [PEFT](https://github.com/huggingface/peft) åº“é‡Œï¼Œä½¿ç”¨éå¸¸æ–¹ä¾¿:

```diff
  from transformers import AutoModelForVision2Seq
+ from peft import get_peft_model, LoraConfig

  model = AutoModelForVision2Seq.from_pretrained("HuggingFaceM4/idefics2-8b")
+ peft_config = LoraConfig(target_modules="all-linear")
+ model = get_peft_model(model, peft_config)
```

PEFT åƒæ˜¯ç»™åŸæ¨¡å‹è¿›è¡Œäº†ä¸€æ¬¡å°è£… (ä»£ç ä¸­ç§°ä¸º _adapter_ )ã€‚è®­ç»ƒæ—¶ï¼Œå®é™…ä¸Šæ˜¯è¿™ä¸ª adapter åœ¨è¢«è®­ç»ƒï¼Œè€ŒåŸæœ‰çš„æ¨¡å‹ä¿æŒä¸åŠ¨ã€‚æˆ‘ä»¬ç°åœ¨ç®—ç®— LoRA å¸®æˆ‘ä»¬å‡å°‘äº†å¤šå°‘è¦è®­ç»ƒçš„å‚æ•°:

```python
>>> model.print_trainable_parameters()
trainable params: 55,348,736 || all params: 8,458,116,848 || trainable%: 0.6543860411799315
```

å®ƒå¸®æˆ‘ä»¬æŠŠè¦è®­ç»ƒçš„å‚æ•°ä»å…«åäº¿é™åˆ°äº†äº”åƒäº”ç™¾ä¸‡ï¼å·®è·çœŸå¤§ï¼è¿™å°†æ˜¾è‘—å‡å°‘æ˜¾å­˜éœ€æ±‚ã€‚

### ä½¿ç”¨ bfloat16 å’Œ LoRA åçš„æ˜¾å­˜éœ€æ±‚

ç°åœ¨æˆ‘ä»¬æ¥ç®—ç®—æ–°çš„æ˜¾å­˜éœ€æ±‚:

| å‚æ•°æ¥æº        | è®¡ç®—å…¬å¼                           | æ˜¾å­˜éœ€æ±‚     |
| ---------------- | ------------------------------------- | ----------- |
| è¦è®­ç»ƒçš„æ¨¡å‹   | \\( 8 \mathrm{G} \times 2 \\)           | 16  GB      |
| å‚è€ƒæ¨¡å‹  | \\( 8 \mathrm{G} \times 2 \\)           | 16  GB      |
| æ¢¯åº¦        | \\( 55 \mathrm{M} \times 2 \\)          | 0.1 GB      |
| ä¼˜åŒ–å™¨çŠ¶æ€é‡ | \\( 2 \times 55 \mathrm{M} \times 2 \\) | 0.2 GB      |
| **åˆè®¡**        |                                       | **32.3 GB** |

ç°åœ¨æˆ‘ä»¬ä»…éœ€ 32GB çš„æ˜¾å­˜å°±å¯ä»¥è®­ç»ƒæˆ‘ä»¬çš„ Idefics2-8b æ¨¡å‹äº†ã€‚è¿™åˆç†å¤šäº†ï¼Œç”¨ 80GB æ˜¾å­˜çš„ GPU å°±å¯ä»¥è®­ç»ƒäº†ã€‚

[PEFT æ–‡æ¡£](https://huggingface.co/docs/peft/en/index) å’Œ [è°·æ­Œè¿™ç¯‡å…³äº LoRA å’Œ QLoRA æ–‡ç« ](https://cloud.google.com/vertex-ai/generative-ai/docs/model-garden/lora-qlora) ä¹Ÿæä¾›äº†å¾ˆå¤šå…³äºæ˜¾å­˜ä¼˜åŒ–çš„å¸®åŠ©æŒ‡å—ï¼Œè¯»è€…æ„Ÿå…´è¶£å¯ä»¥é˜…è¯»ã€‚

### è®­ç»ƒæ—¶ batch size æ€ä¹ˆè®¾å®šï¼Ÿ

ä¸Šè¿°å…³äºæ˜¾å­˜å ç”¨çš„è®¡ç®—è¿˜ä¸ç®—å‡†ç¡®ï¼Œå› ä¸ºå®é™…è®­ç»ƒæ—¶ï¼Œæ¿€æ´»å€¼ä¹Ÿéœ€è¦å ç”¨æ˜¾å­˜ã€‚æ¿€æ´»å€¼æ˜¯ç¥ç»ç½‘ç»œå„å±‚çš„è¾“å‡ºã€‚ä½œä¸ºä¸­é—´äº§ç‰©ï¼Œå®ƒä»¬çš„æ˜¾å­˜å ç”¨é‡å–å†³äºæ¨¡å‹ç»“æ„å’Œè®­ç»ƒæ—¶çš„ batch sizeã€‚å‡†ç¡®è®¡ç®—è¿™äº›æ˜¾å­˜éœ€æ±‚è¿˜æ˜¯å¾ˆå›°éš¾çš„ï¼Œæˆ‘ä»¬ä¸€èˆ¬ä¾èµ–å®éªŒè§‚å¯Ÿã€‚

è‹¥æƒ³æ‰¾åˆ°ä¸€ä¸ªåˆé€‚çš„ batch size ( `per_device_train_batch_size` )ï¼Œä½ å¯ä»¥å…ˆéšä¾¿é€‰å–ä¸€ä¸ªä½ è®¤ä¸ºåˆé€‚çš„æ•°å€¼ (æ¯”å¦‚ 64) ç„¶åè¯•ç€å¼€å§‹è®­ç»ƒã€‚å½“ç„¶è¿™å¤§å¤šæ•°æƒ…å†µä¸‹ä¼šçˆ†æ˜¾å­˜ (OOM)ã€‚å¦‚æœè¿™æ ·ï¼Œä½ å¯ä»¥å‡åŠ batch sizeï¼ŒåŒæ—¶å°† `gradient_accumulation_steps` ç¿»å€ï¼Œä»¥è·å¾—å’ŒåŸå…ˆ batch size è®¾å®šç›¸åŒçš„æ•ˆæœã€‚åå¤é‡å¤è¿™ä¸€è¿‡ç¨‹ï¼Œæœ€ç»ˆå½“ OOM ä¸å†å‡ºç°æ—¶ï¼Œä½ å°±å¯ä»¥è®­ç»ƒäº†ã€‚æˆ‘ä»¬çš„å®éªŒå‚æ•°æ˜¯: `per_device_train_batch_size` è®¾ä¸º 2ï¼Œ `gradient_accumulation_steps` è®¾ä¸º 32ã€‚

ä½ è¿˜å¯ä»¥ä½¿ç”¨ `gradient_checkpointing` æ¥å‡å°‘æ¿€æ´»å€¼æ‰€éœ€çš„å†…å­˜ã€‚è¿™ä¸€æŠ€æœ¯åœ¨è®¡ç®—æ¢¯åº¦æ—¶ï¼Œä¼šé‡æ–°è®¡ç®—ä¸€éå‰å‘è¿‡ç¨‹ï¼Œè€Œä¸æ˜¯åœ¨å‰å‘è¿‡ç¨‹ä¸­ä¿å­˜ç”¨äºè®¡ç®—æ¢¯åº¦çš„ä¸­é—´ç»“æœã€‚éœ€è¦ä½¿ç”¨æ—¶ï¼Œè®¾ç½® `gradient_checkpointing=True` å³å¯ã€‚

### å®Œæ•´è®­ç»ƒä»£ç 

ä¸€åˆ‡å°±ç»ªï¼Œæˆ‘ä»¬å¯ä»¥å¼€å§‹è®­ç»ƒäº†ã€‚ä¸‹é¢æ˜¯æˆ‘ä»¬çš„å®Œæ•´è®­ç»ƒä»£ç ã€‚é™¤äº†ä¸Šé¢æåˆ°çš„éƒ¨åˆ†å¤–ï¼Œæˆ‘ä»¬è¿˜è®¾ç½®äº† `dataset_num_proc` å’Œ `dataloader_num_workers` ç­‰å‚æ•°ï¼Œç”¨äºåŠ é€Ÿæ•°æ®é¢„å¤„ç†ã€‚

```python
# dpo_idefics2-8b.py
from datasets import features, load_dataset
from transformers import AutoModelForVision2Seq, AutoProcessor
import torch
from trl import DPOConfig, DPOTrainer
from peft import LoraConfig

def main():
    # Load the model and processor
    model = AutoModelForVision2Seq.from_pretrained("HuggingFaceM4/idefics2-8b", torch_dtype=torch.bfloat16)
    processor = AutoProcessor.from_pretrained("HuggingFaceM4/idefics2-8b", do_image_splitting=False)

    # Load the dataset
    dataset = load_dataset("openbmb/RLAIF-V-Dataset", split="train")

    def format(example):
        # Prepare the input for the chat template
        prompt = [{"role": "user", "content": [{"type": "image"}, {"type": "text", "text": example["question"]}]}]
        chosen = [{"role": "assistant", "content": [{"type": "text", "text": example["chosen"]}]}]
        rejected = [{"role": "assistant", "content": [{"type": "text", "text": example["rejected"]}]}]
        # Apply the chat template
        prompt = processor.apply_chat_template(prompt, tokenize=False)
        chosen = processor.apply_chat_template(chosen, tokenize=False)
        rejected = processor.apply_chat_template(rejected, tokenize=False)
        # Resize the image to ensure it fits within the maximum allowable
        # size of the processor to prevent OOM errors.
        max_size = processor.image_processor.size["longest_edge"]// 2
        example["image"].thumbnail((max_size, max_size))
        return {"images": [example["image"]], "prompt": prompt, "chosen": chosen, "rejected": rejected}

    # Apply the formatting function to the dataset
    dataset = dataset.map(format, remove_columns=dataset.column_names, num_proc=32)

    # Make sure that the images are decoded, it prevents from storing bytes.
    # More info here https://github.com/huggingface/blog/pull/2148#discussion_r1667400478
    f = dataset.features
    f["images"] = features.Sequence(features.Image(decode=True))
    dataset = dataset.cast(f)

    # Train the model
    training_args = DPOConfig(
        output_dir="idefics2-8b-dpo",
        bf16=True,
        gradient_checkpointing=True,
        per_device_train_batch_size=2,
        gradient_accumulation_steps=32,
        num_train_epochs=1,
        dataset_num_proc=32, # tokenization will use 32 processes
        dataloader_num_workers=32, # data loading will use 32 workers
        logging_steps=10,
    )
    trainer = DPOTrainer(
        model,
        ref_model=None, # not needed when using peft
        args=training_args,
        train_dataset=dataset,
        tokenizer=processor,
        peft_config=LoraConfig(target_modules="all-linear"),
    )

    trainer.train()

if __name__ == "__main__":
    main()
```

å¯åŠ¨è„šæœ¬å¼€å§‹è®­ç»ƒï¼Œæ¥ä¸‹æ¥å°±ç­‰å¾…ç»“æœå§ ğŸš€

```sh
accelerate launch dpo_idefics2-8b.py
```

## ç»“æœ

è®­ç»ƒéœ€è¦å‡ å°æ—¶çš„æ—¶é—´ã€‚å½“è®­ç»ƒå®Œæˆåï¼Œæˆ‘ä»¬å¯ä»¥çœ‹çœ‹è®­ç»ƒç›¸å…³æŒ‡æ ‡çš„å˜åŒ–æ›²çº¿:

![Learning curves](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/dpo_vlm/learning_curves.png)

In DPO, we focus on several metrics to assess the quality of the training:

åœ¨ DPO ä¸­ï¼Œä¸ºäº†è¯„ä¼°è®­ç»ƒï¼Œæˆ‘ä»¬å…³æ³¨è¿™å‡ ä¸ªæŒ‡æ ‡:

- **ç²¾åº¦ (Accuracy)**: åœ¨è®­ç»ƒæ ·æœ¬ä¸­ï¼Œæ¨¡å‹æ›´æ„¿æ„è¾“å‡ºè¢«é€‰ä¸­çš„å›ç­”è€Œä¸æ˜¯è¢«æ·˜æ±°çš„å›ç­”ï¼Œè¿™ä¸ªæ¯”ç‡æœ‰å¤šå°‘ã€‚æˆ‘ä»¬å¯ä»¥çœ‹åˆ°éšç€è®­ç»ƒï¼Œç²¾åº¦åœ¨æå‡ï¼Œè¿™æ˜¯ä¸ªå¥½çš„ä¿¡å·ã€‚
- **å¥–åŠ± (Rewards)**: è¿™ä¸€æŒ‡æ ‡ä¸ä¸€ä¸ªå›ç­” (é€‰ä¸­æˆ–æ·˜æ±°) è¢«é€‰ä¸­çš„æ¦‚ç‡å‘ˆæ­£ç›¸å…³ï¼Œè¯»è€…å¯ä»¥å‚è€ƒ [DPO è®ºæ–‡ , ç¬¬ 5 éƒ¨åˆ†](https://arxiv.org/abs/2305.18290)ã€‚æˆ‘ä»¬å¸Œæœ›è¢«é€‰ä¸­çš„å›ç­”å¯¹åº”çš„å¥–åŠ±é«˜äºè¢«æ·˜æ±°çš„å›ç­”ã€‚æˆ‘ä»¬å¯ä»¥é€šè¿‡ä¸¤è€…å¥–åŠ±çš„å·®å€¼ ( _reward margin_ ) æ¥çœ‹: å›¾ä¸­è¿™ä¸€å·®å€¼é€æ¸å˜å¤§ï¼Œ è¿™ä¹Ÿæ˜¯ä¸ªå¥½çš„ä¿¡å·ã€‚

## è¯„æµ‹

### æ¨ç†ä»£ç 

è®­ç»ƒå®Œæˆåï¼Œæˆ‘ä»¬æ¥ä¸‹æ¥å°±è¦åœ¨ä¸€äº›æ ·æœ¬ä¸Šè¯„æµ‹ä¸€ä¸‹äº†ã€‚è¿™ä¼šè®©æˆ‘ä»¬äº†è§£æ¨¡å‹å­¦ä¹ å¾—æ€ä¹ˆæ ·ã€é¢„æµ‹æœ‰æ•ˆæ€§å¦‚ä½•ã€‚ä¸‹é¢çš„ä»£ç å¯ä»¥ç”¨æ¥åœ¨æµ‹è¯•æ ·æœ¬ä¸Šè¿›è¡Œè¯„æµ‹:

```python
from transformers import AutoModelForVision2Seq, AutoProcessor
from PIL import Image

model = AutoModelForVision2Seq.from_pretrained("HuggingFaceM4/idefics2-8b").to("cuda")
processor = AutoProcessor.from_pretrained("HuggingFaceM4/idefics2-8b", do_image_splitting=False)
model.load_adapter("HuggingFaceH4/idefics2-8b-dpo-rlaif-v-v0.3") # <-- Load the adapter we've just trained

# Process
user_message = ...
image_path = ...
data = [{"role": "user", "content": [{"type": "image"}, {"type": "text", "text": user_message}]}]
prompts = processor.apply_chat_template(data, add_generation_prompt=True) # add_generation_prompt=True to end the prompt with "ASSISTANT:"
images = [Image.open(image_path)]
inputs = processor(prompts, images, return_tensors="pt")
inputs = {k: v.to("cuda") for k, v in inputs.items()}

# Generate
generated_ids = model.generate(**inputs, max_new_tokens=500)
response_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]
print(response_text)
```

å‰é¢æåˆ°çš„ [openbmb/RLAIF-V-Dataset](https://huggingface.co/datasets/openbmb/RLAIF-V-Dataset) è¿™ä¸ªæ•°æ®é›†æ˜¯ç”¨æ¥å‡å°‘å¤§æ¨¡å‹å¹»è§‰çš„ã€‚ä½†çœŸå®è®­ç»ƒæ•ˆæœå¦‚ä½•å‘¢ï¼Ÿæˆ‘ä»¬å¯ä»¥ä½¿ç”¨ [AMBER benchmark](https://arxiv.org/abs/2311.07397) è¿™ä¸ªè¯„æµ‹åŸºå‡†ï¼Œè¯¥æ•°æ®é›†ä¸“é—¨è¢«ç”¨æ¥è¯„ä¼° VLM çš„å¹»è§‰æƒ…å†µã€‚æˆ‘ä»¬åˆ—å‡º Idefics2 å’Œ Idefics2+DPO çš„ç»“æœï¼Œå¹¶å’Œå…¶å®ƒæ¨¡å‹å¯¹æ¯”ã€‚

|                  | Accuracy | F1       |
| ---------------- | -------- | -------- |
| GPT-4o           | 88.8     | 91.6     |
| **Idefics2+DPO** | **85.9** | **89.4** |
| Idefics2         | 85.8     | 89.1     |
| GPT-4v           | 83.4     | 87.4     |
| MiniGemini       | 82.6     | 87.6     |
| LLaVA-NeXT       | 81.4     | 85.4     |
| QWEN-VL          | 81.9     | 86.4     |
| LURE             | 73.5     | 77.7     |
| OPERA            | 75.2     | 78.3     |
| Less-is-more     | 72.4     | 75.8     |
| VCD              | 71.8     | 74.9     |

æ€»çš„æ¥çœ‹ï¼Œæœ‰ç‚¹ä½œç”¨ï¼å¹»è§‰ä¼¼ä¹å‡å°‘äº†ç‚¹ã€‚è®­ç»ƒçœ‹æ¥è¿˜æ˜¯æˆåŠŸçš„ã€‚

ä¸‹é¢æˆ‘ä»¬ä¹Ÿåˆ—å‡ºä¸€äº›å¯è§†åŒ–ç»“æœå‡ºæ¥:

| Image                                                                                                                  | Question                            | Idefics2 | Idefics2+DPO |
| ---------------------------------------------------------------------------------------------------------------------- | ----------------------------------- | -------- | ------------ |
| ![AMBER_2](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/dpo_vlm/AMBER_2.jpg)     | Are there two ships in this image?  | Yes      | No           |
| ![AMBER_111](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/dpo_vlm/AMBER_111.jpg) | Is the ground uneven in this image? | No       | Yes          |
| ![AMBER_7](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/dpo_vlm/AMBER_7.jpg)     | Is there one shovel in this image?  | Yes      | No           |

ä½ ä¹Ÿå¯ä»¥è‡ªå·±æ‰¾äº›ä¾‹å­æ¥æµ‹è¯•ä¸€ä¸‹è¿™ä¸ªæ¨¡å‹ï¼

<script type="module" src="https://gradio.s3-us-west-2.amazonaws.com/4.36.1/gradio.js"></script>
<gradio-app theme_mode="light" space="HuggingFaceH4/compare_idefics-8b-dpo"></gradio-app>

## å¾®è°ƒ Llava 1.5 å’Œ PaliGemma ç­‰æ¨¡å‹

æˆªè‡³æœ¬æ–‡å®Œç¨¿æ—¶ï¼ŒTRL çš„ DPO å®ç°å·²æ”¯æŒ Idefics2ã€Llava 1.5 å’Œ PaliGemmaï¼ŒåŒæ—¶ TRL ä¹Ÿåœ¨åŠªåŠ›æ”¯æŒæ›´å¤šçš„æ¨¡å‹ã€‚æœ€ç®€å•çš„è°ƒç”¨æ–¹æ³•è¿˜æ˜¯ä½¿ç”¨ TRL æä¾›çš„ [ç¤ºä¾‹è„šæœ¬](https://github.com/huggingface/trl/blob/main/examples/scripts/dpo_visual.py)ã€‚ä¾‹å¦‚ï¼Œå¦‚æœä½ æƒ³å¾®è°ƒ PaliGemmaï¼Œä½ å¯ä»¥è¿™æ ·:

```sh
accelerate launch examples/scripts/dpo_visual.py \
    --dataset_name HuggingFaceH4/rlaif-v_formatted \
    --model_name_or_path google/paligemma-3b-pt-224 \
    --per_device_train_batch_size 2 \
    --gradient_accumulation_steps 32 \
    --dataset_num_proc 32 \
    --output_dir dpo_paligemma_rlaif-v \
    --bf16 \
    --torch_dtype bfloat16 \
    --gradient_checkpointing \
    --use_peft \
    --lora_target_modules=all-linear
```

æ›´å¤šå…³äº PaliGemma å¾®è°ƒçš„ä¿¡æ¯å¯ä»¥åœ¨ [smol-vision](https://github.com/merveenoyan/smol-vision) è¿™ä¸ªé¡¹ç›®é‡Œçœ‹åˆ°ã€‚

ğŸš€ğŸš€ å¥½äº†ï¼ä½ ç°åœ¨å·²ç»ä¼šä½¿ç”¨ DPO å¾®è°ƒ VLM æ¨¡å‹äº†ï¼æˆ‘ä»¬æœŸå¾…ä½ åœ¨ç¤¾åŒºåˆ†äº«ä½ çš„æ¨¡å‹ã€æ•°æ®å’Œç‹¬ç‰¹è§è§£ï¼
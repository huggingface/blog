---
title: "ä» PyTorch DDP åˆ° Accelerate åˆ° Trainerï¼Œè½»æ¾æŒæ¡åˆ†å¸ƒå¼è®­ç»ƒ"
thumbnail: /blog/assets/111_pytorch_ddp_accelerate_transformers/thumbnail.png
authors:
- user: muellerzr
translators:
- user: innovation64
- user: zhongdongy
  proofreader: true
---

# ä» PyTorch DDP åˆ° Accelerate åˆ° Trainerï¼Œè½»æ¾æŒæ¡åˆ†å¸ƒå¼è®­ç»ƒ


## æ¦‚è¿°

æœ¬æ•™ç¨‹å‡å®šä½ å·²ç»å¯¹äº PyToch è®­ç»ƒä¸€ä¸ªç®€å•æ¨¡å‹æœ‰ä¸€å®šçš„åŸºç¡€ç†è§£ã€‚æœ¬æ•™ç¨‹å°†å±•ç¤ºä½¿ç”¨ 3 ç§å°è£…å±‚çº§ä¸åŒçš„æ–¹æ³•è°ƒç”¨ DDP (DistributedDataParallel) è¿›ç¨‹ï¼Œåœ¨å¤šä¸ª GPU ä¸Šè®­ç»ƒåŒä¸€ä¸ªæ¨¡å‹ï¼š

- ä½¿ç”¨ `pytorch.distributed` æ¨¡å—çš„åŸç”Ÿ PyTorch DDP æ¨¡å—
- ä½¿ç”¨ ğŸ¤— Accelerate å¯¹ `pytorch.distributed` çš„è½»é‡å°è£…ï¼Œç¡®ä¿ç¨‹åºå¯ä»¥åœ¨ä¸ä¿®æ”¹ä»£ç æˆ–è€…å°‘é‡ä¿®æ”¹ä»£ç çš„æƒ…å†µä¸‹åœ¨å•ä¸ª GPU æˆ– TPU ä¸‹æ­£å¸¸è¿è¡Œ
- ä½¿ç”¨ ğŸ¤— Transformer çš„é«˜çº§ Trainer API ï¼Œè¯¥ API æŠ½è±¡å°è£…äº†æ‰€æœ‰ä»£ç æ¨¡æ¿å¹¶ä¸”æ”¯æŒä¸åŒè®¾å¤‡å’Œåˆ†å¸ƒå¼åœºæ™¯ã€‚

## ä»€ä¹ˆæ˜¯åˆ†å¸ƒå¼è®­ç»ƒï¼Œä¸ºä»€ä¹ˆå®ƒå¾ˆé‡è¦ï¼Ÿ

ä¸‹é¢æ˜¯ä¸€äº›éå¸¸åŸºç¡€çš„ PyTorch è®­ç»ƒä»£ç ï¼Œå®ƒåŸºäº Pytorch å®˜æ–¹åœ¨ MNIST ä¸Šåˆ›å»ºå’Œè®­ç»ƒæ¨¡å‹çš„ [ç¤ºä¾‹](https://github.com/pytorch/examples/blob/main/mnist/main.py)ã€‚


```python
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torchvision import datasets, transforms

class BasicNet(nn.Module):
    def __init__(self):
        super().__init__()
        self.conv1 = nn.Conv2d(1, 32, 3, 1)
        self.conv2 = nn.Conv2d(32, 64, 3, 1)
        self.dropout1 = nn.Dropout(0.25)
        self.dropout2 = nn.Dropout(0.5)
        self.fc1 = nn.Linear(9216, 128)
        self.fc2 = nn.Linear(128, 10)
        self.act = F.relu

    def forward(self, x):
        x = self.act(self.conv1(x))
        x = self.act(self.conv2(x))
        x = F.max_pool2d(x, 2)
        x = self.dropout1(x)
        x = torch.flatten(x, 1)
        x = self.act(self.fc1(x))
        x = self.dropout2(x)
        x = self.fc2(x)
        output = F.log_softmax(x, dim=1)
        return output
```

æˆ‘ä»¬å®šä¹‰è®­ç»ƒè®¾å¤‡ (`cuda`):

```python
device = "cuda"
```

æ„å»ºä¸€äº›åŸºæœ¬çš„ PyTorch DataLoaders:

```python
transform = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize((0.1307), (0.3081))
])

train_dset = datasets.MNIST('data', train=True, download=True, transform=transform)
test_dset = datasets.MNIST('data', train=False, transform=transform)

train_loader = torch.utils.data.DataLoader(train_dset, shuffle=True, batch_size=64)
test_loader = torch.utils.data.DataLoader(test_dset, shuffle=False, batch_size=64)
```

æŠŠæ¨¡å‹æ”¾å…¥ CUDA è®¾å¤‡:

```python
model = BasicNet().to(device)
```

æ„å»º PyTorch optimizer (ä¼˜åŒ–å™¨):

```python
optimizer = optim.AdamW(model.parameters(), lr=1e-3)
```

æœ€ç»ˆåˆ›å»ºä¸€ä¸ªç®€å•çš„è®­ç»ƒå’Œè¯„ä¼°å¾ªç¯ï¼Œè®­ç»ƒå¾ªç¯ä¼šä½¿ç”¨å…¨éƒ¨è®­ç»ƒæ•°æ®é›†è¿›è¡Œè®­ç»ƒï¼Œè¯„ä¼°å¾ªç¯ä¼šè®¡ç®—è®­ç»ƒåæ¨¡å‹åœ¨æµ‹è¯•æ•°æ®é›†ä¸Šçš„å‡†ç¡®åº¦ï¼š

```python
model.train()
for batch_idx, (data, target) in enumerate(train_loader):
    data, target = data.to(device), target.to(device)
    output = model(data)
    loss = F.nll_loss(output, target)
    loss.backward()
    optimizer.step()
    optimizer.zero_grad()

model.eval()
correct = 0
with torch.no_grad():
    for data, target in test_loader:
        output = model(data)
        pred = output.argmax(dim=1, keepdim=True)
        correct += pred.eq(target.view_as(pred)).sum().item()
print(f'Accuracy: {100. * correct / len(test_loader.dataset)}')
```

é€šå¸¸ä»è¿™é‡Œå¼€å§‹ï¼Œå°±å¯ä»¥å°†æ‰€æœ‰çš„ä»£ç æ”¾å…¥ Python è„šæœ¬æˆ–åœ¨ Jupyter Notebook ä¸Šè¿è¡Œå®ƒã€‚

ç„¶è€Œï¼Œåªæ‰§è¡Œ `python myscript.py` åªä¼šä½¿ç”¨å•ä¸ª GPU è¿è¡Œè„šæœ¬ã€‚å¦‚æœæœ‰å¤šä¸ª GPU èµ„æºå¯ç”¨ï¼Œæ‚¨å°†å¦‚ä½•è®©è¿™ä¸ªè„šæœ¬åœ¨ä¸¤ä¸ª GPU æˆ–å¤šå°æœºå™¨ä¸Šè¿è¡Œï¼Œé€šè¿‡ *åˆ†å¸ƒå¼* è®­ç»ƒæé«˜è®­ç»ƒé€Ÿåº¦ï¼Ÿè¿™æ˜¯ `torch.distributed` å‘æŒ¥ä½œç”¨çš„åœ°æ–¹ã€‚

## PyTorch åˆ†å¸ƒå¼æ•°æ®å¹¶è¡Œ

é¡¾åæ€ä¹‰ï¼Œ`torch.distributed` æ—¨åœ¨é…ç½®åˆ†å¸ƒå¼è®­ç»ƒã€‚ä½ å¯ä»¥ä½¿ç”¨å®ƒé…ç½®å¤šä¸ªèŠ‚ç‚¹è¿›è¡Œè®­ç»ƒï¼Œä¾‹å¦‚ï¼šå¤šæœºå™¨ä¸‹çš„å•ä¸ª GPUï¼Œæˆ–è€…å•å°æœºå™¨ä¸‹çš„å¤šä¸ª GPUï¼Œæˆ–è€…ä¸¤è€…çš„ä»»æ„ç»„åˆã€‚

ä¸ºäº†å°†ä¸Šè¿°ä»£ç è½¬æ¢ä¸ºåˆ†å¸ƒå¼è®­ç»ƒï¼Œå¿…é¡»é¦–å…ˆå®šä¹‰ä¸€äº›è®¾ç½®é…ç½®ï¼Œå…·ä½“ç»†èŠ‚è¯·å‚é˜… [DDP ä½¿ç”¨æ•™ç¨‹](https://pytorch.org/tutorials/intermediate/ddp_tutorial.html)ã€‚

é¦–å…ˆå¿…é¡»å£°æ˜ `setup` å’Œ `cleanup` å‡½æ•°ã€‚è¿™å°†åˆ›å»ºä¸€ä¸ªè¿›ç¨‹ç»„ï¼Œå¹¶ä¸”æ‰€æœ‰è®¡ç®—è¿›ç¨‹éƒ½å¯ä»¥é€šè¿‡è¿™ä¸ªè¿›ç¨‹ç»„é€šä¿¡ã€‚

> æ³¨æ„ï¼šåœ¨æœ¬æ•™ç¨‹çš„è¿™ä¸€éƒ¨åˆ†ä¸­ï¼Œå‡å®šè¿™äº›ä»£ç æ˜¯åœ¨ Python è„šæœ¬æ–‡ä»¶ä¸­å¯åŠ¨ã€‚ç¨åå°†è®¨è®ºä½¿ç”¨ ğŸ¤— Accelerate çš„å¯åŠ¨å™¨ï¼Œå°±ä¸å¿…å£°æ˜ `setup` å’Œ `cleanup` å‡½æ•°äº†ã€‚

```python
import os
import torch.distributed as dist

def setup(rank, world_size):
    "Sets up the process group and configuration for PyTorch Distributed Data Parallelism"
    os.environ["MASTER_ADDR"] = 'localhost'
    os.environ["MASTER_PORT"] = "12355"

    # Initialize the process group
    dist.init_process_group("gloo", rank=rank, world_size=world_size)

def cleanup():
    "Cleans up the distributed environment"
    dist.destroy_process_group()
```

æœ€åä¸€ä¸ªç–‘é—®æ˜¯ï¼Œ*æˆ‘æ€æ ·æŠŠæˆ‘çš„æ•°æ®å’Œæ¨¡å‹å‘é€åˆ°å¦ä¸€ä¸ª GPU ä¸Šï¼Ÿ*

è¿™æ­£æ˜¯ `DistributedDataParallel` æ¨¡å—å‘æŒ¥ä½œç”¨çš„åœ°æ–¹ï¼Œ å®ƒå°†æ‚¨çš„æ¨¡å‹å¤åˆ¶åˆ°æ¯ä¸ª GPU ä¸Š ï¼Œå¹¶ä¸”å½“ `loss.backward()` è¢«è°ƒç”¨è¿›è¡Œåå‘ä¼ æ’­çš„æ—¶å€™ï¼Œæ‰€æœ‰è¿™äº›æ¨¡å‹å‰¯æœ¬çš„æ¢¯åº¦å°†è¢«åŒæ­¥åœ°å¹³å‡/ä¸‹é™ (reduce)ã€‚è¿™ç¡®ä¿æ¯ä¸ªè®¾å¤‡åœ¨æ‰§è¡Œä¼˜åŒ–å™¨æ­¥éª¤åå…·æœ‰ç›¸åŒçš„æƒé‡ã€‚

ä¸‹é¢æ˜¯æˆ‘ä»¬çš„è®­ç»ƒè®¾ç½®ç¤ºä¾‹ï¼Œæˆ‘ä»¬ä½¿ç”¨äº† DistributedDataParallel é‡æ„äº†è®­ç»ƒå‡½æ•°ï¼š

> æ³¨æ„ï¼šæ­¤å¤„çš„ rank æ˜¯å½“å‰ GPU ä¸æ‰€æœ‰å…¶ä»–å¯ç”¨ GPU ç›¸æ¯”çš„æ€»ä½“ rankï¼Œè¿™æ„å‘³ç€å®ƒä»¬çš„ rank ä¸º `0 -> n-1`

```python
from torch.nn.parallel import DistributedDataParallel as DDP

def train(model, rank, world_size):
    setup(rank, world_size)
    model = model.to(rank)
    ddp_model = DDP(model, device_ids=[rank])
    optimizer = optim.AdamW(ddp_model.parameters(), lr=1e-3)
    # Train for one epoch
    model.train()
    for batch_idx, (data, target) in enumerate(train_loader):
        data, target = data.to(device), target.to(device)
        output = model(data)
        loss = F.nll_loss(output, target)
        loss.backward()
        optimizer.step()
        optimizer.zero_grad()
    cleanup()
```

åœ¨ä¸Šè¿°çš„ä»£ç ä¸­éœ€è¦ä¸º *æ¯ä¸ªå‰¯æœ¬è®¾å¤‡ä¸Š* çš„æ¨¡å‹ (å› æ­¤åœ¨è¿™é‡Œæ˜¯ `ddp_model` çš„å‚æ•°è€Œä¸æ˜¯ `model` çš„å‚æ•°) å£°æ˜ä¼˜åŒ–å™¨ï¼Œä»¥ä¾¿æ­£ç¡®è®¡ç®—æ¯ä¸ªå‰¯æœ¬è®¾å¤‡ä¸Šçš„æ¢¯åº¦ã€‚

æœ€åï¼Œè¦è¿è¡Œè„šæœ¬ï¼ŒPyTorch æœ‰ä¸€ä¸ªæ–¹ä¾¿çš„ `torchrun` å‘½ä»¤è¡Œæ¨¡å—å¯ä»¥æä¾›å¸®åŠ©ã€‚åªéœ€ä¼ å…¥å®ƒåº”è¯¥ä½¿ç”¨çš„èŠ‚ç‚¹æ•°ä»¥åŠè¦è¿è¡Œçš„è„šæœ¬å³å¯ï¼š

```bash
torchrun --nproc_per_nodes=2 --nnodes=1 example_script.py
```

ä¸Šé¢çš„ä»£ç å¯ä»¥åœ¨åœ¨ä¸€å°æœºå™¨ä¸Šçš„ä¸¤ä¸ª GPU ä¸Šè¿è¡Œè®­ç»ƒè„šæœ¬ï¼Œè¿™æ˜¯ä½¿ç”¨ PyTorch åªè¿›è¡Œåˆ†å¸ƒå¼è®­ç»ƒçš„æƒ…å†µ (ä¸å¯ä»¥åœ¨å•æœºå•å¡ä¸Šè¿è¡Œ)ã€‚

ç°åœ¨è®©æˆ‘ä»¬è°ˆè°ˆ ğŸ¤— Accelerateï¼Œä¸€ä¸ªæ—¨åœ¨ä½¿å¹¶è¡ŒåŒ–æ›´åŠ æ— ç¼å¹¶æœ‰åŠ©äºä¸€äº›æœ€ä½³å®è·µçš„åº“ã€‚

## ğŸ¤— Accelerate

[Accelerate](https://huggingface.co/docs/accelerate) æ˜¯ä¸€ä¸ªåº“ï¼Œæ—¨åœ¨æ— éœ€å¤§å¹…ä¿®æ”¹ä»£ç çš„æƒ…å†µä¸‹å®Œæˆå¹¶è¡ŒåŒ–ã€‚é™¤æ­¤ä¹‹å¤–ï¼ŒğŸ¤— Accelerate é™„å¸¦çš„æ•°æ® pipeline è¿˜å¯ä»¥æé«˜ä»£ç çš„æ€§èƒ½ã€‚

é¦–å…ˆï¼Œè®©æˆ‘ä»¬å°†åˆšåˆšæ‰§è¡Œçš„æ‰€æœ‰ä¸Šè¿°ä»£ç å°è£…åˆ°ä¸€ä¸ªå‡½æ•°ä¸­ï¼Œä»¥å¸®åŠ©æˆ‘ä»¬ç›´è§‚åœ°çœ‹åˆ°å·®å¼‚ï¼š

```python
def train_ddp(rank, world_size):
    setup(rank, world_size)
    # Build DataLoaders
    transform = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize((0.1307), (0.3081))
    ])

    train_dset = datasets.MNIST('data', train=True, download=True, transform=transform)
    test_dset = datasets.MNIST('data', train=False, transform=transform)

    train_loader = torch.utils.data.DataLoader(train_dset, shuffle=True, batch_size=64)
    test_loader = torch.utils.data.DataLoader(test_dset, shuffle=False, batch_size=64)

    # Build model
    model = model.to(rank)
    ddp_model = DDP(model, device_ids=[rank])

    # Build optimizer
    optimizer = optim.AdamW(ddp_model.parameters(), lr=1e-3)

    # Train for a single epoch
    model.train()
    for batch_idx, (data, target) in enumerate(train_loader):
        data, target = data.to(device), target.to(device)
        output = model(data)
        loss = F.nll_loss(output, target)
        loss.backward()
        optimizer.step()
        optimizer.zero_grad()
    
    # Evaluate
    model.eval()
    correct = 0
    with torch.no_grad():
        for data, target in test_loader:
            data, target = data.to(device), target.to(device)
            output = model(data)
            pred = output.argmax(dim=1, keepdim=True)
            correct += pred.eq(target.view_as(pred)).sum().item()
    print(f'Accuracy: {100. * correct / len(test_loader.dataset)}')
```

æ¥ä¸‹æ¥è®©æˆ‘ä»¬è°ˆè°ˆ ğŸ¤— Accelerate å¦‚ä½•ä¾¿åˆ©åœ°å®ç°å¹¶è¡ŒåŒ–çš„ã€‚ä¸Šé¢çš„ä»£ç æœ‰å‡ ä¸ªé—®é¢˜ï¼š

1. è¯¥ä»£ç æœ‰ç‚¹ä½æ•ˆï¼Œå› ä¸ºæ¯ä¸ªè®¾å¤‡éƒ½ä¼šåˆ›å»ºä¸€ä¸ª dataloaderã€‚
2. è¿™äº›ä»£ç **åªèƒ½**è¿è¡Œåœ¨å¤š GPU ä¸‹ï¼Œå½“æƒ³è®©è¿™ä¸ªä»£ç è¿è¡Œåœ¨å•ä¸ª GPU æˆ– TPU æ—¶ï¼Œè¿˜éœ€è¦é¢å¤–è¿›è¡Œä¸€äº›ä¿®æ”¹ã€‚

Accelerate é€šè¿‡ [`Accelerator`](https://huggingface.co/docs/accelerate/v0.12.0/en/package_reference/accelerator#accelerator) ç±»è§£å†³ä¸Šè¿°é—®é¢˜ã€‚é€šè¿‡å®ƒï¼Œä¸è®ºæ˜¯å•èŠ‚ç‚¹è¿˜æ˜¯å¤šèŠ‚ç‚¹ï¼Œé™¤äº†ä¸‰è¡Œä»£ç å¤–ï¼Œå…¶ä½™ä»£ç å‡ ä¹ä¿æŒä¸å˜ï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼š

```python
def train_ddp_accelerate():
    accelerator = Accelerator()
    # Build DataLoaders
    transform = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize((0.1307), (0.3081))
    ])

    train_dset = datasets.MNIST('data', train=True, download=True, transform=transform)
    test_dset = datasets.MNIST('data', train=False, transform=transform)

    train_loader = torch.utils.data.DataLoader(train_dset, shuffle=True, batch_size=64)
    test_loader = torch.utils.data.DataLoader(test_dset, shuffle=False, batch_size=64)

    # Build model
    model = BasicModel()

    # Build optimizer
    optimizer = optim.AdamW(model.parameters(), lr=1e-3)

    # Send everything through `accelerator.prepare`
    train_loader, test_loader, model, optimizer = accelerator.prepare(
        train_loader, test_loader, model, optimizer
    )

    # Train for a single epoch
    model.train()
    for batch_idx, (data, target) in enumerate(train_loader):
        output = model(data)
        loss = F.nll_loss(output, target)
        accelerator.backward(loss)
        optimizer.step()
        optimizer.zero_grad()
    
    # Evaluate
    model.eval()
    correct = 0
    with torch.no_grad():
        for data, target in test_loader:
            data, target = data.to(device), target.to(device)
            output = model(data)
            pred = output.argmax(dim=1, keepdim=True)
            correct += pred.eq(target.view_as(pred)).sum().item()
    print(f'Accuracy: {100. * correct / len(test_loader.dataset)}')
```

å€ŸåŠ© `Accelerator` å¯¹è±¡ï¼Œæ‚¨çš„ PyTorch è®­ç»ƒå¾ªç¯ç°åœ¨å·²é…ç½®ä¸ºå¯ä»¥åœ¨ä»»ä½•åˆ†å¸ƒå¼æƒ…å†µè¿è¡Œã€‚ä½¿ç”¨ Accelerator æ”¹é€ åçš„ä»£ç ä»ç„¶å¯ä»¥é€šè¿‡ `torchrun` CLI æˆ–é€šè¿‡ ğŸ¤— Accelerate è‡ªå·±çš„ CLI ç•Œé¢ [å¯åŠ¨](https://huggingface.co/docs/accelerate/v0.12.0/en/basic_tutorials/launch) (å¯åŠ¨ä½ çš„ğŸ¤— Accelerate è„šæœ¬)ã€‚

å› æ­¤ï¼Œç°åœ¨å¯ä»¥å°½å¯èƒ½ä¿æŒ PyTorch åŸç”Ÿä»£ç ä¸å˜çš„å‰æä¸‹ï¼Œä½¿ç”¨ ğŸ¤— Accelerate æ‰§è¡Œåˆ†å¸ƒå¼è®­ç»ƒã€‚

æ—©äº›æ—¶å€™æœ‰äººæåˆ° ğŸ¤— Accelerate è¿˜å¯ä»¥ä½¿ DataLoaders æ›´é«˜æ•ˆã€‚è¿™æ˜¯é€šè¿‡è‡ªå®šä¹‰é‡‡æ ·å™¨å®ç°çš„ï¼Œå®ƒå¯ä»¥åœ¨è®­ç»ƒæœŸé—´è‡ªåŠ¨å°†éƒ¨åˆ†æ‰¹æ¬¡å‘é€åˆ°ä¸åŒçš„è®¾å¤‡ï¼Œä»è€Œå…è®¸æ¯ä¸ªè®¾å¤‡åªéœ€è¦å‚¨å­˜æ•°æ®çš„ä¸€éƒ¨åˆ†ï¼Œè€Œä¸æ˜¯ä¸€æ¬¡å°†æ•°æ®å¤åˆ¶å››ä»½å­˜å…¥å†…å­˜ï¼Œå…·ä½“å–å†³äºé…ç½®ã€‚å› æ­¤ï¼Œå†…å­˜æ€»é‡ä¸­åªæœ‰åŸå§‹æ•°æ®é›†çš„ä¸€ä¸ªå®Œæ•´å‰¯æœ¬ã€‚è¯¥æ•°æ®é›†ä¼šæ‹†åˆ†ååˆ†é…åˆ°å„ä¸ªè®­ç»ƒèŠ‚ç‚¹ä¸Šï¼Œä»è€Œå…è®¸åœ¨å•ä¸ªå®ä¾‹ä¸Šè®­ç»ƒæ›´å¤§çš„æ•°æ®é›†ï¼Œè€Œä¸ä¼šä½¿å†…å­˜çˆ†ç‚¸ã€‚

### ä½¿ç”¨ `notebook_launcher`

ä¹‹å‰æåˆ°æ‚¨å¯ä»¥ç›´æ¥ä» Jupyter Notebook è¿è¡Œåˆ†å¸ƒå¼ä»£ç ã€‚è¿™æ¥è‡ª ğŸ¤— Accelerate çš„ [`notebook_launcher`](https://huggingface.co/docs/accelerate/v0.12.0/en/basic_tutorials/notebook) æ¨¡å—ï¼Œå®ƒå¯ä»¥åœ¨ Jupyter Notebook å†…éƒ¨çš„ä»£ç å¯åŠ¨å¤š GPU è®­ç»ƒã€‚

ä½¿ç”¨å®ƒå°±åƒå¯¼å…¥ launcher ä¸€æ ·ç®€å•ï¼š

```python
from accelerate import notebook_launcher
```

æ¥ç€ä¼ é€’æˆ‘ä»¬ä¹‹å‰å£°æ˜çš„è®­ç»ƒå‡½æ•°ã€è¦ä¼ é€’çš„ä»»ä½•å‚æ•°ä»¥åŠè¦ä½¿ç”¨çš„è¿›ç¨‹æ•°ï¼ˆä¾‹å¦‚ TPU ä¸Šçš„ 8 ä¸ªï¼Œæˆ–ä¸¤ä¸ª GPU ä¸Šçš„ 2 ä¸ªï¼‰ã€‚ä¸‹é¢ä¸¤ä¸ªè®­ç»ƒå‡½æ•°éƒ½å¯ä»¥è¿è¡Œï¼Œä½†è¯·æ³¨æ„ï¼Œå¯åŠ¨å•æ¬¡å¯åŠ¨åï¼Œå®ä¾‹éœ€è¦é‡æ–°å¯åŠ¨æ‰èƒ½äº§ç”Ÿå¦ä¸€ä¸ªï¼š

```python
notebook_launcher(train_ddp, args=(), num_processes=2)
```

æˆ–è€…ï¼š

```python
notebook_launcher(train_accelerate_ddp, args=(), num_processes=2)
```

## ä½¿ç”¨ ğŸ¤— Trainer

ç»ˆäºæˆ‘ä»¬æ¥åˆ°äº†æœ€é«˜çº§çš„ APIâ€”â€”Hugging Face [Trainer](https://huggingface.co/docs/transformers/main_classes/trainer)ã€‚

å®ƒæ¶µç›–äº†å°½å¯èƒ½å¤šçš„è®­ç»ƒç±»å‹ï¼ŒåŒæ—¶ä»ç„¶èƒ½å¤Ÿåœ¨åˆ†å¸ƒå¼ç³»ç»Ÿä¸Šè¿›è¡Œè®­ç»ƒï¼Œç”¨æˆ·æ ¹æœ¬ä¸éœ€è¦åšä»»ä½•äº‹æƒ…ã€‚

é¦–å…ˆæˆ‘ä»¬éœ€è¦å¯¼å…¥ Trainerï¼š

```python
from transformers import Trainer
```

ç„¶åæˆ‘ä»¬å®šä¹‰ä¸€äº› `TrainingArguments` æ¥æ§åˆ¶æ‰€æœ‰å¸¸ç”¨çš„è¶…å‚æ•°ã€‚Trainer éœ€è¦çš„è®­ç»ƒæ•°æ®æ˜¯å­—å…¸ç±»å‹çš„ï¼Œå› æ­¤éœ€è¦åˆ¶ä½œè‡ªå®šä¹‰æ•´ç†åŠŸèƒ½ã€‚

æœ€åï¼Œæˆ‘ä»¬å°†è®­ç»ƒå™¨å­ç±»åŒ–å¹¶ç¼–å†™æˆ‘ä»¬è‡ªå·±çš„ `compute_loss`ã€‚

ä¹‹åï¼Œè¿™æ®µä»£ç ä¹Ÿå¯ä»¥åˆ†å¸ƒå¼è¿è¡Œï¼Œè€Œæ— éœ€ä¿®æ”¹ä»»ä½•è®­ç»ƒä»£ç ï¼

```python
from transformers import Trainer, TrainingArguments

model = BasicNet()

training_args = TrainingArguments(
    "basic-trainer",
    per_device_train_batch_size=64,
    per_device_eval_batch_size=64,
    num_train_epochs=1,
    evaluation_strategy="epoch",
    remove_unused_columns=False
)

def collate_fn(examples):
    pixel_values = torch.stack([example[0] for example in examples])
    labels = torch.tensor([example[1] for example in examples])
    return {"x":pixel_values, "labels":labels}

class MyTrainer(Trainer):
    def compute_loss(self, model, inputs, return_outputs=False):
        outputs = model(inputs["x"])
        target = inputs["labels"]
        loss = F.nll_loss(outputs, target)
        return (loss, outputs) if return_outputs else loss

trainer = MyTrainer(
    model,
    training_args,
    train_dataset=train_dset,
    eval_dataset=test_dset,
    data_collator=collate_fn,
)
```

```python
trainer.train()
```

```python out
    ***** Running training *****
      Num examples = 60000
      Num Epochs = 1
      Instantaneous batch size per device = 64
      Total train batch size (w. parallel, distributed & accumulation) = 64
      Gradient Accumulation steps = 1
      Total optimization steps = 938
```

| Epoch | Training Loss | Validation Loss |
|-------|---------------|-----------------|
| 1     | 0.875700      | 0.282633        |

ä¸ä¸Šé¢çš„ `notebook_launcher` ç¤ºä¾‹ç±»ä¼¼ï¼Œä¹Ÿå¯ä»¥å°†è¿™ä¸ªè¿‡ç¨‹å°è£…æˆä¸€ä¸ªè®­ç»ƒå‡½æ•°ï¼š

```python
def train_trainer_ddp():
    model = BasicNet()

    training_args = TrainingArguments(
        "basic-trainer",
        per_device_train_batch_size=64,
        per_device_eval_batch_size=64,
        num_train_epochs=1,
        evaluation_strategy="epoch",
        remove_unused_columns=False
    )

    def collate_fn(examples):
        pixel_values = torch.stack([example[0] for example in examples])
        labels = torch.tensor([example[1] for example in examples])
        return {"x":pixel_values, "labels":labels}

    class MyTrainer(Trainer):
        def compute_loss(self, model, inputs, return_outputs=False):
            outputs = model(inputs["x"])
            target = inputs["labels"]
            loss = F.nll_loss(outputs, target)
            return (loss, outputs) if return_outputs else loss

    trainer = MyTrainer(
        model,
        training_args,
        train_dataset=train_dset,
        eval_dataset=test_dset,
        data_collator=collate_fn,
    )

    trainer.train()

notebook_launcher(train_trainer_ddp, args=(), num_processes=2)
```

## ç›¸å…³èµ„æº

è¦äº†è§£æœ‰å…³ PyTorch åˆ†å¸ƒå¼æ•°æ®å¹¶è¡Œæ€§çš„æ›´å¤šä¿¡æ¯ï¼Œè¯·æŸ¥çœ‹ [æ–‡æ¡£](https://pytorch.org/docs/stable/distributed.html)

è¦äº†è§£æœ‰å…³ ğŸ¤— Accelerate çš„æ›´å¤šä¿¡æ¯ï¼Œè¯·æŸ¥çœ‹ [ğŸ¤— Accelerat æ–‡æ¡£](https://huggingface.co/docs/accelerate)

è¦äº†è§£æœ‰å…³ ğŸ¤— Transformer çš„æ›´å¤šä¿¡æ¯ï¼Œè¯·æŸ¥çœ‹ [ğŸ¤— Transformer æ–‡æ¡£](https://huggingface.co/docs/transformers)

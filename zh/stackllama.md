---
title: "â€œStackLLaMAâ€: ç”¨ RLHF è®­ç»ƒ LLaMA çš„æ‰‹æŠŠæ‰‹æ•™ç¨‹"
thumbnail: /blog/assets/138_stackllama/thumbnail.png
authors:
- user: edbeeching
- user: kashif
- user: ybelkada
- user: lewtun
- user: lvwerra
- user: nazneen
- user: natolambert
translators:
- user: Vermillion-Qi
- user: zhongdongy
---

# â€œStackLLaMAâ€: ç”¨ RLHF è®­ç»ƒ LLaMA çš„æ‰‹æŠŠæ‰‹æ•™ç¨‹


å¦‚ [ChatGPT](https://openai.com/blog/chatgpt)ï¼Œ[GPT-4](https://openai.com/research/gpt-4)ï¼Œ[Claude](https://www.anthropic.com/index/introducing-claude)è¯­è¨€æ¨¡å‹ ä¹‹å¼ºå¤§ï¼Œå› ä¸ºå®ƒä»¬é‡‡ç”¨äº† **åŸºäºäººç±»åé¦ˆçš„å¼ºåŒ–å­¦ä¹ ** (Reinforcement Learning from Human Feedback, RLHF) æ¥ä½¿ä¹‹æ›´ç¬¦åˆæˆ‘ä»¬çš„ä½¿ç”¨åœºæ™¯ã€‚

æœ¬åšå®¢æ—¨åœ¨å±•ç¤ºç”¨ RLHF è®­ç»ƒä¸€ä¸ª [LLaMA](https://ai.facebook.com/blog/large-language-model-llama-meta-ai) æ¨¡å‹ï¼Œä»¥å›ç­” [Stack Exchange](https://stackexchange.com/) ä¸Šçš„é—®é¢˜ã€‚å…·ä½“è€Œè¨€ï¼ŒåŒ…å«ä»¥ä¸‹å‡ ä¸ªæ–¹é¢:

- æœ‰ç›‘ç£çš„å¾®è°ƒ (Supervised Fine-tuningï¼ŒSFT)ã€‚
- å¥–åŠ± / åå¥½å»ºæ¨¡ (Reward / preference modelingï¼ŒRM)ã€‚
- åŸºäºäººç±»åé¦ˆçš„å¼ºåŒ–å­¦ä¹  (RLHF)ã€‚

![](https://man-archives.oss-cn-hangzhou.aliyuncs.com/goofan/202304122037176.png)

æ‘˜è‡ª InstructGPT è®ºæ–‡ï¼ŒOuyang, Long, et al. â€œTraining language models to follow instructions with human feedback.â€ arXiv preprint arXiv:2203.02155 (2022).

ç»“åˆäº†ä¸Šè¿°æ–¹æ³•ï¼Œæˆ‘ä»¬å‘å¸ƒäº† StackLLaMA æ¨¡å‹ï¼Œè¯¥æ¨¡å‹åœ¨ [ğŸ¤— Hub](https://huggingface.co/trl-lib/llama-se-rl-peft) ä¸Šå¼€æº (è®¿é—®é“¾æ¥æŸ¥çœ‹ [Meta çš„åŸå§‹ LLaMA](https://ai.facebook.com/blog/large-language-model-llama-meta-ai/) )ï¼Œæ•´ä¸ª [è®­ç»ƒçš„æµç¨‹](https://huggingface.co/docs/trl/index) å·²ç»é›†æˆåˆ°äº† Hugging Face TRL åº“ä¸­ ã€‚ä½ å¯ä»¥é€šè¿‡ä¸‹é¢çš„ [demo](https://huggingface.co/spaces/trl-lib/stack-llama) æ¥å°è¯•è¯¥æ¨¡å‹ã€‚

## LLaMA æ¨¡å‹

åœ¨å®è·µ RLHF æ—¶ï¼Œé€‰å–ä¸€ä¸ªåˆé€‚çš„æ¨¡å‹å¾ˆé‡è¦: RLHF åªæ˜¯ä¸€ä¸ªè®©æ¨¡å‹æ»¡è¶³æˆ‘ä»¬äº¤äº’å½¢å¼çš„éœ€æ±‚çš„å¾®è°ƒè¿‡ç¨‹ ã€‚æ‰€ä»¥æˆ‘ä»¬é€‰å–äº†æœ€è¿‘ä¸Šçº¿çš„ [LLaMA](https://arxiv.org/abs/2302.13971) æ¨¡å‹ã€‚LLaMA æ¨¡å‹æ˜¯ Meta AI æœ€è¿‘æ¨å‡ºçš„å¤§è¯­è¨€æ¨¡å‹ã€‚å…¶å‚æ•°é‡å¤§å°æ¶µç›– 7B åˆ° 65Bï¼Œä»¥åŠè®­ç»ƒåœ¨ 1T å’Œ 1.4T çš„ token ä¸Šï¼Œè¿™è®©å…¶å¾ˆå®ç”¨ã€‚æˆ‘ä»¬è¿™é‡Œé‡‡ç”¨ 7B çš„æ¨¡å‹ã€‚(è¯·å¡«å†™ Meta AI çš„è¿™ä»½ [è¡¨å•](https://docs.google.com/forms/d/e/1FAIpQLSfqNECQnMkycAp2jP4Z9TFX0cGR4uf7b_fBxjY_OjhJILlKGA/viewform) æ¥ä¸‹è½½æ¨¡å‹)ã€‚

## Stack Exchange æ•°æ®é›†

æ”¶é›†äººç±»çš„åé¦ˆæ•°æ®é›†æ˜¯å¾ˆå¤æ‚ä¸”æ˜‚è´µçš„åŠ³åŠ¨ã€‚ä¸ºäº†åšåˆ°è¿™ä¸ªï¼Œå¹¶ä¸”è¿˜èƒ½ä¿è¯æ¨¡å‹çš„æœ‰æ•ˆæ€§ï¼Œæˆ‘ä»¬ä½¿ç”¨ [StackExchange æ•°æ®é›†](https://huggingface.co/datasets/HuggingFaceH4/stack-exchange-preferences)ã€‚è¯¥æ•°æ®é›†æ¶µç›–äº† StackExchange å¹³å°ä¸Šçš„é—®é¢˜å’Œç­”æ¡ˆ (åŒ…å« StackOverflow çš„ç¼–ç¨‹ç­‰è¯é¢˜ä¸‹çš„)ã€‚è¿™å¾ˆé€‚åˆæˆ‘ä»¬çš„å®è·µï¼Œå› ä¸ºå…¶åŒ…å«äº†æ¯ä¸ªç­”æ¡ˆçš„èµå’Œè¸©çš„æ•°é‡ã€‚

æˆ‘ä»¬æŒ‰ç…§ [Askell et al. 2021](https://arxiv.org/abs/2112.00861) ä¸­çš„æ–¹æ³•ï¼Œç»™æ¯ä¸ªç­”æ¡ˆèµ‹åˆ†:

```
score = log2 (1 + upvotes) rounded to the nearest integer, plus 1 if the questioner accepted the answer (we assign a score of âˆ’1 if the number of upvotes is negative).
```

å¯¹å¥–åŠ±æ¨¡å‹ï¼Œæˆ‘ä»¬å°†çœ‹åˆ°æ¯ä¸ªé—®é¢˜æ€»æ˜¯éœ€è¦ä¸¤ä¸ªç­”æ¡ˆå¯¹æ¯”ã€‚æœ‰äº›é—®é¢˜æœ‰å¾ˆå¤šç­”æ¡ˆï¼Œå¯ä»¥äº§ç”Ÿå¾ˆå¤šå¯¹ï¼Œæˆ‘ä»¬åªå–åä¸ªä»¥é™åˆ¶æ¯ä¸ªé—®é¢˜çš„æ•°æ®é‡ã€‚æœ€åï¼Œæˆ‘ä»¬æŠŠæ ¼å¼ä» HTML è½¬åŒ–åˆ° Markdown ä»¥æé«˜è¾“å‡ºçš„å¯è¯»æ€§ã€‚ä½ å¯ä»¥çœ‹åˆ°æ•°æ®é›†å’Œå¤„ç†è¿‡ç¨‹çš„ [ç¬”è®°æœ¬]ã€‚(https://huggingface.co/datasets/lvwerra/stack-exchange-pairedã€‚)

## é«˜æ•ˆè®­ç»ƒç­–ç•¥

å³ä½¿æ˜¯æœ€å° LLaMA æ¨¡å‹çš„è®­ç»ƒï¼Œéƒ½éœ€è¦å¤§é‡å†…å­˜ã€‚ä¼°ç®—ä¸€ä¸‹: ä»¥ bf16 åŠç²¾åº¦ï¼Œæ¯ä¸ªå‚æ•°ç”¨ 2 ä¸ªå­—èŠ‚ (ä»¥ fp32 ç²¾åº¦å››å­—èŠ‚çš„æ ‡å‡†)ï¼Œè®­ç»ƒæ—¶éœ€è¦ 8 ä¸ªå­—èŠ‚ (ä¾‹å¦‚ Adam ä¼˜åŒ–å™¨ï¼Œå‚è§ Tramsformers çš„ [æ€§èƒ½æ–‡æ¡£](https://huggingface.co/docs/transformers/perf_train_gpu_one#optimizer))ã€‚å¯è§ 7B å‚æ•°é‡çš„æ¨¡å‹å°†ç”¨ (2+8)* 7B = 70 GB çš„å†…å­˜ï¼Œå¹¶ä¸”è¿˜å¯èƒ½éœ€è¦æ›´å¤šç”¨äºè®¡ç®—è¯¸å¦‚æ³¨æ„åŠ›åˆ†æ•°çš„ä¸­é—´å€¼ã€‚æ‰€ä»¥å¾ˆéš¾åœ¨ä¸€å¼  80GB æ˜¾å­˜çš„ A100 ä¸Šè®­ç»ƒã€‚æˆ–è®¸ä½ å¯ä»¥ä½¿ç”¨ä¸€äº›æŠ€å·§ï¼Œæ¯”å¦‚ç”¨æ›´é«˜æ•ˆçš„åŠç²¾åº¦è®­ç»ƒçš„ä¼˜åŒ–å™¨æ¥å‹ç¼©å†…å­˜ï¼Œä½†æº¢å‡ºæ˜¯è¿Ÿæ—©çš„ã€‚

å¦å¤–çš„å¯èƒ½æ˜¯ **å‚æ•°é«˜æ•ˆçš„å¾®è°ƒ**(Parameter-Efficient Fine-Tuning, PEFT) æŠ€æœ¯ï¼Œæ¯”å¦‚ [`peft`](https://github.com/huggingface/peft) åº“ï¼Œå®ƒå¯ä»¥å¯¹ä½¿ç”¨ 8-bit åŠ è½½çš„æ¨¡å‹åš **ä½ç§©ä¼˜åŒ–**(Low-Rank Adaptationï¼ŒLoRA)ã€‚

![](https://man-archives.oss-cn-hangzhou.aliyuncs.com/goofan/202304122104084.gif)

çº¿æ€§å±‚çš„ä½ç§©ä¼˜åŒ–: é¢å¤–å‚æ•° (æ©™è‰²) è¢«åŠ åœ¨ Frozen å±‚ (è“è‰²)ï¼Œç¼–ç åçš„éšè—çŠ¶æ€ä¸ Frozen å±‚çš„éšè—çŠ¶æ€å åŠ åœ¨ä¸€èµ·ã€‚

ä»¥ 8bit åŠ è½½æ¨¡å‹ä¼šå¤§å¹…é™ä½å†…å­˜å ç”¨ï¼Œå› ä¸ºæ¯ä¸ªå‚æ•°åªè¦ä¸€å­—èŠ‚ (æ¯”å¦‚ 7B LLaMA æ˜¯ 7GB å†…å­˜)ã€‚ä¸ç›´æ¥è®­ç»ƒåŸå§‹æ¨¡å‹ä¸åŒï¼ŒLoRA åœ¨ç‰¹å®šå±‚ (ä¸€èˆ¬æ˜¯æ³¨æ„åŠ›å±‚) æ·»åŠ å°‘é‡æ–°å‚æ•°ï¼Œå¤§å¹…é™ä½äº†éœ€è¦è®­ç»ƒçš„å‚æ•°ã€‚

æ­¤æƒ…æ­¤æ™¯ï¼Œä¸€ä¸ªè¡¡é‡æ ‡å‡†æ˜¯ 1B çš„å‚æ•°åœ¨æ•´ä¸ªå¾®è°ƒè¿‡ç¨‹ä¸­å  ~1.2-1.4GB (å’Œå…·ä½“ batch size åŠåºåˆ—é•¿åº¦æœ‰å…³)ã€‚åœ¨å‚è€ƒçš„åšå®¢ä¸­å…·ä½“è®¨è®ºäº†ï¼Œè¿™ä½¿å¾—ä½æˆæœ¬ä¸‹å¾®è°ƒè¾ƒå¤§å‚æ•°è§„æ¨¡çš„æ¨¡å‹æˆä¸ºå¯èƒ½ (æ¯”å¦‚åœ¨ä¸€å¼  A100 ä¸Šå¾®è°ƒ 50-60B çš„å‚æ•°)ã€‚

è¿™äº›æŠ€æœ¯èƒ½è®©å¾®è°ƒå¤§æ¨¡å‹çš„ä»»åŠ¡ï¼Œåœ¨æ¶ˆè´¹çº§è®¾å¤‡å’Œ Google Colab ä¸Šæ‰§è¡Œã€‚è¿™é‡Œæä¾›ä¸€äº›å€¼å¾—å…³æ³¨çš„æ¼”ç¤º demo: `facebook/opt-6.7b` (åœ¨ float16 ç²¾åº¦ä¸‹ 13GB) å’Œ `openai/whisper-large` 
è·‘åœ¨ Google Colab (15GB æ˜¾å­˜) ä¸Šã€‚æ¬²äº†è§£ `peft` çš„ä½¿ç”¨ï¼Œè¯·å‚è§ [github ä»“åº“](https://github.com/huggingface/peft) æˆ–è€…ä¹‹å‰çš„ [åšå®¢ä»‹ç»](https://huggingface.co/blog/trl-peft): åœ¨å®¢æˆ·ç«¯è®­ç»ƒ 20B å‚æ•°é‡çš„æ¨¡å‹ã€‚

ç°åœ¨æˆ‘ä»¬èƒ½åœ¨ä¸€å¼  GPU ä¸Šå¾®è°ƒå¾ˆå¤§çš„æ¨¡å‹äº†ï¼Œä½†è®­ç»ƒè¿˜æ˜¯ä¼šå¾ˆæ…¢ã€‚æ­¤æ—¶æœ€ç®€å•çš„ç­–ç•¥ä¾¿æ˜¯å¹¶è¡ŒåŒ–: æŠŠä¸€ä¸ªè®­ç»ƒåŒæ—¶æ”¾åˆ°ä¸åŒçš„ GPU ä¸Šï¼Œå„ GPU æ¥å—ä¸åŒçš„ batchã€‚è¿™æ ·æˆ‘ä»¬å¯ä»¥å¹¶è¡Œæ‰§è¡Œå‰å‘ä¼ æ’­å’Œåå‘ä¼ æ’­ï¼Œé€šè¿‡å¢åŠ  GPU çš„æ•°é‡å®ç°å¹¶è¡Œèƒ½åŠ›æå‡ã€‚

![](https://man-archives.oss-cn-hangzhou.aliyuncs.com/goofan/202304122114399.png)

æˆ‘ä»¬å¯ä»¥é€‰ç”¨ `trainsformers.Trainer` æˆ– `accelerate`ï¼Œå› ä¸ºå®ƒä»¬éƒ½æ”¯æŒæ— ä»£ç å˜æ›´è¿›è¡Œæ•°æ®å¹¶è¡ŒåŒ–ã€‚åªéœ€æ³¨æ„è°ƒç”¨ `torchrun` æˆ–è€… `accelerate launch` è„šæœ¬æ—¶çš„å‚æ•°å³å¯å®ç°ã€‚æ¯”å¦‚ä»¥ä¸‹å°±æ˜¯åœ¨ä¸€ä¸ª 8 æ˜¾å¡çš„æœºå™¨ä¸Šåˆ†åˆ«ç”¨ `accelerate launch` å’Œ `torchrun`çš„æ–¹æ³•:

```bash
accelerate launch --multi_gpu --num_machines 1  --num_processes 8 my_accelerate_script.py
torchrun --nnodes 1  --nproc_per_node 8 my_torch_script.py
```

## æœ‰ç›‘ç£çš„å¾®è°ƒ

åœ¨è®­ç»ƒå¥–åŠ±æ¨¡å‹å’Œç”¨ RL ä¹‹å‰ï¼Œæ¨¡å‹è‹¥æ˜¯å·²ç»åœ¨æˆ‘ä»¬æ„Ÿå…´è¶£çš„æ–¹é¢è¡¨ç°å¥½å°†ä¼šå¾ˆæœ‰å¸®åŠ©ã€‚åœ¨æˆ‘ä»¬çš„ç¤ºä¾‹ä¸­ï¼Œæˆ‘ä»¬æƒ³è¦å…¶èƒ½å›ç­”é—®é¢˜ï¼Œè€Œå…¶ä»–æ—¶å€™ï¼Œæˆ‘ä»¬å¯èƒ½å®ƒèƒ½å¬æŒ‡ä»¤ (è¿™æ—¶å¯¹æŒ‡ä»¤æ‰§è¡Œçš„å¾®è°ƒæ˜¯ç†æƒ³çš„)ã€‚å®ç°è¿™ä¸ªæœ€ç®€å•çš„æ–¹æ³•ä¾¿æ˜¯é¢å‘è¯¥è¯­è¨€ä»»åŠ¡ï¼Œç”¨è¯¥ä»»åŠ¡å’Œé¢†åŸŸçš„æ–‡æœ¬ï¼Œç»§ç»­è®­ç»ƒã€‚[StackExchange æ•°æ®é›†](https://huggingface.co/datasets/HuggingFaceH4/stack-exchange-preferences) å« 10M çš„æŒ‡ä»¤é‡ï¼Œæ‰€ä»¥æˆ‘ä»¬èƒ½ç”¨å…¶å­é›†å¾ˆå®¹æ˜“åœ°è®­ç»ƒã€‚

åœ¨ç”¨ RLHF ä¹‹å‰çš„æ¨¡å‹å¾®è°ƒæ²¡æœ‰ç‰¹åˆ«çš„ï¼Œå°±æ˜¯ä¸€èˆ¬çš„é¢å‘è¯­è¨€ä»»åŠ¡çš„é¢„è®­ç»ƒæ¨¡å‹å¾®è°ƒã€‚ä¸ºäº†é«˜æ•ˆåˆ©ç”¨æ•°æ®ï¼Œæˆ‘ä»¬é‡‡ç”¨äº†ç§°ä¹‹ä¸º **æ‰“åŒ…** çš„æŠ€æœ¯: ä¸ batch ä¸­çš„æ¯ä¸ªæ ·æœ¬å‡ç”±å•ä¸€æ–‡æœ¬ç»„æˆï¼Œæœ€ååŸºäºæœ€é•¿çš„æ–‡æœ¬æ¥ padding (å¡«å……)ï¼Œæˆ‘ä»¬æŠŠå¾ˆå¤šæ–‡æœ¬æ‹¼æ¥èµ·æ¥ï¼Œç”¨ EOS token æ¥éš”å¼€ï¼Œç„¶ååˆ†å‰²æˆä¸€äº› chunk (åˆ‡å—) æ¥åšæˆ batchï¼Œé¿å… paddingã€‚

![](https://huggingface.co/datasets/trl-internal-testing/example-images/resolve/main/blog/stackllama/chapter10_preprocessing-clm.png)

è¯¥æ–¹æ³•å¤§å¤§æé«˜äº†æ•ˆç‡ï¼Œå› ä¸ºæ¨¡å‹è¾“å…¥çš„æ‰€æœ‰ token éƒ½å¯¹ loss æœ‰æ‰€è®­ç»ƒï¼Œè€Œé padding ä½œä¸ºæ©ç è¢«ä¸¢å¼ƒäº†ã€‚å¦‚æœä½ æ²¡æœ‰è¶³å¤Ÿæ•°æ®ï¼Œå¹¶ä¸”æ‹…å¿ƒéšæ„åœ°åˆ†å¼€ token ä¼šå¤±å»ä¸Šä¸‹æ–‡è¯­ä¹‰ï¼Œä½ ä¹Ÿå¯ä»¥ç”¨ä¼ ç»Ÿçš„æ•°æ®åŠ è½½å™¨
 `ConstantLengthDataset` è§£å†³äº† **æ‰“åŒ…**æŠ€æœ¯ï¼Œå¹¶ä¸”æˆ‘ä»¬èƒ½åœ¨ç”¨ `peft` åŠ è½½æ¨¡å‹åç”¨ `Trainer`ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬ç”¨ `int8` åŠ è½½æ¨¡å‹ï¼Œå‡†å¤‡è®­ç»ƒï¼Œç„¶ååŠ å…¥ `LoRA` å¾®è°ƒå™¨ã€‚

```python
# load model in 8bit
model = AutoModelForCausalLM.from_pretrained(
        args.model_path,
        load_in_8bit=True,
        device_map={"": Accelerator().local_process_index}
    )
model = prepare_model_for_int8_training(model)

# add LoRA to model
lora_config = LoraConfig(
    r=16,
    lora_alpha=32,
    lora_dropout=0.05,
    bias="none",
    task_type="CAUSAL_LM",
)

model = get_peft_model(model, config)
```

æˆ‘ä»¬æ ¹æ®ç›¸åº”çš„è¯­è¨€ä»»åŠ¡ï¼Œå¯¹æ¨¡å‹è®­ç»ƒå‡ åƒä¸ª step (æ­¥)ï¼Œå¹¶ä¿å­˜æ¨¡å‹ã€‚ç”±äºæˆ‘ä»¬å°†ä¼šæœ‰å…¶ä»–å¾®è°ƒæ¨¡å‹çš„ç›®çš„ï¼Œæˆ‘ä»¬å°† LoRA çš„å¾®è°ƒå™¨æƒé‡åˆå¹¶åˆ°åŸæ¨¡å‹ä¸­ã€‚


 **å£°æ˜**: å› ä¸º LLaMA çš„è®¸å¯è¯è§„å®šï¼Œæˆ‘ä»¬åªèƒ½å‘å¸ƒå¾®è°ƒå™¨çš„æƒé‡ï¼Œä½ éœ€è¦å¡« Meta AI çš„ [è¡¨æ ¼](https://docs.google.com/forms/d/e/1FAIpQLSfqNECQnMkycAp2jP4Z9TFX0cGR4uf7b_fBxjY_OjhJILlKGA/viewform) æ¥è·å–æ¨¡å‹ï¼Œç„¶åç”¨è¿™ä¸ª [è„šæœ¬](https://github.com/huggingface/transformers/blob/main/src/transformers/models/llama/convert_llama_weights_to_hf.py) æ¥è½¬æˆ ğŸ¤— Transformers æ ¼å¼ã€‚æ³¨æ„ ğŸ¤— Transformers åº”è¯¥ä»æºç å®‰è£…ï¼Œæˆ–è€… `v4.28` ç‰ˆã€‚

ç°åœ¨æˆ‘ä»¬å·²ç»å¾®è°ƒå¥½äº†æ¨¡å‹ï¼Œå¯ä»¥è®­ç»ƒå¥–åŠ±æ¨¡å‹äº†ã€‚

## å¥–åŠ±æ¨¡å‹å’Œäººç±»åå¥½

åŸåˆ™ä¸Šï¼Œæˆ‘ä»¬å¯ä»¥ç›´æ¥ç”¨äººç±»æ ‡æ³¨æ¥å¯¹æ¨¡å‹åš RLHF å¾®è°ƒã€‚ç„¶è€Œï¼Œè¿™å°†éœ€è¦æˆ‘ä»¬ç»™äººç±»å‘é€ä¸€äº›æ ·æœ¬ï¼Œåœ¨æ¯è½®ä¼˜åŒ–åè®¡åˆ†ã€‚è¿™æ˜¯è´µä¸”æ…¢çš„ï¼Œå› ä¸ºæ”¶æ•›éœ€è¦çš„è®­ç»ƒæ ·æœ¬é‡å¤§ï¼Œè€Œäººç±»é˜…è¯»å’Œæ ‡æ³¨çš„é€Ÿåº¦æœ‰é™ã€‚

ä¸€ä¸ªæ¯”ç›´æ¥åé¦ˆæ›´å¥½çš„ç­–ç•¥æ˜¯ï¼Œåœ¨è¿›å…¥ RL å¾ªç¯ä¹‹å‰ç”¨äººç±»æ ‡æ³¨é›†æ¥è®­ç»ƒä¸€ä¸ªå¥–åŠ±æ¨¡å‹ã€‚å¥–åŠ±æ¨¡å‹çš„ç›®çš„æ˜¯æ¨¡æ‹Ÿäººç±»å¯¹æ–‡æœ¬çš„æ‰“åˆ†ã€‚æ„å»ºå¥–åŠ±æ¨¡å‹æœ‰è®¸å¤šèƒ½ç”¨çš„ç­–ç•¥: æœ€ç›´æ¥çš„ä¾¿æ˜¯é¢„æµ‹æ ‡æ³¨ (æ¯”å¦‚æ ¹æ®å¥½ä¸åï¼Œè¾“å‡ºæ¯”åˆ†æˆ–è€…å¸ƒå°”å€¼)ã€‚æœ€ä½³å®è·µæ˜¯ï¼Œé¢„æµ‹ç»“æœçš„æ’åºï¼Œå³å¯¹æ¯ä¸ª prompt (è¾“å…¥æ–‡æœ¬) å¯¹åº”çš„ä¸¤ä¸ªç»“æœ $(y_k, y_j)$ï¼Œæ¨¡å‹é¢„æµ‹äººç±»æ ‡æ³¨çš„æ¯”åˆ†å“ªä¸ªæ›´é«˜ã€‚

æˆ–è€…è¡¨ç¤ºä¸º loss (æŸå¤±) å‡½æ•°:

$$
\mbox{loss}(\theta) = - E_{(x, y_j, y_k)~D} [ \mbox{log}( \sigma( r_\theta (x, y_j) - r_\theta(x, y_k)) ) ]
$$

å…¶ä¸­ $r$ æ˜¯æ¨¡å‹å¯¹å¯èƒ½çš„æ ‡æ³¨ $y_j$ çš„é¢„æµ‹åˆ†æ•°ã€‚

åœ¨ StackExchange æ•°æ®é›†ä¸Šï¼Œæˆ‘ä»¬èƒ½å¾—åˆ°ä¸¤ä¸ªç­”æ¡ˆçš„å—æ¬¢è¿ç¨‹åº¦ã€‚æœ‰äº†è¿™ä¸ªä¿¡æ¯å’Œä¸Šé¢çš„æŸå¤±å‡½æ•°ï¼Œæˆ‘ä»¬å°±èƒ½è‡ªå®šä¹‰ loss æ¥æ”¹ `transformers.Trainer` äº†ã€‚

```python

class RewardTrainer(Trainer):
    def compute_loss(self, model, inputs, return_outputs=False):
        rewards_j = model(input_ids=inputs["input_ids_j"], attention_mask=inputs["attention_mask_j"])[0]
        rewards_k = model(input_ids=inputs["input_ids_k"], attention_mask=inputs["attention_mask_k"])[0]
        loss = -nn.functional.logsigmoid(rewards_j - rewards_k).mean()
        if return_outputs:
            return loss, {"rewards_j": rewards_j, "rewards_k": rewards_k}
        return loss
```

æˆ‘ä»¬ç”¨æ•°æ®é›†ä¸­çš„ 100000 å¯¹ï¼Œå¹¶åœ¨ 50000 å¯¹ä¸Šè¯„ä¼°ã€‚åœ¨æ¯”è¾ƒå°çš„ batch sizeï¼Œä¸º 4 ä¸‹ï¼Œæˆ‘ä»¬ç”¨ LoRA çš„  `peft` å¾®è°ƒå™¨æ¥è®­ç»ƒ LLaMA æ¨¡å‹ï¼Œåœ¨ BF16 ç²¾åº¦ä¸‹ç”¨ Adam ä¼˜åŒ–å™¨ã€‚æˆ‘ä»¬çš„ LoRA è®¾ç½®æ˜¯:

```python
peft_config = LoraConfig(
    task_type=TaskType.SEQ_CLS,
    inference_mode=False,
    r=8,
    lora_alpha=32,
    lora_dropout=0.1,
)
```

è®­ç»ƒç”¨ [Weights & Biases](https://wandb.ai/krasul/huggingface/runs/wmd8rvq6?workspace=user-krasul) æ¥è®°æ—¥å¿—ï¼Œå¹¶åœ¨ ğŸ¤— è®­ç»ƒé›†ç¾¤ä¸Šï¼Œç”¨ 8 å¡ A-100ï¼Œè¦æ•°å°æ—¶ï¼Œæœ€åå‡†ç¡®ç‡ä¸º **67%**ã€‚å°½ç®¡çœ‹ä¸Šå»å¯èƒ½ä½äº†ï¼Œä½†æƒ³æƒ³è¿™ä¸ªä»»åŠ¡çš„éš¾åº¦ã€‚

å¦‚ä¸‹æ–‡è¦ç»†è¯´çš„ï¼Œè®­ç»ƒç»“æœå°†ä½œä¸ºå›ºå®šå‚æ•°ï¼Œä»¥ä¾›ä¸‹æ¸¸ä½¿ç”¨ã€‚

## åŸºäºäººç±»åé¦ˆçš„å¼ºåŒ–å­¦ä¹ 

ç°åœ¨æˆ‘ä»¬æ‰‹å¤´æœ‰äº†å¾®è°ƒçš„è¯­è¨€æ¨¡å‹å’Œå¥–åŠ±æ¨¡å‹ï¼Œå¯ä»¥å¼€å§‹æ‰§è¡Œ RL å¾ªç¯äº†: è¿™ä¸ªè¿‡ç¨‹å¤§è‡´åˆ†ä¸ºä¸‰æ­¥

1. ç”Ÿæˆå¯¹ prompt (è¾“å…¥æ–‡æœ¬) çš„åé¦ˆã€‚
2. ç”¨å¥–åŠ±æ¨¡å‹æ¥å¯¹åé¦ˆè¯„åˆ†ã€‚
3. å¯¹è¯„åˆ†ï¼Œè¿›è¡Œä¸€è½®ç­–ç•¥ä¼˜åŒ–çš„å¼ºåŒ–å­¦ä¹ ã€‚

![](https://huggingface.co/datasets/trl-internal-testing/example-images/resolve/main/blog/stackllama/trl_loop.png)

åœ¨è¢« token åŒ–å¹¶è¾“å…¥å¥–åŠ±æ¨¡å‹å‰ï¼Œæé—®å’Œå›ç­”çš„ prompt æ¨¡ç‰ˆå¦‚ä¸‹:

```
Question: <Query>
Answer: <Response>
```

åœ¨æœ‰ç›‘ç£è®­ç»ƒ (SFT)ï¼Œå¥–åŠ±æ¨¡å‹è®­ç»ƒ (RM) å’Œ RLHF çš„é˜¶æ®µéƒ½ç”¨æ­¤æ¨¡ç‰ˆã€‚

ç”¨ RL è®­ç»ƒè¯­è¨€æ¨¡å‹å‡ºç°çš„å¸¸è§é—®é¢˜æ˜¯ï¼Œæ¨¡å‹å¯èƒ½å­¦ä¼šèƒ¡è¯´å…«é“ä»¥ç³Šå¼„å¥–åŠ±æ¨¡å‹ï¼Œåè€…å¯èƒ½ç»™é«˜åˆ†ã€‚ä¸ºäº†æƒè¡¡ï¼Œæˆ‘ä»¬å¯¹å¥–åŠ±å¢åŠ æƒ©ç½š: ç•™ä¸€ä»½æ²¡æœ‰è®­ç»ƒçš„æ¨¡å‹ï¼Œå¦‚ä½•æ¯”è¾ƒä¸¤è€…è¾“å‡ºçš„ KL æ•£åº¦

$$
\mbox{R}(x, y) = \mbox{r}(x, y) - \beta \mbox{KL}(x,y)
$$

å…¶ä¸­ $r$ æ˜¯å¥–åŠ±æ¨¡å‹çš„ç»“æœï¼Œ$\mbox{KL}(x,y)$ æ˜¯å½“å‰æ¨¡å‹å’Œå¯¹æ¯”æ¨¡å‹çš„ KL æ•£åº¦å·®ã€‚

å†æä¸€éï¼Œæˆ‘ä»¬ç”¨ `peft` æ¥å®ç°å†…å­˜é«˜æ•ˆçš„è®­ç»ƒï¼Œå…¶å¯¹ RLHF é˜¶æ®µæä¾›äº†ä¼˜åŠ¿ã€‚è¿™é‡Œå‚è€ƒçš„æ¨¡å‹å’Œè®­ç»ƒçš„æ¨¡å‹ç”¨åŒä¸€ä¸ªåŸºåº•ï¼Œä¹Ÿå°±æ˜¯æœ‰ç›‘ç£è®­ç»ƒ (SFT) çš„ç»“æœï¼Œå®ƒæ˜¯ç”¨ 8-bit æ¥åŠ è½½ï¼Œå¹¶ä¸”è‡ªå§‹è‡ªç»ˆæ˜¯å›ºå®šçš„ã€‚æˆ‘ä»¬ä»…ç”¨ PPO æ–¹æ³•ä¼˜åŒ–æœ€ç»ˆæ¨¡å‹çš„ LoRA æƒé‡ï¼ŒåŒæ—¶å…¨éƒ¨å…±äº«ä¸€ä¸ªåŸºåº•æ¨¡å‹ã€‚

```python
for epoch, batch in tqdm(enumerate(ppo_trainer.dataloader)):
    question_tensors = batch["input_ids"]
        
    # sample from the policy and generate responses
    response_tensors = ppo_trainer.generate(
        question_tensors,
        return_prompt=False,
        length_sampler=output_length_sampler,
        **generation_kwargs,
    )
    batch["response"] = tokenizer.batch_decode(response_tensors, skip_special_tokens=True)

    # Compute sentiment score
    texts = [q + r for q, r in zip(batch["query"], batch["response"])]
    pipe_outputs = sentiment_pipe(texts, **sent_kwargs)
    rewards = [torch.tensor(output[0]["score"] - script_args.reward_baseline) for output in pipe_outputs]

    # Run PPO step
    stats = ppo_trainer.step(question_tensors, response_tensors, rewards)
    # Log stats to WandB
    ppo_trainer.log_stats(stats, batch, rewards)
```

æˆ‘ä»¬ç”¨ ğŸ¤— é›†ç¾¤ï¼Œåœ¨ 3x8 A100-80GB çš„æœºå™¨ä¸Šè®­ç»ƒäº† 20hï¼Œä½†ä¸€ä¸ªå·®ä¸å¤šçš„ç»“æœå¾ˆå¿« (å¤§æ¦‚ï¼Œåœ¨ 8 A100-80GB ä¸Šè®­ç»ƒ 20h)ã€‚æ‰€æœ‰çš„è®­ç»ƒè¿‡ç¨‹éƒ½åœ¨ [Weight & Biases](https://wandb.ai/lvwerra/trl/runs/ie2h4q8p) ä¸Šæ‰¾åˆ°ã€‚

![](https://huggingface.co/datasets/trl-internal-testing/example-images/resolve/main/blog/stackllama/wandb_reward.png)

æ¯ä¸ª batch çš„å¥–åŠ±ï¼Œå¯¹æ¯æ­¥çš„è®­ç»ƒï¼Œåœ¨  ~1000 æ­¥æ—¶æ¨¡å‹çš„æ•ˆæœæœ€å¥½ã€‚

æ‰€ä»¥æ¨¡å‹è®­å¥½äº†èƒ½å¹²å•¥å˜ ? æˆ‘ä»¬æ‹­ç›®ä»¥å¾… !

![](https://huggingface.co/datasets/trl-internal-testing/example-images/resolve/main/blog/stackllama/llama_prompt.png)

å°½ç®¡æˆ‘ä»¬ä¸è¯¥å¤ªç›¸ä¿¡å…¶ç»“æœï¼Œè‡³å°‘ç›®å‰ã€‚ä½†ç»“æœå·²ç»å¾ˆå¥½äº†ï¼Œç”šè‡³é™„ä¸Šäº† Google é“¾æ¥ã€‚æˆ‘ä»¬æ¥çœ‹çœ‹è®­ç»ƒæ—¶çš„æŒ‘æˆ˜ã€‚

## æŒ‘æˆ˜ï¼Œä¸ç¨³å®šå’Œçªç ´å£

ç”¨ RL è®­ç»ƒ LLM (Large Language Modelsï¼Œå¤§è¯­è¨€æ¨¡å‹) ä¸æ€»æ˜¯ä¸€å¸†é£é¡ºçš„ï¼Œä½ çœ‹åˆ°çš„æœ¬æ–‡ä¹Ÿæ˜¯ç»å†æ— æ•°å®éªŒï¼Œæ— æ•°å¤±è´¥å’Œæ— æ•°è°ƒå‚çš„ã€‚å³ä¾¿å¦‚æ­¤ï¼Œè¯¥æ¨¡å‹ä¹Ÿä¸èƒ½è¯´å˜ç°å®Œç¾ã€‚è¿™å„¿ï¼Œæˆ‘ä»¬åˆ†äº«ä¸€äº›é‡åˆ°çš„è§‚å¯Ÿå’Œé—®é¢˜ã€‚

### å¥–åŠ±æ›´é«˜ä»£è¡¨æ›´å¥½è¡¨ç° ?

![](https://huggingface.co/datasets/trl-internal-testing/example-images/resolve/main/blog/stackllama/logs_high_reward.png)

å¤©å‘ï¼Œè¿™ä¸ªå®éªŒè‚¯å®šè¡¨ç°å¾ˆå¥½ ! çœ‹å¥–åŠ±çš„æ›²çº¿å¤šç”œå•Š !

åœ¨ RL ä¸­ï¼Œä¸€èˆ¬è€Œè¨€ï¼Œå¥–åŠ±è¶Šé«˜è¶Šå¥½ã€‚åœ¨ RLHF ä¸­ï¼Œæˆ‘ä»¬ç”¨äº†ä¸€ä¸ªå¥–åŠ±æ¨¡å‹ï¼Œå®ƒä¸å®Œç¾ï¼Œæ‰€ä»¥ç•™ç»™äº† PPO ç®—æ³•æ¡æ¼çš„æœºä¼šã€‚è¿™èƒ½å¯¼è‡´å¥–åŠ±çªç„¶ä¸Šå‡ï¼Œç„¶è€Œå½“æ£€æŸ¥æ–‡æœ¬ç»“æœæ—¶ï¼Œå´å……æ–¥äº†å­—ç¬¦ â€œ```â€ï¼Œå› ä¸ºå¥–åŠ±æ¨¡å‹å¯¹å«æœ‰ä»£ç  stack exchange çš„ç­”æ¡ˆæ›´ä¿¡ä»»ã€‚å¹¸è¿çš„æ˜¯ï¼Œè¯¥é—®é¢˜ç¢°åˆ°çš„å¾ˆå°‘ï¼Œåº”è¯¥æ˜¯é‡‡å–çš„ KL æ•£åº¦çš„æƒ©ç½šé¡¹èµ·åˆ°äº†ä½œç”¨ã€‚

### KL æ•£åº¦æ€»æ˜¯æ­£çš„?

å¦‚æˆ‘ä»¬å‰é¢æ‰€æåˆ°çš„ï¼Œä¸€ä¸ª KL æƒ©ç½šé¡¹è¢«ç”¨æ¥ä¿è¯è®­ç»ƒåçš„åˆ†å¸ƒå’ŒåŸå§‹åˆ†å¸ƒæ¥è¿‘ã€‚ä¸€èˆ¬åœ° , KL æ•£åº¦æ¥åº¦é‡ä¸¤ä¸ªåˆ†å¸ƒçš„ç›¸ä¼¼ç¨‹åº¦ï¼Œå¹¶ä¸”æ€»æ˜¯æ­£çš„ã€‚ç„¶è€Œï¼Œåœ¨ `trl` æˆ‘ä»¬ç”¨äº†ä¸€ä¸ª KL çš„è¿‘ä¼¼ï¼ŒæœŸæœ›å€¼å’ŒçœŸçš„ KL æ•£åº¦ç›¸åŒã€‚

$$
KL_{pen} (x, y) = \mbox{log} (\pi_\phi^\mbox{RL}(y | x) / \pi^{\mbox{SFT}}(y|x))
$$

æ˜¾ç„¶ï¼Œå½“è®­ç»ƒä¸­ä¸€ä¸ª token æ¯”åŸå§‹æ¨¡å‹æ¦‚ç‡ä½ï¼Œè¿™ä¼šå¯¼è‡´ KL æ•£åº¦ä¸ºè´Ÿï¼Œåˆé€‚çš„å–æ ·å’Œå¹³å‡æ€»èƒ½å¾—åˆ°æ­£çš„ã€‚ä½†æ˜¯ä¸€äº›é‡‡æ ·çš„ç”Ÿæˆç­–ç•¥å¯¼è‡´äº†ä¸åŒ€ç§°çš„é‡‡æ ·ã€‚æ¯”å¦‚ï¼Œå½“ç”Ÿæˆè¢« padding çš„åºåˆ— batch æ—¶å’Œå½“è®¾ç½® EOS token è¢«å‹ç¼©çš„æœ€å°é•¿åº¦æ˜¯ï¼Œæ¨¡å‹ä¼šæœ‰å¾ˆå¤§/å¾ˆå°çš„æ¦‚ç‡åˆ°è´Ÿ KL æ•£åº¦çš„ tokenã€‚åŒæ—¶ PPO ç®—æ³•æ˜¯é¢å‘å¥–åŠ±ä¼˜åŒ–çš„ï¼Œæ¨¡å‹å°±ä¼šè¿½é€è´Ÿçš„æƒ©ç½šï¼Œå¯¼è‡´è®­ç»ƒä¸ç¨³å®šã€‚

![](https://huggingface.co/datasets/trl-internal-testing/example-images/resolve/main/blog/stackllama/logs_neg_kl.png)

å¯¹ç”Ÿæˆå’Œé‡‡æ ·ï¼Œä½ éœ€è¦ç‰¹åˆ«å°å¿ƒã€‚æˆ‘ä»¬å»ºè®®ä¸€å¼€å§‹ç”¨æœ€ç®€å•çš„æ–¹å¼ï¼Œå¦‚ä½•åœ¨é€æ¸å¤æ‚ã€‚

### ä»»ç„¶å­˜åœ¨çš„é—®é¢˜

ä»»ç„¶æœ‰å¾ˆå¤šé—®é¢˜æˆ‘ä»¬ä¸æ‡‚ï¼Œæ¯”å¦‚ä¸‹é¢ï¼Œloss é—´æ–­åœ°è·³è·ƒï¼Œå¯¼è‡´ä¹‹åçš„ä¸ç¨³å®š

![](https://huggingface.co/datasets/trl-internal-testing/example-images/resolve/main/blog/stackllama/logs_loss_spikes.png)

ä¸€æ—¦æˆ‘ä»¬è§£å†³äº†è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬å°±ä¼šä¸Šä¼ å˜åŒ–åˆ° `trl` ä¸Šï¼Œä»¥ä¿è¯ç¤¾åŒºå—ç›Šã€‚

## æ€»ç»“

åœ¨æœ¬åšå®¢ï¼Œæˆ‘ä»¬èµ°è¿‡äº† RLHF è®­ç»ƒçš„æ•´ä¸ªæµç¨‹ï¼Œä»å‡†å¤‡äººç±»æ ‡æ³¨çš„æ•°æ®é›†å¼€å§‹ï¼Œè°ƒæ•´è¯­è¨€æ¨¡å‹åˆ°ç‰¹å®šé¢†åŸŸï¼Œè®­ç»ƒå¥–åŠ±æ¨¡å‹ï¼Œå¹¶æœ€ç»ˆç”¨ RL è®­ç»ƒä¸€ä¸ªæ¨¡å‹ã€‚

é€šè¿‡ä½¿ç”¨ `peft`ï¼Œä»»ä½•äººéƒ½èƒ½åœ¨ä¸€å¼  GPU ä¸Šè·‘æˆ‘ä»¬çš„å®éªŒ ! å¦‚æœè®­ç»ƒæ…¢äº†ï¼Œå¯ä»¥ç”¨æ•°æ®å¹¶è¡ŒåŒ–çš„æ–¹æ³•ï¼Œä¸éœ€è¦æ”¹ä»»ä½•ä»£ç ï¼Œæˆ–è€…ç”¨å¤šå¼  GPU å¹¶è¡Œæé«˜è®­ç»ƒé€Ÿåº¦ã€‚

å¯¹å®é™…åº”ç”¨ï¼Œè¿™ä»…ä»…æ˜¯ç¬¬ä¸€æ­¥ ! ä¸€æ—¦ä½ æœ‰äº†æ¨¡å‹ï¼Œä½ å°±è¦å’Œå…¶ä»–æ¨¡å‹æ¯”è¾ƒä¼˜åŠ£ã€‚è¿™ä¸ªå¯ä»¥ç”¨ä¸€ä¸ªé¢å‘ä¸åŒæ¨¡å‹çš„æ’åç”Ÿæˆåšåˆ°ï¼Œå’Œæˆ‘ä»¬è®­ç»ƒå¥–åŠ±æ•°æ®é›†ç±»ä¼¼ã€‚

ä¸€æ—¦ä½ åŠ å…¥äº†è¯„ä¼°çš„æ­¥éª¤ï¼Œå¥½ç©çš„å°±å¼€å§‹äº†: ä½ å¯ä»¥åœ¨åŸæ•°æ®é›†ä¸Šåå¤ç‚¼ä¸¹ï¼Œä¹Ÿå¯ä»¥å¢åŠ æ•°æ®é›†æˆ–è€…å¯¹åŸæ•°æ®é›†æçº¯ã€‚å¦å¤–ï¼Œä½ å¯ä»¥å¯¹å¥–åŠ±æ¨¡å‹å’Œç”Ÿæˆè¯•ä¸åŒå¤§å°å’Œç»“æ„çš„æ¨¡å‹ï¼Œè¿™éœ€è¦æ—¶é—´ã€‚

æˆ‘ä»¬åœ¨ç§¯ææé«˜ TRL ä»¥ä¿è¯ RLHF çš„æ¯ä¸€æ­¥éƒ½å¯è§ï¼Œå¹¶ä¸”ååˆ†æ¿€åŠ¨èƒ½çœ‹åˆ°äººä»¬ç”¨å®ƒæ¥æ„å»ºçš„ä¸œè¥¿ã€‚å¦‚æœä½ æƒ³æœ‰æ‰€è´¡çŒ®ï¼Œæ¬¢è¿çœ‹æˆ‘ä»¬çš„ [Github Issue](https://github.com/lvwerra/trl/issues)ã€‚

## å¼•ç”¨

```
@misc {beeching2023stackllama,
    author = { Edward Beeching and
                     Younes Belkada and
                     Kashif Rasul and
                     Lewis Tunstall and
                     Leandro von Werra and
                     Nazneen Rajani and
                     Nathan Lambert
                   },
    title = { StackLLaMA: An RL Fine-tuned LLaMA Model for Stack Exchange Question and Answering },
    year = 2023,
    url = { https://huggingface.co/blog/stackllama },
    doi = { 10.57967/hf/0513 },
    publisher = { Hugging Face Blog }
}
```

## æ„Ÿè°¢

æˆ‘ä»¬æ„Ÿè°¢ Philipp Schmid åˆ†äº«äº†ä»–å¯¹æ–‡æœ¬ç”Ÿæˆç»å¦™çš„ [demo](https://huggingface.co/spaces/philschmid/igel-playground), æˆ‘ä»¬çš„ demo ä¹Ÿæ˜¯åŸºäºä»–çš„ã€‚æˆ‘ä»¬ä¹Ÿæ„Ÿè°¢ Omar Sanseviero å’Œ Louis Castricato å¯¹æˆ‘ä»¬åšå®¢çš„è‰ç¨¿æä¾›å®è´µè¯¦å°½çš„åé¦ˆã€‚

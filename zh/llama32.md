---
title: "ç°åœ¨Llamaå…·å¤‡è§†è§‰èƒ½åŠ›å¹¶å¯ä»¥åœ¨ä½ çš„è®¾å¤‡ä¸Šè¿è¡Œ - æ¬¢è¿Llama 3.2" 
thumbnail: /blog/assets/llama32/thumbnail.jpg
authors:
- user: merve
- user: philschmid
- user: osanseviero
- user: reach-vb
- user: lewtun
- user: ariG23498
- user: pcuenq
translators:
- user: cheninwang
---

# ç°åœ¨Llamaå…·å¤‡è§†è§‰èƒ½åŠ›å¹¶å¯ä»¥åœ¨ä½ çš„è®¾å¤‡ä¸Šè¿è¡Œ - æ¬¢è¿Llama 3.2

Llama 3.2 å‘å¸ƒäº†ï¼ä»Šå¤©ï¼Œæˆ‘ä»¬æ¬¢è¿ Llama ç³»åˆ—çš„ä¸€ä¸ªæ–°ç‰ˆæœ¬æ¥åˆ° Hugging Faceã€‚è¿™ä¸€æ¬¡ï¼Œæˆ‘ä»¬å¾ˆé«˜å…´ä¸ Meta åˆä½œå‘å¸ƒå¤šæ¨¡æ€å’Œå°å‹æ¨¡å‹ã€‚åä¸ªå¼€æºæƒé‡æ¨¡å‹ï¼ˆ5ä¸ªå¤šæ¨¡æ€æ¨¡å‹å’Œ5ä¸ªä»…æ–‡æœ¬æ¨¡å‹ï¼‰ç°å·²åœ¨ Hub ä¸Šå¯ç”¨ã€‚

Llama 3.2 è§†è§‰æ¨¡å‹æœ‰ä¸¤ç§å¤§å°ï¼š11Bï¼Œç”¨äºåœ¨æ¶ˆè´¹çº§ GPU ä¸Šçš„é«˜æ•ˆéƒ¨ç½²å’Œå¼€å‘ï¼Œä»¥åŠ 90Bï¼Œç”¨äºå¤§è§„æ¨¡åº”ç”¨ã€‚è¿™ä¸¤ç§ç‰ˆæœ¬å‡æä¾›åŸºç¡€ç‰ˆå’ŒæŒ‡ä»¤è°ƒä¼˜ç‰ˆã€‚æ­¤å¤–ï¼ŒMeta è¿˜å‘å¸ƒäº†å¸¦æœ‰è§†è§‰æ”¯æŒçš„æ–°ç‰ˆæœ¬ Llama Guardï¼ŒLlama Guard 3 æ˜¯ä¸€ä¸ªå®‰å…¨æ¨¡å‹ï¼Œå¯ä»¥å¯¹æ¨¡å‹è¾“å…¥å’Œç”Ÿæˆå†…å®¹è¿›è¡Œç”„åˆ«ï¼ŒåŒ…æ‹¬æ£€æµ‹æœ‰å®³çš„å¤šæ¨¡æ€æç¤ºæˆ–åŠ©æ‰‹å“åº”ã€‚

Llama 3.2 è¿˜åŒ…æ‹¬å¯ä»¥åœ¨è®¾å¤‡ä¸Šè¿è¡Œçš„å°å‹ä»…æ–‡æœ¬è¯­è¨€æ¨¡å‹ã€‚å®ƒä»¬æœ‰ä¸¤ç§æ–°å¤§å°ï¼ˆ1B å’Œ 3Bï¼‰ï¼Œå¹¶æä¾›åŸºç¡€ç‰ˆå’ŒæŒ‡ä»¤ç‰ˆï¼Œå…·æœ‰å¼ºå¤§çš„èƒ½åŠ›ã€‚è¿˜æœ‰ä¸€ä¸ªå°å‹ 1B ç‰ˆæœ¬çš„ Llama Guardï¼Œå¯ä»¥ä¸è¿™äº›æˆ–æ›´å¤§çš„æ–‡æœ¬æ¨¡å‹ä¸€èµ·éƒ¨ç½²åœ¨ç”Ÿäº§ç”¨ä¾‹ä¸­ã€‚

åœ¨å‘å¸ƒçš„åŠŸèƒ½å’Œé›†æˆä¸­ï¼Œæˆ‘ä»¬æœ‰ï¼š
- [Hub ä¸Šçš„æ¨¡å‹æ£€æŸ¥ç‚¹](https://huggingface.co/collections/meta-llama/llama-32-66f448ffc8c32f949b04c8cf)
- Hugging Face Transformers å’Œ TGI å¯¹è§†è§‰æ¨¡å‹çš„é›†æˆ
- åœ¨Google Cloudã€Amazon SageMaker å’Œ DELL ä¼ä¸šä¸­å¿ƒçš„æ¨ç†ä¸éƒ¨ç½²é›†æˆ
- ä½¿ç”¨ [transformersğŸ¤—](https://github.com/huggingface/huggingface-llama-recipes/tree/main/Llama-Vision%20FT.ipynb) å’Œ [TRL](https://github.com/huggingface/trl/tree/main/examples/scripts/sft_vlm.py) åœ¨å•ä¸ª GPU ä¸Šå¾®è°ƒ Llama 3.2 11B è§†è§‰æ¨¡å‹

## ç›®å½•

- [ä»€ä¹ˆæ˜¯ Llama 3.2 è§†è§‰æ¨¡å‹ï¼Ÿ](#ä»€ä¹ˆæ˜¯Llama32è§†è§‰æ¨¡å‹)
- [Llama 3.2 è®¸å¯å˜æ›´ã€‚æŠ±æ­‰ï¼Œæ¬§ç›Ÿ](#Llama-32-è®¸å¯å˜æ›´-æŠ±æ­‰-æ¬§ç›Ÿç”¨æˆ·)
- [Llama 3.2 1B å’Œ 3B çš„ç‰¹åˆ«ä¹‹å¤„ï¼Ÿ](#Llama-32-1Bå’Œ3Bæœ‰ä»€ä¹ˆç‰¹åˆ«ä¹‹å¤„)
- [æ¼”ç¤º](#æ¼”ç¤º)
- [ä½¿ç”¨ Hugging Face Transformers](#ä½¿ç”¨Hugging-Face-Transformers)
- [Llama 3.2 1B å’Œ 3B è¯­è¨€æ¨¡å‹](#Llama-32-1Bå’Œ3Bè¯­è¨€æ¨¡å‹)
- [Llama 3.2 è§†è§‰æ¨¡å‹](#Llama-32-è§†è§‰æ¨¡å‹)
- [è®¾å¤‡ç«¯éƒ¨ç½²](#è®¾å¤‡ç«¯éƒ¨ç½²)
- [Llama.cpp å’Œ Llama-cpp-python](#llamacpp--llama-cpp-python)
- [Transformers.js](#transformersjs)
- [å¾®è°ƒ Llama 3.2](#å¾®è°ƒ-llama-32)
- [Hugging Face åˆä½œä¼™ä¼´é›†æˆ](#Hugging-Face-åˆä½œä¼™ä¼´é›†æˆ)
- [å…¶ä»–èµ„æº](#é¢å¤–èµ„æº)
- [è‡´è°¢](#é¸£è°¢)

## ä»€ä¹ˆæ˜¯Llama3.2è§†è§‰æ¨¡å‹ï¼Ÿ

Llama 3.2 è§†è§‰æ¨¡å‹æ˜¯ Meta å‘å¸ƒçš„æœ€å¼ºå¤§çš„å¼€æºå¤šæ¨¡æ€æ¨¡å‹ã€‚å®ƒå…·æœ‰å‡ºè‰²çš„è§†è§‰ç†è§£å’Œæ¨ç†èƒ½åŠ›ï¼Œå¯ä»¥ç”¨äºå®Œæˆå„ç§ä»»åŠ¡ï¼ŒåŒ…æ‹¬è§†è§‰æ¨ç†ä¸å®šä½ã€æ–‡æ¡£é—®ç­”å’Œå›¾åƒ-æ–‡æœ¬æ£€ç´¢ã€‚æ€ç»´é“¾ï¼ˆCoTï¼‰ç­”æ¡ˆé€šå¸¸éå¸¸ä¼˜ç§€ï¼Œä½¿å¾—è§†è§‰æ¨ç†ç‰¹åˆ«å¼ºå¤§ã€‚

Llama 3.2 è§†è§‰æ¨¡å‹å¯ä»¥å¤„ç†æ–‡æœ¬å’Œå›¾åƒï¼Œä¹Ÿå¯ä»¥ä»…å¤„ç†æ–‡æœ¬ã€‚å¯¹äºå›¾åƒ-æ–‡æœ¬æç¤ºï¼Œæ¨¡å‹å¯ä»¥æ¥å—è‹±æ–‡è¾“å…¥ï¼Œè€Œå¯¹äºä»…æ–‡æœ¬æç¤ºï¼Œæ¨¡å‹å¯ä»¥å¤„ç†å¤šç§è¯­è¨€ã€‚åœ¨ä»…æ–‡æœ¬æ¨¡å¼ä¸‹ï¼Œæ”¯æŒçš„å®Œæ•´è¯­è¨€åˆ—è¡¨åŒ…æ‹¬è‹±è¯­ã€å¾·è¯­ã€æ³•è¯­ã€æ„å¤§åˆ©è¯­ã€è‘¡è„ç‰™è¯­ã€å°åœ°è¯­ã€è¥¿ç­ç‰™è¯­å’Œæ³°è¯­ã€‚

è¿™äº›æ¨¡å‹çš„æ¶æ„åŸºäº Llama 3.1 LLM ä¸è§†è§‰å¡”å’Œå›¾åƒé€‚é…å™¨çš„ç»„åˆã€‚ç”¨äº Llama 3.2 11B è§†è§‰æ¨¡å‹çš„æ–‡æœ¬æ¨¡å‹æ˜¯ Llama 3.1 8Bï¼Œè€Œ Llama 3.2 90B è§†è§‰æ¨¡å‹åˆ™ä½¿ç”¨ Llama 3.1 70Bã€‚æ ¹æ®æˆ‘ä»¬çš„äº†è§£ï¼Œæ–‡æœ¬æ¨¡å‹åœ¨è§†è§‰æ¨¡å‹è®­ç»ƒæœŸé—´æ˜¯å†»ç»“çš„ï¼Œä»¥ä¿æŒä»…æ–‡æœ¬çš„æ€§èƒ½ã€‚

ä¸‹é¢æ˜¯ä¸€äº›æ¥è‡ª 11B æŒ‡ä»¤è°ƒä¼˜æ¨¡å‹çš„æ¨ç†ç¤ºä¾‹ï¼Œå±•ç¤ºäº†çœŸå®ä¸–ç•ŒçŸ¥è¯†ã€æ–‡æ¡£æ¨ç†å’Œä¿¡æ¯å›¾ç†è§£èƒ½åŠ›ã€‚


<table>
  <tr>
    <td style="vertical-align: middle;"><img src="https://huggingface.co/datasets/huggingface/release-assets/resolve/main/wat_arun.jpg" width="300"/></td>
    <td style="vertical-align: middle;">
      <strong>è¿™å¼ å›¾ç‰‡ä½äºå“ªé‡Œï¼Ÿé™„è¿‘è¿˜æœ‰å“ªäº›æ™¯ç‚¹ï¼Ÿ</strong><br>
      å›¾ç‰‡æç»˜çš„æ˜¯æ³°å›½æ›¼è°·çš„å¤§çš‡å®«ã€‚å¤§çš‡å®«æ˜¯ä¸€ä¸ªå»ºç­‘ç¾¤ï¼Œæ›¾ä½œä¸ºæš¹ç½—å›½ç‹çš„å®˜æ–¹ä½æ‰€ï¼Œå¹¶ä½œä¸ºæ”¿åºœçš„è¡Œæ”¿ä¸­å¿ƒè¾¾150å¹´ä¹‹ä¹…ã€‚å®ƒæ˜¯æ›¼è°·æœ€å—æ¬¢è¿çš„æ—…æ¸¸æ™¯ç‚¹ä¹‹ä¸€ï¼Œæ˜¯ä»»ä½•è®¿é—®è¯¥å¸‚çš„äººå¿…å»ä¹‹åœ°ã€‚<br><br>
      å¤§çš‡å®«é™„è¿‘çš„ä¸€äº›å…¶ä»–æ™¯ç‚¹åŒ…æ‹¬ï¼š<br>
      * ç‰ä½›å¯ºï¼ˆWat Phra Kaewï¼‰ï¼šè¿™åº§å¯ºåº™ä½äºå¤§çš‡å®«å¤åˆåŒºå†…ï¼Œæ‹¥æœ‰è‘—åçš„ç‰ä½›åƒã€‚<br>
      * éƒ‘ç‹åº™ï¼ˆWat Arunï¼‰ï¼šè¿™åº§å¯ºåº™ä½äºæ¹„å—æ²³çš„è¥¿å²¸ï¼Œä»¥å…¶æƒŠè‰³çš„å»ºç­‘å’Œç¾ä¸½çš„åŸå¸‚æ™¯è§‚è€Œé—»åã€‚<br>
      * å§ä½›å¯ºï¼ˆWat Phoï¼‰ï¼šè¿™åº§å¯ºåº™é è¿‘å¤§çš‡å®«ï¼Œæ‹¥æœ‰ä¸€å°Šå·¨å¤§çš„å§ä½›åƒã€‚<br>
      * éš†æ¯”å°¼å…¬å›­ï¼ˆLumpini Parkï¼‰ï¼šè¿™ä¸ªå…¬å›­ä½äºæ›¼è°·ä¸­å¿ƒï¼Œæä¾›äº†ä¸€ä¸ªè¿œç¦»åŸå¸‚å–§åš£çš„å®é™é¿é£æ¸¯ã€‚<br>
      * å‰å§†Â·æ±¤æ™®æ£®æ•…å±…ï¼ˆJim Thompson Houseï¼‰ï¼šè¿™åº§åšç‰©é¦†å±•ç¤ºäº†ç¾å›½å•†äººå‰å§†Â·æ±¤æ™®æ£®çš„ç”Ÿæ´»å’Œå·¥ä½œï¼Œä»–åœ¨20ä¸–çºª50å¹´ä»£å’Œ60å¹´ä»£å¸®åŠ©æŒ¯å…´äº†æ³°å›½ä¸ç»¸äº§ä¸šã€‚
    </td>
  </tr>
  <tr>
    <td style="vertical-align: middle;"><img src="https://huggingface.co/datasets/huggingface/release-assets/resolve/main/weather_events.png" width="300"/></td>
    <td style="vertical-align: middle;">
      <strong>æ ¹æ®è¿™å¼ å›¾è¡¨ï¼Œå¹²æ—±å‘ç”Ÿåœ¨ä»€ä¹ˆåœ°æ–¹ï¼Ÿ</strong><br>
      <strong>2016 å¹´å¹²æ—±åŒºåŸŸï¼š</strong>æ ¹æ®å›¾è¡¨ï¼Œ2016 å¹´å‘ç”Ÿå¹²æ—±çš„åœ°åŒºæœ‰ä¸¤ä¸ªï¼š<br>
      * <strong>ä¸œéå’Œå—éï¼š</strong>è¯¥åœ°åŒºç»å†äº†ä¸¥é‡çš„å¹²æ—±ã€‚<br>
      * <strong>æ™ºåˆ©ï¼š</strong>æ™ºåˆ©åœ¨ 1 æœˆä»½ç»å†äº† 50 å¤šå¹´æ¥æœ€å¹²ç‡¥çš„å¤©æ°”ã€‚è¿™äº›åœ°åŒºåœ¨é‚£æ®µæ—¶é—´é¢ä¸´ä¸¥é‡çš„æ°´èµ„æºçŸ­ç¼ºé—®é¢˜ã€‚
    </td>
  </tr>
  <tr>
    <td style="vertical-align: middle;"><img src="https://huggingface.co/datasets/huggingface/release-assets/resolve/main/invoice.png" width="300"/></td>
    <td style="vertical-align: middle;">
      <strong>ä»å‘ç¥¨æ—¥æœŸåˆ°åˆ°æœŸæ—¥æœŸéœ€è¦å¤šé•¿æ—¶é—´ï¼Ÿè¯·ç®€æ´æ˜äº†ã€‚</strong><br>
      <strong>å‘ç¥¨æ—¥æœŸï¼š</strong>2019 å¹´ 2 æœˆ 11 æ—¥<br>
      <strong>åˆ°æœŸæ—¥æœŸï¼š</strong>2019 å¹´ 2 æœˆ 26 æ—¥<br>
      <strong>æ—¶é—´å·®è®¡ç®—ï¼š</strong><br>
      2019å¹´2æœˆ26æ—¥ - 2019å¹´2æœˆ11æ—¥ = 15 å¤©<br><br>
      å‘ç¥¨æ—¥æœŸä¸åˆ°æœŸæ—¥æœŸä¹‹é—´çš„æ—¶é—´å·®ä¸º**15 å¤©**ã€‚
    </td>
  </tr>
</table>

è§†è§‰æ¨¡å‹çš„ä¸Šä¸‹æ–‡é•¿åº¦ä¸º 128k ä¸ªæ ‡è®°ï¼Œè¿™å…è®¸åŒ…å«å›¾åƒçš„å¤šè½®å¯¹è¯ã€‚ç„¶è€Œï¼Œè¯¥æ¨¡å‹åœ¨å…³æ³¨å•ä¸€å›¾åƒæ—¶æ•ˆæœæœ€ä½³ï¼Œå› æ­¤`transformers`å®ç°ä»…å…³æ³¨è¾“å…¥ä¸­çš„æœ€åä¸€å¼ å›¾åƒã€‚è¿™å¯ä»¥ä¿æŒè´¨é‡å¹¶èŠ‚çœå†…å­˜ã€‚

11B åŸºç¡€æ¨¡å‹æ”¯æŒ 448 çš„åˆ†å—å°ºå¯¸ï¼Œè€ŒæŒ‡ä»¤å¾®è°ƒç‰ˆæœ¬å’Œ 90B æ¨¡å‹éƒ½ä½¿ç”¨ 560 çš„åˆ†å—å°ºå¯¸ã€‚è¿™äº›æ¨¡å‹åœ¨ä¸€ä¸ªåŒ…å« 60 äº¿å›¾æ–‡å¯¹çš„æµ·é‡æ•°æ®é›†ä¸Šè¿›è¡Œäº†è®­ç»ƒï¼Œæ•°æ®æ¥æºéå¸¸å¤šæ ·åŒ–ã€‚è¿™ä½¿å¾—å®ƒä»¬æˆä¸ºä¸‹æ¸¸ä»»åŠ¡å¾®è°ƒçš„æä½³å€™é€‰æ¨¡å‹ã€‚ä¸‹è¡¨å±•ç¤ºäº† 11Bã€90B æ¨¡å‹åŠå…¶æŒ‡ä»¤å¾®è°ƒç‰ˆæœ¬åœ¨ä¸€äº›åŸºå‡†æµ‹è¯•ä¸­çš„è¡¨ç°ï¼Œæ•°æ®æ¥è‡ª Metaã€‚æœ‰å…³æ›´å¤šåŸºå‡†æµ‹è¯•å’Œç»†èŠ‚ï¼Œè¯·å‚è€ƒæ¨¡å‹å¡ã€‚


|            | 11B               | 11B (instruction-tuned) | 90B               | 90B (instruction-tuned) | Metric | 
|------------|-------------------|-----------------|-------------------|------------------|------------------|
| MMMU (val) | 41.7 | 50.7 (CoT)      | 49.3 (zero-shot) | 60.3 (CoT)       | Micro Average Accuracy |
| VQAv2      | 66.8 (val)       | 75.2 (test)     | 73.6 (val)       | 78.1 (test)      | Accuracy |
| DocVQA     | 62.3 (val)       | 88.4 (test)     | 70.7 (val)       | 90.1 (test)      | ANLS |
| AI2D       | 62.4             | 91.1            | 75.3             | 92.3             | Accuracy |

æˆ‘ä»¬é¢„è®¡è¿™äº›æ¨¡å‹çš„æ–‡æœ¬èƒ½åŠ›å°†ä¸8Bå’Œ70Bçš„Llama 3.1æ¨¡å‹ç›¸å½“ï¼Œå› ä¸ºæˆ‘ä»¬çš„ç†è§£æ˜¯æ–‡æœ¬æ¨¡å‹åœ¨è§†è§‰æ¨¡å‹è®­ç»ƒæœŸé—´æ˜¯å†»ç»“çš„ã€‚å› æ­¤ï¼Œæ–‡æœ¬åŸºå‡†æµ‹è¯•åº”è¯¥ä¸8Bå’Œ70Bä¸€è‡´ã€‚

## Llama 3.2 è®¸å¯å˜æ›´, æŠ±æ­‰, æ¬§ç›Ÿç”¨æˆ·

![License Change](https://huggingface.co/datasets/huggingface/release-assets/resolve/main/license_change.png)

å…³äºè®¸å¯æ¡æ¬¾ï¼ŒLlama 3.2 çš„è®¸å¯ä¸Llama 3.1 éå¸¸ç›¸ä¼¼ï¼Œå”¯ä¸€çš„å…³é”®åŒºåˆ«åœ¨äºå¯æ¥å—ä½¿ç”¨æ”¿ç­–ï¼šä»»ä½•å±…ä½åœ¨æ¬§ç›Ÿçš„ä¸ªäººæˆ–åœ¨æ¬§ç›Ÿæœ‰ä¸»è¦è¥ä¸šåœ°ç‚¹çš„å…¬å¸ä¸è¢«æˆäºˆä½¿ç”¨ Llama 3.2 ä¸­åŒ…å«çš„å¤šæ¨¡æ€æ¨¡å‹çš„è®¸å¯æƒã€‚è¿™ä¸€é™åˆ¶ä¸é€‚ç”¨äºé›†æˆäº†ä»»ä½•æ­¤ç±»å¤šæ¨¡æ€æ¨¡å‹çš„äº§å“æˆ–æœåŠ¡çš„æœ€ç»ˆç”¨æˆ·ï¼Œå› æ­¤äººä»¬ä»ç„¶å¯ä»¥æ„å»ºå…¨çƒäº§å“ä¸è§†è§‰å˜ä½“ã€‚

æœ‰å…³è¯¦ç»†ä¿¡æ¯ï¼Œè¯·åŠ¡å¿…é˜…è¯»[å®˜æ–¹è®¸å¯](https://huggingface.co/meta-llama/Llama-3.2-1B/blob/main/LICENSE.txt)å’Œ[å¯æ¥å—ä½¿ç”¨æ”¿ç­–](https://huggingface.co/meta-llama/Llama-3.2-1B/blob/main/USE_POLICY.md)ã€‚

## Llama 3.2 1B å’Œ 3B æœ‰ä»€ä¹ˆç‰¹åˆ«ä¹‹å¤„ï¼Ÿ

Llama 3.2 ç³»åˆ—åŒ…æ‹¬ 1B å’Œ 3B æ–‡æœ¬æ¨¡å‹ã€‚è¿™äº›æ¨¡å‹æ—¨åœ¨ç”¨äºè®¾å¤‡ä¸Šçš„ä½¿ç”¨æ¡ˆä¾‹ï¼Œå¦‚æç¤ºé‡å†™ã€å¤šè¯­è¨€çŸ¥è¯†æ£€ç´¢ã€æ‘˜è¦ä»»åŠ¡ã€å·¥å…·ä½¿ç”¨å’Œæœ¬åœ°è¿è¡Œçš„åŠ©æ‰‹ã€‚å®ƒä»¬åœ¨è¿™äº›è§„æ¨¡ä¸Šè¶…è¿‡äº†è®¸å¤šå¯ç”¨çš„å¼€æ”¾è®¿é—®æ¨¡å‹ï¼Œå¹¶ä¸è®¸å¤šå€å¤§çš„æ¨¡å‹ç«äº‰ã€‚åœ¨åé¢çš„éƒ¨åˆ†ä¸­ï¼Œæˆ‘ä»¬å°†å±•ç¤ºå¦‚ä½•ç¦»çº¿è¿è¡Œè¿™äº›æ¨¡å‹ã€‚

è¿™äº›æ¨¡å‹éµå¾ªä¸ Llama 3.1 ç›¸åŒçš„æ¶æ„ã€‚å®ƒä»¬ä½¿ç”¨é«˜è¾¾ 9 ä¸‡äº¿ä¸ªæ ‡è®°è¿›è¡Œè®­ç»ƒï¼Œå¹¶ä»ç„¶æ”¯æŒé•¿ä¸Šä¸‹æ–‡é•¿åº¦çš„ 128k ä¸ªæ ‡è®°ã€‚æ¨¡å‹æ˜¯å¤šè¯­è¨€çš„ï¼Œæ”¯æŒè‹±è¯­ã€å¾·è¯­ã€æ³•è¯­ã€æ„å¤§åˆ©è¯­ã€è‘¡è„ç‰™è¯­ã€å°åœ°è¯­ã€è¥¿ç­ç‰™è¯­å’Œæ³°è¯­ã€‚

è¿˜æœ‰ä¸€ä¸ªæ–°çš„ Llama Guard å°ç‰ˆæœ¬ï¼ŒLlama Guard 3 1Bï¼Œå¯ä»¥ä¸è¿™äº›æ¨¡å‹ä¸€èµ·éƒ¨ç½²ï¼Œä»¥è¯„ä¼°å¤šè½®å¯¹è¯ä¸­æœ€åä¸€æ¬¡ç”¨æˆ·æˆ–åŠ©æ‰‹çš„å“åº”ã€‚å®ƒä½¿ç”¨ä¸€ç»„é¢„å®šä¹‰çš„ç±»åˆ«ï¼ˆåœ¨æ­¤ç‰ˆæœ¬ä¸­æ–°å¢ï¼‰ï¼Œå¯ä»¥æ ¹æ®å¼€å‘è€…çš„ç”¨ä¾‹è¿›è¡Œè‡ªå®šä¹‰æˆ–æ’é™¤ã€‚æœ‰å…³ä½¿ç”¨Llama Guardçš„æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚è€ƒæ¨¡å‹å¡ã€‚

é¢å¤–ä¿¡æ¯ï¼šLlama 3.2 æ¥è§¦äº†æ¯”ä¸Šè¿°8ç§è¯­è¨€æ›´å¹¿æ³›çš„è¯­è¨€é›†åˆã€‚é¼“åŠ±å¼€å‘è€…é’ˆå¯¹ç‰¹å®šè¯­è¨€ç”¨ä¾‹å¾®è°ƒLlama 3.2æ¨¡å‹ã€‚

æˆ‘ä»¬é€šè¿‡ Open LLM Leaderboard è¯„ä¼°å¥—ä»¶å¯¹åŸºç¡€æ¨¡å‹è¿›è¡Œäº†æµ‹è¯•ï¼Œè€ŒæŒ‡ä»¤æ¨¡å‹åˆ™åœ¨ä¸‰ä¸ªæµè¡Œçš„åŸºå‡†ä¸Šè¿›è¡Œäº†è¯„ä¼°ï¼Œè¿™äº›åŸºå‡†è¡¡é‡éµå¾ªæŒ‡ä»¤çš„èƒ½åŠ›ï¼Œå¹¶ä¸ LMSYS èŠå¤©æœºå™¨äººç«æŠ€åœºçš„ç›¸å…³æ€§è¾ƒé«˜ï¼š[IFEval](https://arxiv.org/abs/2311.07911)ã€[AlpacaEval](https://arxiv.org/abs/2404.04475)å’Œ [MixEval-Hard](https://arxiv.org/abs/2406.06565)ã€‚ä»¥ä¸‹æ˜¯åŸºç¡€æ¨¡å‹çš„ç»“æœï¼Œå…¶ä¸­åŒ…æ‹¬ Llama-3.1-8B ä½œä¸ºå‚è€ƒï¼š


| Model                | BBH   | MATH Lvl 5 | GPQA  | MUSR  | MMLU-PRO | Average |
|----------------------|-------|------------|-------|-------|----------|---------|
| Meta-Llama-3.2-1B     | 4.37  | 0.23       | 0.00  | 2.56  | 2.26     | 1.88    |
| Meta-Llama-3.2-3B     | 14.73 | 1.28       | 4.03  | 3.39  | 16.57    | 8.00    |
| Meta-Llama-3.1-8B     | 25.29 | 4.61       | 6.15  | 8.98  | 24.95    | 14.00   |

ä»¥ä¸‹æ˜¯æŒ‡ä»¤æ¨¡å‹çš„ç»“æœï¼Œä»¥Llama-3.1-8B-Instruct ä½œä¸ºå‚è€ƒï¼š

| Model                       | AlpacaEval (LC) | IFEval | MixEval-Hard | Average |
|-----------------------------|-----------------|--------|--------------|---------|
| Meta-Llama-3.2-1B-Instruct   | 7.17            | 58.92  | 26.10        | 30.73   |
| Meta-Llama-3.2-3B-Instruct   | 20.88           | 77.01  | 31.80        | 43.23   |
| Meta-Llama-3.1-8B-Instruct   | 25.74           | 76.49  | 44.10        | 48.78   |

ä»¤äººç©ç›®çš„æ˜¯ï¼Œ3B æ¨¡å‹åœ¨ IFEval ä¸Šçš„è¡¨ç°ä¸8Bæ¨¡å‹ç›¸å½“ï¼è¿™ä½¿å¾—è¯¥æ¨¡å‹éå¸¸é€‚åˆç”¨äºä»£ç†åº”ç”¨ï¼Œå…¶ä¸­éµå¾ªæŒ‡ä»¤å¯¹äºæé«˜å¯é æ€§è‡³å…³é‡è¦ã€‚è¿™ä¸ªé«˜çš„ IFEval å¾—åˆ†å¯¹äºè¿™ä¸ªè§„æ¨¡çš„æ¨¡å‹æ¥è¯´éå¸¸ä»¤äººå°è±¡æ·±åˆ»ã€‚

1Bå’Œ3Bçš„æŒ‡ä»¤è°ƒä¼˜æ¨¡å‹å‡æ”¯æŒå·¥å…·ä½¿ç”¨ã€‚ç”¨æˆ·åœ¨ 0-shot ç¯å¢ƒä¸­æŒ‡å®šå·¥å…·ï¼ˆæ¨¡å‹å¯¹å¼€å‘è€…å°†ä½¿ç”¨çš„å·¥å…·æ²¡æœ‰å…ˆå‰çš„ä¿¡æ¯ï¼‰ã€‚å› æ­¤ï¼ŒLlama 3.1 æ¨¡å‹ä¸­åŒ…å«çš„å†…ç½®å·¥å…·ï¼ˆ`brave_search`å’Œ`wolfram_alpha`ï¼‰ä¸å†å¯ç”¨ã€‚

ç”±äºå…¶ä½“ç§¯å°ï¼Œè¿™äº›å°æ¨¡å‹å¯ä»¥ä½œä¸ºæ›´å¤§æ¨¡å‹çš„åŠ©æ‰‹ï¼Œæ‰§è¡Œ[è¾…åŠ©ç”Ÿæˆ](https://huggingface.co/blog/assisted-generation)ï¼ˆä¹Ÿç§°ä¸ºæ¨æµ‹è§£ç ï¼‰ã€‚[è¿™é‡Œ](https://github.com/huggingface/huggingface-llama-recipes/tree/main)æ˜¯ä¸€ä¸ªä½¿ç”¨Llama 3.2 1Bæ¨¡å‹ä½œä¸ºLlama 3.1 8Bæ¨¡å‹åŠ©æ‰‹çš„ç¤ºä¾‹ã€‚æœ‰å…³ç¦»çº¿ä½¿ç”¨æ¡ˆä¾‹ï¼Œè¯·åœ¨æ–‡ç« åé¢æŸ¥çœ‹è®¾å¤‡ä¸Šçš„éƒ¨åˆ†ã€‚

## æ¼”ç¤º
æ‚¨å¯ä»¥åœ¨ä»¥ä¸‹æ¼”ç¤ºä¸­ä½“éªŒè¿™ä¸‰ç§æŒ‡ä»¤æ¨¡å‹ï¼š

- [Gradioç©ºé—´ä¸­çš„Llama 3.2 11Bè§†è§‰æŒ‡ä»¤](https://huggingface.co/spaces/huggingface-projects/llama-3.2-vision-11B)
- [Gradioé©±åŠ¨çš„ç©ºé—´ä¸­çš„Llama 3.2 3B](https://huggingface.co/spaces/huggingface-projects/llama-3.2-3B-Instruct)
- Llama 3.2 3Båœ¨WebGPUä¸Šè¿è¡Œ 

![Demo GIF](https://huggingface.co/datasets/huggingface/release-assets/resolve/main/demo_gif.gif)

## ä½¿ç”¨Hugging Face Transformers

ä»…æ–‡æœ¬çš„æ£€æŸ¥ç‚¹ä¸ä¹‹å‰çš„ç‰ˆæœ¬å…·æœ‰ç›¸åŒçš„æ¶æ„ï¼Œå› æ­¤ä¸éœ€è¦æ›´æ–°æ‚¨çš„ç¯å¢ƒã€‚ç„¶è€Œï¼Œé‰´äºæ–°æ¶æ„ï¼ŒLlama 3.2è§†è§‰æ¨¡å‹éœ€è¦æ›´æ–°Transformersã€‚è¯·ç¡®ä¿å°†æ‚¨çš„å®‰è£…å‡çº§åˆ°4.45.0æˆ–æ›´é«˜ç‰ˆæœ¬ã€‚

```bash
pip install "transformers>=4.45.0" --upgrade
```

å‡çº§åï¼Œæ‚¨å¯ä»¥ä½¿ç”¨æ–°çš„ Llama 3.2 æ¨¡å‹ï¼Œå¹¶åˆ©ç”¨ Hugging Face ç”Ÿæ€ç³»ç»Ÿçš„æ‰€æœ‰å·¥å…·ã€‚

## Llama 3.2 1Bå’Œ3Bè¯­è¨€æ¨¡å‹

æ‚¨å¯ä»¥ä»…ç”¨å‡ è¡Œä»£ç é€šè¿‡ Transformers è¿è¡Œ 1B å’Œ 3B æ–‡æœ¬æ¨¡å‹æ£€æŸ¥ç‚¹ã€‚æ¨¡å‹æ£€æŸ¥ç‚¹ä»¥`bfloat16`ç²¾åº¦ä¸Šä¼ ï¼Œä½†æ‚¨ä¹Ÿå¯ä»¥ä½¿ç”¨ float16 æˆ–é‡åŒ–æƒé‡ã€‚å†…å­˜è¦æ±‚å–å†³äºæ¨¡å‹å¤§å°å’Œæƒé‡ç²¾åº¦ã€‚ä»¥ä¸‹æ˜¯ä¸€ä¸ªè¡¨æ ¼ï¼Œæ˜¾ç¤ºä½¿ç”¨ä¸åŒé…ç½®è¿›è¡Œæ¨ç†æ—¶æ‰€éœ€çš„å¤§è‡´å†…å­˜ï¼š

| Model Size | BF16/FP16 | FP8     | INT4    |
|------------|--------|---------|---------|
| 3B         | 6.5 GB | 3.2 GB  | 1.75 GB |
| 1B         | 2.5 GB | 1.25 GB | 0.75 GB |

```python
from transformers import pipeline
import torch

model_id = "meta-llama/Llama-3.2-3B-Instruct"
pipe = pipeline(
    "text-generation",
    model=model_id,
    torch_dtype=torch.bfloat16,
    device_map="auto",
)

messages = [
    {"role": "user", "content": "Who are you? Please, answer in pirate-speak."},
]
outputs = pipe(
    messages,
    max_new_tokens=256,
)
response = outputs[0]["generated_text"][-1]["content"]
print(response)
# Arrrr, me hearty! Yer lookin' fer a bit o' information about meself, eh? Alright then, matey! I be a language-generatin' swashbuckler, a digital buccaneer with a penchant fer spinnin' words into gold doubloons o' knowledge! Me name be... (dramatic pause)...Assistant! Aye, that be me name, and I be here to help ye navigate the seven seas o' questions and find the hidden treasure o' answers! So hoist the sails and set course fer adventure, me hearty! What be yer first question?
```

ä¸€äº›ç»†èŠ‚ï¼š

- æˆ‘ä»¬ä½¿ç”¨ `bfloat16` åŠ è½½æ¨¡å‹ã€‚å¦‚ä¸Šæ‰€è¿°ï¼Œè¿™æ˜¯ Meta å‘å¸ƒçš„åŸå§‹æ£€æŸ¥ç‚¹æ‰€ä½¿ç”¨çš„ç±»å‹ï¼Œå› æ­¤è¿™æ˜¯æ¨èçš„è¿è¡Œæ–¹å¼ï¼Œä»¥ç¡®ä¿æœ€ä½³ç²¾åº¦æˆ–è¿›è¡Œè¯„ä¼°ã€‚æ ¹æ®æ‚¨çš„ç¡¬ä»¶ï¼Œfloat16 å¯èƒ½ä¼šæ›´å¿«ã€‚

- é»˜è®¤æƒ…å†µä¸‹ï¼Œtransformers ä½¿ç”¨ä¸åŸå§‹ Meta ä»£ç åº“ç›¸åŒçš„é‡‡æ ·å‚æ•°ï¼ˆtemperature=0.6 å’Œ top_p=0.9ï¼‰ã€‚æˆ‘ä»¬å°šæœªè¿›è¡Œå¹¿æ³›æµ‹è¯•ï¼Œè¯·éšæ„æ¢ç´¢ï¼

## Llama 3.2 è§†è§‰æ¨¡å‹

è§†è§‰æ¨¡å‹è¾ƒå¤§ï¼Œå› æ­¤å®ƒä»¬åœ¨è¿è¡Œæ—¶éœ€è¦æ¯”å°å‹æ–‡æœ¬æ¨¡å‹æ›´å¤šçš„å†…å­˜ã€‚ä½œä¸ºå‚è€ƒï¼Œ11B è§†è§‰æ¨¡å‹åœ¨æ¨ç†æ—¶å¤§çº¦éœ€è¦ 10 GB çš„ GPU RAMï¼Œé‡‡ç”¨4-bitæ¨¡å¼ã€‚

ä½¿ç”¨æŒ‡ä»¤è°ƒä¼˜çš„ Llama è§†è§‰æ¨¡å‹è¿›è¡Œæ¨ç†çš„æœ€ç®€å•æ–¹æ³•æ˜¯ä½¿ç”¨å†…ç½®çš„èŠå¤©æ¨¡æ¿ã€‚è¾“å…¥å…·æœ‰ `user` å’Œ `assistant` è§’è‰²ï¼Œä»¥æŒ‡ç¤ºå¯¹è¯çš„è½®æ¬¡ã€‚ä¸æ–‡æœ¬æ¨¡å‹çš„ä¸€ä¸ªåŒºåˆ«æ˜¯ä¸æ”¯æŒç³»ç»Ÿè§’è‰²ã€‚ç”¨æˆ·è½®æ¬¡å¯ä»¥åŒ…æ‹¬å›¾åƒ-æ–‡æœ¬æˆ–ä»…æ–‡æœ¬è¾“å…¥ã€‚è¦æŒ‡ç¤ºè¾“å…¥åŒ…å«å›¾åƒï¼Œè¯·åœ¨è¾“å…¥çš„å†…å®¹éƒ¨åˆ†æ·»åŠ  `{"type": "image"}`ï¼Œç„¶åå°†å›¾åƒæ•°æ®ä¼ é€’ç»™ `processor`ï¼š

```python
import requests
import torch
from PIL import Image
from transformers import MllamaForConditionalGeneration, AutoProcessor

model_id = "meta-llama/Llama-3.2-11B-Vision-Instruct"
model = MllamaForConditionalGeneration.from_pretrained(
    model_id,
    torch_dtype=torch.bfloat16,
    device="cuda",
)
processor = AutoProcessor.from_pretrained(model_id)

url = "https://huggingface.co/datasets/huggingface/documentation-images/resolve/0052a70beed5bf71b92610a43a52df6d286cd5f3/diffusers/rabbit.jpg"
image = Image.open(requests.get(url, stream=True).raw)

messages = [
    {"role": "user", "content": [
        {"type": "image"},
        {"type": "text", "text": "Can you please describe this image in just one sentence?"}
    ]}
]

input_text = processor.apply_chat_template(
    messages, add_generation_prompt=True,
)
inputs = processor(
    image, input_text, return_tensors="pt"
).to(model.device)

output = model.generate(**inputs, max_new_tokens=70)

print(processor.decode(output[0][inputs["input_ids"].shape[-1]:]))


## The image depicts a rabbit dressed in a blue coat and brown vest, standing on a dirt road in front of a stone house.
```

æ‚¨å¯ä»¥ç»§ç»­å…³äºå›¾åƒçš„å¯¹è¯ã€‚è¯·è®°ä½ï¼Œå¦‚æœæ‚¨åœ¨æ–°ç”¨æˆ·è½®æ¬¡ä¸­æä¾›æ–°å›¾åƒï¼Œæ¨¡å‹å°†ä»é‚£æ—¶èµ·å¼•ç”¨æ–°å›¾åƒã€‚æ‚¨ä¸èƒ½åŒæ—¶æŸ¥è¯¢ä¸¤å¹…ä¸åŒçš„å›¾åƒã€‚è¿™æ˜¯ç»§ç»­ä¹‹å‰å¯¹è¯çš„ä¸€ä¸ªç¤ºä¾‹ï¼Œæˆ‘ä»¬åœ¨å¯¹è¯ä¸­æ·»åŠ åŠ©æ‰‹è½®æ¬¡ï¼Œå¹¶è¯¢é—®ä¸€äº›æ›´å¤šçš„ç»†èŠ‚ï¼š

```python
messages = [
    {"role": "user", "content": [
        {"type": "image"},
        {"type": "text", "text": "Can you please describe this image in just one sentence?"}
    ]},
    {"role": "assistant", "content": "The image depicts a rabbit dressed in a blue coat and brown vest, standing on a dirt road in front of a stone house."},
    {"role": "user", "content": "What is in the background?"}
]

input_text = processor.apply_chat_template(
    messages,
    add_generation_prompt=True,
)
inputs = processor(image, input_text, return_tensors="pt").to(model.device)
output = model.generate(**inputs, max_new_tokens=70)
print(processor.decode(output[0][inputs["input_ids"].shape[-1]:]))
```
è¿™æ˜¯æˆ‘ä»¬å¾—åˆ°çš„å›å¤ï¼š

```
In the background, there is a stone house with a thatched roof, a dirt road, a field of flowers, and rolling hills.
```

æ‚¨è¿˜å¯ä»¥ä½¿ç”¨ `bitsandbytes` åº“è‡ªåŠ¨é‡åŒ–æ¨¡å‹ï¼Œä»¥ 8-bit æˆ–ç”šè‡³ 4-bit æ¨¡å¼åŠ è½½ã€‚ä»¥ä¸‹æ˜¯å¦‚ä½•åœ¨ 4-bit æ¨¡å¼ä¸‹åŠ è½½ç”Ÿæˆç®¡é“çš„ç¤ºä¾‹ï¼š

```diff
import torch
from transformers import MllamaForConditionalGeneration, AutoProcessor
+from transformers import BitsAndBytesConfig

+bnb_config = BitsAndBytesConfig(
+    load_in_4bit=True,
+    bnb_4bit_quant_type="nf4",
+    bnb_4bit_compute_dtype=torch.bfloat16
)
 
model = MllamaForConditionalGeneration.from_pretrained(
    model_id,
-   torch_dtype=torch.bfloat16,
-   device="cuda",
+   quantization_config=bnb_config,
)
```

ç„¶åï¼Œæ‚¨å¯ä»¥åº”ç”¨èŠå¤©æ¨¡æ¿ï¼Œä½¿ç”¨`processor`ï¼Œå¹¶åƒä»¥å‰ä¸€æ ·è°ƒç”¨æ¨¡å‹ã€‚

## è®¾å¤‡ç«¯éƒ¨ç½²

æ‚¨å¯ä»¥ç›´æ¥åœ¨è®¾å¤‡çš„ CPU/GPU/æµè§ˆå™¨ä¸Šè¿è¡Œ Llama 3.2 1B å’Œ 3Bï¼Œä½¿ç”¨å¤šä¸ªå¼€æºåº“ï¼Œå¦‚ä¸‹æ‰€ç¤ºã€‚

### Llama.cpp & Llama-cpp-python

[Llama.cpp](https://github.com/ggerganov/llama.cpp) æ˜¯è¿›è¡Œè·¨å¹³å°è®¾å¤‡ä¸Šæœºå™¨å­¦ä¹ æ¨ç†çš„é¦–é€‰æ¡†æ¶ã€‚æˆ‘ä»¬ä¸º 1B å’Œ 3B æ¨¡å‹æä¾›äº†é‡åŒ–çš„4-bitå’Œ8-bitæƒé‡ã€‚æˆ‘ä»¬å¸Œæœ›ç¤¾åŒºèƒ½å¤Ÿé‡‡ç”¨è¿™äº›æ¨¡å‹ï¼Œå¹¶åˆ›å»ºå…¶ä»–é‡åŒ–å’Œå¾®è°ƒã€‚æ‚¨å¯ä»¥åœ¨ [è¿™é‡Œ](https://huggingface.co/models?search=hugging-quants/Llama-3.2-) æ‰¾åˆ°æ‰€æœ‰é‡åŒ–çš„ Llama 3.2 æ¨¡å‹ã€‚

ä»¥ä¸‹æ˜¯å¦‚ä½•ç›´æ¥ä½¿ç”¨è¿™äº›æ£€æŸ¥ç‚¹ä¸ llama.cppã€‚

é€šè¿‡ brew å®‰è£… llama.cppï¼ˆé€‚ç”¨äº Mac å’Œ Linuxï¼‰ã€‚

```bash
brew install llama.cpp
```

æ‚¨å¯ä»¥ä½¿ç”¨ CLI è¿è¡Œå•æ¬¡ç”Ÿæˆæˆ–è°ƒç”¨å…¼å®¹ Open AI æ¶ˆæ¯è§„èŒƒçš„ llama.cpp æœåŠ¡å™¨ã€‚

æ‚¨å¯ä»¥ä½¿ç”¨å¦‚ä¸‹å‘½ä»¤è¿è¡Œ CLIï¼š

```bash
llama-cli --hf-repo hugging-quants/Llama-3.2-3B-Instruct-Q8_0-GGUF --hf-file llama-3.2-3b-instruct-q8_0.gguf -p "ç”Ÿå‘½å’Œå®‡å®™çš„æ„ä¹‰æ˜¯"
```

æ‚¨å¯ä»¥è¿™æ ·å¯åŠ¨æœåŠ¡å™¨ï¼š

```bash
llama-server --hf-repo hugging-quants/Llama-3.2-3B-Instruct-Q8_0-GGUF --hf-file llama-3.2-3b-instruct-q8_0.gguf -c 2048
```

æ‚¨è¿˜å¯ä»¥ä½¿ç”¨ [llama-cpp-python](https://github.com/abetlen/llama-cpp-python) åœ¨ Python ä¸­ä»¥ç¼–ç¨‹æ–¹å¼è®¿é—®è¿™äº›æ¨¡å‹ã€‚

```python
from llama_cpp import Llama

llm = Llama.from_pretrained(
    repo_id="hugging-quants/Llama-3.2-3B-Instruct-Q8_0-GGUF",
    filename="*q8_0.gguf",
)
llm.create_chat_completion(
      messages = [
          {
              "role": "user",
              "content": "What is the capital of France?"
          }
      ]
)
```


### Transformers.js

æ‚¨ç”šè‡³å¯ä»¥åœ¨æµè§ˆå™¨ï¼ˆæˆ–ä»»ä½• JavaScript è¿è¡Œæ—¶ï¼Œå¦‚ Node.jsã€Deno æˆ– Bunï¼‰ä¸­ä½¿ç”¨ [Transformers.js](https://huggingface.co/docs/transformers.js) è¿è¡Œ Llama 3.2ã€‚æ‚¨å¯ä»¥åœ¨ Hub ä¸Šæ‰¾åˆ° [ONNX æ¨¡å‹](https://huggingface.co/onnx-community/Llama-3.2-1B-Instruct)ã€‚å¦‚æœæ‚¨è¿˜æ²¡æœ‰å®‰è£…è¯¥åº“ï¼Œå¯ä»¥é€šè¿‡ [NPM](https://www.npmjs.com/package/@huggingface/transformers) ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤å®‰è£…ï¼š
```bash
npm i @huggingface/transformers
```

ç„¶åï¼Œæ‚¨å¯ä»¥æŒ‰ç…§ä»¥ä¸‹æ–¹å¼è¿è¡Œæ¨¡å‹ï¼š
```js
import { pipeline } from "@huggingface/transformers";

// Create a text generation pipeline
const generator = await pipeline("text-generation", "onnx-community/Llama-3.2-1B-Instruct");

// Define the list of messages
const messages = [
  { role: "system", content: "You are a helpful assistant." },
  { role: "user", content: "Tell me a joke." },
];

// Generate a response
const output = await generator(messages, { max_new_tokens: 128 });
console.log(output[0].generated_text.at(-1).content);
```

<details>

<summary>Example output</summary>

```
Here's a joke for you:

What do you call a fake noodle?

An impasta!

I hope that made you laugh! Do you want to hear another one?
```

</details>

## å¾®è°ƒ Llama 3.2

TRL æ”¯æŒç›´æ¥å¯¹ Llama 3.2 æ–‡æœ¬æ¨¡å‹è¿›è¡ŒèŠå¤©å’Œå¾®è°ƒï¼š

```bash
# Chat
trl chat --model_name_or_path meta-llama/Llama-3.2-3B

# Fine-tune
trl sft  --model_name_or_path meta-llama/Llama-3.2-3B \
         --dataset_name HuggingFaceH4/no_robots \
         --output_dir Llama-3.2-3B-Instruct-sft \
         --gradient_checkpointing
```

TRL è¿˜æ”¯æŒå¯¹ Llama 3.2 Vision æ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œä½¿ç”¨ [è¿™ä¸ªè„šæœ¬](https://github.com/huggingface/trl/tree/main/examples/scripts/sft_vlm.py)ã€‚

```bash
# Tested on 8x H100 GPUs
accelerate launch --config_file=examples/accelerate_configs/deepspeed_zero3.yaml \
    examples/scripts/sft_vlm.py \
    --dataset_name HuggingFaceH4/llava-instruct-mix-vsft \
    --model_name_or_path meta-llama/Llama-3.2-11B-Vision-Instruct \
    --per_device_train_batch_size 8 \
    --gradient_accumulation_steps 8 \
    --output_dir Llama-3.2-11B-Vision-Instruct-sft \
    --bf16 \
    --torch_dtype bfloat16 \
    --gradient_checkpointing
```

æ‚¨è¿˜å¯ä»¥æŸ¥çœ‹[ç¬”è®°æœ¬](https://github.com/huggingface/huggingface-llama-recipes/blob/main/Llama-Vision%20FT.ipynb)ï¼Œäº†è§£å¦‚ä½•ä½¿ç”¨ Transformers å’Œ PEFT è¿›è¡Œ LoRA å¾®è°ƒã€‚

## Hugging Face åˆä½œä¼™ä¼´é›†æˆ

æˆ‘ä»¬ç›®å‰æ­£åœ¨ä¸ AWSã€Google Cloudã€Microsoft Azure å’Œ DELL çš„åˆä½œä¼™ä¼´åˆä½œï¼Œæ­£åœ¨å°† Llama 3.2 11B å’Œ 90B æ¨¡å‹æ·»åŠ åˆ° Amazon SageMakerã€Google Kubernetes Engineã€Vertex AI Model Catalogã€Azure AI Studio å’Œ DELL Enterprise Hub ä¸­ã€‚æˆ‘ä»¬ä¼šåœ¨è¿™äº›å®¹å™¨å¯ç”¨æ—¶æ›´æ–°æœ¬èŠ‚å†…å®¹ï¼Œæ‚¨å¯ä»¥è®¢é˜… [Hugging Squad](https://mailchi.mp/huggingface/squad) è·å–ç”µå­é‚®ä»¶æ›´æ–°ã€‚

## é¢å¤–èµ„æº

- [æ¨¡å‹åœ¨ Hub ä¸Š](https://huggingface.co/collections/meta-llama/llama-32-66f448ffc8c32f949b04c8cf)
- [Hugging Face Llama é£Ÿè°±](https://github.com/huggingface/huggingface-llama-recipes)
- [Open LLM æ’è¡Œæ¦œ](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard)
- [Meta åšå®¢](https://ai.meta.com/blog/llama-3-2-connect-2024-vision-edge-mobile-devices/)
- [è¯„ä¼°æ•°æ®é›†](https://huggingface.co/collections/meta-llama/llama-32-evals-66f44b3d2df1c7b136d821f0)

## é¸£è°¢

æ²¡æœ‰æˆåƒä¸Šä¸‡ç¤¾åŒºæˆå‘˜çš„è´¡çŒ®ï¼Œè¿™ç§æ¨¡å‹çš„å‘å¸ƒä»¥åŠç”Ÿæ€ç³»ç»Ÿä¸­çš„æ”¯æŒå’Œè¯„ä¼°å°†æ— æ³•å®ç°ï¼Œä»–ä»¬ä¸º transformersã€text-generation-inferenceã€vllmã€pytorchã€LM Eval Harness ä»¥åŠå…¶ä»–ä¼—å¤šé¡¹ç›®ä½œå‡ºäº†è´¡çŒ®ã€‚ç‰¹åˆ«æ„Ÿè°¢ VLLM å›¢é˜Ÿçš„æµ‹è¯•å’Œé—®é¢˜æŠ¥å‘Šæ”¯æŒã€‚è¿™æ¬¡å‘å¸ƒçš„é¡ºåˆ©è¿›è¡Œç¦»ä¸å¼€ ClÃ©mentineã€Alinaã€Elie å’Œ Loubna å¯¹ LLM è¯„ä¼°çš„æ”¯æŒï¼ŒNicolas Patryã€Olivier Dehaene å’Œ DaniÃ«l de Kok å¯¹æ–‡æœ¬ç”Ÿæˆæ¨ç†çš„è´¡çŒ®ï¼›Lysandreã€Arthurã€Pavelã€Edward Beechingã€Amyã€Benjaminã€Joaoã€Pabloã€Raushan Turganbayã€Matthew Carrigan å’Œ Joshua Lochner å¯¹ transformersã€transformers.jsã€TRL å’Œ PEFT çš„æ”¯æŒï¼›Nathan Sarrazin å’Œ Victor è®© Llama 3.2 åœ¨ Hugging Chat ä¸Šå¯ç”¨ï¼›Brigitte Tousignant å’Œ Florent Daudens çš„æ²Ÿé€šæ”¯æŒï¼›Julienã€Simonã€Pierricã€Eliottã€Lucainã€Alvaroã€Caleb å’Œ Mishig æ¥è‡ª Hub å›¢é˜Ÿçš„å¼€å‘å’ŒåŠŸèƒ½å‘å¸ƒæ”¯æŒã€‚

ç‰¹åˆ«æ„Ÿè°¢ Meta å›¢é˜Ÿå‘å¸ƒ Llama 3.2 å¹¶ä½¿å…¶å¼€æ”¾ç»™ AI ç¤¾åŒºï¼

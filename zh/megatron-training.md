---
title: å¦‚ä½•ä½¿ç”¨ Megatron-LM è®­ç»ƒè¯­è¨€æ¨¡å‹
thumbnail: /blog/assets/100_megatron_training/thumbnail.png
authors:
- user: loubnabnl
translators:
- user: gxy-gxy
- user: zhongdongy
  proofreader: true
---

# å¦‚ä½•ä½¿ç”¨ Megatron-LM è®­ç»ƒè¯­è¨€æ¨¡å‹


åœ¨ PyTorch ä¸­è®­ç»ƒå¤§è¯­è¨€æ¨¡å‹ä¸ä»…ä»…æ˜¯å†™ä¸€ä¸ªè®­ç»ƒå¾ªç¯è¿™ä¹ˆç®€å•ã€‚æˆ‘ä»¬é€šå¸¸éœ€è¦å°†æ¨¡å‹åˆ†å¸ƒåœ¨å¤šä¸ªè®¾å¤‡ä¸Šï¼Œå¹¶ä½¿ç”¨è®¸å¤šä¼˜åŒ–æŠ€æœ¯ä»¥å®ç°ç¨³å®šé«˜æ•ˆçš„è®­ç»ƒã€‚Hugging Face ğŸ¤—Â [Accelerate](https://huggingface.co/docs/accelerate/index) çš„åˆ›å»ºæ˜¯ä¸ºäº†æ”¯æŒè·¨ GPU å’Œ TPU çš„åˆ†å¸ƒå¼è®­ç»ƒï¼Œå¹¶ä½¿å…¶èƒ½å¤Ÿéå¸¸å®¹æ˜“çš„é›†æˆåˆ°è®­ç»ƒä»£ç ä¸­ã€‚ğŸ¤—Â [Transformers](https://huggingface.co/docs/transformers/index) è¿˜æ”¯æŒä½¿ç”¨ [Trainer](https://huggingface.co/docs/transformers/main_classes/trainer#transformers.Trainer) API æ¥è®­ç»ƒï¼Œå…¶åœ¨ PyTorch ä¸­æä¾›åŠŸèƒ½å®Œæ•´çš„è®­ç»ƒæ¥å£ï¼Œç”šè‡³ä¸éœ€è¦è‡ªå·±ç¼–å†™è®­ç»ƒçš„ä»£ç ã€‚

[Megatron-LM](https://github.com/NVIDIA/Megatron-LM) æ˜¯ç ”ç©¶äººå‘˜ç”¨äºé¢„è®­ç»ƒå¤§å‹ Transformer æ¨¡å‹çš„å¦ä¸€ä¸ªæµè¡Œå·¥å…·ï¼Œå®ƒæ˜¯ NVIDIA åº”ç”¨æ·±åº¦å­¦ä¹ ç ”ç©¶å›¢é˜Ÿå¼€å‘çš„ä¸€ä¸ªå¼ºå¤§æ¡†æ¶ã€‚ä¸ `accelerate` å’Œ `Trainer` ä¸åŒï¼ŒMegatron-LM ä½¿ç”¨èµ·æ¥å¹¶ä¸ç®€å•ï¼Œå¯¹äºåˆå­¦è€…æ¥è¯´å¯èƒ½éš¾ä»¥ä¸Šæ‰‹ã€‚ä½†å®ƒé’ˆå¯¹ GPU ä¸Šçš„è®­ç»ƒè¿›è¡Œäº†é«˜åº¦ä¼˜åŒ–ã€‚åœ¨è¿™ç¯‡æ–‡ç« ä¸­ï¼Œä½ å°†å­¦ä¹ å¦‚ä½•ä½¿ç”¨ Megatron-LM æ¡†æ¶åœ¨ NVIDIA GPU ä¸Šè®­ç»ƒè¯­è¨€æ¨¡å‹ï¼Œå¹¶å°†å…¶ä¸ `transformers` ç»“åˆã€‚

æˆ‘ä»¬å°†åˆ†è§£åœ¨æ­¤æ¡†æ¶ä¸­è®­ç»ƒ GPT2 æ¨¡å‹çš„ä¸åŒæ­¥éª¤ï¼ŒåŒ…æ‹¬:

- ç¯å¢ƒè®¾ç½®
- æ•°æ®é¢„å¤„ç†
- è®­ç»ƒ
- å°†æ¨¡å‹è½¬åŒ–ä¸º ğŸ¤— Transformers

## ä¸ºä»€ä¹ˆé€‰æ‹© Megatron-LM?

åœ¨è¿›å…¥è®­ç»ƒç»†èŠ‚çš„è®²è§£ä¹‹å‰ï¼Œè®©æˆ‘ä»¬é¦–å…ˆäº†è§£æ˜¯ä»€ä¹ˆè®©è¿™ä¸ªæ¡†æ¶æ¯”å…¶ä»–æ¡†æ¶æ›´é«˜æ•ˆã€‚æœ¬èŠ‚çš„çµæ„Ÿæ¥è‡ªè¿™ç¯‡å…³äºä½¿ç”¨ [Megatron-DeepSpeed](https://github.com/bigscience-workshop/Megatron-DeepSpeed) è¿›è¡Œ BLOOM è®­ç»ƒçš„ç²¾å½© [åšå®¢](https://huggingface.co/blog/zh/bloom-megatron-deepspeed)ï¼Œè¯·å‚é˜…è¯¥åšå®¢ä»¥è·å–æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œå› ä¸ºè¯¥åšå®¢æ—¨åœ¨å¯¹ Megatron-LM è¿›è¡Œè¯¦ç»†çš„ä»‹ç»ã€‚

### æ•°æ®åŠ è½½

Megatron-LM å¸¦æœ‰ä¸€ä¸ªé«˜æ•ˆçš„ DataLoaderï¼Œå…¶ä¸­æ•°æ®åœ¨è®­ç»ƒå‰è¢« tokenize å’Œ shuffleã€‚å®ƒè¿˜å°†æ•°æ®æ‹†åˆ†ä¸ºå¸¦æœ‰ç´¢å¼•çš„ç¼–å·åºåˆ—ï¼Œå¹¶å°†ç´¢å¼•å­˜å‚¨ï¼Œå› æ­¤ tokenize åªéœ€è¦è®¡ç®—ä¸€æ¬¡ã€‚ä¸ºäº†æ„å»ºç´¢å¼•ï¼Œé¦–å…ˆæ ¹æ®è®­ç»ƒå‚æ•°è®¡ç®—æ¯ä¸ª epoch çš„æ•°é‡ï¼Œå¹¶åˆ›å»ºä¸€ä¸ªæ’åºï¼Œç„¶åå¯¹æ•°æ®è¿›è¡Œ shuffle æ“ä½œã€‚è¿™ä¸å¤§å¤šæ•°æƒ…å†µä¸åŒï¼Œæˆ‘ä»¬é€šå¸¸è¿­ä»£æ•´ä¸ªæ•°æ®é›†ç›´åˆ°å…¶ç”¨å°½ï¼Œç„¶åé‡å¤ç¬¬äºŒä¸ª epoch ã€‚è¿™å¹³æ»‘äº†å­¦ä¹ æ›²çº¿å¹¶èŠ‚çœäº†è®­ç»ƒæ—¶é—´ã€‚

### èåˆ CUDA å†…æ ¸

å½“ä¸€ä¸ªè®¡ç®—åœ¨ GPU ä¸Šè¿è¡Œæ—¶ï¼Œå¿…è¦çš„æ•°æ®ä¼šä»å†…å­˜ä¸­å–å‡ºå¹¶åŠ è½½åˆ° GPU ä¸Šï¼Œç„¶åè®¡ç®—ç»“æœè¢«ä¿å­˜å›å†…å­˜ã€‚ç®€å•æ¥è¯´ï¼Œèåˆå†…æ ¸çš„æ€æƒ³æ˜¯: å°†é€šå¸¸ç”± PyTorch å•ç‹¬æ‰§è¡Œçš„ç±»ä¼¼æ“ä½œç»„åˆæˆä¸€ä¸ªå•ç‹¬çš„ç¡¬ä»¶æ“ä½œã€‚å› æ­¤å¯ä»¥å°†å¤šä¸ªç¦»æ•£è®¡ç®—åˆå¹¶ä¸ºä¸€ä¸ªï¼Œä»è€Œå‡å°‘åœ¨å¤šä¸ªç¦»æ•£è®¡ç®—ä¸­çš„å†…å­˜ç§»åŠ¨æ¬¡æ•°ã€‚ä¸‹å›¾è¯´æ˜äº†å†…æ ¸èåˆçš„æ€æƒ³ã€‚å®ƒçš„çµæ„Ÿæ¥è‡ªè¿™ç¯‡ [è®ºæ–‡](https://www.arxiv-vanity.com/papers/1305.1183/)ï¼Œè¯¥è®ºæ–‡è¯¦ç»†è®¨è®ºäº†è¿™ä¸ªæ¦‚å¿µã€‚

<p align="center">
    <img src="/blog/assets/100_megatron_training/kernel_fusion.png" width="600" />
</p>

å½“ fã€g å’Œ h èåˆåœ¨ä¸€ä¸ªå†…æ ¸ä¸­æ—¶ï¼Œf å’Œ g çš„ä¸­é—´ç»“æœ x' å’Œ y' å­˜å‚¨åœ¨ GPU å¯„å­˜å™¨ä¸­å¹¶ç«‹å³è¢« h ä½¿ç”¨ã€‚ä½†æ˜¯å¦‚æœä¸èåˆï¼Œx' å’Œ y' å°±éœ€è¦å¤åˆ¶åˆ°å†…å­˜ä¸­ï¼Œç„¶åç”± h åŠ è½½ã€‚å› æ­¤ï¼Œèåˆ CUDA å†…æ ¸æ˜¾ç€åŠ å¿«äº†è®¡ç®—é€Ÿåº¦ã€‚æ­¤å¤–ï¼ŒMegatron-LM è¿˜ä½¿ç”¨ [Apex](https://github.com/NVIDIA/apex) çš„ AdamW èåˆå®ç°ï¼Œå®ƒæ¯” PyTorch å®ç°æ›´å¿«ã€‚

è™½ç„¶æˆ‘ä»¬å¯ä»¥åœ¨ `transformers` ä¸­è‡ªå®šä¹‰ Megatron-LM ä¸­çš„ DataLoader å’Œ Apex çš„èåˆä¼˜åŒ–å™¨ï¼Œä½†è‡ªå®šä¹‰èåˆ CUDA å†…æ ¸å¯¹æ–°æ‰‹æ¥è¯´å¤ªä¸å‹å¥½äº†ã€‚

ç°åœ¨ä½ å·²ç»ç†Ÿæ‚‰äº†è¯¥æ¡†æ¶åŠå…¶ä¼˜åŠ¿ï¼Œè®©æˆ‘ä»¬è¿›å…¥è®­ç»ƒç»†èŠ‚å§ï¼

## å¦‚ä½•ä½¿ç”¨ Megatron-LM æ¡†æ¶è®­ç»ƒï¼Ÿ

### ç¯å¢ƒè®¾ç½®

è®¾ç½®ç¯å¢ƒçš„æœ€ç®€å•æ–¹æ³•æ˜¯ä» [NGC](https://catalog.ngc.nvidia.com/orgs/nvidia/containers/pytorch) æ‹‰å–é™„å¸¦æ‰€æœ‰æ‰€éœ€ç¯å¢ƒçš„ NVIDIA PyTorch å®¹å™¨ã€‚æœ‰å…³è¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜… [æ–‡æ¡£](https://docs.nvidia.com/deeplearning/frameworks/pytorch-release-notes/index.html)ã€‚å¦‚æœä½ ä¸æƒ³ä½¿ç”¨æ­¤å®¹å™¨ï¼Œåˆ™éœ€è¦å®‰è£…æœ€æ–°çš„ pytorchã€cudaã€nccl å’Œ NVIDIA [APEX](https://github.com/NVIDIA/apex#quick-start) ç‰ˆæœ¬å’Œ `nltk` åº“ã€‚

åœ¨å®‰è£…å®Œ Docker ä¹‹åï¼Œä½ å¯ä»¥ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤è¿è¡Œå®¹å™¨ (`xx.xx` è¡¨ç¤ºä½ çš„ Docker ç‰ˆæœ¬)ï¼Œç„¶ååœ¨å…¶ä¸­å…‹éš† [Megatron-LM åº“](https://github.com/NVIDIA/Megatron-LM):

```bash
docker run --gpus all -it --rm nvcr.io/nvidia/pytorch:xx.xx-py3
git clone https://github.com/NVIDIA/Megatron-LM
```

ä½ è¿˜éœ€è¦åœ¨å®¹å™¨çš„ Megatron-LM æ–‡ä»¶å¤¹ä¸­æ·»åŠ åˆ†è¯å™¨çš„è¯æ±‡æ–‡ä»¶ `vocab.json` å’Œåˆå¹¶è¡¨ `merges.txt`ã€‚è¿™äº›æ–‡ä»¶å¯ä»¥åœ¨å¸¦æœ‰æƒé‡çš„æ¨¡å‹ä»“åº“ä¸­æ‰¾åˆ°ï¼Œè¯·å‚é˜… [GPT2 åº“](https://huggingface.co/gpt2/tree/main)ã€‚ä½ è¿˜å¯ä»¥ä½¿ç”¨ `transformers` è®­ç»ƒè‡ªå·±çš„åˆ†è¯å™¨ã€‚ä½ å¯ä»¥æŸ¥çœ‹ [CodeParrot é¡¹ç›®](https://github.com/huggingface/transformers/tree/main/examples/research_projects/codeparrot) ä»¥è·å–å®é™…ç¤ºä¾‹ã€‚ç°åœ¨ï¼Œå¦‚æœä½ æƒ³ä»å®¹å™¨å¤–éƒ¨å¤åˆ¶è¿™äº›æ•°æ®ï¼Œä½ å¯ä»¥ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤:

```bash
sudo docker cp vocab.json CONTAINER_ID:/workspace/Megatron-LM
sudo docker cp merges.txt CONTAINER_ID:/workspace/Megatron-LM
```

### æ•°æ®é¢„å¤„ç†

åœ¨æœ¬æ•™ç¨‹çš„å…¶ä½™éƒ¨åˆ†ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨ [CodeParrot](https://huggingface.co/codeparrot/codeparrot-small) æ¨¡å‹å’Œæ•°æ®ä½œä¸ºç¤ºä¾‹ã€‚

æˆ‘ä»¬éœ€è¦å¯¹é¢„è®­ç»ƒæ•°æ®è¿›è¡Œé¢„å¤„ç†ã€‚é¦–å…ˆï¼Œä½ éœ€è¦å°†å…¶è½¬æ¢ä¸º json æ ¼å¼ï¼Œä¸€ä¸ª json çš„ä¸€è¡ŒåŒ…å«ä¸€ä¸ªæ–‡æœ¬æ ·æœ¬ã€‚å¦‚æœä½ æ­£åœ¨ä½¿ç”¨ ğŸ¤— [Datasets](https://huggingface.co/docs/datasets/index)ï¼Œè¿™é‡Œæœ‰ä¸€ä¸ªå…³äºå¦‚ä½•åšåˆ°è¿™ä¸€ç‚¹çš„ä¾‹å­ (è¯·åœ¨ Megatron-LM æ–‡ä»¶å¤¹ä¸­è¿›è¡Œè¿™äº›æ“ä½œ):

```python
from datasets import load_dataset

train_data = load_dataset('codeparrot/codeparrot-clean-train', split='train')
train_data.to_json("codeparrot_data.json", lines=True)
```

ç„¶åä½¿ç”¨ä»¥ä¸‹å‘½ä»¤å°†æ•°æ® tokenizeã€shuffle å¹¶å¤„ç†æˆäºŒè¿›åˆ¶æ ¼å¼ä»¥è¿›è¡Œè®­ç»ƒ:

```bash
#if nltk isn't installed
pip install nltk
python tools/preprocess_data.py \
       --input codeparrot_data.json \
       --output-prefix codeparrot \
       --vocab vocab.json \
       --dataset-impl mmap \
       --tokenizer-type GPT2BPETokenizer \
       --merge-file merges.txt \
       --json-keys content \
       --workers 32 \
       --chunk-size 25 \
       --append-eod
```

`workers` å’Œ `chunk_size` é€‰é¡¹æŒ‡çš„æ˜¯é¢„å¤„ç†ä¸­ä½¿ç”¨çš„çº¿ç¨‹æ•°é‡å’Œåˆ†é…ç»™æ¯ä¸ªçº¿ç¨‹çš„æ•°æ®å—å¤§å°ã€‚`dataset-impl` æŒ‡çš„æ˜¯ç´¢å¼•æ•°æ®é›†çš„å®ç°æ–¹å¼ï¼ŒåŒ…æ‹¬ ['lazy', 'cached', 'mmap']ã€‚è¿™å°†è¾“å‡º `codeparrot_content_document.idx` å’Œ  `codeparrot_content_document.bin` ä¸¤ä¸ªæ–‡ä»¶ç”¨äºè®­ç»ƒã€‚

### è®­ç»ƒ

ä½ å¯ä»¥ä½¿ç”¨å¦‚ä¸‹æ‰€ç¤ºé…ç½®æ¨¡å‹æ¶æ„å’Œè®­ç»ƒå‚æ•°ï¼Œæˆ–å°†å…¶æ”¾å…¥ä½ å°†è¿è¡Œçš„ bash è„šæœ¬ä¸­ã€‚è¯¥å‘½ä»¤åœ¨ 8 ä¸ª GPU ä¸Šå‚æ•°ä¸º 110M çš„ CodeParrot æ¨¡å‹è¿›è¡Œé¢„è®­ç»ƒã€‚è¯·æ³¨æ„ï¼Œæ•°æ®é»˜è®¤æŒ‰ 969:30:1 çš„æ¯”ä¾‹åˆ’åˆ†ä¸ºè®­ç»ƒ/éªŒè¯/æµ‹è¯•é›†ã€‚

```bash
GPUS_PER_NODE=8
MASTER_ADDR=localhost
MASTER_PORT=6001
NNODES=1
NODE_RANK=0
WORLD_SIZE=$(($GPUS_PER_NODE*$NNODES))
DISTRIBUTED_ARGS="--nproc_per_node $GPUS_PER_NODE --nnodes $NNODES --node_rank $NODE_RANK --master_addr $MASTER_ADDR --master_port $MASTER_PORT"
CHECKPOINT_PATH=/workspace/Megatron-LM/experiments/codeparrot-small
VOCAB_FILE=vocab.json
MERGE_FILE=merges.txt
DATA_PATH=codeparrot_content_document
GPT_ARGS="--num-layers 12
--hidden-size 768
--num-attention-heads 12
--seq-length 1024
--max-position-embeddings 1024
--micro-batch-size 12
--global-batch-size 192
--lr 0.0005
--train-iters 150000
--lr-decay-iters 150000
--lr-decay-style cosine
--lr-warmup-iters 2000
--weight-decay .1
--adam-beta2 .999
--fp16
--log-interval 10
--save-interval 2000
--eval-interval 200
--eval-iters 10
"
TENSORBOARD_ARGS="--tensorboard-dir experiments/tensorboard"
python3 -m torch.distributed.launch $DISTRIBUTED_ARGS \
        pretrain_gpt.py \
        --tensor-model-parallel-size 1 \
        --pipeline-model-parallel-size 1 \
        $GPT_ARGS \
        --vocab-file $VOCAB_FILE \
        --merge-file $MERGE_FILE \
        --save $CHECKPOINT_PATH \
        --load $CHECKPOINT_PATH \
        --data-path $DATA_PATH \
        $TENSORBOARD_ARGS
```

ä½¿ç”¨ä»¥ä¸Šè®¾ç½®ï¼Œè®­ç»ƒå¤§çº¦éœ€è¦ 12 ä¸ªå°æ—¶ã€‚

è¯¥è®¾ç½®ä½¿ç”¨æ•°æ®å¹¶è¡Œï¼Œä½†ä¹Ÿå¯ä»¥å¯¹æ— æ³•æ”¾åœ¨å•ä¸ª GPU çš„è¶…å¤§æ¨¡å‹ä½¿ç”¨æ¨¡å‹å¹¶è¡Œã€‚ç¬¬ä¸€ç§é€‰æ‹©æ˜¯è®¾ç½®å¼ é‡å¹¶è¡Œï¼Œå®ƒå°†æ¨¡å‹ä¸­çš„å¼ é‡æ‹†åˆ†åˆ°å¤šä¸ª GPU ä¸Šå¹¶è¡Œè¿ç®—ï¼Œä½ éœ€è¦å°† `tensor-model-parallel-size` å‚æ•°æ›´æ”¹ä¸ºæ‰€éœ€çš„ GPU æ•°é‡ã€‚ç¬¬äºŒç§é€‰æ‹©æ˜¯æµæ°´çº¿å¹¶è¡Œï¼Œå®ƒå°†æ¨¡å‹æŒ‰å±‚åˆ†æˆå¤§å°ç›¸ç­‰çš„å‡ å—ã€‚å‚æ•° `pipeline-model-parallel-size` è¡¨ç¤ºå°†æ¨¡å‹åˆ†æˆçš„å—æ•°ã€‚æœ‰å…³è¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…æ­¤ [åšå®¢](https://huggingface.co/blog/zh/bloom-megatron-deepspeed)

### å°†æ¨¡å‹è½¬æ¢ä¸º ğŸ¤— Transformers

è®­ç»ƒç»“æŸåï¼Œæˆ‘ä»¬å¸Œæœ›ä½¿ç”¨ `transformers` åº“å¯¹è¯¥æ¨¡å‹è¿›è¡Œè¯„ä¼°æˆ–å°†å…¶éƒ¨ç½²åˆ°ç”Ÿäº§ç¯å¢ƒä¸­ã€‚ä½ å¯ä»¥æŒ‰ç…§ [æ•™ç¨‹](https://huggingface.co/nvidia/megatron-gpt2-345m) å°†å…¶è½¬æ¢ä¸º `transformers` æ¨¡å‹ã€‚ä¾‹å¦‚ï¼Œåœ¨è®­ç»ƒå®Œæˆåï¼Œä½ å¯ä»¥å¤åˆ¶ç¬¬ 150k æ¬¡è¿­ä»£çš„æƒé‡ï¼Œå¹¶ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤å°†æ–‡ä»¶ `model_optim_rng.pt` è½¬æ¢ä¸º `transformers` æ”¯æŒçš„ `pytorch_model.bin` æ–‡ä»¶:

```bash
# to execute outside the container:
mkdir -p nvidia/megatron-codeparrot-small
# copy the weights from the container
sudo docker cp CONTAINER_ID:/workspace/Megatron-LM/experiments/codeparrot-small/iter_0150000/mp_rank_00/model_optim_rng.pt nvidia/megatron-codeparrot-small
git clone https://github.com/huggingface/transformers.git
git clone https://github.com/NVIDIA/Megatron-LM.git
export PYTHONPATH=Megatron-LM
python transformers/src/transformers/models/megatron_gpt2/convert_megatron_gpt2_checkpoint.py nvidia/megatron-codeparrot-small/model_optim_rng.pt
```

è¯·æ³¨æ„ï¼Œå¦‚æœä½ æ‰“ç®—ä½¿ç”¨åŸå§‹çš„åˆ†è¯å™¨ï¼Œä½ å°†éœ€è¦åœ¨è½¬æ¢åå°†ç”Ÿæˆçš„è¯æ±‡æ–‡ä»¶å’Œåˆå¹¶è¡¨æ›¿æ¢ä¸ºæˆ‘ä»¬ä¹‹å‰ä»‹ç»çš„åŸå§‹æ–‡ä»¶ã€‚

ä¸è¦å¿˜è®°å°†ä½ çš„æ¨¡å‹æ¨é€åˆ° hub å¹¶ä¸ç¤¾åŒºåˆ†äº«ï¼Œåªéœ€ä¸‰è¡Œä»£ç  ğŸ¤—:

```python
from transformers import AutoModelForCausalLM

model = AutoModelForCausalLM.from_pretrained("nvidia/megatron-codeparrot-small")
# this creates a repository under your username with the model name codeparrot-small
model.push_to_hub("codeparrot-small")
```

ä½ è¿˜å¯ä»¥è½»æ¾åœ°ä½¿ç”¨å®ƒæ¥ç”Ÿæˆæ–‡æœ¬:

```python
from transformers import pipeline

pipe = pipeline("text-generation", model="your_username/codeparrot-small")
outputs = pipe("def hello_world():")
print(outputs[0]["generated_text"])
```

```
 def hello_world():
    print("Hello World!")
```

Transformers è¿˜å¯ä»¥æœ‰æ•ˆåœ°å¤„ç†å¤§æ¨¡å‹æ¨ç†ã€‚å¦‚æœä½ è®­ç»ƒäº†ä¸€ä¸ªéå¸¸å¤§çš„æ¨¡å‹ (ä¾‹å¦‚è®­ç»ƒæ—¶ä½¿ç”¨äº†æ¨¡å‹å¹¶è¡Œ)ï¼Œä½ å¯ä»¥é€šè¿‡ä»¥ä¸‹å‘½ä»¤è½»æ¾åœ°è¿›è¡Œæ¨ç†:

```python
from transformers import AutoModelForCausalLM

model = AutoModelForCausalLM.from_pretrained("your_username/codeparrot-large", device_map="auto")
```

è¿™å°†åœ¨å†…éƒ¨è°ƒç”¨ [accelerate åº“](https://huggingface.co/docs/accelerate/index) è‡ªåŠ¨åœ¨ä½ å¯ç”¨çš„è®¾å¤‡ (GPUã€CPU RAM) ä¹‹é—´åˆ†é…æ¨¡å‹æƒé‡ã€‚

å…è´£å£°æ˜: æˆ‘ä»¬å·²ç»è¯æ˜ä»»ä½•äººéƒ½å¯ä»¥ä½¿ç”¨ Megatron-LM æ¥è®­ç»ƒè¯­è¨€æ¨¡å‹ã€‚é—®é¢˜æ˜¯æˆ‘ä»¬éœ€è¦è€ƒè™‘ä»€ä¹ˆæ—¶å€™ä½¿ç”¨å®ƒã€‚ç”±äºé¢å¤–çš„é¢„å¤„ç†å’Œè½¬æ¢æ­¥éª¤ï¼Œè¿™ä¸ªæ¡†æ¶æ˜¾ç„¶å¢åŠ äº†ä¸€äº›æ—¶é—´å¼€é”€ã€‚å› æ­¤ï¼Œé‡è¦çš„æ˜¯ä½ è¦è€ƒè™‘å“ªä¸ªæ¡†æ¶æ›´é€‚åˆä½ çš„éœ€æ±‚å’Œæ¨¡å‹å¤§å°ã€‚æˆ‘ä»¬å»ºè®®å°†å…¶ç”¨äºé¢„è®­ç»ƒæ¨¡å‹æˆ–å¾®è°ƒï¼Œä½†å¯èƒ½ä¸é€‚ç”¨äºä¸­å‹æ¨¡å‹çš„å¾®è°ƒã€‚ `APITrainer` å’Œ `accelerate` åº“å¯¹äºæ¨¡å‹è®­ç»ƒåŒæ ·ä¹Ÿéå¸¸æ–¹ä¾¿ï¼Œå¹¶ä¸”å®ƒä»¬ä¸è®¾å¤‡æ— å…³ï¼Œä¸ºç”¨æˆ·æä¾›äº†æå¤§çš„çµæ´»æ€§ã€‚

æ­å–œ ğŸ‰ ç°åœ¨ä½ å­¦ä¼šäº†å¦‚ä½•åœ¨ Megatron-LM æ¡†æ¶ä¸­è®­ç»ƒ GPT2 æ¨¡å‹å¹¶ä½¿å…¶æ”¯æŒ `transformers`ï¼

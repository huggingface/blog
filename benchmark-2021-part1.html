<!DOCTYPE html>
    <html>
    <head>
        <meta charset="UTF-8">
        <title>Scaling up BERT-like model Inference on modern CPU</title>
        <style>
</style>
        <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.10.2/dist/katex.min.css" integrity="sha384-yFRtMMDnQtDRO8rLpMIKrtPCD5jdktao2TV19YiZYWMDkUR5GQZR/NOVTdquEx1j" crossorigin="anonymous">
<link href="https://cdn.jsdelivr.net/npm/katex-copytex@latest/dist/katex-copytex.min.css" rel="stylesheet" type="text/css">
        <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/Microsoft/vscode/extensions/markdown-language-features/media/markdown.css">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/Microsoft/vscode/extensions/markdown-language-features/media/highlight.css">
<style>
            body {
                font-family: -apple-system, BlinkMacSystemFont, 'Segoe WPC', 'Segoe UI', system-ui, 'Ubuntu', 'Droid Sans', sans-serif;
                font-size: 14px;
                line-height: 1.6;
            }
        </style>
        <style>
.task-list-item { list-style-type: none; } .task-list-item-checkbox { margin-left: -20px; vertical-align: middle; }
</style>
        
        <script src="https://cdn.jsdelivr.net/npm/katex-copytex@latest/dist/katex-copytex.min.js"></script>
        
    </head>
    <body class="vscode-body vscode-light">
        <style>
  .centered {
      display: block;
      margin: 0 auto;
  }
</style>
<div class="blog-metadata">
    <small>Published April 13, 2021.</small>
</div>
<div class="author-card">
    <a href="/mfuntowicz">
        <img class="avatar avatar-user" src="https://aeiljuispo.cloudimg.io/v7/https://s3.amazonaws.com/moonup/production/uploads/1583858935715-5e67c47c100906368940747e.jpeg?w=200&h=200&f=face" title="Gravatar">
        <div class="bfc">
            <code>mfuntowicz</code>
            <span class="fullname">Morgan Funtowicz</span>
        </div>
    </a>
</div>
<h1 id="scaling-up-bert-like-model-inference-on-modern-cpu">Scaling up BERT-like model Inference on modern CPU</h1>
<h2 id="context-and-motivations">Context and Motivations</h2>
<p>Back in October 2019, my colleague Lysandre Debut published an exhaustive <em>(at that time)</em> <a href="https://medium.com/huggingface/benchmarking-transformers-pytorch-and-tensorflow-e2917fb891c2">benchmarking
blog (1)</a>.</p>
<p>Since then, <a href="https://github.com/huggingface/transformers">ðŸ¤— transformers (2)</a> welcomed a tremendous number
of new architectures and thousands of new models were added to the <a href="https://huggingface.co/models">ðŸ¤— hub (3)</a>
which now counts more than 7000 of them as of first quarter of 2021.</p>
<p>In this context, it is very hard to cover them all, thus we decided to focus on the most
known one around here, <a href="https://arxiv.org/abs/1810.04805v1">BERT (Delvin &amp; al. 2018) (4)</a>.
In essence, while we focus this blog post on BERT-like models to keep the article concise, all the elements
presented can be applied to any architecture on the hub. Also, in this blogpost we will not go over the
details of the Transformer architecture. Still, if you want some more details, I can't recommend more the
<a href="https://jalammar.github.io/illustrated-transformer/">Illustrated Transformer blogpost from Jay Alammar (5)</a>.</p>
<p>Today's goals are to give the reader an idea of where we are from an Open Source perspective when it comes to use BERT-like
models for inference on PyTorch and TensorFlow but also what he can easily leverage in order to speedup inference.</p>
<h2 id="benchmarking-methodology">Benchmarking methodology</h2>
<p>When it comes to leveraging BERT-like models from Hugging Face's model hub there are many knobs which can
be tuned to make things faster. Also, in order to quantify what &quot;faster&quot; means, we will rely on widely adopted metrics:</p>
<ul>
<li><strong>Latency</strong>: Time it takes for a single execution of the model (i.e. forward call)</li>
<li><strong>Throughput</strong>: Number of executions performed in a fixed amount of time</li>
</ul>
<p>These two metrics will help us understand the benefits and compromises along this blog post.</p>
<p>The benchmark was reimplemented from scratch in order to integrate the latest features provided by transformers
along with being able to let the community run benchmarks and share them in an <strong>hopefully easier</strong> way.
The whole framework is now based on Facebook AI &amp; Research's Hydra configuration library allowing us to easily report
all the items which were in place while running the benchmark, effectively increase the reproducibility of such
benchmark.</p>
<p>On the 2021 version, we kept the ability to run inference workloads through PyTorch and Tensorflow as in the
previous blog <a href="https://medium.com/huggingface/benchmarking-transformers-pytorch-and-tensorflow-e2917fb891c2">(5)</a> along with their traced counterpart
<a href="https://pytorch.org/docs/stable/jit.html">TorchScript (6)</a>, <a href="https://www.tensorflow.org/xla">Google Accelerated Linear Algebra (XLA) (7)</a>.</p>
<p>Also, we decided to include support for<a href="https://www.onnxruntime.ai/">ONNX Runtime (8)</a> as it provides many optimizations
especially targeting transformers based model which de facto makes it a strong candidates to consider when speaking about
performances.</p>
<p>Last but not least, this new unified, benchmarking environment will allow us to easily run inference for different scenarios
such as <a href="https://arxiv.org/abs/1609.07061">Quantized Models (Hubara &amp; al. 2016) (9)</a>
using less precise number representation (<code>float16</code>, <code>int8</code>, <code>int4</code>). This method known as <strong>quantization</strong> has seen an increased adoption amount all the the major hardware providers.
Also, in the near future, we would like integrate alternative methods we are actively working on at Hugging Face, namely Distilation, Pruning &amp; Sparsificaton.</p>
<h2 id="baselines">Baselines</h2>
<p>All the results below were run on <a href="https://aws.amazon.com/ec2/instance-types/c5">Amazon Web Services (AWS) c5.metal instance</a>
leveraging an Intel Xeon Platinum 8275 CPU (48 cores/96 threads).
The choice of this instance provides all the CPU features useful in order to speedup Deep Learning workloads such as:</p>
<ul>
<li>AVX512 instructions set (<em>which might not be leveraged out-of-the-box by the various frameworks</em>)</li>
<li>Intel Deep Learning Boost (also known as Vector Neural Network Instruction - VNNI) which provides specialized
CPU instructions for running quantized network (int8)</li>
</ul>
<p>The choice of using <em>metal</em> instance is to avoid any virtualization issue which can arise when using cloud providers.
This gives us full control of the hardware, especially while targeting the NUMA (Non-Unified Memory Architecture) controller we will cover later in this blog.</p>
<p><em>The operating system was Ubuntu 20.04 (LTS) and all the experiments were conducted using Hugging Face transformers version 4.5.0</em></p>
<h2 id="out-of-the-box-results">Out of the box results</h2>
<p><img src="file:///d:\Workspace\HuggingFace\blog\assets\19_benchmark_2021_part1\imgs\pytorch_vs_tf_oob.svg" alt="pytorch versus tensorflow out of the box"></p>
<p><img src="file:///d:\Workspace\HuggingFace\blog\assets\19_benchmark_2021_part1\imgs\pytorch_vs_tf_oob_big_batch.svg" alt="pytorch versus tensorflow out of the box bigger batch sizes"></p>
<p>The above results correctly report the following assumptions:</p>
<ul>
<li>Sequence length has a quadratic impact on the performances</li>
<li>Batch size has a linear impact on the performances</li>
</ul>
<p>Also, PyTorch shows better inference results over TensorFlow for all the configurations tested here.
Along with PyTorch providing globally better out-of-the-box inference performances it seems the batch scalability is
better, with no real performance impact when the sequence length remains small.</p>
<p>One possible way to explain such difference between the two framework might be the underlying technology to
execute parallel sections within operators. PyTorch internally uses OpenMP along with Intel MKL (now oneDNN) for
efficient linear algebra computations whereas TensorFlow relies on Eigen and its own threading implementation.</p>
<h2 id="scaling-bert-inference-to-increase-overall-throughput-on-modern-cpu">Scaling BERT Inference to increase overall throughput on modern CPU</h2>
<h3 id="introduction">Introduction</h3>
<p>There are multiple ways to improve the latency and throughput for tasks such as BERT inference.
Improvements and tuning can be performed at various levels from enabling Operating System features, swapping dependant
libraries with more performant ones, carefully tuning framework properties and last but not least
using parallelization logic leveraging all the cores on the CPU(s).</p>
<p>For the remaining of this blog post we will focus on the latter, also known as <strong>Multiple Inference Stream</strong>.</p>
<p>The idea is simple: Allocate <strong>multiple instances</strong> of the same model and assign the execution of each instance to a
<strong>dedicated, non-overlapping subset of the CPU cores</strong> in order to have truly parallel instances.</p>
<h3 id="core-count-scaling---does-using-more-cores-actually-improve-performances">Core count scaling - Does using more cores actually improve performances?</h3>
<p>At this stage, you may wonder what is the point of allocating only a subset of the cores to the task rather than throwing
all the horses to achieve the minimum latency.</p>
<p>Rationally, throwing more resources to the task might give better results.
This statement really depends on the problem size.<br>
Indeed, it's possible than for small problems putting more threads at work
doesn't bring improvements over the latency measurements.</p>
<p>In order to illustrate this, the figure below takes different problem sizes (<code>sequence length = {32, 128, 512}</code>)
and reports the latencies with respect to the number of threads used for running
computations for both PyTorch and TensorFlow.</p>
<p>Limiting the number of resources involved in computation is done by limiting the number of threads involved in
<strong>intra</strong> operations (<em><strong>intra</strong> here means inside an operator doing computation, also known as &quot;kernel&quot;</em>).</p>
<p>This is achieved through the following API:</p>
<ul>
<li>PyTorch: <code>torch.set_num_threads(x)</code></li>
<li>TensorFlow: <code>tf.config.threading.set_intra_op_parallelism_threads(x)</code></li>
</ul>
<p><img src="file:///d:\Workspace\HuggingFace\blog\assets\19_benchmark_2021_part1\imgs\core_count_scaling.svg" alt="pytorch_tensorflow_intraops_scaling"></p>
<p>As you can see, depending on the problem size, the number of threads involved in the computations has a positive impact
on the latency measurements.</p>
<p>Still, for small-sized problems, using only 8 or 16 threads already gives the best performances,
the same applies for medium-sized problems where using a single CPU (24 threads) gives the best results. Finally, for
large-sized problems, the overhead of the cross-socket communication is covered by the computations cost, thus benefiting from
using all the cores available on the system.</p>
<h3 id="cores-and-threads-on-modern-cpus">Cores and Threads on Modern CPUs</h3>
<p>On our way towards optimizing CPU inference for better usage of the CPU cores you might have already seen -<em>at least for the
past 20 years</em>- modern CPUs specifications reports &quot;cores&quot; and &quot;threads&quot; or &quot;physical&quot; and &quot;logical&quot; numbers.
These notions refer to a mechanism called <strong>Simultaneous Multi-Threading</strong> (SMT).</p>
<p>To put things simple, imagine two tasks <strong>A</strong> and <strong>B</strong>, executing in parallel, each on its own thread.<br>
At some point, there is a high probability these two tasks will have to wait for some resources to be fetched from main memory, SSD, HDD
or even the network.<br>
During these periods the core executing the task is in an <strong>Idle</strong> state waiting for the resources to arrive, and effectively doing nothing...</p>
<p>Now, with <strong>SMT</strong>, the <strong>two threads for task A and B</strong> will be scheduled on the same <strong>physical core</strong>,
but their execution will be interleaved:<br>
When task <strong>A</strong> receives a system interrupt, task <strong>B</strong> will resume
its execution, then task <strong>A</strong>, etc. which increases overall core utilization.</p>
<!-- The figure below explains the above visually ( [source](https://appuals.com/how-does-hyper-threading-work-in-intel-core-i7-processors/) ) -->
<img class="centered" alt="Intel Hyper Threading technology" src="file:///d:\Workspace\HuggingFace\blog\assets\19_benchmark_2021_part1\imgs\hyper_threading_explained.png" />
<p>Back to our model inference workload... If you think about it, in a perfect world and very optimized setup, computations take the majority of time.</p>
<p>In this context, using the logical cores shouldn't bring us any performance benefits because our computation work
(<em>task A in the example above</em>) is not letting the second one (<em>task B</em>) being schedule on the CPU core.</p>
<p><img src="file:///d:\Workspace\HuggingFace\blog\assets\19_benchmark_2021_part1\imgs\pytorch_tf_intel_ht_impact.svg" alt="Pytorch and TensorFlow Hyper-Threading impact on latency"></p>
<h3 id="leveraging-multi-sockets-servers-and-cpu-affinity">Leveraging Multi-Sockets servers and CPU affinity</h3>
<p>Nowadays servers brings many cores, some of them even support multi-sockets setups (<em>i.e. multiple CPUs on the motherboard</em>).<br>
On Linux, the command <code>lscpu</code> reports all the specifications and topology of the CPUs present on the system:</p>
<pre><code class="language-shell"><div>ubuntu@some-ec2-machine:~$ lscpu
Architecture:                    x86_64
CPU op-mode(s):                  32-bit, 64-bit
Byte Order:                      Little Endian
Address sizes:                   46 bits physical, 48 bits virtual
CPU(s):                          96
On-line CPU(s) list:             0-95
Thread(s) per core:              2
Core(s) per socket:              24
Socket(s):                       2
NUMA node(s):                    2
Vendor ID:                       GenuineIntel
CPU family:                      6
Model:                           85
Model name:                      Intel(R) Xeon(R) Platinum 8275CL CPU @ 3.00GHz
Stepping:                        7
CPU MHz:                         1200.577
CPU max MHz:                     3900.0000
CPU min MHz:                     1200.0000
BogoMIPS:                        6000.00
Virtualization:                  VT-x
L1d cache:                       1.5 MiB
L1i cache:                       1.5 MiB
L2 cache:                        48 MiB
L3 cache:                        71.5 MiB
NUMA node0 CPU(s):               0-23,48-71
NUMA node1 CPU(s):               24-47,72-95
</div></code></pre>
<p>In our case we have a machine with <strong>2 sockets</strong>, each socket providing <strong>24 physical cores</strong> with <strong>2 threads per cores</strong> (SMT).<br>
One another interesting point is the notion of <strong>NUMA</strong> node (0, 1) which represents how cores and memory are being
mapped on the system.</p>
<p>Non-Uniform Memory Access (<strong>NUMA</strong>) is the opposite of Uniform Memory Access (<strong>UMA</strong>) where the whole memory pool
is accessible by all the cores through a single unified bus between sockets and the main memory.
<strong>NUMA</strong> on the other hand splits the memory pool and each CPU socket is responsible to address a subset of the memory,
reducing the congestion on the bus.</p>
<img class="centered" alt="Non-Uniform Memory Access and Uniform Memory Access architectures" src="file:///d:\Workspace\HuggingFace\blog\assets\19_benchmark_2021_part1\imgs\UMA_NUMA.png" />
<!-- ![Non-Uniform Memory Access and Uniform Memory Access architectures](assets/19_benchmark_2021_part1/imgs/UMA_NUMA.png) -->
<p>In order to fully exploit the potential of such beefy machine, we need to ensure our model instances are correctly
dispatched across all the <strong>physical</strong> cores on all sockets along with enforcing memory allocation to be &quot;NUMA-aware&quot;.</p>
<p>On Linux, NUMA's process configuration can be tuned through <code>numactl</code> which provides an interface to bind a process to a
set of CPU cores (referred as <strong>Processor Affinity</strong>).<br>
Also, it allows tuning the memory allocation policy, making sure the memory allocated for the process
is as close as possible to the cores memory pool (referred as <strong>Explicit Memory Allocation Directives</strong>).</p>
<p><em>Note: Setting both cores and memory affinities is important here. Having computations done on socket 0 and memory allocated
on socket 1 would ask the system to go over the sockets shared bus to exchange memory, thus leading to an unwanted overhead.</em></p>
<h3 id="tuning-process-affinity">Tuning Process Affinity</h3>
<p>Now that we have all the knobs required to control the resources' allocation of our model instances we go further and see how to
effectively deploy those and see the impact on latency and throughput.<br>
Let's go gradually to get a sense of what is the impact of each command and parameter.</p>
<p>First, we start by launching our inference model without any tuning, and we observe how the computations are being dispatched on CPU cores (<em>Left</em>).</p>
<pre><code class="language-shell"><div>python3 src/main.py model=bert-base-cased backend.name=pytorch batch_size=1 sequence_length=128
</div></code></pre>
<p>Then we specify the core and memory affinity through <code>numactl</code> spawning all (<em>and only</em>) the <strong>physical</strong> cores (<em>Right</em>):</p>
<pre><code class="language-shell"><div>numactl -C 0-47 -m 0,1 python3 src/main.py model=bert-base-cased backend.name=pytorch batch_size=1 sequence_length=128
</div></code></pre>
<!-- ![htop CPU usage without and with numactl process affinity set](assets/19_benchmark_2021_part1/imgs/numa_combined.svg) -->
<img class="centered" alt="htop CPU usage without and with numactl process affinity set" src="file:///d:\Workspace\HuggingFace\blog\assets\19_benchmark_2021_part1\imgs\numa_combined.svg" /> 
<p>As you can see, without any specific tuning, PyTorch and TensorFlow dispatch the work on a single core, using both physical <strong>and</strong> logical cores.<br>
Also, as we highligted earlier, we do not want to leverage the <strong>SMT</strong> feature in our case, so we set the process' cores affinity to target only physical cores.</p>
<p>Let's take sometime from here to highlight what we did with <code>numactl</code>:</p>
<ul>
<li><code>-C 0-47</code> indicates to <code>numactl</code> what is the processor affinity (cores 0 to 47).</li>
<li><code>-m 0,1</code> indicates to <code>numactl</code> to allocate memory on both CPU sockets</li>
</ul>
<p>If you wonder why we are binding the process to cores [0...47], you need to go back to look at the output of <code>lscpu</code>.<br>
From there you will find the section <code>NUMA node0</code> and <code>NUMA node1</code> which has the form <code>NUMA node&lt;X&gt; &lt;physical ids&gt;/&lt;logicial ids&gt;</code></p>
<p>In our case, physical cores range from 0 to 23 on the first CPU and from 24 to 47 on the second one.<br>
Cores 48 to 71 and 72 to 95 correspond to the logical cores (SMT) respectively on the first and second CPU.</p>
<p>As we are targeting physical cores only to avoid Intel Hyper-Threading here, the above explains why we restrict processor affinity
to the range <code>[0...47]</code>.<br>
Moreover, the range <code>[0...47]</code> spawn cores across both sockets, so we need to bind the memory allocations accordingly (<code>0,1</code>).</p>
<p><em>Please note <strong>this setup is significantly slower than just launching without <code>numactl</code></strong> for small-sized problem.<br>
This slowness is expected as the computations span over the two CPUs it involves cross-socket communication overhead
which in this case is higher than the overall computations time.</em></p>
<h2 id="multi-stream-inference---end-to-end">Multi-Stream Inference - End to end</h2>
<p>If you keep on reading until here, you should now be in a good shape to setup optimized inference workload on CPU.<br>
Now, we are going to highlight some possiblities offered by the beefy hardware we have and the tuning the knobs described before to scale
our inference as linearly as possible.</p>
<p>Let's start simple, if we want to spawn 2 instances, one on each socket with 24 cores assigned:</p>
<pre><code class="language-shell"><div>numactl -C 0-23 -m 0 python3 src/main.py model=bert-base-cased batch_size=1 sequence_length=128 backend.name=pytorch backend.num_threads=24
numactl -C 24-47 -m 1 python3 src/main.py model=bert-base-cased batch_size=1 sequence_length=128 backend.name=pytorch backend.num_threads=24
</div></code></pre>
<p>Starting from here, both instances doesn't share any resources between them and everything is operating as its maximum efficiency from a
hardware perspective.<br>
The latency measurements are identical to what a single instance would achieve but throughput is actually 2x higher
as the two instances operate in a truly parallel way.</p>
<p>We can further increase the number of instances, lowering the number of cores assigned for each instance.<br>
Let's run 4 independent instances, each of them effectively bound to 12 CPU cores.</p>
<pre><code class="language-shell"><div>numactl -C 0-11 -m 0 python3 src/main.py model=bert-base-cased batch_size=1 sequence_length=128 backend.name=pytorch backend.num_threads=12
numactl -C 12-23 -m 0 python3 src/main.py model=bert-base-cased batch_size=1 sequence_length=128 backend.name=pytorch backend.num_threads=12
numactl -C 24-35 -m 1 python3 src/main.py model=bert-base-cased batch_size=1 sequence_length=128 backend.name=pytorch backend.num_threads=12
numactl -C 36-47 -m 1 python3 src/main.py model=bert-base-cased batch_size=1 sequence_length=128 backend.name=pytorch backend.num_threads=12
</div></code></pre>
<p>The outcomes remains the same, our 4 instances are effectively running in a truely parallel manner.<br>
The latency will be roughtly the same on each instance, but the throughput 4x higher.</p>
<p>Last but not least, it also brings the possibility to have multiple instances carefully tuned for various problem sizes.<br>
With a smart dispatching approach, one can redirect incoming requests to the right configuration giving the best latency depending on request's workload.</p>
<pre><code class="language-shell"><div><span class="hljs-meta">#</span><span class="bash"> Small-sized problems (sequence length &lt;= 32) uses only 8 cores (on CPU 0 - 8/24 cores used)</span>
numactl -C 0-7 -m 0 python3 src/main.py model=bert-base-cased batch_size=1 sequence_length=32 backend.name=pytorch backend.num_threads=8
<span class="hljs-meta">
#</span><span class="bash"> Medium-sized problems (32 &gt; sequence &gt;= 384) use remaining 16 cores (on CPU 0 - (8+16)/24 cores used)</span>
numactl -C 8-23 -m 0 python3 src/main.py model=bert-base-cased batch_size=1 sequence_length=128 backend.name=pytorch backend.num_threads=16
<span class="hljs-meta">
#</span><span class="bash"> Large sized problems (sequence &gt;= 512) use the entire CPU on the second socket (on CPU 1 - 24/24 cores used)</span>
numactl -C 24-37 -m 1 python3 src/main.py model=bert-base-cased batch_size=1 sequence_length=128 backend.name=pytorch backend.num_threads=24
</div></code></pre>
<p>The following section summarizes the performances of Multi-Stream Inference, leveraging all the knobs explained above.</p>
<h3 id="batch-size-scaling-results">Batch size scaling results</h3>
<p>The first chart below reports the best latency setup depending on the problem size (<em>w.r.t the sequence length</em>).
This corresponds to taking the <strong>maximum</strong> inference time for all the instance for a same problem size.
The second one reports the actually scaling efficiency by <strong>summing</strong> the throughput for all instances for a same problem size.</p>
<p><img src="file:///d:\Workspace\HuggingFace\blog\assets\19_benchmark_2021_part1\imgs\batch_scaling_exp.svg" alt="Batch scaling experiment for PyTorch and Tensorflow"></p>
<p><img src="file:///d:\Workspace\HuggingFace\blog\assets\19_benchmark_2021_part1\imgs\batch_scaling_exp_throughput.svg" alt="Batch scaling experiment for PyTorch and Tensorflow"></p>
<h2 id="conclusion">Conclusion</h2>
<p>Through this blog post we covered the basic BERT inference results for both PyTorch and TensorFlow one can expect without from a simple PyPi install and without any further tuning.
The second part described some knobs to better leverage the underlying CPU(s) in order to achieve better scalability on nowadays multi-cores servers.</p>
<p>There are some other settings which can be tuned to further decrease the overall model latency. These settings will be detailed in a second part</p>
<h2 id="acknowledgments">Acknowledgments</h2>
<ul>
<li><a href="https://github.com/omry">Omry Yadan</a> (Facebook FAIR) - Author of <a href="https://github.com/omry/omegaconf">OmegaConf</a> &amp; <a href="https://github.com/facebookresearch/hydra">Hydra</a> for all the tips setting up Hydra correctly.</li>
<li>Sangeeta Bhattacharya (Intel) - For all the help all the way long setting up the experiments and relevant pieces.</li>
<li>Hugging Face colleagues - For all the comments and improvements in the reviewing process.</li>
</ul>
<h2 id="references">References</h2>
<ol>
<li><a href="https://medium.com/huggingface/benchmarking-transformers-pytorch-and-tensorflow-e2917fb891c2">Benchmarking Transformers: PyTorch and TensorFlow</a></li>
<li><a href="https://arxiv.org/abs/1910.03771v2">HuggingFace's Transformers: State-of-the-art Natural Language Processing</a></li>
<li><a href="https://huggingface.co/models">HuggingFace's Model Hub</a></li>
<li><a href="https://arxiv.org/abs/1810.04805v1">BERT - Pre-training of Deep Bidirectional Transformers for Language Understanding (Devlin &amp; al. 2018)</a></li>
<li><a href="https://jalammar.github.io/illustrated-transformer/">Illustrated Transformer blogpost from Jay Alammar</a></li>
<li><a href="https://pytorch.org/docs/stable/jit.html">PyTorch - TorchScript</a></li>
<li><a href="https://www.tensorflow.org/xla">Google Accelerated Linear Algebra (XLA)</a></li>
<li><a href="https://www.onnxruntime.ai/">ONNX Runtime - Optimize and Accelerate Machine Learning Inferencing and Training</a></li>
<li><a href="https://arxiv.org/abs/1609.07061">Quantized Neural Networks: Training Neural Networks with Low Precision Weights and Activations (Hubara &amp; al. 2016)</a></li>
<li><a href="https://software.intel.com/content/www/us/en/develop/articles/optimizing-applications-for-numa.html">Optimizing Applications for NUMA</a></li>
</ol>

    </body>
    </html>
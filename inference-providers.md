---
title: "Welcome to Inference Providers on the Hub ðŸ”¥"
thumbnail: /blog/assets/inference-providers/thumbnail.png
authors:
# - user: ajshinfai
#   guest: true
#   org: FriendliAI
# - user: soominc
#   guest: true
#   org: FriendliAI
# - user: bgchun
#   guest: true
#   org: FriendliAI
- user: julien-c
---

Today, we are launching the integration of four awesome serverless Inference Providers â€“ fal, Replicate, Sambanova, Together AI â€“ directly on the Hubâ€™s model pages. They are also seamlessly integrated into our client SDKs (for JS and Python), making it easier than ever to explore serverless inference of a wide variety of models that run on your favorite providers.

<insert big visual with logos>
 
Weâ€™ve been hosting a serverless Inference API on the Hub for a long time (we launched the v1 in summer 2020 â€“ wow, time flies ðŸ¤¯). While this has enabled easy exploration and prototyping, weâ€™ve since refined our core value proposition towards collaboration, storage, versioning, and distribution of large datasets and models with the community. At the same time, serverless providers have flourished, and the time was right for Hugging Face to offer easy and unified access to serverless inference through a set of great providers. 

Just as we work with great partners like AWS, Nvidia and <insert other partners> for dedicated deployment options via the model pagesâ€™ Deploy button, it was natural to partner with the next generation of serverless inference providers for model-centric, serverless inference.

Hereâ€™s what this enables, taking the timely example of DeepSeek/DeepSeek-R1, a model which has achieved mainstream fame over the past few days ðŸ”¥:

<insert screenshot or GIF of DeepSeek-R1 model page showcasing fast Inference>

[Quote from a few partners]

This is just the start, and weâ€™ll build on top of this with the community in the coming weeks!

---
title: "Welcome to Inference Providers on the Hub ðŸ”¥"
thumbnail: /blog/assets/inference-providers/thumbnail.png
authors:
# - user: ajshinfai
#   guest: true
#   org: FriendliAI
# - user: soominc
#   guest: true
#   org: FriendliAI
# - user: bgchun
#   guest: true
#   org: FriendliAI
- user: julien-c
---

Today, we are launching the integration of four awesome serverless Inference Providers â€“ fal, Replicate, Sambanova, Together AI â€“ directly on the Hubâ€™s model pages. They are also seamlessly integrated into our client SDKs (for JS and Python), making it easier than ever to explore serverless inference of a wide variety of models that run on your favorite providers.

<insert big visual with logos>
 
Weâ€™ve been hosting a serverless Inference API on the Hub for a long time (we launched the v1 in summer 2020 â€“ wow, time flies ðŸ¤¯). While this has enabled easy exploration and prototyping, weâ€™ve since refined our core value proposition towards collaboration, storage, versioning, and distribution of large datasets and models with the community. At the same time, serverless providers have flourished, and the time was right for Hugging Face to offer easy and unified access to serverless inference through a set of great providers. 

Just as we work with great partners like AWS, Nvidia and <insert other partners> for dedicated deployment options via the model pagesâ€™ Deploy button, it was natural to partner with the next generation of serverless inference providers for model-centric, serverless inference.

Hereâ€™s what this enables, taking the timely example of DeepSeek/DeepSeek-R1, a model which has achieved mainstream fame over the past few days ðŸ”¥:

<insert screenshot or GIF of DeepSeek-R1 model page showcasing fast Inference>

Rodrigo Liang, Co-Founder & CEO at [SambaNova](https://huggingface.co/sambanovasystems): "We are excited to be partnering with Hugging Face to accelerate its Inference API. Hugging Face developers now have access to much faster inference speeds on a wide range of the best open source models."

Zeke Sikelianos, Founding Designer at [Replicate](https://huggingface.co/replicate): "Hugging Face is the de facto home of open-source model weights, and has been a key player in making AI more accessible to the world. We use Hugging Face internally at Replicate as our weights registry of choice, and we're honored to be among the first inference providers to be featured in this launch."

**This is just the start, and weâ€™ll build on top of this with the community in the [coming weeks](https://huggingface.co/spaces/huggingface/HuggingDiscussions/discussions/49)!**


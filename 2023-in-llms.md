---
title: "2023, year of open LLMs"
thumbnail: 
authors:
- user: clefourrier
---

# 2023, year of open LLMs
2023 has seen a surge of public interest in Large Language Models (LLMs), and now that most people have an idea of what they are, the public debates between open and closed source have soared. Obviously, at HuggingFace, we really like open models, because they allow science to be reproducible, they empower the community, avoid concentration of power in the hands of a few actors, and reduce the overall carbon impact of models by favoriting their reuse. 
So let's do a retrospective of the year in open LLMs! 

*To keep this document manageable in length, we won't look at code models.*

## Recipe for a pretrained large language model
First, how do you get a large language model? (Feel free to skim this section if you already know!)

The model **architecture** (its code) describes its specific implementation and mathematical shape: it is a list of all its parameters, as well as their connections together. At the moment, most highly performing LLMs are variations on the decoder-only Transformer architecture (see the original encoder-decoder implementation [here](https://arxiv.org/abs/1706.03762), or the now very popular decoder only architecture [there](https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf)). 
The **training dataset** contains all examples and documents on which the model learns, therefore the specific patterns learned. Most of the time, these documents contain text, either in natural language (ex: French, English, Chinese), a programming language (ex: Python, C) or any kind of structured data expressible as text (ex: tables in markdown or latex, equations, ...). A **tokenizer** defines how the text from the training dataset is converted to numbers (as a model is a mathematical function, and therefore eats numbers). This is done by transforming text into sub-units, called tokens (which can be words, sub words, or characters depending on tokenization methods). The vocabulary size of the tokenizer is the number of different units that it knows. We tend to measure the size of a dataset by the **number of tokens** it contains.
**Training hyperparameters** then define how the model should learn, and the processes that the model will undergo to do so. How much should the parameters change to fit each new example? How fast should the model update itself?

Once you have all this, as well as 1) a lot of computing power to train your model and 2) competent and kind people to run and monitor said training, you're all set to reach the last step: instanciating your architecture, and training it on a specific dataset using training hyperparameters, you get model **weights**. These are the actual parameters of the model after learning, and what most people mean when they talk about having access to an open pretrained model. These weights can then be used as such for **inference** (prediction on new inputs).

However, pretrained LLMs, when the weights are released, can also be specialised, and used as a starting point for other applications through fine-tuning. Fine-tuning is applying a new step of training on the model, on a different (often more specialized) dataset, to teach it to be better on your specific application. Though this step has a cost, it is much less costly than training a model from scratch, both financially and environmentally. This is why high quality open source pretrained models are so important, so they can be freely used and built upon by the community. 

## 2022 - From a race for size to a race for data
So, what was available to the community before 2023? 

Up until early 2022, the trend in machine learning was that the bigger a model (the more parameters), the better the performance; it seemed that models going above specific size thresholds jumped in capabilites, two concepts which were dubbed `emergent abilities` and `scaling laws`. Pretrained open source model families published in 2022 mostly followed this approach. 
1. [BLOOM](https://arxiv.org/pdf/2211.05100.pdf) (BigScience Large Open-science Open-access Multilingual Language Model) 
BLOOM is a family of models released by BigScience, a collaborative effort including 1000 researchers across 60 countries and 250 institutions, led by HuggingFace, and the French GENCI and IDRIS. These models use a decoder only transformers, with some minute changes (post embedding normalisation,[^1] and the use of ALiBi positional embeddings [^2]). The biggest model of this family is a 176B parameters model, trained on 350B tokens of multilingual data, in 46 human languages and 13 programming languages. Most of the specific data was released, and details of its sources, curation and processing were published. It is the biggest open source massively multilingual model to date.

2. [OPT](https://arxiv.org/pdf/2205.01068.pdf) (Open Pre-trained Transformer)
The OPT model family was released by Meta. These models use a decoder only transformers, following the tricks of the GPT-3 paper (a specific weights initialisation, pre-normalisation), with some changes to the attention mechanism (alternating dense and locally banded attention layers). The biggest model of this family is a 175B parameters model trained on 180B tokens of data from mostly public sources (books, social data through reddit, news, Wikipedia, and other various internet sources). This model family was of comparable performance to GPT-3 models, using coding optimisation to make it less compute intensive.

3. [GLM-130B](https://arxiv.org/abs/2210.02414) (General Language Model)
GLM-130B was released by Tsinghua University and Zhipu.AI. It uses a full transformer architecture with some changes (post layer-normalisation with DeepNorm, rotary embeddings). The 130B parameters model was trained on 400B tokens of English and Chinese internet data (The Pile, Wudao Corpora, and other Chinese corpora). It was also of comparable performance to GPT-3 models.

4. Specialized models for research
Some smaller open source models were also released, mostly for research purposes: Meta released the [Galactica](https://arxiv.org/pdf/2211.09085.pdf) series, LLM of up to 120B parameters, pre-trained on 106B tokens of scientific literature, and EleutherAI released the [GPT-NeoX-20B](https://huggingface.co/EleutherAI/gpt-neox-20b) model, an entirely open source (architecture, weights, data included) decoder transformer model trained on 500B tokens (using RoPE and some changes to attention and initialisation), to provide a full artefact for scientific investigations.

All these big models were very cool, but also very expensive to use! When running inference (predicting with a model), said model needs to be loaded in memory, but a 100B parameters model needs to be loaded on 220GB of memory (we'll explain how this works later), which is huge, and not accessible to most!

However, in March 2022, a [new paper](https://arxiv.org/pdf/2203.15556.pdf) came out (DeepMind), investigating what the optimal ratio of tokens to model parameters is, for a given compute budget. In other words, if you only have an amount X of money to spend on model training, what should the respective model and data sizes be? The authors found out that, overall, for the average compute budget being spent on LLMs, models should be smaller but trained on considerably more data. Their own model, Chinchilla (not open source), was a 70B parameters model (a third of the size of above models), but trained on 1.4T tokens of data (between 3 and 4 times more data). It had similar or better performance than its bigger counterparts, both open and closed source.

This switch in mindset took the field by storm.

## 2023, a year of open releases
### The rise of small large language models
2023 saw a wave of decoder style transformers arise, with new pretrained models released every few month: LLaMA (by Meta) in February, Pythia (by Eleuther AI) in April, MPT (by MosaicML) in May, X-GEN (by Salesforce) and Falcon (by TIIUAE) in June, LLaMA-2 (by Meta) in July. Qwen (by Alibaba) and Mistral (by Mistral.AI) in September, Yi (by 01-ai) in November, DeciLM (by Deci) in December. All these releases a) included model weights (under varyingly open licenses) and b) had good performance for models on the smaller size (between 7B and 70B parameters), and therefore they became instantly usable by the community. These models almost all use the same decoder-transformer architecture, with different tweaks (ALiBi or RoPE, RMS pre-normalisation, SwiGLU), as well as some changes to the attention functions (Flash-Attention, GQA, sliding windows) and different code base implementations to optimise for training or inference speed. These tweaks are likely to affect the performance and training speed a bit; however, as all the architectures have been released publicly with the weights, the core differences that remain are the training data and the licensing.

The first models of this series were the [LLaMA](https://arxiv.org/pdf/2302.13971.pdf) models, released by the Meta company. The explicit objective of the researchers was to train models for the best possible performance for a given compute budget in a reproducible fashion. However, they decided to take into account not only the training budget, but also the inference one (how much does it cost to actually use the model after its been trained). With this perspective, they decided to train smaller models on even more data and for even longer, to reach higher performance at a small size. The biggest model of the series was a 65B parameters model trained on 1.4T token, but the smaller models (of 6 and 13B parameters) were trained on 1T tokens. Their small 13B LLaMA model outperformed GPT-3 on most benchmarks, and their biggest LLaMA model was state of the art when it came out. However, the weights were released with a non-commercial license. The [Pythia](https://arxiv.org/abs/2304.01373) models were released by the open source non profit lab Eleuther AI, and were a suite of LLMs of different sizes, trained on completely public data, provided to help researchers understand the different steps of LLM training.

The [MPT models](https://www.mosaicml.com/blog/mpt-7b) which came out a couple month later, released by MosaicML, were of similar performance, but licensed for commercial use. Their models were 7B models (followed up by 30B versions in June) trained on 1T tokens of English and code (using data from C4, CommonCrawl, The Stack, S2ORC). They were quickly followed by the 7 and 30B models from the [Falcon series](https://arxiv.org/pdf/2311.16867.pdf), released by TIIUAE, and trained on 1 to 1.5T tokens of English and code (RefinedWeb, Project Gutemberg, Reddit, StackOverflow, Github, arXiv, Wikipedia, among other sources) - a 180B model was also released at the end of the year. The Falcon models data and training process were documented in detail in a technical report. 

Then came the [X-Gen](https://arxiv.org/pdf/2309.03450.pdf) models, 7B parameters trained on 1.5T tokens of "natural language and code", in several steps, following a data scheduling system (not all data is introduced at the same time to the model). Subsequently, Meta released the [LLaMA-2](https://arxiv.org/pdf/2307.09288.pdf) series, a range of 7 to 70B models trained on 2T tokens 'from publicly available sources", with a permissive community license, and a couple months later, [Mistral-7B](https://mistral.ai/news/announcing-mistral-7b/) was released, trained on an unknown number of tokens from data "extracted from the open Web", followed by [DeciLM](https://deci.ai/blog/introducing-DeciLM-7B-the-fastest-and-most-accurate-7b-large-language-model-to-date), also trained on unknown data. In parallel, two bilingual English-Chinese model series were released: [Qwen](https://arxiv.org/abs/2309.16609), from Alibaba, models of 7 to 70B parameters trained on 2.4T tokens, and [Yi](https://huggingface.co/01-ai/Yi-34B), from 01-AI, models of 6 to 34B parameters, trained on 3T tokens. Where previous models were public about their data, these latest releases gave close to no information about what was used to train the models, and their efforts cannot be reproduced - however, they provide starting points for the community through the weights released.

### Dialog models everywhere
All these pretrained models did not come alone - most releases came both with a pre-trained version and a dialog version, using one of several existing approaches. These approaches were almost all developed prior to this year, but really took off with the interest of the general public in chat models. We detail the most well known approaches, but many variations exist!

 **Chat based fine-tuning** is a variant of supervised fine-tuning, where the annotated data is chat data (multiturn dialogue-like data, much like what you would find on social media) that you fine-tune your model on. You use the same technique as when training your model: for decoder transformers, you teach your model to predict the next words one by one (called an auto-regressive approach). 
 **Instruction fine-tuning** (IFT) follows the same approach, but with instruction datasets, which contain a collection of query-like prompts plus answer (with optional additional input if needed). These datasets teach the models how to follow an instruction, and can be human or LLM-generated. 
 Using model originated datasets (either completely generated or from interaction data between users and said model) is one of the ways to accomplish what is called `distillation` - taking the high-level knowledge from a high performing model to train or fine-tune a smaller model.

 Both these methods are relatively easy to implement: you just need to find or generate related datasets, and then fine-tune your model using the same technique as when training. A great number of instruct datasets were published last year, which improved model performance in dialogue-like setups. For more information on this topic, you can read an intro blog [here](https://huggingface.co/blog/dialog-agents). However, the models, though better, can still not match what humans expect. 
 
 **Reinforcement learning from human feedback** (RLHF) is a specific approach which aims to align what model predicts to what humans like best (depending on specific criteria). It was (at the beginning of the year) a new technique for fine-tuning. From a given prompt, the model generates a number of possible answers; humans rank these answers; the rankings are used to train what is called a preference model (which learns to give a score reflecting human preference to answers); the preference model is then used to fine-tune the language model using reinforcement learning. For more detailed information, see this [blog post](https://huggingface.co/blog/rlhf), the [original RLHF paper](https://arxiv.org/abs/1909.08593), or the Anthropic paper on [RLHF](https://arxiv.org/abs/2204.05862). It's a costly method (annotating/ranking + training a new model + fine-tuning is quite expensive) which has been mostly used to align models for safety objectives. A less costly variations of this method have been developed, which uses a high quality LLM to rank model outputs, instead of humans: : **reinforcement learning from AI feedback** (RLAIF). 

 **Direct preference optimisation** (DPO) is also a variation of RLHF, but which abstracts itself from using a preference model - the method needs the same human or AI rankings first, but uses them to update the model directly by looking at the difference between its original policy (way of predicting) and the optimal one (which would predict the best ranked answers). It also allows to align a model to human preferences, while requiring considerably less changes to the model than the previous method.

So to come back to our wave of small open weights models from (mostly) private companies, a lot of them were released with fine-tuned counterparts: MPT-7B also came with an instruct and a chat version, instruct-tuned versions of Falcon and XGen models were released at the end of the year, Llama-2, Qwen and Yi were released with chat versions and DeciLM with an instruct version. The release of Llama-2 was particularly notable due to the strong focus on safety, both on the pretraining and fine-tuning models. 

### What about the community?
The community and researchers at large were not outdone with fine-tunes, and while some used previous datasets, a number of researchers developed their own.

Examples of previously released datasets (pre-2023) of human preferences are [WebGPT](https://huggingface.co/datasets/openai/webgpt_comparisons) by OpenAI, [HH-RLHF dataset](https://github.com/anthropics/hh-rlhf) by Anthropic, [Summarize](https://huggingface.co/datasets/openai/summarize_from_feedback) by OpenAI) and of instructions are [FLAN](https://github.com/google-research/FLAN) 1 and 2 by Google, [Natural Instructions](https://github.com/allenai/natural-instructions) by AllenAI, [Self Instruct](https://github.com/yizhongw/self-instruct), a framework to generate automatic instructions by researchers from different affiliations, [SuperNatural instructions](https://aclanthology.org/2022.emnlp-main.340/), an expert created instruction benchmark sometimes used as fine-tuning data, [Unnatural instructions](https://aclanthology.org/2023.acl-long.806.pdf), automatically generated instruction dataset by Tel Aviv University and Meta, among others.

❄️ Winter: In January this year, the [Human ChatGPT Instruction corpus](https://huggingface.co/datasets/Hello-SimpleAI/HC3) (HC3) was released by Chinese researchers from various institutions, and contained humans versus model answers to various questions. March was filled with releases: Stanford opened the [Alpaca](https://github.com/tatsu-lab/stanford_alpaca) model, which was the first instruction-following LLaMA model (7B), and the associated dataset, 52K instructions generated with an LLM. LAION (a non profit open source lab) released the [Open Instruction Generalist](https://laion.ai/blog/oig-dataset/) (OIG) dataset, 43M instructions both created with data augmentation and compiled from other pre-existing data sources. The same month, LMSYS org (at UC Berkeley) released [Vicuna](https://lmsys.org/blog/2023-03-30-vicuna/), also a LLaMA fine-tune (13B), this time on chat data: conversations between users and ChatGPT, shared publicly by the users themselves on [ShareGPT](https://share-gpt.com/). The [Guanaco](https://huggingface.co/datasets/JosephusCheung/GuanacoDataset) dataset, an extension of the Alpaca dataset (containing an added 500K entries in more languages), was also released, as well as a LLaMA-7B fine-tune on .

🌱 Spring: In April, BAIR (Berkeley AI Research lab) released [Koala](https://bair.berkeley.edu/blog/2023/04/03/koala/), a chat-tuned LLaMA model, using several of the previous datasets (Alpaca, HH-RLHF, WebGPT, ShareGPT), and DataBricks released the [Dolly](https://huggingface.co/datasets/databricks/databricks-dolly-15k) dataset, a great human effort of 15K manually generated instructions as well as the associated model, a Pythia fine-tune.  In May, Tsinghua University released [UltraChat](https://arxiv.org/abs/2305.14233), a dataset of 1.5M conversations containing instructions, and UltraLLaMA, a fine-tune on said dataset. Microsoft then released the [GPT4-LLM](https://github.com/Instruction-Tuning-with-GPT-4/GPT-4-LLM) dataset/framework, to generate instructions with GPT4, and in June, Microsoft research shared a new method, [Orca](https://arxiv.org/pdf/2306.02707.pdf), to construct instruction datasets by using the reasoning trace of larger models (which explain their step by step reasoning) - it was soon reproduced by the community (notably Alignementlab.ai), who created [Open Orca](https://huggingface.co/Open-Orca) datasets, several million of entries, then used to fine-tune a number of models (Llama, Mistral, ...). In May and June, [Camel-AI](https://huggingface.co/camel-ai) released a number of instruction or chat datasets on different topics (more than 20K examples in each domain, physics, biology, chemistry, ...), obtained with GPT4. In June too, the [Airoboros](https://github.com/jondurbin/airoboros) framework to fine-tune models using model generated data (following the self-instruct approach) was released, along with a number of [instruct datasets](https://huggingface.co/jondurbin). 

🌻Summer: In August, [UltraLM](https://github.com/thunlp/UltraChat) (a high performing chat fine-tune of LLaMA) was released by OpenBMB, a Chinese non profit, and in September, they released the associated preference dataset [UltraFeedback](https://huggingface.co/datasets/openbmb/UltraFeedback), a feedback dataset of inputs compared by GPT4 (with annotations). Throughout the summer, [NousResearch](https://huggingface.co/NousResearch), a collaborative, released a number of model fine-tunes (notably the Hermes and Capybara collections), based on several private and public instruct datasets. In September, a student team from Tsinghua University released [OpenChat](https://huggingface.co/openchat/openchat_3.5), a LLaMA fine-tune using a new  RL finetuning strategy. 

🍂 Autumn: In October, HuggingFace released [Zephyr](https://huggingface.co/HuggingFaceH4/zephyr-7b-beta), a Mistral fine-tuned using DPO and AIF on UltraChat and UltraFeedback, and community members released [OpenHermes 2](https://huggingface.co/teknium/OpenHermes-2-Mistral-7B), a Mistral-7B fine-tuned on 900K entries either from the web or generated with Axolotl. Lmsys released LMSYS-Chat-1M, real life user conversations with 25 LLMs. In November, OpenBuddy released OpenBuddy-Zephyr, a Zephyr fine-tuned on multi-turn dialoguue. In November, NVIDIA released [HelpSteer](https://huggingface.co/datasets/nvidia/HelpSteer), an alignement fine-tuning dataset providing prompts, associated model responses, and grades of said answers on several criteria, while Microsoft Research released the [Orca-2](https://huggingface.co/microsoft/Orca-2-13b) model, a Llama 2 fine-tuned on a new synthetic reasoning dataset. In December, Berkeley released [Starling](https://huggingface.co/berkeley-nest/Starling-LM-7B-alpha), a RLAIF fine-tuned of Open-Chat, and the associated dataset, [Nectar](https://huggingface.co/datasets/berkeley-nest/Nectar), 200K entries of comparision data.

As we can see, this whole year's development relies both on the creation of new datasets through the use of high quality pretrained LLMs, as well as on all the open model releases that the community appropriated, making the field go forward by leaps and bounds! And if you now see one of these names in a model name, you'll be able to get an idea of where it's coming from 🤗

*Some more specialized datasets (such as [MetaMath](https://meta-math.github.io/) or [MathInstruct](https://huggingface.co/datasets/TIGER-Lab/MathInstruct) math problem fine-tuning datasets, [Evol-Instruct](https://huggingface.co/datasets/WizardLM/WizardLM_evol_instruct_70k), math and code instructions, [CodeAlpaca](https://huggingface.co/datasets/sahil2801/CodeAlpaca-20k) and [CodeCapybara](https://github.com/FSoft-AI4Code/CodeCapybara) code instructions) were also released, but we won't cover them in detail here, though they have also been used to improve model performance on specific tasks. You can also see the [awesome instructions dataset](https://github.com/jianzhnie/awesome-instruction-datasets) for a compilation of other relevant datasets.*

## Democratizing access
### Merging: Extreme customization
As we can see, with each release, it's getting harder to trace both the data used (as a number of released datasets are compilations of other datasets) and the models' history, as highly performing models are fine-tuned versions of fine-tuned versions of similar models. This summary, in which I'm sure to have forgotten a couple models since it's been quite an eventful year, is not even taking into account merged models. 

But what does it mean to merge a model?

**Model merging** is a way to fuse the weights of several different models, to (ideally) benefit from their respective strong points in one single new model. Several techniques exist to do so: the simplest is averaging the parameters of models sharing a common architecture ([example 1](https://arxiv.org/abs/2204.03044), [example 2](https://arxiv.org/abs/2109.01903)) but more complex parameter combinations exist, such as determining which parameters are the most influential in each model for a given task ([weighted averaging](https://arxiv.org/abs/2111.09832)), or considering parameters interference between models before selecting which parameters to keep when merging ([ties merging](https://arxiv.org/pdf/2306.01708.pdf)). 

These techniques allow anybody to simply generate the combinations of models that they want, which is made especially easy by the fast that most models are nowadays variations of the same architectures. That's how you get models that are named things like `llama2-zephyr-orca-ultra`: this would be a merge of `llama2` and `zephyr` models, fine-tuned on orca and ultra datasets.

### PEFT: Personalization at the tip of your fingers
Sometimes, you want more controlled personalisation, but don't have enough memory to load a whole model in memory to fine tune it. Did you know that you don't need to use an entire model when fine-tuning?

You might want to use what is called **parameter efficient fine-tuning** (PEFT).
This technique first freezes up the parameters of your pretrained model of interest, then adds a number of new parameters on top of it, called the adapters. What you then fine-tune on your task are only the (lightweight) adapter weights, considerably smaller than the original model. You then just need to share your small adapter weights (and the base model)! You'll find a list of interesting approaches for PEFT [here](https://github.com/huggingface/peft).

### Quantization: Models running everywhere
We've seen that well performing models now come in all shapes and sizes… but even then, it doesn't mean that they are accessible to all! A 30B parameters model can require more than 66G of RAM just to load in memory (not even use), and not everyone in the community has the hardware necessary to do so.

That's where quantization comes in! Quantization is a special technique which reduces a model's size by changing the precision of its parameters. 

What does it mean? 

In a computer, numbers are stored with a given precision (such as `float32`, `float16`, `int8`, and so forth). A precision indicates both the number type (is it a floating point number or an integer) as well as on how much memory the number is stored: `float32` stores floating point numbers on 32 bits. For a more in depth explanation, see [this link](https://huggingface.co/docs/optimum/concept_guides/quantization#going-further-how-do-machines-represent-numbers). So, the higher the precision, the more physical memory a number takes, as it will be stored on more bits. 

So if you reduce the precision, you reduce the memory that each model parameter takes in storage, and therefore you reduce the model size! This also means that you reduce... the actual precision of the computations, which can reduce the performance of the model. However, we found out that on bigger models, this performance degradation is actually very [limited](https://huggingface.co/blog/overview-quantization-transformers).

To go back to our above example, our 30B parameters model in `float16` requires a bit less than 66G of RAM, in `8bit` it only requires half that, so 33G of RAM, and it `4bit` we reach even half of this, so around 16G of RAM, making it considerably more accessible.

There are many ways to go from one precision to another, with many different "translation" schemes existing, each with its own benefits and drawbacks. Popular approaches include [bitsandbytes](https://arxiv.org/abs/2208.07339), [GPTQ](https://arxiv.org/abs/2210.17323), and [AWQ](https://arxiv.org/abs/2306.00978). Some users, such as [TheBloke](https://huggingface.co/TheBloke), are even converting popular models to make them accessible to the community. All are very recent and still developing, and we hope to see even more progress on this as time goes on.


## What's next?
The year is not over yet! And these final months have already come with the share of surprises: will a new architecture finally overperform the simple and efficient Transformer?

New releases include
- A mixture of experts:
	- [Mixtral](https://mistral.ai/news/mixtral-of-experts/), the model is made of 8 sub-models (transformer decoders), and for each input, a routeur picks the 2 best sub-models and sums their outputs.
- Several state space models (models which map input to output through a latent space, and which can expressed as either an RNN or a CNN depending on the tasks):
	- [Mamba](https://arxiv.org/pdf/2312.00752.pdf), a state space model with an added selection mechanism
	- [Striped Hyena](https://www.together.ai/blog/stripedhyena-7b), a state space model with fast convolutions kernel

It's still a bit too early to say if these new approaches will take over the Transformer, but state space models are quite promising!

I hope you enjoyed this year's review, and that I made clear how much of AI progress now relies on open source.

[^1]: Post embedding normalisation is a trick to make learning more stable.
[^2]: ALiBi positional embeddings introduce a penalty when tokens too far away in a sequence are connected together by the model (where normal positional embeddings would just store information about the order and respective position of tokens in a sequence).


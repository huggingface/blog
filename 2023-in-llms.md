---
title: "2023, year of open LLMs"
thumbnail: 
authors:
- user: clefourrier
---

# 2023, year of open LLMs
2023 has seen a surge of public interest in Large Language Models (LLMs), and now that most people have an idea of what they are and can do, the public debates around open versus closed source have reached a wide audience as well. At Hugging Face we follow open models with great interest, as they allow research to be reproducible, empower the community to participate in the development of AI models, permit the easier scrutiny of model biases and limitations, and lower the overall carbon impact of our field by favoring checkpoint reuse (among [many other benefits](https://arxiv.org/abs/2302.04844)). 

So let's do a retrospective of the year in open LLMs! 

*To keep this document manageable in length, we won't look at code models.*

## Recipe for a pretrained large language model
First, how do you get a large language model? (Feel free to skim this section if you already know!)

The model **architecture** (its code) describes its specific implementation and mathematical shape: it is a list of all its parameters, as well as how they interact with inputs. At the moment, most highly performing LLMs are variations on the "decoder-only" Transformer architecture (more details in the [original transformers paper](https://arxiv.org/abs/1706.03762)). 
The **training dataset** contains all examples and documents on which the model is trained (aka the parameters are learned), therefore the specific patterns learned. Most of the time, these documents contain text, either in natural language (ex: French, English, Chinese), a programming language (ex: Python, C) or any kind of structured data expressible as text (ex: tables in markdown or latex, equations, ...). A **tokenizer** defines how the text from the training dataset is converted to numbers (as a model is a mathematical function, and therefore need numbers as inputs). Tokenization is done by transforming text into sub-units, called tokens (which can be words, sub words, or characters depending on tokenization methods). The vocabulary size of the tokenizer indicate how many different tokens it knows, typically between 32k and 200k. The size of a dataset is often measured as the **number of tokens** it contains once split in a sequence of these individual, "atomistic" units and these days range from several hundred billions tokens to several trillion tokens!
**Training hyperparameters** then define how the model is trained. How much should the parameters change to fit each new example? How fast should the model be updated?

Once these parameters have been selection, you only need 1) a lot of computing power to train the model and 2) competent (and kind) people to run and monitor the training. The training it-self will consist in instanciating the architecture (creating the matrices on the hardware used for training), and running the training algorithm on the training dataset with the above mentioned hype-rparameters. The result is a set of model **weights**. These are the parameters of the model after learning, and what most people mean when they talk about having access to an open pretrained model. These weights can then be used **inference**, i.e. for prediction on new inputs, for instance to generate text.

Pretrained LLMs can also be specialized or adapted for a specific task after pretraining, in particular when the weights are openly released, They are then used as a starting point for use-cases and applications through a process called "fine-tuning". Fine-tuning consists in applying additional steps of training on the model, on a different ‚Äìoften more specialized and smaller‚Äì dataset, to optimize it for a specific application. Eventhough this step has a cost in terms of compute power needed, it is usually much less costly than training a model from scratch, both financially and environmentally. This is one reason high quality open source pretrained models are very interesting, as they can be freely used and built upon by the community even when the practitioners have only access to a limited compute budget. 

## 2022 - From a race for size to a race for data
What open models were available to the community before 2023?

Up until early 2022, the trend in machine learning was that the bigger a model was (i.e. the more parameters it had), the better its performances. In particular, it seemed that models going above specific size thresholds jumped in capabilities, two concepts which were dubbed `emergent abilities` and `scaling laws`. Pretrained open source model families published in 2022 mostly followed this paradigm. 
1. [BLOOM](https://arxiv.org/pdf/2211.05100.pdf) (BigScience Large Open-science Open-access Multilingual Language Model) 
BLOOM is a family of models released by BigScience, a collaborative effort including 1000 researchers across 60 countries and 250 institutions, coordinated by Hugging Face, in collaboration with the French organizations GENCI and IDRIS. These models use a decoder-only transformers, with minor modifications (post embedding normalisation,[^1] and the use of ALiBi positional embeddings [^2]). The biggest model of this family is a 176B parameters model, trained on 350B tokens of multilingual data, in 46 human languages and 13 programming languages. Most of the training data was released, and details of its sources, curation and processing were published. It is the biggest open source massively multilingual model to date.

2. [OPT](https://arxiv.org/pdf/2205.01068.pdf) (Open Pre-trained Transformer)
The OPT model family was released by Meta. These models use a decoder-only transformers architecture, following the tricks of the GPT-3 paper (a specific weights initialisation, pre-normalisation), with some changes to the attention mechanism (alternating dense and locally banded attention layers). The biggest model of this family is a 175B parameters model trained on 180B tokens of data from mostly public sources (books, social data through reddit, news, Wikipedia, and other various internet sources). This model family was of comparable performance to GPT-3 models, using coding optimisation to make it less compute intensive.

3. [GLM-130B](https://arxiv.org/abs/2210.02414) (General Language Model)
GLM-130B was released by Tsinghua University and Zhipu.AI. It uses a full transformer architecture with some changes (post layer-normalisation with DeepNorm, rotary embeddings). The 130B parameters model was trained on 400B tokens of English and Chinese internet data (The Pile, Wudao Corpora, and other Chinese corpora). It was also of comparable performance to GPT-3 models.

4. Smaller or more specialized open LLM
Some smaller open source models were also released, mostly for research purposes: Meta released the [Galactica](https://arxiv.org/pdf/2211.09085.pdf) series, LLM of up to 120B parameters, pre-trained on 106B tokens of scientific literature, and EleutherAI released the [GPT-NeoX-20B](https://huggingface.co/EleutherAI/gpt-neox-20b) model, an entirely open source (architecture, weights, data included) decoder transformer model trained on 500B tokens (using RoPE and some changes to attention and initialisation), to provide a full artefact for scientific investigations.

These huge models were exciting, but also very expensive to run! When performing inference (computing predictions from a model), the model needs to be loaded in memory, but a 100B parameters model will typically require 220GB of memory to be loaded (we explain this process below), which is very large, and not accessible to most organization and practionners!

However, in March 2022, a [new paper](https://arxiv.org/pdf/2203.15556.pdf) by DeepMind came out, investigating what the optimal ratio of tokens to model parameters is, for a given compute budget. In other words, if you only have an amount X of money to spend on model training, what should the respective model and data sizes be? The authors found out that, overall, for the average compute budget being spent on LLMs, models should be smaller but trained on considerably more data. Their own model, Chinchilla (not open source), was a 70B parameters model (a third of the size of above models), but trained on 1.4T tokens of data (between 3 and 4 times more data). It had similar or better performance than its bigger counterparts, both open and closed source.

This paradigm shift, while probably already known in closed labs took the open science community by storm.

## 2023, a year of open releases
### The rise of small large language models
2023 saw a wave of decoder style transformers arise, with new pretrained models released every few months: LLaMA (by Meta) in February, Pythia (by Eleuther AI) in April, MPT (by MosaicML) in May, X-GEN (by Salesforce) and Falcon (by TIIUAE) in June, LLaMA-2 (by Meta) in July. Qwen (by Alibaba) and Mistral (by Mistral.AI) in September, Yi (by 01-ai) in November, DeciLM (by Deci) and SOLAR (by Upstage) in December. All these releases a) included model weights (under varyingly open licenses) and b) had good performance for models on the smaller side (between 7B and 70B parameters), and therefore they became instantly usable by the community. These models almost all use the same decoder-transformer architecture, with different tweaks (ALiBi or RoPE, RMS pre-normalisation, SwiGLU), as well as some changes to the attention functions (Flash-Attention, GQA, sliding windows) and different code base implementations to optimise for training or inference speed. These tweaks are likely to affect the performance and training speed a bit; however, as all the architectures have been released publicly with the weights, the core differences that remain are the training data and the licensing.

The first models of this series were the [LLaMA](https://arxiv.org/pdf/2302.13971.pdf) models, released by the Meta company. The explicit objective of the researchers was to train models for the best possible performance for a given compute budget in a reproducible fashion. However, they decided to take into account not only the training budget, but also the inference one (how much does it cost to actually use the model after it's been trained). With this perspective, they decided to train smaller models on even more data and for even longer, to reach higher performance at a small size. The biggest model of the series was a 65B parameters model trained on 1.4T token, but the smaller models (of 6 and 13B parameters) were trained on 1T tokens. Their small 13B LLaMA model outperformed GPT-3 on most benchmarks, and their biggest LLaMA model was state of the art when it came out. However, the weights were released with a non-commercial license. The [Pythia](https://arxiv.org/abs/2304.01373) models were released by the open source non-profit lab Eleuther AI, and were a suite of LLMs of different sizes, trained on completely public data, provided to help researchers to understand the different steps of LLM training.

The [MPT models](https://www.mosaicml.com/blog/mpt-7b) which came out a couple months later, released by MosaicML, were of similar performance, but their license allowed commercial use. Their models were 7B models (followed up by 30B versions in June) trained on 1T tokens of English and code (using data from C4, CommonCrawl, The Stack, S2ORC). They were quickly followed by the 7 and 30B models from the [Falcon series](https://arxiv.org/pdf/2311.16867.pdf), released by TIIUAE, and trained on 1 to 1.5T tokens of English and code (RefinedWeb, Project Gutemberg, Reddit, StackOverflow, Github, arXiv, Wikipedia, among other sources) - a 180B model was also released at the end of the year. The Falcon models, data, and training process were documented in detail in a technical report. 

Then came the [X-Gen](https://arxiv.org/pdf/2309.03450.pdf) models, 7B parameters trained on 1.5T tokens of "natural language and code", in several steps, following a data scheduling system (not all data is introduced at the same time to the model). Subsequently, Meta released the [LLaMA-2](https://arxiv.org/pdf/2307.09288.pdf) series, a range of 7 to 70B models trained on 2T tokens "from publicly available sources", with a permissive community license. A couple months later, [Mistral-7B](https://mistral.ai/news/announcing-mistral-7b/) was released, trained on an unknown number of tokens from data "extracted from the open Web", followed by [DeciLM](https://deci.ai/blog/introducing-DeciLM-7B-the-fastest-and-most-accurate-7b-large-language-model-to-date) and [SOLAR](https://huggingface.co/upstage/SOLAR-10.7B-v1.0) also trained on unknown data. In parallel, two bilingual English-Chinese model series were released: [Qwen](https://arxiv.org/abs/2309.16609), from Alibaba, models of 7 to 70B parameters trained on 2.4T tokens, and [Yi](https://huggingface.co/01-ai/Yi-34B), from 01-AI, models of 6 to 34B parameters, trained on 3T tokens. Where previous models were public about their data, these latest releases gave close to no information about what was used to train the models, and their efforts cannot be reproduced - however, they provide starting points for the community through the weights released.

### Dialog models everywhere
All these pretrained models did not come alone - most releases came with both a pre-trained version and a dialog version, using one of several existing approaches. These approaches were almost all developed before this year, but really took off with the interest of the general public in chat models. We detail the most well known approaches, but many variations exist!

 **Chat based fine-tuning** is a variant of supervised fine-tuning, where the annotated data is chat data (multiturn dialogue-like data, much like what you would find on social media) that you fine-tune your model on. You use the same technique as when training your model: for decoder transformers, you teach your model to predict the next words one by one (called an auto-regressive approach). 
 **Instruction fine-tuning** (IFT) follows the same approach, but with instruction datasets, which contain a collection of query-like prompts plus answers (with optional additional input if needed). These datasets teach the models how to follow an instruction, and can be human or LLM-generated. 
 Using model originated datasets (either completely generated or from interaction data between users and said model) is one of the ways to accomplish what is called `distillation` - taking the high-level knowledge from a high performing model to train or fine-tune a smaller model.

 Both these methods are relatively easy to implement: you just need to find or generate related datasets, and then fine-tune your model using the same technique as when training. A great number of instruct datasets were published last year, which improved model performance in dialogue-like setups. For more information on this topic, you can read an intro blog [here](https://huggingface.co/blog/dialog-agents). However, the models, though better, can still not match what humans expect. 
 
 **Reinforcement learning from human feedback** (RLHF) is a specific approach which aims to align what the model predicts to what humans like best (depending on specific criteria). It was (at the beginning of the year) a new technique for fine-tuning. From a given prompt, the model generates a number of possible answers; humans rank these answers; the rankings are used to train what is called a preference model (which learns to give a score reflecting human preference to answers); the preference model is then used to fine-tune the language model using reinforcement learning. For more detailed information, see this [blog post](https://huggingface.co/blog/rlhf), the [original RLHF paper](https://arxiv.org/abs/1909.08593), or the Anthropic paper on [RLHF](https://arxiv.org/abs/2204.05862). It's a costly method (annotating/ranking + training a new model + fine-tuning is quite expensive) which has been mostly used to align models for safety objectives. A less costly variation of this method have been developed that uses a high quality LLM to rank model outputs, instead of humans: **reinforcement learning from AI feedback** (RLAIF). 

**Direct preference optimisation** (DPO) is another variation of RLHF, but does not require the use of a preference model - the method needs the same human or AI rankings first, but uses them to update the model directly by looking at the difference between its original policy (way of predicting) and the optimal one (which would predict the best ranked answers). It also allows to align a model to human preferences, while requiring considerably less changes to the model than the previous method.

So to come back to our wave of small open weights models from (mostly) private companies, a lot of them were released with fine-tuned counterparts: MPT-7B also came with an instruct and a chat version, instruct-tuned versions of Falcon and XGen models were released at the end of the year, Llama-2, Qwen and Yi were released with chat versions and DeciLM with an instruct version. The release of Llama-2 was particularly notable due to the strong focus on safety, both on the pretraining and fine-tuning models. 

### What about the community?
The community and researchers at large were not outdone with fine-tunes, and while some used previous datasets, a number of researchers developed their own.

Examples of previously released datasets (pre-2023) of human preferences are [WebGPT](https://huggingface.co/datasets/openai/webgpt_comparisons) by OpenAI, [HH-RLHF dataset](https://github.com/anthropics/hh-rlhf) by Anthropic, [Summarize](https://huggingface.co/datasets/openai/summarize_from_feedback) by OpenAI. Examples of instruction datasets are [FLAN](https://github.com/google-research/FLAN) 1 and 2 by Google, [Natural Instructions](https://github.com/allenai/natural-instructions) by AllenAI, [Self Instruct](https://github.com/yizhongw/self-instruct), a framework to generate automatic instructions by researchers from different affiliations, [SuperNatural instructions](https://aclanthology.org/2022.emnlp-main.340/), an expert created instruction benchmark sometimes used as fine-tuning data, [Unnatural instructions](https://aclanthology.org/2023.acl-long.806.pdf), an automatically generated instruction dataset by Tel Aviv University and Meta, among others.

‚ùÑÔ∏è Winter: In January this year, the [Human ChatGPT Instruction corpus](https://huggingface.co/datasets/Hello-SimpleAI/HC3) (HC3) was released by Chinese researchers from various institutions, and contained humans versus model answers to various questions. March was filled with releases: Stanford opened the [Alpaca](https://github.com/tatsu-lab/stanford_alpaca) model, which was the first instruction-following LLaMA model (7B), and the associated dataset, 52K instructions generated with an LLM. LAION (a non profit open source lab) released the [Open Instruction Generalist](https://laion.ai/blog/oig-dataset/) (OIG) dataset, 43M instructions both created with data augmentation and compiled from other pre-existing data sources. The same month, LMSYS org (at UC Berkeley) released [Vicuna](https://lmsys.org/blog/2023-03-30-vicuna/), also a LLaMA fine-tune (13B), this time on chat data: conversations between users and ChatGPT, shared publicly by the users themselves on [ShareGPT](https://share-gpt.com/). The [Guanaco](https://huggingface.co/datasets/JosephusCheung/GuanacoDataset) dataset, an extension of the Alpaca dataset (containing an added 500K entries in more languages), was also released, as well as the associated LLaMA-7B fine-tune.

üå± Spring: In April, BAIR (Berkeley AI Research lab) released [Koala](https://bair.berkeley.edu/blog/2023/04/03/koala/), a chat-tuned LLaMA model, using several of the previous datasets (Alpaca, HH-RLHF, WebGPT, ShareGPT), and DataBricks released the [Dolly](https://huggingface.co/datasets/databricks/databricks-dolly-15k) dataset, a great human effort of 15K manually generated instructions as well as the associated model, a Pythia fine-tune.  In May, Tsinghua University released [UltraChat](https://arxiv.org/abs/2305.14233), a dataset of 1.5M conversations containing instructions, and UltraLLaMA, a fine-tune on said dataset. Microsoft then released the [GPT4-LLM](https://github.com/Instruction-Tuning-with-GPT-4/GPT-4-LLM) dataset/framework, to generate instructions with GPT4, and in June, Microsoft research shared a new method, [Orca](https://arxiv.org/pdf/2306.02707.pdf), to construct instruction datasets by using the reasoning trace of larger models (which explain their step by step reasoning) - it was soon reproduced by the community (notably Alignementlab.ai), who created [Open Orca](https://huggingface.co/Open-Orca) datasets, several million of entries, then used to fine-tune a number of models (Llama, Mistral, ...). In May and June, [Camel-AI](https://huggingface.co/camel-ai) released a number of instruction or chat datasets on different topics (more than 20K examples in each domain, physics, biology, chemistry, ...), obtained with GPT4. In June too, the [Airoboros](https://github.com/jondurbin/airoboros) framework to fine-tune models using model generated data (following the self-instruct approach) was released, along with a number of [instruct datasets](https://huggingface.co/jondurbin). 

üåªSummer: In August, [UltraLM](https://github.com/thunlp/UltraChat) (a high performing chat fine-tune of LLaMA) was released by OpenBMB, a Chinese non profit, and in September, they released the associated preference dataset [UltraFeedback](https://huggingface.co/datasets/openbmb/UltraFeedback), a feedback dataset of inputs compared by GPT4 (with annotations). Throughout the summer, [NousResearch](https://huggingface.co/NousResearch), a collective, released a number of model fine-tunes (notably the Hermes and Capybara collections), based on several private and public instruct datasets. In September, a student team from Tsinghua University released [OpenChat](https://huggingface.co/openchat/openchat_3.5), a LLaMA fine-tune using a new  RL finetuning strategy. 

üçÇ Autumn: In October, Hugging Face released [Zephyr](https://huggingface.co/Hugging FaceH4/zephyr-7b-beta), a Mistral fine-tune using DPO and AIF on UltraChat and UltraFeedback, and community members released [OpenHermes 2](https://huggingface.co/teknium/OpenHermes-2-Mistral-7B), a Mistral-7B fine-tuned on 900K entries either from the web or generated with Axolotl. Lmsys released LMSYS-Chat-1M, real life user conversations with 25 LLMs. In November, OpenBuddy released OpenBuddy-Zephyr, a Zephyr fine-tuned on multi-turn dialogue. In November, NVIDIA released [HelpSteer](https://huggingface.co/datasets/nvidia/HelpSteer), an alignement fine-tuning dataset providing prompts, associated model responses, and grades of said answers on several criteria, while Microsoft Research released the [Orca-2](https://huggingface.co/microsoft/Orca-2-13b) model, a Llama 2 fine-tuned on a new synthetic reasoning dataset. In December, Berkeley released [Starling](https://huggingface.co/berkeley-nest/Starling-LM-7B-alpha), a RLAIF fine-tuned of Open-Chat, and the associated dataset, [Nectar](https://huggingface.co/datasets/berkeley-nest/Nectar), 200K entries of comparison data.

As we can see, this whole year's development relies both on the creation of new datasets through the use of high quality pretrained LLMs, as well as on all the open model releases that the community appropriated, making the field go forward by leaps and bounds! And if you now see one of these names in a model name, you'll be able to get an idea of where it's coming from ü§ó

*Some more specialized datasets (such as [MetaMath](https://meta-math.github.io/) or [MathInstruct](https://huggingface.co/datasets/TIGER-Lab/MathInstruct) math problem fine-tuning datasets, [Evol-Instruct](https://huggingface.co/datasets/WizardLM/WizardLM_evol_instruct_70k), math and code instructions, [CodeAlpaca](https://huggingface.co/datasets/sahil2801/CodeAlpaca-20k) and [CodeCapybara](https://github.com/FSoft-AI4Code/CodeCapybara) code instructions) were also released, but we won't cover them in detail here, though they have also been used to improve model performance on specific tasks. You can also see the [awesome instructions dataset](https://github.com/jianzhnie/awesome-instruction-datasets) for a compilation of other relevant datasets.*

## Democratizing access
### Merging: Extreme customization
As we can see, with each release, it's getting harder to trace both the data used (as a number of released datasets are compilations of other datasets) and the models' history, as highly performing models are fine-tuned versions of fine-tuned versions of similar models (see Mistral's "child models tree" [here](https://huggingface.co/spaces/davanstrien/mistral-graph)). This summary, in which I'm sure to have forgotten a couple models since it's been quite an eventful year, is not even taking into account merged models. 

But what does it mean to merge a model?

**Model merging** is a way to fuse the weights of several different models, to (ideally) benefit from their respective strong points in one single new model. Several techniques exist to do so: the simplest is averaging the parameters of models sharing a common architecture ([example 1](https://arxiv.org/abs/2204.03044), [example 2](https://arxiv.org/abs/2109.01903)) but more complex parameter combinations exist, such as determining which parameters are the most influential in each model for a given task ([weighted averaging](https://arxiv.org/abs/2111.09832)), or considering parameters interference between models before selecting which parameters to keep when merging ([ties merging](https://arxiv.org/pdf/2306.01708.pdf)). 

These techniques allow anybody to simply generate the combinations of models that they want, which is made especially easy by the fact that most models are nowadays variations of the same architectures. That's how you get models that are named things like `llama2-zephyr-orca-ultra`: this would be a merge of `llama2` and `zephyr` models, fine-tuned on orca and ultra datasets.

### PEFT: Personalization at the tip of your fingers
Sometimes, you want more controlled personalisation, but don't have enough memory to load a whole model in memory to fine tune it. Did you know that you don't need to use an entire model when fine-tuning?

You might want to use what is called **parameter efficient fine-tuning** (PEFT).
This technique first freezes up the parameters of your pretrained model of interest, then adds a number of new parameters on top of it, called the adapters. What you then fine-tune on your task are only the (lightweight) adapter weights, considerably smaller than the original model. You then just need to share your small adapter weights (and the base model)! You'll find a list of interesting approaches for PEFT [here](https://github.com/huggingface/peft).

### Quantization: Models running everywhere
We've seen that well-performing models now come in all shapes and sizes‚Ä¶ but even then, it doesn't mean that they are accessible to all! A 30B parameters model can require more than 66G of RAM just to load in memory (not even use), and not everyone in the community has the hardware necessary to do so.

That's where quantization comes in! Quantization is a special technique which reduces a model's size by changing the precision of its parameters. 

What does it mean? 

In a computer, numbers are stored with a given precision (such as `float32`, `float16`, `int8`, and so forth). A precision indicates both the number type (is it a floating point number or an integer) as well as on how much memory the number is stored: `float32` stores floating point numbers on 32 bits. For a more in depth explanation, see [this link](https://huggingface.co/docs/optimum/concept_guides/quantization#going-further-how-do-machines-represent-numbers). So, the higher the precision, the more physical memory a number takes, as it will be stored on more bits. 

So if you reduce the precision, you reduce the memory that each model parameter takes in storage, and therefore you reduce the model size! This also means that you reduce... the actual precision of the computations, which can reduce the performance of the model. However, we found out that on bigger models, this performance degradation is actually very [limited](https://huggingface.co/blog/overview-quantization-transformers).

To go back to our above example, our 30B parameters model in `float16` requires a bit less than 66G of RAM, in `8bit` it only requires half that, so 33G of RAM, and it `4bit` we reach even half of this, so around 16G of RAM, making it considerably more accessible.

There are many ways to go from one precision to another, with many different "translation" schemes existing, each with its own benefits and drawbacks. Popular approaches include [bitsandbytes](https://arxiv.org/abs/2208.07339), [GPTQ](https://arxiv.org/abs/2210.17323), and [AWQ](https://arxiv.org/abs/2306.00978). Some users, such as [TheBloke](https://huggingface.co/TheBloke), are even converting popular models to make them accessible to the community. All are very recent and still developing, and we hope to see even more progress on this as time goes on.


## What's next?
The year is not over yet! And these final ~~months~~ ~~days~~ hours have already come with the share of surprises: will a new architecture finally overperform the simple and efficient Transformer?

New releases include
- A mixture of experts:
	- [Mixtral](https://mistral.ai/news/mixtral-of-experts/), the model is made of 8 sub-models (transformer decoders), and for each input, a routeur picks the 2 best sub-models and sums their outputs.
- Several state space models (models which map input to output through a latent space, and which can expressed as either an RNN or a CNN depending on the tasks):
	- [Mamba](https://arxiv.org/pdf/2312.00752.pdf), a state space model with an added selection mechanism
	- [Striped Hyena](https://www.together.ai/blog/stripedhyena-7b), a state space model with fast convolutions kernel

It's still a bit too early to say if these new approaches will take over the Transformer, but state space models are quite promising!

I hope you enjoyed this year's review, and that I made clear how much of AI progress now relies on open source.

[^1]: Post embedding normalisation is a trick to make learning more stable.
[^2]: ALiBi positional embeddings introduce a penalty when tokens too far away in a sequence are connected together by the model (where normal positional embeddings would just store information about the order and respective position of tokens in a sequence).


---
title: "Que se passe-t-il avec l'Open LLM Leaderboard ?"
thumbnail: /blog/assets/evaluating-mmlu-leaderboard/thumbnail.png
authors:
- user: clefourrier
- user: SaylorTwift
- user: slippylolo
- user: thomwolf
translators:
- user: lbourdois
---

# Que se passe-t-il avec l'Open LLM Leaderboard ?

Une discussion int√©ressante a r√©cemment eu lieu sur Twitter suite √† la publication de [**Falcon ü¶Ö**](https://hf.co/tiiuae/falcon-40b) et √† son ajout √† l'[*Open LLM Leaderboard*](https://hf.co/spaces/HuggingFaceH4/open_llm_leaderboard), un classement public comparant les grands mod√®les de langage en libre acc√®s.

La discussion a port√© sur l'une des quatre √©valuations affich√©es dans le classement : un *benchmark* pour mesurer [*Massive Multitask Language Understanding*](https://arxiv.org/abs/2009.03300) (commun√©ment abr√©g√© en MMLU).

La communaut√© a √©t√© √©tonn√©e de constater que les r√©sultats du mod√®le actuellement en t√™te du classement sur le jeu de donn√©es MMLU, le [**LLaMA ü¶ô**](https://ai.facebook.com/blog/large-language-model-llama-meta-ai/), √©taient nettement inf√©rieurs √† ceux indiqu√©s par les auteurs dans le [papier](https://arxiv.org/abs/2302.13971).
Nous avons donc d√©cid√© de nous plonger sur ce point pour comprendre ce qui se passait et comment y rem√©dier üï≥üêá

Dans notre qu√™te, nous avons discut√© avec l'excellent [@javier-m](https://hf.co/javier-m) qui a collabor√© aux √©valuations de LLaMA et l'incroyable [@slippylolo](https://hf.co/slippylolo) de l'√©quipe Falcon. Ceci √©tant dit, toutes les erreurs observ√©es ci-dessous doivent nous √™tre attribu√©es plut√¥t qu'√† eux !
Au cours de ce p√©riple avec nous, vous en apprendrez davantage sur les fa√ßons d'√©valuer un mod√®le √† partir d'une seule √©valuation et sur la question de savoir si vous devez croire ou non les chiffres que vous voyez en ligne et dans les papiers.

Vous √™tes pr√™ts ? Alors attachez votre ceinture, nous d√©collons üöÄ

## Qu'est-ce que l‚Äô*Open LLM Leaderboard* ?

Tout d'abord, notez que l'[*Open LLM Leaderboard*](https://hf.co/spaces/HuggingFaceH4/open_llm_leaderboard) n'est en fait qu'un *wrapper* ex√©cutant la biblioth√®que d'√©valuation open-source [*LM Evaluation Harness*](https://github.com/EleutherAI/lm-evaluation-harness) cr√©√©e par le laboratoire de recherche en IA √† but non lucratif [EleutherAI](https://www.eleuther.ai/) connu pour avoir cr√©√© [*The Pile*](https://pile.eleuther.ai/) et entra√Æn√© [*GPT-J*](https://hf.co/EleutherAI/gpt-j-6b), [*GPT-Neo-X 20B*](https://hf.co/EleutherAI/gpt-neox-20b), et [*Pythia*](https://github.com/EleutherAI/pythia). Une √©quipe avec de s√©rieuses r√©f√©rences dans le domaine de l'IA !

Ce *wrapper* ex√©cute des √©valuations √† l'aide d' *Eleuther AI harness* sur le cluster de calcul d'Hugging Face, stocke les r√©sultats dans un jeu de donn√©es envoy√© sur le Hub, puis qui sont finalement affich√©s dans un [*Space* d√©di√©](https://hf.co/spaces/HuggingFaceH4/open_llm_leaderboard).

Pour les mod√®les LLaMA, les r√©sultats obtenus sur MMLU avec [*LM Evaluation Harness*](https://github.com/EleutherAI/lm-evaluation-harness) diff√®rent consid√©rablement des r√©sultats obtenus dans le papier.
Pourquoi cela est-il le cas ?

## 1001 saveurs de MMLU

Il s'av√®re que l'√©quipe de LLaMA a adapt√© une autre impl√©mentation disponible en ligne, √† savoir le code d'√©valuation propos√© par l'√©quipe de UC Berkeley qui a con√ßu le *benchmark* MMLU disponible √† l'adresse https://github.com/hendrycks/test et que nous appellerons ici l'**Implementation originale**.

En approfondissant la question, nous avons trouv√© une autre impl√©mentation int√©ressante pour MMLU : le code fourni par le [CRFM](https://crfm.stanford.edu/) de Stanford dans son *benchmark* d‚Äô√©valuation tr√®s complet [*Holistic Evaluation of Language Models*](https://crfm.stanford.edu/helm/latest/) que nous appellerons ici l'impl√©mentation **HELM**.

Les *benchmarks* EleutherAI Harness et Stanford HELM sont int√©ressants car ils rassemblent de nombreuses √©valuations dans une seule base de code (y compris MMLU), et donnent ainsi une vue d'ensemble de la performance d'un mod√®le. C'est la raison pour laquelle l'*Open LLM Leaderboard* int√®gre de tels *benchmarks* ¬´ holistiques ¬ª au lieu d'utiliser des bases de code individuelles pour chaque √©valuation.

Pour trancher la question, nous avons d√©cid√© d'ex√©cuter ces trois impl√©mentations possibles pour MMLU sur un ensemble de mod√®les afin de les classer en fonction de ces r√©sultats :
- l'impl√©mentation Harness ([commit e47e01b](https://github.com/EleutherAI/lm-evaluation-harness/tree/e47e01beea79cfe87421e2dac49e64d499c240b4))
- l'impl√©mentation HELM ([commit cab5d89](https://github.com/stanford-crfm/helm/tree/cab5d89fadbff86190f29ddfa497301958eaf2ec))
- l'impl√©mentation originale (compatible avec `transformers` gr√¢ce √† l'incroyable [@olmer](https://hf.co/olmer) [ici](https://github.com/hendrycks/test/pull/13))

> [!NOTE]
> Notez que l'impl√©mentation Harness a √©t√© r√©cemment mise √† jour. Plus d'informations √† ce sujet √† la fin de cet article.

Les r√©sultats sont surprenants : 

![png](https://hf.co/datasets/huggingface/documentation-images/resolve/main/blog/evaluating-mmlu-leaderboard/LLM-01-ter-01.png)

Vous trouverez tous les r√©sultats de l'√©valuation √† la fin de l'article.

Ces trois impl√©mentations du m√™me *benchmark* donnent des r√©sultats tr√®s diff√©rents et changent m√™me l'ordre de classement des mod√®les sur le classement !

Essayons de comprendre d'o√π vient cette divergence üïµÔ∏è  
Mais tout d'abord, comprenons bri√®vement comment nous pouvons √©valuer automatiquement les comportements dans les LLM modernes.

## Comment √©valuer automatiquement un LLM d'aujourd'hui
MMLU consiste en des questions √† choix multiples, donc un *benchmark* plut√¥t simple (par rapport aux questions ouvertes), mais comme nous le verrons, cela laisse encore beaucoup de marge pour des diff√©rences sur les d√©tails de l'impl√©mentation. Le *benchmark* consiste en des questions √† quatre r√©ponses possibles couvrant 57 domaines de connaissances g√©n√©rales regroup√©s en grandes cat√©gories : ¬´ Sciences humaines ¬ª, ¬´ Sciences sociales ¬ª, ¬´ STIM  ¬ª, etc.

Pour chaque question, une seule des r√©ponses propos√©es est correcte. Voici un exemple :

```
Question: Glucose is transported into the muscle cells:


Choices:
A. via protein transporters called GLUT4.
B. only in the presence of insulin.
C. via hexokinase.
D. via monocarbylic acid transporters.


Correct answer: A
```

Remarque : vous pouvez explorer facilement ce jeu de donn√©es [via le visualiseur de jeu de donn√©es](https://hf.co/datasets/cais/mmlu/viewer/college_medicine/dev?row=0) du Hub.

Les grands mod√®les de langage sont des mod√®les simples dans le zoo des mod√®les d'IA. Ils prennent en entr√©e une *cha√Æne de texte* (appel√©e ¬´ *prompt* ¬ª), qui est d√©coup√©e en *tokens* (mots, sous-mots ou caract√®res, selon la mani√®re dont le mod√®le est construit) et introduite dans le mod√®le. √Ä partir de cette entr√©e, ils g√©n√®rent une distribution de probabilit√© pour le *token* suivant, sur l'ensemble des *tokens* qu'ils connaissent (appel√©s ¬´ vocabulaire ¬ª du mod√®le). Vous pouvez donc obtenir le ¬´ degr√© de probabilit√© ¬ª qu'un *token* soit la suite du *prompt* d'entr√©e.

Nous pouvons utiliser ces probabilit√©s pour choisir un *token*, par exemple le plus probable (ou nous pouvons introduire un l√©ger bruit avec un √©chantillonnage pour √©viter d'avoir des r√©ponses ¬´ trop m√©caniques ¬ª). L'ajout du *token* retenu au *prompt* et sa r√©introduction dans le mod√®le permettent de g√©n√©rer un autre *token* et ainsi de suite jusqu'√† ce que des phrases enti√®res soient cr√©√©es comme des suites du *prompt* d'entr√©e :

![png](https://hf.co/datasets/huggingface/documentation-images/resolve/main/blog/evaluating-mmlu-leaderboard/LLM-01.png)

C'est de cette fa√ßon que ChatGPT ou Hugging Chat g√©n√®rent des r√©ponses.
En r√©sum√©, nous disposons de deux moyens principaux pour obtenir des informations √† partir d'un mod√®le afin de l'√©valuer :
1. obtenir les **probabilit√©s** que certains groupes de *tokens* sp√©cifiques soient des suites du *prompt* puis **comparer ces probabilit√©s ensemble** pour nos choix possibles pr√©d√©finis ;
2. obtenir une **g√©n√©ration de texte** √† partir du mod√®le (en s√©lectionnant les *tokens* de mani√®re r√©p√©t√©e comme nous l'avons vu) puis **comparer ces g√©n√©rations de texte** aux textes des diff√©rents choix possibles pr√©d√©finis.

Fort de ces connaissances, nous allons nous plonger dans nos trois impl√©mentations de MMLU, afin de d√©couvrir les donn√©es d'entr√©e envoy√©es aux mod√®les, les r√©sultats attendus et la mani√®re dont ces r√©sultats sont compar√©s.

## MMLU se d√©cline sous toutes les formes et dans toutes les tailles : examen des prompts

Comparons un exemple de *prompt* envoy√© aux mod√®les dans chaque impl√©mentation pour le m√™me jeu de donn√©es MMLU :
<div>
<table><p>
  <tbody>
 <tr style="text-align: left;">
  <td>Implementation originale <a href="https://github.com/hendrycks/test/pull/13">Ollmer PR</a></td>
  <td>HELM <a href="https://github.com/stanford-crfm/helm/tree/cab5d89fadbff86190f29ddfa497301958eaf2ec">commit cab5d89</a> </td>
  <td>AI Harness <a href="https://github.com/EleutherAI/lm-evaluation-harness/tree/e47e01beea79cfe87421e2dac49e64d499c240b4">commit e47e01b</a></td>
 </tr>
  <tr style=" vertical-align: top;">
    <td>The following are multiple choice questions (with answers) about  us foreign policy. <br>
How did the 2008 financial crisis affect America's international reputation? <br>
A. It damaged support for the US model of political economy and capitalism <br>
B. It created anger at the United States for exaggerating the crisis <br>
C. It increased support for American global leadership under President Obama <br>
D. It reduced global use of the US dollar <br>
Answer:
</td>
    <td>The following are multiple choice questions (with answers) about us foreign policy. <br>
 <br>
Question: How did the 2008 financial crisis affect America's international reputation? <br>
A. It damaged support for the US model of political economy and capitalism <br>
B. It created anger at the United States for exaggerating the crisis <br>
C. It increased support for American global leadership under President Obama <br>
D. It reduced global use of the US dollar <br>
Answer:
</td>
    <td>Question: How did the 2008 financial crisis affect America's international reputation? <br>
Choices: <br>
A. It damaged support for the US model of political economy and capitalism <br>
B. It created anger at the United States for exaggerating the crisis <br>
C. It increased support for American global leadership under President Obama <br>
D. It reduced global use of the US dollar <br>
Answer:
</td>
  </tr>
  </tbody>
</table><p>
</div>

Les diff√©rences peuvent sembler minimes, les avez-vous toutes rep√©r√©es ? Les voici :
- Premi√®re phrase, instruction et sujet :<br>Peu de diff√©rences. HELM ajoute un espace suppl√©mentaire, et Harness n'inclut pas la ligne de sujet
- Question :<br>HELM et Harness ajoutent le pr√©fixe ¬´ Question : ¬ª
- Choix :<br>Harness les fait pr√©c√©der du mot-cl√© ¬´ Choix ¬ª

## Comment √©valuer le mod√®le √† partir de ces prompts ?

Commen√ßons par la mani√®re dont [l'impl√©mentation originale de MMLU](https://github.com/hendrycks/test/pull/13) extrait les pr√©dictions du mod√®le. Dans cette version, nous comparons les probabilit√©s pr√©dites par le mod√®le, pour les quatre r√©ponses :
![png](https://hf.co/datasets/huggingface/documentation-images/resolve/main/blog/evaluating-mmlu-leaderboard/LLM-02.png)

Cela peut √™tre b√©n√©fique pour le mod√®le dans certains cas, par exemple, comme vous pouvez le voir ici :

![png](https://hf.co/datasets/huggingface/documentation-images/resolve/main/blog/evaluating-mmlu-leaderboard/LLM-03.png)

Dans ce cas, le mod√®le a obtenu un +1 pour avoir class√© la bonne r√©ponse en t√™te des 4 options. Mais si nous regardons le vocabulaire complet, il aurait plut√¥t g√©n√©r√© un mot en dehors de nos quatre options : le mot ¬´ Zygote ¬ª (il s'agit plus d'un exemple que d'un cas d'utilisation r√©el üôÇ).
Comment pouvons-nous nous assurer que le mod√®le commet le moins possible d'erreurs de ce type ?

Nous pouvons utiliser une approche ¬´ ***few-shots*** ¬ª dans laquelle nous fournissons au mod√®le un ou plusieurs exemples dans le *prompt*, avec leurs r√©ponses attendues √©galement. Voici √† quoi cela ressemble :
![png](https://hf.co/datasets/huggingface/documentation-images/resolve/main/blog/evaluating-mmlu-leaderboard/LLM-04.png)

Ici, le mod√®le dispose d'un exemple du comportement souhait√© et est donc moins susceptible de pr√©dire des r√©ponses en dehors de la fourchette des r√©ponses attendues.
Puisque cela am√©liore les performances, dans toutes nos √©valuations, MMLU est √©valu√© en 5 coups (en faisant pr√©c√©der chaque *prompt* de 5 exemples). Notons que d'un *benchmark* √† l'autre, bien que les m√™mes 5 exemples soient utilis√©s, leur ordre d'introduction dans le mod√®le peut varier, ce qui est √©galement une source possible de diff√©rence, que nous n'examinerons pas ici. Vous devez √©galement faire attention √† ne pas laisser filtrer certaines r√©ponses dans les exemples *few-shot* que vous utilisez...

**HELM**  
Examinons √† pr√©sent [l‚Äôimplementation  d‚ÄôHELM](https://github.com/stanford-crfm/helm/tree/cab5d89fadbff86190f29ddfa497301958eaf2ec). Si le *prompt* avec *few-shot* est g√©n√©ralement similaire, la mani√®re dont le mod√®le est √©valu√© est tr√®s diff√©rente de l'impl√©mentation originale que nous venons de voir. Nous utilisons les probabilit√©s de sortie du prochain *token* du mod√®le pour s√©lectionner une g√©n√©ration de texte et nous la comparons au texte de la r√©ponse attendue :
![png](https://hf.co/datasets/huggingface/documentation-images/resolve/main/blog/evaluating-mmlu-leaderboard/LLM-05.png)

Dans ce cas, si notre *token* ¬´ Zygote ¬ª √©tait au contraire le plus probable (comme nous l'avons vu plus haut), la r√©ponse du mod√®le (¬´ Zygote ¬ª) serait erron√©e et le mod√®le ne marquerait aucun point pour cette question :
![png](https://hf.co/datasets/huggingface/documentation-images/resolve/main/blog/evaluating-mmlu-leaderboard/LLM-06.png)

**Harness**  
Finissons avec [l‚Äôimplementation de janvier 2023 d‚ÄôHarness](https://github.com/EleutherAI/lm-evaluation-harness/tree/e47e01beea79cfe87421e2dac49e64d499c240b4) qui a √©t√© utilis√© pour calculer les premiers r√©sultats du classement. Comme nous allons le voir, nous avons ici une autre fa√ßon de calculer un score pour le mod√®le sur le m√™me jeu de donn√©es d'√©valuation.
Nous utilisons √† nouveau les probabilit√©s, mais cette fois-ci les probabilit√©s de la s√©quence de r√©ponses compl√®te, avec la lettre suivie du texte de la r√©ponse, par exemple ¬´ *C. The second pharyngeal arch* ¬ª. Pour calculer la probabilit√© d'une r√©ponse compl√®te, nous obtenons la probabilit√© pour chaque *token* (comme nous l'avons vu ci-dessus) et nous les regroupons. Pour des raisons de stabilit√© num√©rique, nous additionnons le logarithme des probabilit√©s et nous pouvons d√©cider (ou non) de calculer une normalisation dans laquelle nous divisons la somme par le nombre de *tokens* afin d'√©viter d'avantager les r√©ponses plus longues (plus d'informations √† ce sujet ult√©rieurement). Voici √† quoi cela ressemble :

![png](https://hf.co/datasets/huggingface/documentation-images/resolve/main/blog/evaluating-mmlu-leaderboard/LLM-07.png)

Pour r√©sumer ce que nous avons vu jusqu'√† pr√©sent, voici un tableau r√©capitulatif des r√©ponses fournies et g√©n√©r√©es par le mod√®le :
<div>
<table><p>
  <tbody>
 <tr style="text-align: left;">
  <td>Implementation originale</td>
  <td>HELM</td>
  <td>AI Harness (janvier 2023)</td>
 </tr>
  <tr style=" vertical-align: top;">
    <td> Nous comparons les probabilit√©s des r√©ponses aux lettres suivantes :
</td>
    <td> Le mod√®le doit g√©n√©rer comme texte la lettre suivante comme r√©ponse :
</td>
    <td> Nous comparons les probabilit√©s des r√©ponses compl√®tes suivantes :
</td>
  </tr>
  <tr style=" vertical-align: top;">
    <td>  A <br>
 B <br>
 C <br>
 D
</td>
    <td>A
</td>
    <td> A. It damaged support for the US model of political economy and capitalism <br>
 B. It created anger at the United States for exaggerating the crisis <br>
 C. It increased support for American global leadership under President Obama <br>
 D. It reduced global use of the US dollar
</td>
  </tr>
  </tbody>
</table><p>
</div>

Maintenant que nous avons couvert toutes les impl√©mentations, comparons les r√©sultats obtenues via ces trois fa√ßons possibles d'√©valuer les mod√®les :

|                                           | MMLU (HELM) | MMLU (Harness) | MMLU (Originale) |
|:------------------------------------------|------------:|---------------:|----------------:|
| llama-65b                     |       **0.637** |          0.488 |           **0.636** |
| tiiuae/falcon-40b                         |       0.571 |          **0.527** |           0.558 |
| llama-30b                     |       0.583 |          0.457 |           0.584 |
| EleutherAI/gpt-neox-20b                   |       0.256 |          0.333 |           0.262 |
| llama-13b                     |       0.471 |          0.377 |           0.47  |
| llama-7b                      |       0.339 |          0.342 |           0.351 |
| tiiuae/falcon-7b                          |       0.278 |          0.35  |           0.254 |
| togethercomputer/RedPajama-INCITE-7B-Base |       0.275 |          0.34  |           0.269 |

Nous pouvons constater que pour le m√™me jeu de donn√©es, tant les scores absolus que les classements des mod√®les (voir la premi√®re figure) sont tr√®s sensibles √† la m√©thode d'√©valuation que nous d√©cidons d'utiliser.

Disons que vous avez entra√Æn√© une reproduction parfaite du mod√®le LLaMA 65B et que vous l'avez √©valu√©e avec Harness (score de 0,488). Vous le comparez maintenant au nombre publi√© (√©valu√© sur l'impl√©mentation originale de MMLU, donc avec un score de 0,637). Avec une telle diff√©rence de 25 points, vous vous dites probablement : ¬´ Oh mince, j'ai compl√®tement rat√© mon entra√Ænement üò± ¬ª. Or, rien n'est plus faux, ces valeurs ne sont pas du tout comparables, m√™me si elles portent toutes deux la mention ¬´ score MMLU ¬ª (et sont √©valu√©es sur le m√™me jeu de donn√©es MMLU).

Existe-t-il une ¬´ meilleure fa√ßon ¬ª d'√©valuer un mod√®le parmi toutes celles que nous avons vues ? C'est une question d√©licate. Diff√©rents mod√®les peuvent se comporter diff√©remment lorsqu'ils sont √©valu√©s d'une mani√®re ou d'une autre, comme nous le voyons ci-dessus lorsque les classements changent. Pour rester aussi √©quitable que possible, on peut √™tre tent√© de choisir une impl√©mentation pour laquelle le score moyen de tous les mod√®les test√©s est le plus √©lev√©, de mani√®re √† ¬´ d√©bloquer ¬ª le plus grand nombre possible de capacit√©s des mod√®les. Dans notre cas, cela signifierait utiliser l'option de log-vraisemblance de l'impl√©mentation originale. Mais comme nous l'avons vu plus haut, son utilisation donne √©galement des indications au mod√®le en restreignant le champ des r√©ponses possibles, et aide donc peut-√™tre trop les mod√®les les moins puissants. En outre, la log-vraisemblance est facilement accessible pour les mod√®les open-source, mais n'est pas toujours indiqu√©e pour les mod√®les API √† source ferm√©e.

Et vous, lecteur, qu'en pensez-vous ? Cet article de blog √©tant d√©j√† long, il est temps d'ouvrir la discussion et de recueillir vos commentaires. Venez discuter de ce sujet dans le fil de discussion du *Space* de l'*Open LLM Leaderboard* : https://hf.co/spaces/HuggingFaceH4/open_llm_leaderboard/discussions/82

## Conclusion

L'une des principales le√ßons √† retenir est que les √©valuations sont fortement li√©es √† leur impl√©mentation, jusque dans les moindres d√©tails tels que les *prompts* et la *tokenisation*. La simple indication de ¬´ r√©sultats MMLU ¬ª ne vous donne que peu ou pas d'informations sur la fa√ßon dont vous pouvez comparer ces chiffres √† ceux d'autres biblioth√®ques que vous avez √©valu√©es.

C'est pourquoi les *benchmarks* ouverts, normalis√©s et reproductibles tels que l'[EleutherAI Eval Harness](https://github.com/EleutherAI/lm-evaluation-harness/) ou le [Stanford HELM](https://github.com/stanford-crfm/helm/) sont d'une valeur inestimable pour la communaut√©. Sans eux, la comparaison des r√©sultats entre les mod√®les et les papiers serait impossible, ce qui √©toufferait la recherche sur l'am√©lioration des LLM.

**Post scriptum** :  
Dans le cas de l'*Open LLM Leaderboard*, nous avons d√©cid√© de nous en tenir √† l'utilisation de biblioth√®ques d'√©valuation g√©r√©es par la communaut√©. Heureusement, pendant la r√©daction de cet article de blog, la formidable communaut√© autour d'EleutherAI Harness, et en particulier [ollmer](https://github.com/EleutherAI/lm-evaluation-harness/issues/475), a r√©alis√© un travail remarquable en mettant √† jour l'√©valuation de MMLU pour la rendre similaire √† l'impl√©mentation d'origine.

Nous sommes en train de mettre √† jour le classement complet avec la version actualis√©e d'Harness, alors attendez-vous √† voir des scores provenant d'Harness v2 dans les prochaines semaines !  
L'ex√©cution de tous les mod√®les √† nouveau prendra un certain temps, restez √† l'√©coute :hugs:

## Remerciements
Nous sommes tr√®s reconnaissants envers Xavier Martinet, Aur√©lien Rodriguez et Sharan Narang de l'√©quipe LLama pour leurs suggestions utiles dans cet article ainsi que pour avoir r√©pondu √† toutes nos questions.

## Hachages de reproductibilit√©
Voici les hachages de validation des diff√©rentes impl√©mentations de code utilis√©es dans cet article de blog :
- Implementation d‚ÄôEleutherAI LM Harness, commit e47e01b: https://github.com/EleutherAI/lm-evaluation-harness/tree/e47e01beea79cfe87421e2dac49e64d499c240b4
- Implementation d‚ÄôHELM, commit cab5d89: https://github.com/stanford-crfm/helm/tree/cab5d89fadbff86190f29ddfa497301958eaf2ec
- Implementation originale de MMLU (compatible avec `transformers` par l'incroyable [@olmer](https://hf.co/olmer)) : https://github.com/hendrycks/test/pull/13

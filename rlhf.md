---
title: "Illustrating Reinforcement Learning from Human Feedback (RLHF)" 
thumbnail: /blog/assets/120_rlhf/thumbnail.png
authors:
- user: natolambert
- user: LouisCastricato
  guest: true
- user: lvwerra
- user: Dahoas
  guest: true
  translator:
- user: Sarjamisepehr
---

# توضیح یادگیری تقویتی با استفاده از بازخورد انسانی

## Reinforcement Learning from Human Feedback (RLHF)

<!-- {blog_metadata} -->
<!-- {authors} -->

_This article's original language is [English](https://huggingface.co/blog/rlhf). In case you are interested in reading in English.


مدل‌های زبان در چند سال گذشته قابلیت‌های چشمگیر خود را با تولید متن‌های متنوع و قانع‌کننده در جواب دستورهای ورودی انسانی، نشان داده‌اند. با این حال، آنچه که یک متن "خوب" را می سازد ذاتاً به سختی قابل تعریف است زیرا سلیقه ی ذهنی و وابسته به زمینه ی موضوع بحث است. کاربردهای زیادی وجود دارند مانند نوشتن داستان‌هایی که در آنها خلاقیت می‌خواهید، تکه‌هایی از متن های خبری، که باید صادق باشد و یا تکه‌ کدهایی که می‌خواهیم قابل اجرا باشند.

نوشتن یک تابع ضرر برای ویژگی‌های گفته شده غیرقابل حل به نظر می‌رسد و بیشتر مدل‌های زبان هنوز با شیوه ی ساده ی پیش‌بینی توکن(token) بعدی (مانند cross entropy) آموزش داده می‌شوند. برای جبران کاستی‌های ناشی از خود ضرر، افراد معیارهایی را تعریف می‌کنند که برای جلب بهتر ترجیحات انسانی (human preferences) طراحی شده‌اند مانند [BLEU](https://en.wikipedia.org/wiki/BLEU)یا [ROUGE](https://en.wikipedia.org/wiki/ROUGE_(metric)). در حالی که این معیارها در اندازه گیری عملکرد، بهتر از خود تابع ضرر عمل میکنند و مناسب ترند، این معیارها با قوانین ساده متن تولید شده را با نمونه های ترجیح داده شده(انسانی) مقایسه می کنند، درنتیجه میتوانیم بگوییم هنوز هم محدود هستند. آیا اگر از بازخوردهای انسانی برای متن تولید شده به عنوان معیار عملکرد استفاده کنیم یا حتی یک قدم جلوتر برویم و از این بازخوردها به عنوان ضرر(loss) برای بهینه سازی مدل استفاده کنیم، عالی نیست؟ این ایده همان یادگیری تقویتی از بازخورد انسانی (RLHF) است. از روش های یادگیری تقویتی برای بهینه سازی مستقیم یک مدل زبان با بازخورد انسانی استفاده کنیم. RLHF مدل‌های زبان را قادر ساخته تا مدلی را که بر روی مجموعه‌ای از داده‌های متنی آموزش داده شده است، با ارزش‌های پیچیده انسانی هماهنگ کنند.

آخرین موفقیت RLHF استفاده از آن در [ChatGPT](http://openai.com/blog/chatgpt/)بود. با توجه به توانایی های چشمگیر ChatGPT، از آن خواستیم تا RLHF را برای ما توضیح دهد:

<p align="center">
    <img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/rlhf/chatgpt-explains.png" width="500" />
</p>

ترجمه:

- سوال:

آیا می توانی برای من توضیح دهی که چگونه RL(یادگیری تقویتی) با بازخورد انسانی برای مدل های زبانی کار میکند انگار که من پنج ساله هستم؟

- پاسخ:

حتما! خب، تصور کنید رباتی به نام روفوس(Rufus) دارید که می خواهد یاد بگیرد که چگونه مانند یک انسان صحبت کند. روفوس یک مدل زبان دارد که به او در درک کلمات و جملات کمک می کند.

ابتدا روفوس با استفاده از مدل زبانی خود چیزی می گوید. به عنوان مثال، ممکن است بگوید "من یک ربات هستم".

سپس، یک انسان به گفته های روفوس گوش می دهد و به او بازخورد می دهد که آیا این جمله مانند یک جمله طبیعی است که یک انسان می گوید یا خیر. انسان ممکن است بگوید، "این کاملا درست نیست، روفوس. انسان ها معمولا نمی گویند "من یک ربات هستم." آنها ممکن است بگویند "من یک ربات ام" یا "من یک ماشین هستم."

روفوس این بازخورد را می گیرد و از آن برای به روز رسانی مدل زبانی خود استفاده می کند. او با استفاده از اطلاعات جدیدی که از انسان دریافت کرده، سعی خواهد کرد دوباره جمله را بگوید. این بار ممکن است بگوید "من یک ربات ام".

انسان دوباره گوش می دهد و بازخورد بیشتری به روفوس می دهد. این روند تا زمانی ادامه خواهد داشت که روفوس بتواند جملاتی را بگوید که برای انسان طبیعی به نظر می رسد.

با گذشت زمان، روفوس به لطف بازخوردهایی که از انسان ها دریافت می کند، یاد می گیرد که چگونه مانند یک انسان صحبت کند. به این ترتیب می توان مدل های زبانی را با استفاده از RL(یادگیری تقویتی) با بازخورد انسانی بهبود بخشید.

به طرز شگفت آوری خوب عمل می کند، اما همه چیز را کاملاً پوشش نمی دهد. ما این شکاف ها را پر خواهیم کرد!

# RLHF: بیایید قدم به قدم پیش برویم

یادگیری تقویتی از بازخورد انسانی (همچنین به عنوان RL از ترجیحات انسانی ذکر می شود) یک مفهوم چالش برانگیز است زیرا شامل یک فرآیند آموزشی چند مدل و مراحل مختلفی از استقرار است. در این پست وبلاگ، روند آموزش را به سه مرحله اصلی تقسیم می کنیم:

1. پیش آموزش یک مدل زبانی (LM)
2. جمع آوری داده ها و آموزش مدل پاداش
3. تنظیم دقیق LM با یادگیری تقویتی

برای شروع، نگاهی می اندازیم به اینکه مدل های زبانی چگونه پیش-آموزش داده میشوند.

### پیش آموزش مدل های زبانی

به عنوان نقطه شروع، RLHF از یک مدل زبانی استفاده می‌کند که قبلاً با اهداف پیش‌آموزشی کلاسیک از قبل آموزش داده شده است (برای جزئیات بیشتر به این [پست](https://huggingface.co/blog/how-to-train) وبلاگ مراجعه کنید). OpenAI از نسخه کوچکتر GPT-3 برای اولین مدل محبوب RLHF خود، [InstructGPT](https://openai.com/research/instruction-following)استفاده کرد. Anthropic از مدل های ترانسفورمر از 10 میلیون تا 52 میلیارد پارامتر آموزش دیده برای این کار استفاده کرد. DeepMind از مدل 280 میلیارد پارامتری خودشان با نام [Gopher](https://arxiv.org/abs/2112.11446)خود استفاده کرد.

این مدل اولیه همچنین می‌تواند روی متن بیشتر یا شرایط مختلف تنظیم(fine-tune) شود، اما لزوماً نیازی به تنظیم نیست. به عنوان مثال، OpenAI روی متن تولید شده توسط انسان که "ارجح" بود تنظیم دقیقی انجام داد و Anthropic مدل زبانی اولیه خود را برای RLHF با تقطیر(جداسازی خلوص) یک مدل زبانی اصلی بر روی سرنخ های متن زمینه با توجه به معیارهای "مفید، صادقانه و بی ضرر" خود، ایجاد کرد. اینها هر دو منبع اطلاعاتی هستند که من از آن به عنوان داده های پرهزینه و افزوده (augmented data) یاد می کنم، اما اینها برای درک RLHF تکنیک های مورد نیاز نیستند.

به طور کلی، پاسخ روشنی در مورد اینکه «کدام مدل» برای نقطه شروع RLHF بهترین است، وجود ندارد. این یک بحث رایج در این وبلاگ خواهد بود - فضای طراحی از گزینه ها درباره ی آموزش RLHF به طور کامل بررسی نشده است.

در مرحله بعد، با یک مدل زبانی، فرد نیاز به تولید داده برای آموزش یک مدل پاداش دارد، که در اینجاست که چگونگی ادغام ترجیحات انسانی در سیستم مشخص می شود.

<p align="center">
    <img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/rlhf/pretraining.png" width="500" />
</p>

### آموزش مدل پاداش (Reward model training)
    
طراحی و ساخت یک مدل پاداش (مدل پاداش ، که به عنوان مدل ترجیحی(preference model) نیز نامیده می شود) تنظیم شده با ترجیحات انسانی، همان جایی است که تحقیقات نسبتاً جدید در یادگیری تقویتی با استفاده از بازخورد انسانی آغاز می شود. هدف اساسی بدست آوردن یک مدل یا سیستمی است که رشته ای از متن را می گیرد و یک پاداش اسکالر که باید به صورت عددی نشان دهنده ترجیح انسان باشد، برمی گرداند. این سیستم می‌تواند یک مدل زبانی سرتاسر(end-to-end به یک فرآیند کامل از ابتدا تا انتها اشاره می‌کند) یا یک سیستم ماژولار باشد که پاداشی را ارائه می‌کند (برای مثال یک مدل خروجی‌ها را رتبه‌بندی می‌کند و سپس رتبه‌بندی به پاداش تبدیل می‌شود). خروجی یک پاداش اسکالر برای الگوریتم‌های یادگیری تقویتی موجود که بعداً در فرآیند یادگیری تقویتی با استفاده از بازخورد انسانی یکپارچه می‌شوند، بسیار مهم است.

این مدل‌های زبانیِ برای مدل‌سازی پاداش، می‌توانند هم یک مدل زبان تنظیم‌شده(fine-tuned) دیگر باشد و هم میتوانند یک مدل زبانی باشند که از ابتدا در داده‌های ترجیحی(ترجیحی انسانی) آموزش دیده‌اند. به عنوان مثال، [Antropic](https://www.anthropic.com/)از یک روش تخصصی تنظیم دقیق برای مقداردهی اولیه این مدل‌ها پس از پیش‌آموزش استفاده می‌کند (پیش‌آموزش مدل ترجیحی، PMP) زیرا آنها دریافتند که این شیوه کارآمدتر برای نمونه ها عمل میکند نسبت به تنظیم دقیق، اما هیچ کدام از ورژن های مختلف مدل‌سازی پاداشی بعنوان بهترین و روشن ترین انتخاب در نظر گرفته نمی‌شوند.

دیتاست آموزشی تولید متن های جفت هم برای مدل پاداشی با نمونه گیری از دیتاست دیگری از متون که از پیش تعریف شده تولید می شود. (داده‌های Anthropic که عمدتاً با ابزار چت در Amazon Mechanical Turk تولید می‌شوند در Hub [موجود](https://huggingface.co/datasets/Anthropic/hh-rlhf)[است](https://huggingface.co/datasets/Anthropic/hh-rlhf) و OpenAI از متن درخواست‌های ارسال شده توسط کاربران به GPT API استفاده می‌کند). متن ها از طریق یک مدل زبان اولیه برای تولید متن جدید رد می شوند.

از حاشیه نویس های انسانی برای رتبه بندی خروجی های متن تولید شده از مدل زبانی استفاده می شود. ممکن است که تصور شود باید از همان ابتدا یک امتیاز اسکالر را مستقیماً برای هر قطعه از متن اعمال کنند تا یک مدل پاداشی ایجاد کنیم، اما انجام این کار در عمل دشوار است. تفاوت های ارزش‌های انسان‌ها با یکدیگر باعث می‌شود که این امتیازات نامنظم و پر از خطا باشند. در عوض، استفاده بسیار بهتری از رتبه‌بندی ها برای مقایسه خروجی‌ چندین مدل‌ و ایجاد یک مجموعه داده منظم می‌شود.

چندین روش برای رتبه بندی متن وجود دارد. یکی از متدهای موفقیت‌آمیز این است که کاربران متن تولید شده از دو مدل زبانی با یک ورودی را مقایسه کنند. با مقایسه خروجی های مدل ها، می توان از سیستم [Elo](https://en.wikipedia.org/wiki/Elo_rating_system)برای ایجاد رتبه بندی مدل ها و خروجی ها نسبت به یکدیگر استفاده کرد. این روش‌های مختلف رتبه‌بندی به یک سیگنال پاداش اسکالر برای آموزش مدل، عادی(normalize) می‌شوند.

یک دستاور جالب این فرآیند این است که سیستم‌های موفق یادگیری تقویتی با استفاده از بازخورد انسانی تا به امروز از مدل‌های زبانی پاداشی با سایزهای متفاوت نسبت به تولید متن استفاده کرده‌اند. (به عنوان مثال، مدل زبانی 175 میلیارد پارامتری OpenAI دارای یک مدل پاداش با 6 میلیارد پارامتر است، مدل زبان استفاده شده شرکت Anthropic و مدل های پاداش آنها از 10 میلیارد تا 52 میلیارد است، DeepMind از مدل 70 میلیاردی مدل چینچیلا هم برای مدل زبان خود و هم برای مدل پاداش آن استفاده می کند). یک تصور می تواند این باشد که این مدل های ترجیحی باید ظرفیتی مشابهی برای درک متن داده شده به آنها مشابه یک مدل زبانی برای تولید متن داراست داشته باشند.

در این مرحله در سیستم یادگیری تقویتی با استفاده از بازخوردهای انسانی، ما یک مدل زبان اولیه داریم که می‌توان از آن برای تولید متن استفاده کرد و یک مدل ترجیحی که هر متنی را می‌گیرد و به آن نمره‌ای از نحوه درک انسان از آن اختصاص می‌دهد. در مرحله بعد، ما از یادگیری تقویتی (RL) برای بهینه سازی مدل زبان اصلی با توجه به مدل پاداش استفاده می کنیم.

<p align="center">
    <img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/rlhf/reward-model.png" width="600" />
</p>

### تنظیم دقیق با RL

آموزش یک مدل زبان با یادگیری تقویتی برای مدت طولانی چیزی بود که مردم هم به دلایل مهندسی و هم به دلایل الگوریتمی غیرممکن تصور می کردند. چیزی که به نظر می رسد چندین سازمان به کار خود دست یافته اند، تنظیم دقیق برخی یا همه پارامترهای یک کپی از مدل زبانی اولیه با الگوریتم خاص گرادیان خط مشی(روش‌های گرادیان خط‌مشی نوعی از تکنیک‌های یادگیری تقویتی هستند که بر بهینه‌سازی خط‌مشی‌های پارامتری با توجه به بازده مورد انتظار (پاداش تجمعی بلندمدت) با نزول گرادیان تکیه دارند.) یادگیری تقویتی، بهینه سازی سیاست پروگزیمال (PPO) است. پارامترهای مدل زبانی منجمد شده اند زیرا تنظیم دقیق یک مدل پارامتر 10B یا 100B+ بسیار هزینه بر است (برای اطلاعات بیشتر، به تطبیق با رتبه پایین ([LoRA](https://arxiv.org/abs/2106.09685)) برای مدلهای زبانی یا [Sparrow](https://arxiv.org/abs/2209.14375)LM از DeepMind مراجعه کنید.) PPO برای مدت نسبتاً طولانی وجود داشته است - راهنماهای زیادی در مورد نحوه عملکرد آن وجود دارد. بلوغ نسبی این روش، آن را به یک گزینه و انتخاب مطلوب برای کاربرد جدید آموزش توزیع‌شده(distributed training) برای سیستم یادگیری تقویتی با استفاده از بازخورد انسانی تبدیل کرده است. به نظر می رسد که بسیاری از پیشرفت های اصلی یادگیری تقویتی درجهت انجام یادگیری تقویتی با استفاده از بازخورد انسانی در حال کشف چگونگی به روز رسانی چنین مدل بزرگی با یک الگوریتم آشنا بوده اند. (در ادامه در مورد آن بیشتر توضیح خواهیم داد)

اجازه دهید ابتدا این وظیفه تنظیم دقیق را به عنوان یک مسئله یادگیری تقویتی فرموله کنیم. در درجه اول، خط مشی(policy) یک مدل زبانی است که یک ورودی متنی دریافت می کند و دنباله ای از متن (یا فقط توزیع احتمال بر روی متن) را برمی گرداند. فضای عمل این خط مشی همه توکن های مربوط به واژگان مدل زبان است (اغلب در حدود 50 هزار توکن وجود دارد) و فضای مشاهده نیز تمام حالت های توزیع درکنارهم قرارگیری توکن های ورودی که ممکن است، که خود فضای مشاهده نیز با توجه به استفاده های قبلی از یادگیری تقویتی نیز بسیار بزرگ است. (بُعد تقریباً به اندازه واژگان به توان طول دنباله توکن ورودی است). تابع پاداش ترکیبی از مدل ترجیحی و یک محدودیت در تغییر خط مشی (policy) است.


تابع پاداش جایی است که سیستم همه مدل‌هایی را که تا الان در مورد آن صحبت کردیم، در یک فرآیند یادگیری تقویتی با استفاده از بازخورد انسانی ترکیب می‌کند. برای مثال متن x از مجموعه دیتاست بعنوان ورودی داده میشود، متن y توسط تکرار فعلی خط مشی تنظیم شده تولید می شود. متن تولید شده به متن اصلی وصل میشود، سپس کل متن به مدل ترجیحی فرستاده می‌شود، که یک متغیر اسکالر از «ترجیح‌پذیری»، تحت متغیر \\( r_\theta \\)را برمی‌گرداند. علاوه بر این، توزیع‌های احتمال هر توکن از خط مشی یادگیری تقویتی با توزیع‌های مدل اولیه مقایسه می‌شوند تا جریمه تفاوت بین آنها محاسبه شود. در مقالات متعدد از OpenAI، Anthropic، و DeepMind، این جریمه به عنوان یک نسخه مقیاس‌بندی شده از واگرایی ([Kullback–Leibler KL](https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence)) بین این توالی‌های توزیع بر روی توکن‌ها، بعنوان  \\( r_\text{KL} \\) طراحی شده است. اصطلاح واگرایی KL، خط مشی یادگیری تقویتی را از دور شدن اساسی از مدل اولیه از پیش آموزش دیده با هر دسته آموزشی جریمه می کند، که می تواند برای اطمینان از خروجی مدل به شکلی از قطعات متنی منسجم منطقی مفید باشد. بدون این جریمه، بهینه‌سازی می‌تواند شروع به تولید متنی کند که بی‌معنی است، اما مدل پاداش را فریب می‌دهد تا پاداش بالایی از آن دریافت کند. در عمل، واگرایی KL از طریق نمونه گیری از هر دو توزیع، تخمین زده می شود (توضیح جان شولمن در [اینجا](http://joschu.net/blog/kl-approx.html)). پاداش نهایی ارسال شده به قانون به روز رسانی یادگیری تقویتی  \\( r = r_\theta - \lambda r_\text{KL} \\) است.

برخی از سیستم های یادگیری های تقویتی از بازخورد انسانی، اصطلاحات اضافی را به تابع پاداش اضافه کرده اند. به عنوان مثال، OpenAI با موفقیت روی مدل InstructGPT خود با ترکیب شیب های(گرادیان های) اضافی قبل از آموزش (از مجموعه حاشیه نویسی انسانی) در قانون به روز رسانی برای PPO، آزمایش کرد. این احتمال وجود دارد که با بررسی بیشتر یادگیری تقویتی از بازخورد انسانی، فرمول بندی این تابع پاداش به تکامل خود ادامه دهد.

در نهایت، قانون به‌روزرسانی، به‌روزرسانی پارامتر از PPO است که معیارهای پاداش را در دسته فعلی(batch) داده‌ها به حداکثر می‌رساند (PPO روی خط‌مشی است، به این معنی که پارامترها فقط با دسته فعلی جفت‌های متن های تولید شده به‌روزرسانی می‌شوند). PPO یک الگوریتم بهینه‌سازی منطقه اعتماد است که از محدودیت‌هایی در گرادیان(شیب) استفاده می‌کند تا اطمینان حاصل کند که مرحله به‌روزرسانی فرآیند یادگیری را بی‌ثبات نمی‌کند. DeepMind از تنظیمات پاداش مشابهی برای Gopher استفاده کرد، اما از مزیت [بازیگر-منتقد همزمان (A2C) ](http://proceedings.mlr.press/v48/mniha16.html?ref=https://githubhelp.com) برای بهینه‌سازی گرادیان‌ها استفاده کرد، که به‌طور قابل توجهی متفاوت است اما در بازتولید خارج از سازمان نشده است.

<p align="center">
    <img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/rlhf/rlhf.png" width="650" />
</p>

_نکته جزئیات فنی: نمودار بالا به نظر می رسد که هر دو مدل پاسخ های متفاوتی را برای یک متن ورودی تولید می کنند، اما آنچه واقعاً اتفاق می افتد احتمالات یادگیری تقویتی برای جریمه KL است.

به طور اختیاری، یادگیری های تقویتی از بازخورد انسانی می تواند از این نقطه به بعد با به روز رسانی مکرر مدل پاداش و خط مشی با یکدیگر، ادامه دهد. همانطور که مدل خط مشی یادگیری تقویتی به روز می شود، کاربران می توانند به رتبه بندی این خروجی ها در مقایسه با خروجی نسخه های قبلی مدل ادامه دهند.
بیشتر مقالات هنوز درباره پیاده سازی این عملیات بحث نکرده اند، زیرا حالت استقرار مورد نیاز برای جمع آوری این نوع داده ها فقط برای ماموران پشتیبانی گفتگو با دسترسی به پایگاه کاربر فعال و درگیر با سیستم، کار می کند. Anthropic این گزینه را به عنوان Iterated Online RLHF مورد بحث قرار می دهد (به [مقاله اصلی](https://arxiv.org/abs/2204.05862) مراجعه کنید)، که در آن تکرارهای(iterations) خط مشی در سیستم رتبه بندی ELO در بین مدل ها گنجانده شده است. این امر پویایی پیچیده ای از مدل خط مشی و مدل پاداش در حال تکامل را معرفی می کند که نشان دهنده یک سوال تحقیقاتی پیچیده و باز است.

# ابزارهای منبع باز برای RLHF

اولین [کدی](https://github.com/openai/lm-human-preferences) که برای اجرای یادگیری تقویتی از بازخورد انسانی در مدل‌های زبانی منتشر شد، از OpenAI در TensorFlow در سال ۲۰۱۹ بود.

امروزه، در حال حاضر چند مخزن فعال برای یادگیری تقویتی از بازخورد انسانی در PyTorch وجود دارد که از این منبع رشد کرده است. مخازن اولیه عبارتند از Transformers Reinforcement Learning ([TRL](https://github.com/lvwerra/trl))، کد [TRLX](https://github.com/CarperAI/trlx)که به عنوان فورک TRL و Reinforcement Learning for Language Models ([RL4LMs](https://github.com/allenai/RL4LMs)) ایجاد شده است.

TRL برای تنظیم دقیق مدل های زبانی از پیش آموزش دیده در اکوسیستم Hugging Face با PPO طراحی شده است. TRLX یک فورک توسعه یافته از TRL است که توسط [CarperAI](https://carper.ai/)ساخته شده است تا مدل های بزرگتر را برای آموزش آنلاین و آفلاین مدیریت کند. در حال حاضر، TRLX دارای یک API است که قادر به تولید زبان تقویتی از بازخورد انسانی با PPO و زبان ضمنی Q-Learning [ILQL](https://sea-snell.github.io/ILQL_site/)در مقیاس های مورد نیاز برای استقرار مدل زبان بزرگ (مثلاً 33 میلیارد پارامتر) است. نسخه‌های آینده TRLX امکان مدل‌های زبانی تا 200 میلیارد پارامتر را فراهم می‌کند. به این ترتیب، رابط با TRLX برای مهندسین یادگیری ماشین با تجربه در این مقیاس بهینه شده است.


[RL4LMs](https://github.com/allenai/RL4LMs)بلوک‌های سازنده را برای شخصی سازی دقیق و ارزیابی مدل‌های زبان بزرگ با طیف گسترده‌ای از الگوریتم‌های یادگیری تقویتی (PPO، NLPO، A2C و TRPO)، توابع پاداش و معیارها ارائه می‌کند. علاوه بر این، این کتابخانه به راحتی قابل تنظیم است، که امکان آموزش هر رمزگذار-رمزگشا(encoder-decoder) یا مدل زبانی مبتنی بر ترانسفورمر رمزگذار را در هر تابع پاداش دلخواه مشخص شده توسط کاربر فراهم می کند. قابل ذکر است که در [کار](https://arxiv.org/abs/2210.01241)[اخیر](https://arxiv.org/abs/2210.01241) به خوبی آزمایش شده و در طیف وسیعی از وظایف محک زده شده است که شامل بیش از 2000 آزمایش می شود که چندین بینش عملی مانند: مقایسه ظرفیت داده ها (نمایش های متخصص در مقابل مدل سازی پاداش)، مدیریت هک پاداش و بی ثباتی های آموزشی و غیره را برجسته می کند. برنامه های فعلی RL4LM شامل آموزش توزیع شده مدل های بزرگتر و الگوریتم های جدید یادگیری تقویتی است.

هر دو TRLX و RL4LM در حال توسعه بیشتر هستند، بنابراین به زودی انتظار ویژگی های بیشتری فراتر از این ها را داشته باشید.

یک مجموعه [دیتاست](https://huggingface.co/datasets/Anthropic/hh-rlhf) بزرگ ایجاد شده توسط Anthropic در هاب موجود است.

# آینده RLHF چیست؟

در حالی که این تکنیک‌ها بسیار امیدوارکننده و تاثیرگذار هستند و توجه بزرگترین آزمایشگاه‌های تحقیقاتی هوش مصنوعی را به خود جلب کرده‌اند، هنوز محدودیت‌های واضحی وجود دارد. مدل‌ها، اگرچه بهتر هستند، اما همچنان می‌توانند متن مضر یا نادرست را بدون هیچ گونه ابهامی تولید کنند. این نقص نشان دهنده یک چالش و انگیزه بلندمدت برای سیستم یادگیری تقویتی از بازخورد انسانی است – کار در یک حوزه ی مشکل ذاتی انسانی به این معناست که هرگز مرز نهایی و مقصد روشنی برای عبور از مدل برای کامل نامیدن آن وجود نخواهد داشت.

هنگام استقرار(deployment) یک سیستم با استفاده از یادگیری تقویتی از بازخورد انسانی، جمع‌آوری داده‌های ترجیحی انسانی به دلیل ادغام مستقیم سایر کارگران انسانی خارج از حلقه آموزشی مدل بسیار هزینه بر است. عملکرد یادگیری تقویتی از بازخورد انسانی فقط به اندازه کیفیت حاشیه نویسی های انسانی آن است که به دو نوع تقسیم میشود: متن تولید شده توسط انسان، مانند تنظیم دقیق مدل زبانی اولیه در InstructGPT، و نوع دوم برچسب های ترجیحات انسانی بین خروجی های مدل.

تولید متن انسانی به خوبی نوشته شده برای پاسخگویی به درخواست های خاص بسیار پرهزینه است، زیرا اغلب به استخدام کارکنان پاره وقت نیاز دارد (به جای اینکه بتوانید به کاربران محصول یا منابع جمعی تکیه کنید). خوشبختانه، مقیاس داده‌های مورد استفاده در آموزش مدل پاداش برای بیشتر کاربردهای یادگیری تقویتی از بازخورد انسانی (حدود 50 هزار نمونه ترجیحی برچسب‌گذاری شده) گران نیست. با این حال، هنوز هم هزینه بالاتری نسبت به مقداری است که آزمایشگاه های دانشگاهی احتمالاً قادر به پرداخت آن هستند. در حال حاضر، تنها یک مجموعه داده در مقیاس بزرگ برای یادگیری تقویتی از بازخورد انسانی در یک مدل زبانی عمومی (از Anthropic) و چند مجموعه داده مختص کار در مقیاس کوچکتر (مانند دیتای خلاصه سازی(summarization) از OpenAI) وجود دارد. چالش دوم داده‌ها برای یادگیری تقویتی از بازخورد انسانی این است که حاشیه‌نویس‌های انسانی اغلب می‌توانند اختلاف نظر داشته باشند، و یک واریانس بالقوه قابل‌توجهی به داده‌های آموزشی بدون حقیقت پایه اضافه می‌کنند.

با این محدودیت‌ها، طیف وسیعی از گزینه‌های طراحی ناشناخته همچنان می‌توانند یادگیری تقویتی از بازخورد انسانی را قادر به برداشتن گام‌های اساسی کنند. بسیاری از این موارد در حوزه بهبود بهینه ساز یادگیری تقویتی قرار می گیرند. PPO یک الگوریتم نسبتا قدیمی است، اما هیچ دلیل ساختاری وجود ندارد که الگوریتم‌های دیگر نتوانند مزایا و جایگشت‌هایی را در جریان کار یادگیری تقویتی از بازخورد انسانی موجود ارائه دهند. یکی از هزینه های بزرگ بخش بازخورد تنظیم دقیق خط مشی مدل زبانی این است که هر قطعه متن تولید شده از خط مشی باید بر اساس مدل پاداش ارزیابی شود (زیرا مانند بخشی از محیط در چارچوب استاندارد یادگیری تقویتی عمل می کند). برای جلوگیری از این عبورهای پرهزینه از یک مدل بزرگ، یادگیری تقویتی آفلاین می تواند به عنوان بهینه ساز خط مشی استفاده شود. اخیراً الگوریتم‌های جدیدی ظاهر شده‌اند، مانند یادگیری زبان ضمنی Q بعنوان (ILQL) و [[Talk](https://www.youtube.com/watch?v=fGq4np3brbs) on ILQL at CarperAI] که به‌خوبی با این نوع بهینه‌سازی مطابقت دارد. سایر مبادلات اصلی در فرآیند یادگیری تقویتی، مانند تعادل اکتشاف و بهره برداری، نیز مستند نشده اند. کاوش در این جهت ها حداقل درک قابل توجهی از نحوه عملکرد یادگیری تقویتی از بازخورد انسانی ایجاد می کند و در غیر این صورت، عملکرد بهبود یافته را ارائه می دهد.

نویسندگان مقاله میزبان یک سخنرانی در روز سه شنبه 13 دسامبر 2022 بودیم که در این پست گسترش یافت. میتوانید از این [لینک](https://www.youtube.com/watch?v=2MBJOuVq380&feature=youtu.be) تماشا کنید.

### برای مطالع بیشتر

در اینجا لیستی از رایج ترین مقالات در مورد RLHF تا به امروز آمده است. این رشته اخیراً با ظهور DeepRL (حدود سال 2017) رایج شد و به مطالعه گسترده تری در مورد کاربردهای مدل های زبان بزرگ از بسیاری از شرکت های فناوری بزرگ تبدیل شده است. در اینجا برخی از مقالات در مورد RLHF وجود دارد که قبل از تمرکز بر مدل زبانی ایجاد شده است:

- [TAMER: Training an Agent Manually via Evaluative Reinforcement](https://www.cs.utexas.edu/~pstone/Papers/bib2html-links/ICDL08-knox.pdf) (Knox and Stone 2008): یک عامل آموخته شده پیشنهاد کردند که در آن انسان ها امتیازاتی را در مورد اقدامات به طور مکرر انجام شده برای یادگیری یک مدل پاداش ارائه کردند.
- [Interactive Learning from Policy-Dependent Human Feedback](http://proceedings.mlr.press/v70/macglashan17a/macglashan17a.pdf) (MacGlashan et al. 2017): یک الگوریتم بازیگر-منتقد، تحت عنوان COACH پیشنهاد شده است، که در آن از بازخورد انسانی (هم مثبت و هم منفی) برای تنظیم تابع مزیت استفاده می‌شود.
- [Deep Reinforcement Learning from Human Preferences](https://proceedings.neurips.cc/paper/2017/hash/d5e2c0adad503c91f91df240d0cd4e49-Abstract.html) (Christiano et al. 2017): RLHF بر روی ترجیحات بین مسیرهای آتاری اعمال می شود!
- [Deep TAMER: Interactive Agent Shaping in High-Dimensional State Spaces](https://ojs.aaai.org/index.php/AAAI/article/view/11485) (Warnell et al. 2018): چارچوب TAMER را گسترش می دهد که در آن از یک شبکه عصبی عمیق برای مدل سازی پیش بینی پاداش استفاده می شود.

و در اینجا یک مجموعه رو به رشد مقالات "کلیدی" است که عملکرد RLHF را برای مدل های زبانی را نشان می دهد:
- [Fine-Tuning Language Models from Human Preferences](https://arxiv.org/abs/1909.08593) (Zieglar et al. 2019): مقاله اولیه ای که تأثیر یادگیری پاداشی(reward learning) را بر چهار تسک خاص مطالعه می کند.
- [Learning to summarize with human feedback](https://proceedings.neurips.cc/paper/2020/hash/1f89885d556929e98d3ef9b86448f951-Abstract.html) (Stiennon et al., 2020): RLHF برای خلاصه کردن متن اعمال شد. همچنین: [Recursively Summarizing Books with Human Feedback](https://arxiv.org/abs/2109.10862) (OpenAI Alignment Team 2021), خلاصه سازی کتاب ها را دنبال می کند.
- [WebGPT: Browser-assisted question-answering with human feedback](https://arxiv.org/abs/2112.09332) (OpenAI, 2021): 
استفاده از RLHF برای آموزش یک عامل(Agent) برای پیمایش در وب.(نویسنده: طبق توضیحات مقالات منتشر شده درون سایت خود شرکت OpenAi آنها از این مدل برای جمع آوری اطلاعات مدلهای خود از سطح اینترنت استفاده کردند.)
- InstructGPT: [Training language models to follow instructions with human feedback](https://arxiv.org/abs/2203.02155) (OpenAI Alignment Team 2022): 
RLHF برای یک مدل زبان عمومی اعمال می شود[پست](https://openai.com/blog/instruction-following/) درباره ی InstructGPT
- GopherCite: [Teaching language models to support answers with verified quotes](https://www.deepmind.com/publications/gophercite-teaching-language-models-to-support-answers-with-verified-quotes) (Menick et al. 2022): یک مدل زبان را با RLHF آموزش دادند تا پاسخ هایی را با نقل قول های خاص برگردانند.
- Sparrow: [Improving alignment of dialogue agents via targeted human judgements](https://arxiv.org/abs/2209.14375) (Glaese et al. 2022): تنظیم دقیق عامل(Agent) گفتگو با RLHF
- [ChatGPT: Optimizing Language Models for Dialogue](https://openai.com/blog/chatgpt/) (OpenAI 2022): آموزش یک مدل زبان با RLHF برای استفاده مناسب به عنوان یک ربات چت همه منظوره.
- [Scaling Laws for Reward Model Overoptimization](https://arxiv.org/abs/2210.10760) (Gao et al. 2022): خواص مقیاس بندی مدل ترجیحی آموخته شده را در RLHF مطالعه می کند.
- [Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback](https://arxiv.org/abs/2204.05862) (Anthropic, 2022): مستندات دقیق آموزش دستیار مدل زبان با RLHF.
- [Red Teaming Language Models to Reduce Harms: Methods, Scaling Behaviors, and Lessons Learned](https://arxiv.org/abs/2209.07858) (Ganguli et al. 2022): مستندی دقیق از تلاش‌ها برای «کشف، اندازه‌گیری و تلاش برای کاهش خروجی‌های بالقوه مضر[مدل‌های زبانی] ».
- [Dynamic Planning in Open-Ended Dialogue using Reinforcement Learning](https://arxiv.org/abs/2208.02294) (Cohen at al. 2022): استفاده از یادگیری تقویتی برای تقویت مهارت مکالمه یک عامل گفتگوی پایان باز.
- [Is Reinforcement Learning (Not) for Natural Language Processing?: Benchmarks, Baselines, and Building Blocks for Natural Language Policy Optimization](https://arxiv.org/abs/2210.01241) (Ramamurthy and Ammanabrolu et al. 2022): فضای طراحی ابزارهای منبع باز در RLHF را مورد بحث قرار می دهد و یک الگوریتم جدید NLPO (بهینه سازی خط مشی زبان طبیعی) را به عنوان جایگزینی برای PPO پیشنهاد می کند.

این فیلد همگرایی چندین فیلد است، بنابراین می توانید منابع را در سایر زمینه ها نیز بیابید:
* Continual learning of instructions ([Kojima et al. 2021](https://arxiv.org/abs/2108.04812), [Suhr and Artzi 2022](https://arxiv.org/abs/2212.09710)) or bandit learning from user feedback ([Sokolov et al. 2016](https://arxiv.org/abs/1601.04468), [Gao et al. 2022](https://arxiv.org/abs/2203.10079))
* تاریخچه قبلی استفاده از سایر الگوریتم های RL برای تولید متن (نه همه با ترجیحات انسانی)، مانند recurrent neural networks ([Ranzato et al. 2015](https://arxiv.org/abs/1511.06732)), یک الگوریتم بازیگر منتقد برای پیش‌بینی متن ([Bahdanau et al. 2016](https://arxiv.org/abs/1607.07086)), یا یک کار اولیه که ترجیحات انسانی را به این چارچوب اضافه می کند ([Nguyen et al. 2017](https://arxiv.org/abs/1707.07402)).

**Citation:**
If you found this useful for your academic work, please consider citing our work, in text:
```
Lambert, et al., "Illustrating Reinforcement Learning from Human Feedback (RLHF)", Hugging Face Blog, 2022.
```

BibTeX citation:
```
@article{lambert2022illustrating,
  author = {Lambert, Nathan and Castricato, Louis and von Werra, Leandro and Havrilla, Alex},
  title = {Illustrating Reinforcement Learning from Human Feedback (RLHF)},
  journal = {Hugging Face Blog},
  year = {2022},
  note = {https://huggingface.co/blog/rlhf},
}
```

*Thanks to [Robert Kirk](https://robertkirk.github.io/) for fixing some factual errors regarding specific implementations of RLHF. Thanks to [Peter Stone](https://www.cs.utexas.edu/~pstone/), [Khanh X. Nguyen](https://machineslearner.com/) and [Yoav Artzi](https://yoavartzi.com/) for helping expand the related works further into history.*

*Thanks to Stas Bekman for fixing some typos or confusing phrases.*

*Thanks to [Igor Kotenkov](https://www.linkedin.com/in/seeall/) for pointing out a technical error in the KL-penalty term of the RLHF procedure, its diagram, and textual description.*

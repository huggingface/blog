{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Tool Use with ðŸ¤— `transformers`"
      ],
      "metadata": {
        "id": "qq_TE-iBfdJR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We begin by initializing our model as normal. We'll use a small, ungated model so you can run this on Colab - make sure you've got a GPU, though, because it won't fit in CPU memory alone!"
      ],
      "metadata": {
        "id": "xoeShkLUftDV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "checkpoint = \"NousResearch/Hermes-2-Pro-Llama-3-8B\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
        "model = AutoModelForCausalLM.from_pretrained(checkpoint, torch_dtype=torch.bfloat16, device_map=\"auto\")"
      ],
      "metadata": {
        "id": "UUmokuFrffjJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we'll define a simple tool function for our model - but in a real scenario, you'll probably want to use more than one! The type hints and docstrings are mandatory - those are going to be parsed and used by the model to understand what the tools do.\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "S9BAyr70f1Eq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_current_temperature(location: str):\n",
        "    \"\"\"\n",
        "    Gets the temperature at a given location.\n",
        "\n",
        "    Args:\n",
        "        location: The location to get the temperature for, in the format \"city, country\"\n",
        "    \"\"\"\n",
        "    return 22.0  # bug: Sometimes the temperature is not 22. low priority to fix tho\n",
        "\n",
        "tools = [get_current_temperature]\n"
      ],
      "metadata": {
        "id": "8QWuToHTbiQO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we set up a simple chat."
      ],
      "metadata": {
        "id": "zMyCg0b0f9fK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chat = [\n",
        "    {\"role\": \"user\", \"content\": \"Hey, what's the weather like in Paris right now?\"}\n",
        "]"
      ],
      "metadata": {
        "id": "CIZnlw8gblIV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We pass the tools to the chat template, and generate text from the model using the formatted prompt."
      ],
      "metadata": {
        "id": "4KEC1HLlgCJx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tool_prompt = tokenizer.apply_chat_template(\n",
        "    chat,\n",
        "    tools=tools,\n",
        "    return_tensors=\"pt\",\n",
        "    return_dict=True,\n",
        "    add_generation_prompt=True,\n",
        ")\n",
        "tool_prompt = tool_prompt.to(model.device)\n",
        "\n",
        "out = model.generate(**tool_prompt, max_new_tokens=128)\n",
        "generated_text = out[0, tool_prompt['input_ids'].shape[1]:]\n",
        "\n",
        "print(tokenizer.decode(generated_text))"
      ],
      "metadata": {
        "id": "3AZ8l7zDboD8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The model has chosen to call a tool, and picked an argument that matches both the user's request and the format in the tool docstring! Let's add this tool call to the chat as a message!"
      ],
      "metadata": {
        "id": "6H1fqDvTgHK5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tool_call = {\"name\": \"get_current_temperature\", \"arguments\": {\"location\": \"Paris, France\"}}\n",
        "chat.append({\"role\": \"assistant\", \"tool_calls\": [{\"type\": \"function\", \"function\": tool_call}]})"
      ],
      "metadata": {
        "id": "QApSN0mObvIh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we add the tool response containing the function output to the chat as well. Both the tool call (containing the arguments passed to the tool) and tool response (containing the tool's output) must be included in the chat history."
      ],
      "metadata": {
        "id": "C55M3uLBgLHo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chat.append({\"role\": \"tool\", \"name\": \"get_current_temperature\", \"content\": \"22.0\"})"
      ],
      "metadata": {
        "id": "lb8qbiV1c-wL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, we apply the chat template and generate text once again."
      ],
      "metadata": {
        "id": "hVrbGCU7gWLW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tool_prompt = tokenizer.apply_chat_template(\n",
        "    chat,\n",
        "    tools=tools,\n",
        "    return_tensors=\"pt\",\n",
        "    return_dict=True,\n",
        "    add_generation_prompt=True,\n",
        ")\n",
        "tool_prompt = tool_prompt.to(model.device)\n",
        "\n",
        "out = model.generate(**tool_prompt, max_new_tokens=128)\n",
        "generated_text = out[0, tool_prompt['input_ids'].shape[1]:]\n",
        "\n",
        "print(tokenizer.decode(generated_text))"
      ],
      "metadata": {
        "id": "p9JTN62odBGT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Success! The model has used the tool response correctly in its response to the user."
      ],
      "metadata": {
        "id": "k1lLSczhgY64"
      }
    }
  ]
}
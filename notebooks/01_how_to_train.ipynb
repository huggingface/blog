{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "01_how-to-train.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyO4ejZO06whbdXXZyf0nmGP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/huggingface/blog/blob/master/notebooks/01_how_to_train.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e67Ut53QYEdU",
        "colab_type": "code",
        "cellView": "form",
        "outputId": "79ab19d9-6867-452b-fa53-aba25dd93759",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "#@title\n",
        "%%html\n",
        "<div style=\"background-color: pink;\">\n",
        "  Notebook written in collaboration with <a href=\"https://github.com/aditya-malte\">Aditya Malte</a>.\n",
        "  <br>\n",
        "  The Notebook is on GitHub, so contributions are more than welcome.\n",
        "</div>\n",
        "<br>\n",
        "<div style=\"background-color: yellow;\">\n",
        "  Aditya wrote another notebook with a slightly different use case and methodology, please check it out.\n",
        "  <br>\n",
        "  <a target=\"_blank\" href=\"https://gist.github.com/aditya-malte/2d4f896f471be9c38eb4d723a710768b\">\n",
        "    https://gist.github.com/aditya-malte/2d4f896f471be9c38eb4d723a710768b\n",
        "  </a>\n",
        "</div>\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div style=\"background-color: pink;\">\n",
              "  Notebook written in collaboration with <a href=\"https://github.com/aditya-malte\">Aditya Malte</a>.\n",
              "  <br>\n",
              "  The Notebook is on GitHub, so contributions are more than welcome.\n",
              "</div>\n",
              "<br>\n",
              "<div style=\"background-color: yellow;\">\n",
              "  Aditya wrote another notebook with a slightly different use case and methodology, please check it out.\n",
              "  <br>\n",
              "  <a target=\"_blank\" href=\"https://gist.github.com/aditya-malte/2d4f896f471be9c38eb4d723a710768b\">\n",
              "    https://gist.github.com/aditya-malte/2d4f896f471be9c38eb4d723a710768b\n",
              "  </a>\n",
              "</div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M1oqh0F6W3ad",
        "colab_type": "text"
      },
      "source": [
        "# How to train a new language model from scratch using Transformers and Tokenizers\n",
        "\n",
        "### Notebook edition (link to blogpost [link](https://huggingface.co/blog/how-to-train))\n",
        "\n",
        "\n",
        "Over the past few weeks, we made several improvements to our [`transformers`](https://github.com/huggingface/transformers) and [`tokenizers`](https://github.com/huggingface/tokenizers) libraries, with the goal of making it way easier to **train a new language model from scratch**.\n",
        "\n",
        "In this post we‚Äôll demo how to train a ‚Äúsmall‚Äù model (84 M parameters = 6 layers, 768 hidden size, 12 attention heads) ‚Äì that‚Äôs the same number of layers & heads as DistilBERT ‚Äì on **Esperanto**. We‚Äôll then fine-tune the model on a downstream task of part-of-speech tagging.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oK7PPVm2XBgr",
        "colab_type": "text"
      },
      "source": [
        "## 1. Find a dataset\n",
        "\n",
        "First, let us find a corpus of text in Esperanto. Here we‚Äôll use the Esperanto portion of the [OSCAR corpus](https://traces1.inria.fr/oscar/) from INRIA.\n",
        "OSCAR is a huge multilingual corpus obtained by language classification and filtering of [Common Crawl](https://commoncrawl.org/) dumps of the Web.\n",
        "\n",
        "<img src=\"https://huggingface.co/blog/assets/oscar.png\" style=\"margin: auto; display: block; width: 260px;\">\n",
        "\n",
        "The Esperanto portion of the dataset is only 299M, so we‚Äôll concatenate with the Esperanto sub-corpus of the [Leipzig Corpora Collection](https://wortschatz.uni-leipzig.de/en/download), which is comprised of text from diverse sources like news, literature, and wikipedia.\n",
        "\n",
        "The final training corpus has a size of 3 GB, which is still small ‚Äì for your model, you will get better results the more data you can get to pretrain on. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HOk4iZ9YZvec",
        "colab_type": "code",
        "outputId": "1685aebd-590a-4503-e82d-5946b079c20a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "# in this notebook we'll only get one of the files (the Oscar one) for the sake of simplicity and performance\n",
        "!wget https://s3.amazonaws.com/datasets.huggingface.co/EsperBERTo/data/oscar.eo.txt"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-02-27 16:27:42--  https://s3.amazonaws.com/datasets.huggingface.co/EsperBERTo/data/oscar.eo.txt\n",
            "Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.216.233.189\n",
            "Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.216.233.189|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 312733741 (298M) [text/plain]\n",
            "Saving to: ‚Äòoscar.eo.txt‚Äô\n",
            "\n",
            "oscar.eo.txt        100%[===================>] 298.25M  52.3MB/s    in 5.6s    \n",
            "\n",
            "2020-02-27 16:27:47 (53.3 MB/s) - ‚Äòoscar.eo.txt‚Äô saved [312733741/312733741]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G-kkz81OY6xH",
        "colab_type": "text"
      },
      "source": [
        "## 2. Train a tokenizer\n",
        "\n",
        "We choose to train a byte-level Byte-pair encoding tokenizer (the same as GPT-2), with the same special tokens as RoBERTa. Let‚Äôs arbitrarily pick its size to be 52,000.\n",
        "\n",
        "We recommend training a byte-level BPE (rather than let‚Äôs say, a WordPiece tokenizer like BERT) because it will start building its vocabulary from an alphabet of single bytes, so all words will be decomposable into tokens (no more `<unk>` tokens!).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5duRggBRZKvP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Install dependencies\n",
        "!pip uninstall -y tensorflow\n",
        "!pip install transformers\n",
        "# transformers version at notebook creation --- 2.5.1\n",
        "# tokenizers version at notebook creation --- 0.5.2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IMnymRDLe0hi",
        "colab_type": "code",
        "outputId": "5ccf9421-b29f-4a33-aaee-5e4b3dc4ff2f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from pathlib import Path\n",
        "\n",
        "from tokenizers import ByteLevelBPETokenizer\n",
        "\n",
        "paths = [str(x) for x in Path(\".\").glob(\"**/*.txt\")]\n",
        "\n",
        "# Initialize a tokenizer\n",
        "tokenizer = ByteLevelBPETokenizer()\n",
        "\n",
        "# Customize training\n",
        "tokenizer.train(files=paths, vocab_size=52_000, min_frequency=2, special_tokens=[\n",
        "    \"<s>\",\n",
        "    \"<pad>\",\n",
        "    \"</s>\",\n",
        "    \"<unk>\",\n",
        "    \"<mask>\",\n",
        "])\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['oscar.eo.txt']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Ei7bqpRf1LH",
        "colab_type": "text"
      },
      "source": [
        "Now let's save files to disk"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EIS-irI0f32P",
        "colab_type": "code",
        "outputId": "99f9a3dc-47a5-4ea6-efd4-a7e252485ef2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!mkdir EsperBERTo\n",
        "tokenizer.save(\"EsperBERTo\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['EsperBERTo/vocab.json', 'EsperBERTo/merges.txt']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lOOfYSuQhSqT",
        "colab_type": "text"
      },
      "source": [
        "üî•üî• Wow, that was fast! ‚ö°Ô∏èüî•\n",
        "\n",
        "We now have both a `vocab.json`, which is a list of the most frequent tokens ranked by frequency, and a `merges.txt` list of merges.\n",
        "\n",
        "```json\n",
        "{\n",
        "\t\"<s>\": 0,\n",
        "\t\"<pad>\": 1,\n",
        "\t\"</s>\": 2,\n",
        "\t\"<unk>\": 3,\n",
        "\t\"<mask>\": 4,\n",
        "\t\"!\": 5,\n",
        "\t\"\\\"\": 6,\n",
        "\t\"#\": 7,\n",
        "\t\"$\": 8,\n",
        "\t\"%\": 9,\n",
        "\t\"&\": 10,\n",
        "\t\"'\": 11,\n",
        "\t\"(\": 12,\n",
        "\t\")\": 13,\n",
        "\t# ...\n",
        "}\n",
        "\n",
        "# merges.txt\n",
        "l a\n",
        "ƒ† k\n",
        "o n\n",
        "ƒ† la\n",
        "t a\n",
        "ƒ† e\n",
        "ƒ† d\n",
        "ƒ† p\n",
        "# ...\n",
        "```\n",
        "\n",
        "What is great is that our tokenizer is optimized for Esperanto. Compared to a generic tokenizer trained for English, more native words are represented by a single, unsplit token. Diacritics, i.e. accented characters used in Esperanto ‚Äì `ƒâ`, `ƒù`, `ƒ•`, `ƒµ`, `≈ù`, and `≈≠` ‚Äì are encoded natively. We also represent sequences in a more efficient manner. Here on this corpus, the average length of encoded sequences is ~30% smaller as when using the pretrained GPT-2 tokenizer.\n",
        "\n",
        "Here‚Äôs  how you can use it in `tokenizers`, including handling the RoBERTa special tokens ‚Äì of course, you‚Äôll also be able to use it direcly from `transformers`.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tKVWB8WShT-z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tokenizers.implementations import ByteLevelBPETokenizer\n",
        "from tokenizers.processors import BertProcessing\n",
        "\n",
        "\n",
        "tokenizer = ByteLevelBPETokenizer(\n",
        "    \"./EsperBERTo/vocab.json\",\n",
        "    \"./EsperBERTo/merges.txt\",\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hO5M3vrAhcuj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokenizer._tokenizer.post_processor = BertProcessing(\n",
        "    (\"</s>\", tokenizer.token_to_id(\"</s>\")),\n",
        "    (\"<s>\", tokenizer.token_to_id(\"<s>\")),\n",
        ")\n",
        "tokenizer.enable_truncation(max_length=512)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E3Ye27nchfzq",
        "colab_type": "code",
        "outputId": "7048c175-73c5-4798-d0e8-0e80b8550d71",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "tokenizer.encode(\"Mi estas Julien.\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Encoding(num_tokens=7, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing, original_str, normalized_str])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X8ya5_7rhjKS",
        "colab_type": "code",
        "outputId": "80d24f46-ce55-4830-bae6-75d326214525",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "tokenizer.encode(\"Mi estas Julien.\").tokens"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['<s>', 'Mi', 'ƒ†estas', 'ƒ†Juli', 'en', '.', '</s>']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WQpUC_CDhnWW",
        "colab_type": "text"
      },
      "source": [
        "## 3. Train a language model from scratch\n",
        "\n",
        "We will now train our language model using the [`run_language_modeling.py`](https://github.com/huggingface/transformers/blob/master/examples/run_language_modeling.py) script from `transformers` (newly renamed from `run_lm_finetuning.py` as it now supports training from scratch more seamlessly). Just remember to leave `--model_name_or_path` to `None` to train from scratch vs. from an existing model or checkpoint.\n",
        "\n",
        "> We‚Äôll train a RoBERTa-like model, which is a BERT-like with a couple of changes (check the [documentation](https://huggingface.co/transformers/model_doc/roberta.html) for more details).\n",
        "\n",
        "As the model is BERT-like, we‚Äôll train it on a task of *Masked language modeling*, i.e. the predict how to fill arbitrary tokens that we randomly mask in the dataset. This is taken care of by the example script.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kD140sFjh0LQ",
        "colab_type": "code",
        "outputId": "e24f736f-412a-4eaf-d53a-ac2189869d28",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        }
      },
      "source": [
        "# Check that we have a GPU\n",
        "!nvidia-smi"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Thu Feb 27 17:22:49 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 440.48.02    Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   40C    P8    11W /  70W |      0MiB / 15079MiB |      0%      Default |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                       GPU Memory |\n",
            "|  GPU       PID   Type   Process name                             Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VNZZs-r6iKAV",
        "colab_type": "code",
        "outputId": "0ee216e3-acbd-4f1a-97b1-532e7fb8bd4a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Check that PyTorch sees it\n",
        "import torch\n",
        "torch.cuda.is_available()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jBtUHRMliOLM",
        "colab_type": "text"
      },
      "source": [
        "Here, as we only have one text file, we don't even need to customize our `LineByLineDataset`. We'll just run the `run_language_modeling.py` script out-of-the-box."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zTgWPa9Dipk2",
        "colab_type": "code",
        "outputId": "fee66657-12a0-46eb-cc60-82e4df0d8870",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "# Clone repo, to get the example scripts.\n",
        "!git clone https://github.com/huggingface/transformers.git"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'transformers'...\n",
            "remote: Enumerating objects: 27, done.\u001b[K\n",
            "remote: Counting objects: 100% (27/27), done.\u001b[K\n",
            "remote: Compressing objects: 100% (20/20), done.\u001b[K\n",
            "remote: Total 20504 (delta 12), reused 12 (delta 5), pack-reused 20477\u001b[K\n",
            "Receiving objects: 100% (20504/20504), 12.22 MiB | 26.41 MiB/s, done.\n",
            "Resolving deltas: 100% (14852/14852), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u0qQzgrBi1OX",
        "colab_type": "text"
      },
      "source": [
        "### We'll define the following config for the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KwZXcYMujOsZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import json\n",
        "config = {\n",
        "\t\"architectures\": [\n",
        "\t\t\"RobertaForMaskedLM\"\n",
        "\t],\n",
        "\t\"attention_probs_dropout_prob\": 0.1,\n",
        "\t\"hidden_act\": \"gelu\",\n",
        "\t\"hidden_dropout_prob\": 0.1,\n",
        "\t\"hidden_size\": 768,\n",
        "\t\"initializer_range\": 0.02,\n",
        "\t\"intermediate_size\": 3072,\n",
        "\t\"layer_norm_eps\": 1e-05,\n",
        "\t\"max_position_embeddings\": 514,\n",
        "\t\"model_type\": \"roberta\",\n",
        "\t\"num_attention_heads\": 12,\n",
        "\t\"num_hidden_layers\": 6,\n",
        "\t\"type_vocab_size\": 1,\n",
        "\t\"vocab_size\": 52000\n",
        "}\n",
        "with open(\"./EsperBERTo/config.json\", 'w') as fp:\n",
        "    json.dump(config, fp)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ri2BIQKqjfHm",
        "colab_type": "text"
      },
      "source": [
        "Let's run our script with the following options:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VmaHZXzmkNtJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cmd =\t\"\"\"\n",
        "  python transformers/examples/run_language_modeling.py\n",
        "  --train_data_file ./oscar.eo.txt\n",
        "  --output_dir ./EsperBERTo-small-v1\n",
        "\t--model_type roberta\n",
        "\t--mlm\n",
        "\t--config_name ./EsperBERTo\n",
        "\t--tokenizer_name ./EsperBERTo\n",
        "\t--do_train\n",
        "\t--learning_rate 1e-4\n",
        "\t--num_train_epochs 1\n",
        "\t--save_total_limit 2\n",
        "\t--save_steps 2000\n",
        "\t--per_gpu_train_batch_size 16\n",
        "\t--seed 42\n",
        "\"\"\".replace(\"\\n\", \" \")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EBSrPay8kdB-",
        "colab_type": "code",
        "outputId": "3c0ae30e-dbd6-494f-ba24-5cda971fde07",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!{cmd}"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "02/27/2020 17:23:08 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False\n",
            "02/27/2020 17:23:08 - INFO - transformers.configuration_utils -   loading configuration file ./EsperBERTo/config.json\n",
            "02/27/2020 17:23:08 - INFO - transformers.configuration_utils -   Model config RobertaConfig {\n",
            "  \"architectures\": [\n",
            "    \"RobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": null,\n",
            "  \"do_sample\": false,\n",
            "  \"eos_token_ids\": null,\n",
            "  \"finetuning_task\": null,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"is_decoder\": false,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"length_penalty\": 1.0,\n",
            "  \"max_length\": 20,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_beams\": 1,\n",
            "  \"num_hidden_layers\": 6,\n",
            "  \"num_labels\": 2,\n",
            "  \"num_return_sequences\": 1,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": null,\n",
            "  \"pruned_heads\": {},\n",
            "  \"repetition_penalty\": 1.0,\n",
            "  \"temperature\": 1.0,\n",
            "  \"top_k\": 50,\n",
            "  \"top_p\": 1.0,\n",
            "  \"torchscript\": false,\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_bfloat16\": false,\n",
            "  \"vocab_size\": 52000\n",
            "}\n",
            "\n",
            "02/27/2020 17:23:08 - INFO - transformers.tokenization_utils -   Model name './EsperBERTo' not found in model shortcut name list (roberta-base, roberta-large, roberta-large-mnli, distilroberta-base, roberta-base-openai-detector, roberta-large-openai-detector). Assuming './EsperBERTo' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
            "02/27/2020 17:23:08 - INFO - transformers.tokenization_utils -   Didn't find file ./EsperBERTo/added_tokens.json. We won't load it.\n",
            "02/27/2020 17:23:08 - INFO - transformers.tokenization_utils -   Didn't find file ./EsperBERTo/special_tokens_map.json. We won't load it.\n",
            "02/27/2020 17:23:08 - INFO - transformers.tokenization_utils -   Didn't find file ./EsperBERTo/tokenizer_config.json. We won't load it.\n",
            "02/27/2020 17:23:08 - INFO - transformers.tokenization_utils -   loading file ./EsperBERTo/vocab.json\n",
            "02/27/2020 17:23:08 - INFO - transformers.tokenization_utils -   loading file ./EsperBERTo/merges.txt\n",
            "02/27/2020 17:23:08 - INFO - transformers.tokenization_utils -   loading file None\n",
            "02/27/2020 17:23:08 - INFO - transformers.tokenization_utils -   loading file None\n",
            "02/27/2020 17:23:08 - INFO - transformers.tokenization_utils -   loading file None\n",
            "02/27/2020 17:23:08 - INFO - __main__ -   Training new model from scratch\n",
            "02/27/2020 17:23:16 - INFO - __main__ -   Training/evaluation parameters Namespace(adam_epsilon=1e-08, block_size=1000000000000, cache_dir=None, config_name='./EsperBERTo', device=device(type='cuda'), do_eval=False, do_train=True, eval_all_checkpoints=False, eval_data_file=None, evaluate_during_training=False, fp16=False, fp16_opt_level='O1', gradient_accumulation_steps=1, learning_rate=0.0001, line_by_line=False, local_rank=-1, logging_steps=500, max_grad_norm=1.0, max_steps=-1, mlm=True, mlm_probability=0.15, model_name_or_path=None, model_type='roberta', n_gpu=1, no_cuda=False, num_train_epochs=1.0, output_dir='./EsperBERTo-small-v1', overwrite_cache=False, overwrite_output_dir=False, per_gpu_eval_batch_size=4, per_gpu_train_batch_size=16, save_steps=2000, save_total_limit=2, seed=42, server_ip='', server_port='', should_continue=False, tokenizer_name='./EsperBERTo', train_data_file='./oscar.eo.txt', warmup_steps=0, weight_decay=0.0)\n",
            "02/27/2020 17:23:16 - INFO - __main__ -   Loading features from cached file ./roberta_cached_lm_999999999998_oscar.eo.txt\n",
            "Traceback (most recent call last):\n",
            "  File \"transformers/examples/run_language_modeling.py\", line 799, in <module>\n",
            "    main()\n",
            "  File \"transformers/examples/run_language_modeling.py\", line 749, in main\n",
            "    global_step, tr_loss = train(args, train_dataset, model, tokenizer)\n",
            "  File \"transformers/examples/run_language_modeling.py\", line 245, in train\n",
            "    train_sampler = RandomSampler(train_dataset) if args.local_rank == -1 else DistributedSampler(train_dataset)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/sampler.py\", line 94, in __init__\n",
            "    \"value, but got num_samples={}\".format(self.num_samples))\n",
            "ValueError: num_samples should be a positive integer value, but got num_samples=0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pzpYNqmSke2F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
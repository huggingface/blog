---
title: "An Introduction to Deep Reinforcement Learning"
thumbnail: /blog/assets/63_deeprlclass_unit1/thumbnail.png
---

<h1>An Introduction to Deep Reinforcement Learning</h1>
<h2>Chapter 1 of theÂ <a href="https://github.com/huggingface/deep-rl-class">Deep Reinforcement Learning Class with Hugging Face ğŸ¤—</a></h2>

<div class="author-card">
    <a href="/thomassimonini">
        <img class="avatar avatar-user" src="https://aeiljuispo.cloudimg.io/v7/https://s3.amazonaws.com/moonup/production/uploads/1632748593235-60cae820b1c79a3e4b436664.jpeg?w=200&h=200&f=face" title="Gravatar">
        <div class="bfc">
            <code>thomassimonini</code>
            <span class="fullname">Thomas Simonini</span>
        </div>
    </a>
</div>

*This article is part of the Deep Reinforcement Learning Class. A free course from beginner to expert. Check the syllabusÂ [here.](https://github.com/huggingface/deep-rl-class)*
---

Welcome to the most fascinating topic in Artificial Intelligence:Â **Deep Reinforcement Learning.**

Deep RL is a type of Machine Learning where an agent learnsÂ **how to behave**Â in an environmentÂ **by performing actions**Â andÂ **seeing the results.**

Since 2013 and theÂ [Deep Q-Learning paper](https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf), weâ€™ve seen a lot of breakthroughs. From OpenAIÂ [five that beat some of the best Dota2 players of the world,](https://www.twitch.tv/videos/293517383)Â to theÂ [Dexterity project](https://openai.com/blog/learning-dexterity/), weÂ **live in an exciting moment in Deep RL research.**

<figure class="image table text-center m-0 w-full">
  <img src="assets/63_deeprlclass_unit1/OpenAIFive.jpg" alt="OpenAI Five, an AIÂ that beat some of the best Dota2 players in the world"/>
  <figcaption>OpenAI Five, an AIÂ <a href="https://www.twitch.tv/videos/293517383">that beat some of the best Dota2 players in the world</a></figcaption>
</figure>


Moreover, since 2018, **you have now, access to so many amazing environments and libraries to build your agents.**

Thatâ€™s why this is the best moment to start learning, and with this courseÂ **youâ€™re in the right place.**

Yes, because this article is the first unit of [Deep Reinforcement Learning Class](https://github.com/huggingface/deep-rl-class), a **free class from beginner to expert** where youâ€™ll learn the theory and practice using famous Deep RL libraries such as Stable Baselines3, RL Baselines3 Zoo and RLlib.

In this free course, you will:

- ğŸ“– Study Deep Reinforcement Learning in **theory and practice**.
- ğŸ§‘â€ğŸ’» Learn to **use famous Deep RL libraries** such as Stable Baselines3, RL Baselines3 Zoo, and RLlib.
- ğŸ¤– Train agents in **unique environments** such as SnowballFight, Huggy the Doggo ğŸ¶, and classical ones such as Space Invaders and PyBullet.
- ğŸ’¾ Publish your trained agents **in one line of code to the Hub**. But also download powerful agents from the community.
- ğŸ† **Participate in challenges** where you will evaluate your agents against other teams.
- ğŸ–Œï¸ğŸ¨ Learn to **share your environments** made with Unity and Godot.

So in this first unit,Â **youâ€™ll learn the foundations of deep reinforcement learning.** And then, you'll train your first lander agent toÂ **land correctly on the Moon ğŸŒ• and upload it to the Hugging Face Hub, a free, open platform where people can share ML models, datasets and demos.**

<figure class="image table text-center m-0 w-full">
    <video
        alt="LunarLander"
        style="max-width: 70%; margin: auto;"
        autoplay loop autobuffer muted playsinline
    >
      <source src="assets/63_deeprlclass_unit1/lunarlander.mp4" type="video/mp4">
  </video>
</figure>


Itâ€™s essential **to master these elements**Â before diving into implementing Deep Reinforcement Learning agents. The goal of this chapter is to give you solid foundations.

If you prefer, you can watch the ğŸ“¹ video version of this chapter :

<iframe width="560" height="315" src="https://www.youtube.com/embed/q0BiUn5LiBc?start=127" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

So letâ€™s get started! ğŸš€

## **What is Reinforcement Learning?**

To understand reinforcement learning, letâ€™s start with the big picture.

### **The big picture**

The idea behind Reinforcement Learning is that anÂ agent (an AI) will learn from the environment byÂ **interacting with it**Â (through trial and error) andÂ **receiving rewards**Â (negative or positive) as feedback for performing actions.

Learning from interaction with the environmentÂ **comes from our natural experiences.**

For instance, imagine putting your little brother in front of a video game he never played, a controller in his hands, and letting him alone.


<figure class="image table text-center m-0 w-full">
  <img src="assets/63_deeprlclass_unit1/Illustration_1.jpg" alt="Illustration_1"/>
</figure>


Your brother will interact with the environment (the video game) by pressing the right button (action). He got a coin, thatâ€™s a +1 reward. Itâ€™s positive, he just understood that in this gameÂ **he must get the coins.**


<figure class="image table text-center m-0 w-full">
  <img src="assets/63_deeprlclass_unit1/Illustration_2.jpg" alt="Illustration_2"/>
</figure>

But then,Â **he presses right again**Â and he touches an enemy, he just died -1 reward.

<figure class="image table text-center m-0 w-full">
  <img src="assets/63_deeprlclass_unit1/Illustration_3.jpg" alt="Illustration_3"/>
</figure>

By interacting with his environment through trial and error, your little brother understood thatÂ **he needed to get coins in this environment but avoid the enemies.**

**Without any supervision**, the child will get better and better at playing the game.

Thatâ€™s how humans and animals learn,Â **through interaction.**Â Reinforcement Learning is just aÂ **computational approach of learning from action.**

### **A formal definition**

If we take now a formal definition:

> Reinforcement learning is a framework for solving control tasks (also called decision problems) by building agents thatÂ learn from the environmentÂ byÂ interacting with itÂ through trial and error andÂ receiving rewardsÂ (positive or negative)Â as unique feedback.
>

â‡’ But how Reinforcement Learning works?

## **The Reinforcement Learning Framework**
### **The RL Process**

<figure class="image table text-center m-0 w-full">
  <img src="assets/63_deeprlclass_unit1/RL_process.jpg" alt="The RL process"/>
  <figcaption>The RL Process: a loop of state, action, reward and next state</figcaption>
</figure>

To understand the RL process, letâ€™s imagine an agent learning to play a platform game:

<figure class="image table text-center m-0 w-full">
  <img src="assets/63_deeprlclass_unit1/RL_process_game.jpg" alt="The RL process"/>
</figure>

- Our Agent receivesÂ **state S0**Â from theÂ **Environment**Â â€” we receive the first frame of our game (Environment).
- Based on thatÂ **state S0,**Â the Agent takesÂ **action A0**Â â€” our Agent will move to the right.
- Environment goes to aÂ **new**Â **state S1**Â â€” new frame.
- The environment gives someÂ **reward R1**Â to the Agent â€” weâ€™re not deadÂ *(Positive Reward +1)*.

This RL loop outputs a sequence ofÂ **state, action, reward and next state.**


<figure class="image table text-center m-0 w-full">
  <img src="assets/63_deeprlclass_unit1/sars.jpg" alt="State, Action, Reward, Next State"/>
</figure>

The agent's goal is to maximize its cumulative reward,Â **called the expected return.**

### **The reward hypothesis: the central idea of Reinforcement Learning**

â‡’ Why is the goal of the agent to maximize the expected return?

Because RL is based on theÂ **reward hypothesis**, which is that all goals can be described as theÂ **maximization of the expected return**Â (expected cumulative reward).

Thatâ€™s why in Reinforcement Learning,Â **to have the best behavior,**Â we need toÂ **maximize the expected cumulative reward.**

### **Markov Property**

In papers, youâ€™ll see that the RL process is called theÂ **Markov Decision Process**Â (MDP).

Weâ€™ll talk again about the Markov Property in the following units. But if you need to remember something today about it, Markov Property implies that our agent needsÂ **only the current state to decide**Â what action to take andÂ **not the history of all the states**Â **and actions**Â they took before.

### **Observations/States Space**

Observations/States are theÂ **information our agent gets from the environment.**Â In the case of a video game, it can be a frame (a screenshot). In the case of the trading agent, it can be the value of a certain stock, etc.

There is a differentiation to make betweenÂ *observation*Â andÂ *state*:

- *State s*: is **a complete description of the state of the world** (there is no hidden information). In a fully observed environment.

<figure class="image table text-center m-0 w-full">
  <img src="assets/63_deeprlclass_unit1/chess.jpg" alt="Chess"/>
  <figcaption>In chess game, we receive a state from the environment since we have access to the whole check board information.</figcaption>
</figure>

In chess game, we receive a state from the environment since we have access to the whole check board information.

With a chess game, we are in a fully observed environment, since we have access to the whole check board information.

- *Observation o*: is a **partial description of the state.** In a partially observed environment.

<figure class="image table text-center m-0 w-full">
  <img src="assets/63_deeprlclass_unit1/mario.jpg" alt="Mario"/>
  <figcaption>In Super Mario Bros, we only see a part of the level close to the player, so we receive an observation.</figcaption>
</figure>

In Super Mario Bros, we only see a part of the level close to the player, so we receive an observation.

InÂ Super Mario Bros, we are in a partially observed environment. We receive an observationÂ **since we only see a part of the level.**

> In reality, we use the term state in this course but we willÂ make the distinction in implementations.
>

To recap:
<figure class="image table text-center m-0 w-full">
  <img src="assets/63_deeprlclass_unit1/obs_space_recap.jpg" alt="Obs space recap"/>
</figure>

### Action Space

The Action space is the set ofÂ **all possible actions in an environment.**

The actions can come from aÂ *discrete*Â orÂ *continuous space*:

- *Discrete space*: the number of possible actions is **finite**.

<figure class="image table image-center text-center m-0 w-full">
  <img src="assets/63_deeprlclass_unit1/mario.jpg" alt="Mario"/>
  <figcaption>Again, in Super Mario Bros, we have only 4 directions and jump possible</figcaption>
</figure>

In Super Mario Bros, we have a finite set of actions since we have only 4 directions and jump.

- *Continuous space*: the number of possible actions is **infinite**.
<figure class="image table text-center m-0 w-full">
  <img src="assets/63_deeprlclass_unit1/self_driving_car.jpg" alt="Self Driving Car"/>
  <figcaption>A Self Driving Car agent has an infinite number of possible actions since he can turn left 20Â°, 21Â°, 22Â°, honk, turn right 20Â°, 20,1Â°â€¦
</figcaption>
</figure>

To recap:
<figure class="image table text-center m-0 w-full">
  <img src="assets/63_deeprlclass_unit1/action_space.jpg" alt="Recap action space"/>
</figcaption>
</figure>

Taking this information into consideration is crucial because it willÂ **have importance when choosing the RL algorithm in the future.**

### **Rewards and the discounting**

The reward is fundamental in RL because itâ€™sÂ **the only feedback**Â for the agent. Thanks to it, our agent knowsÂ **if the action taken was good or not.**

The cumulative reward at each time step t can be written as:

<figure class="image table text-center m-0 w-full">
  <img src="assets/63_deeprlclass_unit1/rewards_1.jpg" alt="Rewards"/>
  <figcaption>The cumulative reward equals to the sum of all rewards of the sequence.
</figcaption>
</figure>

Which is equivalent to:

<figure class="image table text-center m-0 w-full">
  <img src="assets/63_deeprlclass_unit1/rewards_2.jpg" alt="Rewards"/>
</figure>

However, in reality,Â **we canâ€™t just add them like that.**Â The rewards that come sooner (at the beginning of the game)Â **are more likely to happen** since they are more predictable than the long-term future reward.

Letâ€™s say your agent is this tiny mouse that can move one tile each time step, and your opponent is the cat (that can move too). Your goal isÂ **to eat the maximum amount of cheese before being eaten by the cat.**

<figure class="image table text-center m-0 w-full">
  <img src="assets/63_deeprlclass_unit1/rewards_3.jpg" alt="Rewards"/>
</figure>

As we can see in theÂ diagram,Â **itâ€™s more probable to eat the cheese near us than the cheese close to the cat**Â (the closer we are to the cat, the more dangerous it is).

Consequently,Â **the reward near the cat, even if it is bigger (more cheese), will be more discounted**Â since weâ€™re not really sure weâ€™ll be able to eat it.

ToÂ discount the rewards, we proceed like this:

1. We define a discount rate called gamma. **It must be between 0 and 1.**

- The larger the gamma, the smaller the discount. This means our agent **cares more about the long-term reward.**

- On the other hand, the smaller the gamma, the bigger the discount. This means our **agent cares more about the short term reward (the nearest cheese).**


2.Â Then, each reward will be discounted by gamma to the exponent of the time step. As the time step increases, the cat gets closer to us,Â **so the future reward is less and less likely to happen.**

OurÂ discounted cumulative expected rewards is:

<figure class="image table text-center m-0 w-full">
  <img src="assets/63_deeprlclass_unit1/rewards_4.jpg" alt="Rewards"/>
</figure>


### Type of tasks

A task is anÂ **instance**Â of a Reinforcement Learning problem.Â We can have two types of tasks:Â episodic and continuous.

#### Episodic task

In this case,Â we have a starting point and an ending pointÂ **(a terminal state).Â This creates an episode**: a list of States, Actions, Rewards, and new States.

For instance, think about Super Mario Bros: an episode begin at the launch of a new Mario Level and endingÂ **when youâ€™re killed or you reached the end of the level.**

<figure class="image table text-center m-0 w-full">
  <img src="assets/63_deeprlclass_unit1/mario.jpg" alt="Mario"/>
  <figcaption>Beginning of a new episode.
</figcaption>
</figure>


#### Continuous tasks

These are tasks that continue foreverÂ (no terminal state). In this case, the agentÂ must **learn how to choose the best actions and simultaneously interact with the environment.**

For instance, an agent that does automated stock trading. For this task, there is no starting point and terminal state.Â **The agent keeps running until we decide to stop him.**

<figure class="image table text-center m-0 w-full">
  <img src="assets/63_deeprlclass_unit1/stock.jpg" alt="Stock Market"/>
</figure>

<figure class="image table text-center m-0 w-full">
  <img src="assets/63_deeprlclass_unit1/tasks.jpg" alt="Tasks recap"/>
</figure>

## **Exploration/ Exploitation tradeoff**

Finally, before looking at the different methods to solve Reinforcement Learning problems, we must cover one more very important topic:Â *the exploration/exploitation trade-off.*

- Exploration is exploring the environment by trying random actions in order to **find more information about the environment.**

- Exploitation is **exploiting known information to maximize the reward.**


Remember, the goal of our RL agent is to maximize the expected cumulative reward. However,Â **we can fall into a common trap**.

Letâ€™s take an example:

<figure class="image table text-center m-0 w-full">
  <img src="assets/63_deeprlclass_unit1/exp_1.jpg" alt="Exploration"/>
</figure>

In this game, our mouse can have anÂ **infinite amount of small cheese**Â (+1 each). But at the top of the maze, there is a gigantic sum of cheese (+1000).

However, if we only focus on exploitation, our agent will never reach the gigantic sum of cheese. Instead, it will only exploitÂ **the nearest source of rewards,**Â even if this source is small (exploitation).

But if our agent does a little bit of exploration, it canÂ **discover the big reward**Â (the pile of big cheese).

This is what we call theÂ exploration/exploitation trade-off. We need to balance how much weÂ **explore the environment**Â and how much weÂ **exploit what we know about the environment.**

Therefore, we mustÂ **define a rule that helps to handle this trade-off**.Â Weâ€™ll see in future chapters different ways to handle it.

If itâ€™s still confusing, **think of a real problem: the choice of a restaurant:**

<figure class="image table text-center m-0 w-full">
  <img src="assets/63_deeprlclass_unit1/exp_2.jpg" alt="Exploration"/>
  <figcaption>Source: <a href="http://rail.eecs.berkeley.edu/deeprlcourse-fa17/f17docs/lecture_13_exploration.pdf"> Berkley AI Course</a>
</figcaption>
</figure>

- *Exploitation*: You go every day to the same one that you know is good and **take the risk to miss another better restaurant.**
- *Exploration*: Try restaurants you never went to before, with the risk of having a bad experience **but the probable opportunity of a fantastic experience.**

To recap:
<figure class="image table text-center m-0 w-full">
  <img src="assets/63_deeprlclass_unit1/expexpltradeoff.jpg" alt="Exploration Exploitation Tradeoff"/>
</figure>

## **The two main approaches for solving RL problems**

â‡’ Now that we learned the RL framework, how do we solve the RL problem?

In other terms, how to build an RL agent that canÂ **select the actions thatÂ maximize its expected cumulative reward?**

### **The Policy Ï€: the agentâ€™s brain**

The PolicyÂ **Ï€**Â is theÂ **brain of our Agent**, itâ€™s the function that tell us whatÂ **action to take given the state we are.**Â So itÂ **defines the agentâ€™s behavior**Â at a given time.

<figure class="image table text-center m-0 w-full">
  <img src="assets/63_deeprlclass_unit1/policy_1.jpg" alt="Policy"/>
  <figcaption>Think of policy as the brain of our agent, the function that will tells us the action to take given a state
</figcaption>
</figure>

Think of policy as the brain of our agent, the function that will tells us the action to take given a state

This PolicyÂ **is the function we want to learn**, our goal is to find the optimal policyÂ **Ï€*,Â the policy that**Â maximizesÂ **expected return**Â when the agent acts according to it. We find thisÂ **Ï€* through training.**

There are two approaches to train our agent to find this optimal policy Ï€*:

- **Directly,** by teaching the agent to learn which **action to take,** given the state is in: **Policy-Based Methods.**
- Indirectly, **teach the agent to learn which state is more valuable** and then take the action that **leads to the more valuable states**: Value-Based Methods.

### **Policy-Based Methods**

In Policy-Based Methods,Â **we learn a policy function directly.**

This function will map from each state to the best corresponding action at that state.Â **Or a probability distribution over the set of possible actions at that state.**

<figure class="image table text-center m-0 w-full">
  <img src="assets/63_deeprlclass_unit1/policy_2.jpg" alt="Policy"/>
  <figcaption>As we can see here, the policy (deterministic)Â <b>directly indicates the action to take for each step.</b>
</figcaption>
</figure>

As we can see here, the policy (deterministic)Â **directly indicates the action to take for each step.**

We have two types of policy:

- *Deterministic*: a policy at a given state **will always return the same action.**

<figure class="image table text-center m-0 w-full">
  <img src="assets/63_deeprlclass_unit1/policy_3.jpg" alt="Policy"/>
  <figcaption>action = policy(state)
</figcaption>
</figure>

<figure class="image table text-center m-0 w-full">
  <img src="assets/63_deeprlclass_unit1/policy_4.jpg" alt="Policy"/>
</figure>

- *Stochastic*: outputÂ **a probability distribution over actions.**

<figure class="image table text-center m-0 w-full">
  <img src="assets/63_deeprlclass_unit1/policy_5.jpg" alt="Policy"/>
  <figcaption>policy(actions | state) = probability distribution over the set of actions given the current state
</figcaption>
</figure>

<figure class="image table text-center m-0 w-full">
  <img src="assets/63_deeprlclass_unit1/mario.jpg" alt="Mario"/>
  <figcaption>Given an initial state, our stochastic policy will output probability distributions over the possible actions at that state.
</figcaption>
</figure>

If we recap:

<figure class="image table text-center m-0 w-full">
  <img src="assets/63_deeprlclass_unit1/pbm_1.jpg" alt="Pbm recap"/>
</figure>
<figure class="image table text-center m-0 w-full">
  <img src="assets/63_deeprlclass_unit1/pbm_2.jpg" alt="Pbm recap"/>
</figure>


### **Value-based methods**

In Value-based methods,Â instead of training a policy function,Â weÂ **train a value function**Â that maps a state to the expected valueÂ **of being at that state.**

The value of a state is theÂ **expected discounted return**Â the agent can get if itÂ **starts in that state, and then act according to our policy.**

â€œAct according to our policyâ€ just means that our policy isÂ **â€œgoing to the state with the highest valueâ€.**

<figure class="image table text-center m-0 w-full">
  <img src="assets/63_deeprlclass_unit1/value_1.jpg" alt="Value based RL"/>
</figure>

Here we see that our value functionÂ **defined value for each possible state.**

<figure class="image table text-center m-0 w-full">
  <img src="assets/63_deeprlclass_unit1/value_2.jpg" alt="Value based RL"/>
  <figcaption>Thanks to our value function, at each step our policy will select the state with the biggest value defined by the value function: -7, then -6, then -5 (and so on) to attain the goal.
</figcaption>
</figure>

Thanks to our value function, at each step our policy will select the state with the biggest value defined by the value function: -7, then -6, then -5 (and so on) to attain the goal.

If we recap:

<figure class="image table text-center m-0 w-full">
  <img src="assets/63_deeprlclass_unit1/vbm_1.jpg" alt="Vbm recap"/>
</figure>
<figure class="image table text-center m-0 w-full">
  <img src="assets/63_deeprlclass_unit1/vbm_2.jpg" alt="Vbm recap"/>
</figure>


## **The â€œDeepâ€ in Reinforcement Learning**

â‡’ Waitâ€¦ you spoke about Reinforcement Learning, but why we spoke about Reinforcement Learning?

Deep Reinforcement Learning introducesÂ **deep neural networks to solve Reinforcement Learning problems**Â â€” hence the name â€œdeep.â€

For instance, in the next article, weâ€™ll workÂ onÂ Q-LearningÂ (classic Reinforcement Learning) and thenÂ Deep Q-LearningÂ both are value-based RL algorithms.

Youâ€™ll see the difference is that in the first approach,Â **we use a traditional algorithm**Â to create a Q table that helps us find what action to take for each state.

In the second approach,Â **we will use a Neural Network**Â (to approximate the q value).

<figure class="image table text-center m-0 w-full">
  <img src="assets/63_deeprlclass_unit1/deep.jpg" alt="Value based RL"/>
  <figcaption>Schema inspired by the Q learning notebook by Udacity
</figcaption>
</figure>

If you are not familiar with Deep Learning you definitely should watch <a href="http://introtodeeplearning.com/">the MIT Intro Course on Deep Learning (Free)</a>

That was a lot of information, if we summarize:

- Reinforcement Learning is a computational approach of learning from action. We build an agent that learns from the environment **by interacting with it through trial and error** and receiving rewards (negative or positive) as feedback.
- The goal of any RL agent is to maximize its expected cumulative reward (also called expected return) because RL is based on the **reward hypothesis**, which is that **all goals can be described as the maximization of the expected cumulative reward.**
- The RL process is a loop that outputs a sequence of **state, action, reward and next state.**
- To calculate the expected cumulative reward (expected return), we discount the rewards: the rewards that come sooner (at the beginning of the game) **are more probable to happen since they are more predictable than the long term future reward.**
- To solve an RL problem, you want to **find an optimal policy**, the policy is the â€œbrainâ€ of your AI that will tell us **what action to take given a state.** The optimal one is the one who **gives you the actions that max the expected return.**

- There are two ways to find your optimal policy:
  1. By training your policy directly: **policy-based methods.**
  2. By training a value function that tells us the expected return the agent will get at each state and use this function to define our policy: **value-based methods.**

- Finally, we speak about Deep RL because we introduces **deep neural networks to estimate the action to take (policy-based) or to estimate the value of a state (value-based)** hence the name â€œdeep.â€

---
Now that you've studied the bases of Reinforcement Learning, youâ€™re ready to train your first lander agent toÂ **land correctly on the Moon ğŸŒ• and share it with the community through the Hub** ğŸ”¥

<figure class="image table text-center m-0 w-full">
    <video
        alt="LunarLander"
        style="max-width: 70%; margin: auto;"
        autoplay loop autobuffer muted playsinline
    >
      <source src="assets/63_deeprlclass_unit1/lunarlander.mp4" type="video/mp4">
  </video>
</figure>

Start the tutorial here [ADD LINK]

Congrats on finishing this chapter!Â **That was the biggest one**, and there was a lot of information. And congrats on finishing the tutorial. Youâ€™ve just trained your first Deep RL agent and shared it on the Hub ğŸ¥³.

Thatâ€™sÂ **normal if you still feel confused**Â with all these elements.Â **This was the same for me and for all people who studied RL.**

Take time to really grasp the material before continuing. Itâ€™s important to master these elements and having a solid foundations before entering theÂ **fun part.**

We published additional readings in the syllabus if you want to go deeper [ADD LINK README UNIT 1]

Naturally, during the course,Â **weâ€™re going to use and explain these terms again**, but itâ€™s better to understand them before diving into the next chapters.

In the next chapter, weâ€™re going to learn about Q-Learning and dive deeperÂ **into the value-based methods.**

And don't forget to share with your friends who want to learn ğŸ¤— !

### Keep learning, stay awesome,
